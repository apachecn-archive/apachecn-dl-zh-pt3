# 5.深度强化学习

在上一章中，我们研究了 ML Agents Toolkit 的大脑学院架构的各个方面，并理解了某些对 agent 根据策略做出决策非常重要的脚本。在本章中，我们将通过 Python 及其与大脑学院架构的 C#脚本的交互来了解深度强化学习(RL)的核心概念。当我们简要讨论使用 OpenAI Gym environment (CartPole)的深度 Q 学习算法时，以及当我们讨论 OpenAI 的基线库时，我们已经瞥见了深度 RL 的一部分。在通过外部大脑在 Tensorflow 中训练 ML 代理的过程中，我们还使用了近似策略优化(PPO)算法以及 trainer_config.yaml 文件中的默认超参数。我们将深入讨论这些算法以及演员评论范例中的其他算法。然而，要完全理解这一章，我们必须了解如何使用 Tensorflow 和 Keras 模块建立深度学习网络。我们还必须了解深度学习的基本概念，以及在当前背景下为什么需要它。通过这一章，我们还将为计算机视觉方法创建神经网络模型，这在我们研究 GridWorld 环境时将是极其重要的。由于我们主要有射线和摄像机传感器为代理提供观察空间，在大多数模型中，我们将有两种策略的变体:多层感知器(基于 MLP 的网络)和卷积神经网络(基于 CNN-2D 的网络)。我们还将研究使用 ML 代理工具包创建的其他模拟和游戏，并将尝试基于 OpenAI 的基线实现来训练我们的模型。然而，让我们首先了解深度学习中通用神经网络模型的基本原理。

## 神经网络基础

由于我们一直在训练我们的代理人，并且也应用了某些深度学习算法，我们应该了解神经网络是如何执行的。神经网络的最简单形式是感知器模型，它由输入层、某些“隐藏”层和输出层组成。基于深度学习或神经网络的模型的一般要求是迭代地优化成本函数，基于某些约束，该成本函数被显著地称为“损失”函数。在许多情况下，每当我们应用神经网络时，都假设相关函数必须是连续可微函数，具有非线性。这也意味着函数的曲线将包含被称为“最大值”和“最小值”的某些轨迹前者指函数的最高值，后者指最低值。在一般机器学习(ML)中，我们感兴趣的是找到曲线的全局最小值，假设函数的相应权重或系数将是最优的并且一致以产生最小误差。然而，在 RL 中，我们将看到全局最小值和全局最大值在策略梯度中扮演着重要的角色。

### 感知器神经网络

为了简单起见，让我们取一个函数，其中 y 值依赖于二维平面上的 x 值。我们将权重或系数与 x 相关联，并添加偏差。在感知器模型的上下文中，权重是最重要的方面，我们将使用简单的 MLP 网络迭代优化这些权重。如果我们表示 w 是权重集，b 是模型的偏差集，我们可以写为:

y= σ (w.x + b)，

其中，σ是一个依赖于 e(自然对数)的非线性函数，通常称为 sigmoid 函数。该非线性 sigmoid 函数表示为:

σ(x)= 1/(1+e <sup>(-x)</sup>

现在，这个特殊的非线性函数的重要性在于，它有助于损失函数向全局最小值收敛。这些非线性函数通常被称为激活函数。sigmoid 函数通常表示 S 形曲线。我们将在接下来的章节中探讨其他几个函数，比如 Softmax、Relu 等等。理解对加权函数的这些非线性变换有助于全局收敛是很重要的。我们可以通过 Jupyter 笔记本使用 Python 创建这个简化的 sigmoid 曲线公式。我们将导入 math 和 matplotlib 库来创建缩放的 sigmoid 图。让我们打开 Sigmoid-Curve.ipynb 笔记本。我们将看到 sigmoid 函数通过使用数组实现了 sigmoid 曲线的指数方程。

```
def sigmoid(x):
    a = []
    for item in x:
        a.append(1/(1+math.exp(-item)))
    return a

```

之后，我们使用 matplotlib 绘制曲线，并指定沿 x 轴从-10 到+10 的点的均匀分布，任意两点之间的差异为 0.2。

```
x = np.arange(-10., 10., 0.2)
sig = sigmoid(x)
plt.plot(x, sig)
plt.show()

```

运行后，我们将在 Jupyter 笔记本中有一个缩放的 sigmoid 曲线，如图 [5-1](#Fig1) 所示，我们已经创建了一个激活函数。当涉及到泛型 ML 中的二进制分类时，这是一个非常重要的功能，我们将对此进行简单的探讨。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig1_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig1_HTML.jpg)

图 5-1

Jupyter 笔记本中的 Sigmoid 曲线

在一般的基于梯度的迭代学习中，有三个基本步骤:

*   前进传球

*   误差计算

*   偶数道次

**前向传递** **:** 这是在将输入向量(inputs)乘以权重向量(weights)并添加偏差向量(bias)之后(如果需要)计算 y 值的第一阶段。需要记住的是，在 ML 中，我们将处理张量，它是所有输入和输出的有效矩阵。因此，字母 y、w 和 x 代表向量或矩阵，而不是单值函数。我们在显示以下等式的部分看到了这一点:

y= σ (w.x + b)

**误差计算** **:** 因为我们将针对某个“基本事实”数据集运行我们的感知器神经网络，以验证网络根据输入张量、权重矩阵和偏差预测正确输出的程度。这里有一个重要的概念，即每个迭代学习阶段之间的误差计算。实际上，每个历元中的误差被认为是预测输出张量和实际输出张量之间的绝对差。

∈| y〖t0〗t1〗y |，

其中，是误差项。现在，我们将考虑一个重要的指标，称为均方误差，其公式如下:

姆塞= 0.5(y<sup>-y)<sup>2</sup></sup>

这个等式被称为损失函数或二次成本函数，在一般学习中非常重要，因为我们将在下一步中使用它来计算梯度。

**向后传递** **:** 这是处理后续更新的非常重要的一步。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig2_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig2_HTML.jpg)

图 5-2

反向传播中的梯度下降算法

*   **计算梯度:**需要对前一方程中计算的均方误差进行部分微分，以更新下一个时期的权重，并找到算法的适当收敛。这意味着我们必须根据权重矩阵来区分上一步中的误差。从数学上讲，这可以表示为:

    j =√t0〖姆塞〗t1〗/√w，

    这实际上意味着我们必须通过使用微分链规则来对误差项相对于权重矩阵进行部分微分。链式法则可以表述为:

    d(xy) = [y(dx) + x(dy)] (dx)，

    其中 xy 是 f(x，y)的连续可微函数。

    我们必须相对于 w 对函数 y 进行偏导数，在一般的偏导数技术中，我们必须计算出导数项的导数，并保持其余部分为常数。例如，如果一个方程的形式为 f(x，y)=xy，那么如果我们需要对 f 相对于 x 进行部分求导，我们将得到:

    * f(x，y))=[y(d(x)/dx)+x(d(y)/dx)]* x，

    其中 d(f(x))/dx 表示函数的正态微分。根据微积分规则，这产生:

    * f(x，y))=[y]x，

    因为常数的微分被计算为 0；因此，第二项消失了。现在，由于我们使用了 sigmoid 曲线作为激活曲线，因此损失函数的偏导数如下:

    *(y)=[[d(σ(y)]/d(y)][d(y)/dw]]w，

    微分中的链式法则。现在，s 形曲线的导数简单地由下式给出:

    d（σ（θ）） = θ（1-θ） d（θ），

    其中，θ为任意多项式函数，本例中为 y，第二部分的导数可通过以下链式法则计算:

    d(y)/dw = w(d(x)/dw)+x(d(w)/dw)+db/dw

    由于这是偏微分，我们将去除常数(d(x)/dw 和 d(b)/dw)的导数，这有效地简化了我们的最终方程，它可能如下所示:

    (y)=[[y(1-y)](x)(d(w)/dw)]* w

    现在我们已经使用了∈<sub>MSE</sub>作为误差函数，如果我们必须计算∈<sub>MSE</sub>的偏导数，那么我们可以写出:

    *(<sub>姆塞</sub>=【0.5 * 2(d | y<sup>'</sup>-y |/dw ')w

    现在 y <sup>'</sup> 是验证的实际值，y 是预测值；因此，实际上我们必须对 y 求导，这已经计算过了。这个整个过程用于在学习过程的每一步寻找损失函数的梯度。

*   **反向传播** **:** 泛型 ML 中权重更新规则最重要的部分。计算完梯度后，我们将这些梯度从输出张量传播到输入张量。因为在我们的例子中，我们使用了一个简单的 sigmoid 感知器模型，没有任何隐藏层，在许多复杂的神经网络中，我们将有许多隐藏层。这意味着每一个隐藏层都将由单独的感知器单元组成，就像我们使用的那样。对于我们的情况，由于没有隐藏层，我们只需使用以下规则更新权重张量:

    w = w-a(j)表示法，

    其中 j =∂∈<sub>MSE</sub>/∂<sub>w</sub>，α为学习率。在这种情况下，该权重更新规则与梯度下降算法相关联。如前所述，监督 ML 的主要骨干是通过更新权重和收敛到全局最小值来优化损失函数。记住这一点，在梯度下降中考虑权重更新策略的轨迹应该沿着收敛的最陡斜率。然而，在许多情况下，梯度下降可能没有用，因为它可能会过度拟合数据集。这意味着该策略可能会在全局最小值附近振荡，并且永远不会收敛，或者它可能会陷入局部最小值。这就是符号α出现的地方。α被称为学习率，它调整策略为达到全局最小值而采取的步长。但是已经观察到这没有解决收敛算法的局部振荡问题；因此，已经有了一些通过使用特定动量来优化梯度下降算法的发展。在整个深度学习模块中，我们将彻底使用优化器，同时为不同的深度 RL 算法编写我们的神经网络。一些最著名的优化器包括 Adam、Adagrad 和 Adadelta，而较老的版本包括 RMSProp 和 SGD。这些优化器使用动量结合随机梯度收敛算法来调整步长。我们可以使用随机梯度下降(SGD)算法来可视化一个简单的梯度下降收敛，如图 [5-2](#Fig2) 所示。

这可以通过运行 Plotting Gradient Descent.ipynb 笔记本来可视化。

实际上，我们使用简单的感知器模型研究了反向传播，这完成了基本神经网络的介绍部分，包括 sigmoid 激活、误差计算和使用梯度下降算法的权重更新策略以及到输入层的反向传播。该模型可如图 [5-3](#Fig3) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig3_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig3_HTML.jpg)

图 5-3

前馈感知器神经网络模型

在下一节中，我们将研究一个相对密集的网络，并理解该模型的反向传播概念。不失一般性，特定神经网络中涉及的所有层都基于某些矩阵/张量进行计算，并且在大多数情况下，乘法被称为两个或更多个张量之间乘积的“点”运算。由于所有这些偏微分规则将适用于这些层，或者更确切地说，这些张量，我们可以认为将需要几个矩阵/张量计算。然而，对于大多数神经网络模型，前向传递、误差计算和后向传递这三个阶段保持不变。我们将着眼于 MLP 密集网络和卷积网络。

### 密集神经网络

现在让我们来理解密集网络的基本原理，该网络具有作为感知器单元的隐藏层。这是我们将使用的最常见的神经网络模型，可以如图 [5-4](#Fig4) 所示进行可视化。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig4_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig4_HTML.jpg)

图 5-4

带隐含层的稠密神经网络

有趣的是，隐藏层由具有不同激活函数的感知器模块组成。隐藏层的这些感知器单元然后被叠加以生成累积损失函数。在这种情况下，我们将考虑前面提到的三个阶段——正向传递、误差计算和反向传递。

**正向传递** **:** 在这种情况下，我们假设隐层中的感知器只有 sigmoid 激活。权重和输入也是张量或矩阵。让我们假设 l 代表隐藏层，而(l-1)代表输入层。第(l-1) <sup>第</sup>层的第 k <sup>第</sup>个神经元与第 l <sup>第</sup>层的第 j <sup>第</sup>个神经元之间的连接对应的权重用 w<sup>l</sup>T14】JK 表示。现在，如果我们假设第 l <sup>层</sup>中的偏置张量为 b <sup>l</sup> 并且来自前一(l-1) <sup>层</sup>中的第 k <sup>神经元的输入张量为 x <sup>l-1</sup> <sub>k</sub> ，则前向传递方程或加权神经元函数可以被构造为:</sup>

y<sub>【j】</sub>=σ((w】<sup>【l5t】<sub>【JK】</sub><sup>【l-1】</sup>【k】+b</sup>

通常，如果每个感知器模型的激活是不同的，那么等式可以修改为:

y<sup><sub>【j】</sub>= f(【w】<sup>【l】</sup><sub>【JK】</sub>【x】<sup>【l-1】<sub>【k】</sub>+b】</sup></sup>

其中 f 是任何激活函数，如 tanh、softmax 等。一旦我们获得了损失函数，我们必须确定误差，这也被称为成本函数。

**误差计算** **:** 我们将考虑均方误差度量，也称为二次成本或损失函数。相应地，这可以由以下等式表示:

姆塞= 0.5(y**【y】**

 **现在，在这种密集网络的情况下，我们将有一个跨 n 个训练样本的采样成本函数。同样，y 应该被我们已经探索过的新的损失函数所代替。改变这些细节导致类似的等式:

<sub>姆塞</sub>=(1/2n)* *和**-和<sup><sub>【j】<sup>【2】</sup></sub></sup>**

 **对于一个特定的训练样本，这个等式可以简化为:

姆塞= c =(1/2)* * y<sup>-y<sup>【l5t】<sub>【j4t 7】)<sup>2</sup>，</sub></sup></sup>

为了简单起见，我们用符号 C 来表示它。

**反向传递** **:** 在这种情况下，如上所述，最重要的方面是梯度计算和误差反向传播。在这种情况下，我们将像以前一样使用偏导数来找出梯度，只是在这种情况下有一个隐藏层。

*   反向传播:这意味着不是像在简单的感知器中那样计算关于权重的单层导数，我们必须计算隐藏层中所有感知器的梯度，然后我们必须将这些梯度反向传播到输入层。首先，我们将计算外层和隐藏层之间的梯度流。

    **输出-隐藏层渐变流**

    我们将对权重 w <sup>l</sup> <sub>jk</sub> 以及偏差 b<sup>l</sup>T6】j 进行部分微分。这可以分别表示为∂c/∂w<sup>l</sup>T10】JK 和∂c/∂b<sup>l</sup>t14】j。为了简单起见，我们将把偏差视为常数。然而，我们也必须计算隐藏层梯度。为了计算这一点，我们必须以如下所示的形式对成本函数进行部分微分:

    √c/√w〖t0〗l〖t1〗〗〗j〖T3〗j〖c/∝y〖T4〗l〖t5〗j〖T6〗j〖T7〗y〖t8〗l〖T9〖T10〗

    其中权重 w<sup>l</sup>T2 j 表示第 l <sup>层中第 j <sup>层</sup>神经元的权重，并且是 y<sup>l</sup>T10】j 层的输出张量。这意味着我们必须对∂y<sup>l</sup>j/∂w<sup>l</sup>的基于 sigmoid 激活的输出进行微分。为简单起见，让我们考虑这一层的微分公式为:</sup>

    √y<sup><sub>【j】</sub>=【d(σ(z))/d w<sup>【l5t】<sub>【j】；【w】<sup>【L9】，</sup></sub></sup></sup>

    其中 z 被定义为对于隐藏层中的特定神经元，z =σ(∑w<sup>l</sup><sub>JK</sub>x<sup>l-1</sup><sub>k</sub>+b<sup>l</sup><sub>j</sub>。如果我们简化这个等式，我们可以用简化的形式来表示它:

    <sub>【j】</sub>=【w】<sup>【l】</sup><sub>【j】</sub>=【c/<sup>【l】</sup>

    这也可以表示为:

    √t0<sub>【j】</sub>=[√c/√和<sup>【l5t】<sub>【j】</sub></sup>

    **其中σ**(z)=【d(σ(z))/d w<sup>【l5t】j</sup>****

    ****下一步是将这些渐变通过隐藏层传播到输入层。这也意味着，如果我们在当前输入层和隐藏层之间有另一个隐藏层，那么渐变也会流过这个层。这意味着随着层数的增加，我们必须使用链式法则重复计算偏导数。****

    ******隐藏到输入层或隐藏到内部** **隐藏层** **:******

    ****因为，在我们的例子中，我们有直接从输入层馈送的隐藏层，梯度传播步骤是简单的:****

    ****√c/√w〖t0〗l〗t1〖T2〗j〖T3 =√w〗(w〖T4〗l〖T6〗JK〖T7〖x〖t8〖l-1〖T9〗T10〗k****

    ****这是因为我们已经通过单个隐藏层计算了相对于权重的梯度。现在剩下的唯一步骤是通过减去/加上梯度来更新权重，以获得更好的最优性:****

    ****w<sub>【JK】</sub>= w<sup>【l6t】【JK】–α[√w/<sup><sub>j</sub></sup></sup>****

    ****其中，α是学习率。对于由单个隐藏层组成的密集网络，这基本上结束了反向传播步骤和梯度更新。必须记住，这些计算是在张量上进行的，通常使用哈达玛积进行计算。两个张量或两个矩阵的 Hadamard 乘积是存在于矩阵内相同位置的矩阵元素的成对乘法。换句话说，它形成了两个张量(向量)的逐元素乘法。例如:****

    ****[1 2] 秒****

    ****[4 5] =[1*4 2*5 ]****

    ****= [4 10]****

    ****现在，我们将研究隐藏层从另一个隐藏层馈电的情况，这是多层密集网络中最常见的架构。在这种情况下，如果我们假设有另一个隐藏层，我们也必须部分区分该层。让我们考虑 w <sup>l-1</sup> <sub>ki</sub> ，它代表第(l-1)<sup>层中第 i <sup>个</sup>神经元和第 l <sup>层中第 k <sup>个</sup>个神经元之间连接的权重。现在，有效的加权神经元函数将类似于此:</sup></sup>****

    ****y<sub>【j】</sub>=σ((w】<sup>【l6t】【JK】(w<sup>【L1】<sub>【ki】</sub></sup></sup>****

    ****在这种情况下，我们将输出层的 x <sup>l-1</sup> <sub>k</sub> 部分替换为前一个隐层的加权神经元函数。在这种情况下，我们正在研究一个基于两个隐藏层的模型，其中输入层 I 将数据导向第一个隐藏层 k，第一个隐藏层 k 又将数据导向第二个隐藏层 j。每个层都有其相应的偏差，为简单起见，我们将它们视为非功能常数。这就是为什么第二个隐藏层(j <sup>th</sup> 层)本质上采用由σ(∑w<sup>l-1</sup>T8】kix<sup>l-2</sup>T12】I+b<sup>l</sup><sub>k</sub>给出的第一个加权神经元隐藏层作为输入，而第一个隐藏层只是一个从输入层获取输入的 sigmoid 激活的加权隐藏层。在这种情况下，如果我们要反向传播梯度，这将是一个链偏导数规则如下。****

    ****对于最外层的隐藏层，梯度可以表示为:****

    ****√c/√w〖t0〗l〖t1〗〗〗j〖T3〗j〖c/∝y〖T4〗l〖t5〗j〖T6〗j〖T7〗y〖t8〗l〖T9〖T10〗****

    ****对于这一层，权重更新规则可以如前面讨论的那样编写:****

    ****w<sub>【JK】</sub>= w<sup>【l6t】【JK】–α[？c/【w8】</sup><sub>j【t】</sub>****

    ****对于最后一个隐藏层到倒数第二个隐藏层(或本例中的第一个隐藏层):****

    ****√c/√w〖t0〗l-1〗t1〗T2〖k〗T3 =[√c/√y〖T4〗l〖t5〗T6〖j〖T7〗〗y〖t8〗l〖T9〗****

    ****现在，对于第一个隐藏层中的权重更新，我们只需计算:****

    ****w【ki】= w<sup>【l-1】</sup>【KL】–α[？c/【w】<sup>【l-1】</sup>k****

    ****现在让我们将这种方法推广到基于 n 个隐藏层的密集网络。梯度流可以概括为:****

    ****√c/√w〖t0〗net〗t1 =[√c/√y〖T2〗net〖T3〗net〖y〖T4〗net〗t5/∝w〖T6〗net〖T7〗net〗和〗t****

    ****……c/√y<sup>net</sup>[√y<sup>1</sup>/√w<sup>1</sup>]****

    ****这是通过密集网络的梯度流的一般形式，其中优化函数有助于全局收敛。这就形成了反向传播算法，它是深度学习和收敛的核心。这很重要，因为当我们使用不同的 RL 算法为我们的 ML 代理设计网络时，如深度 Q 网络(DQN)、演员评论网络、策略梯度和其他非策略算法，我们将使用 Keras (Tensorflow)的密集 MLP 神经网络库。总结一下反向传播模块，用于外部神经元(外部隐藏层)的权重更新是∂C/∂ w <sup>网</sup>=【∂c/∂y<sup>网</sup>】【∂y<sup>网</sup> / ∂ w <sup>网</sup>，而对于其他神经元(隐藏-先前隐藏或隐藏输入)，更新规则变成各个加权函数(具有激活)相对于权重的链偏微分。****

 ****我们已经理解了密集神经网络架构，并且还发现了偏导数如何在确定误差更新和通过网络的梯度流中发挥重要作用。这在监督形式的学习中很重要，监督形式的学习通过最小化每一步的错误来进行。我们在第一章中创建的 DQN 基于通过多层感知器神经网络(密集网络)最小化价值函数的误差。随着我们的进一步发展，我们将看到价值和策略功能是如何相互作用的，以及为什么策略梯度在深度 RL 中非常重要。但是在我们探索算法之前，我们应该熟悉使用 Keras 编写我们自己的密集网络。让我们在进入代码段之前也探索一下另一个神经网络模型——CNN。

### 卷积神经网络

这是神经网络的另一种变体，最常用于图像分析。CNN 包括应用某些非线性函数对图像中的像素进行空间分析。这些被称为从图像中提取的特征。这些特征然后作为输入通过密集神经网络模型传递。这非常重要，因为在 GridWorld 中，PPO 算法使用卷积层(2D)进行图像分析，然后指示代理做出相应的决策。在大多数雅达利 2D 游戏中，这些 CNN 模型对于理解特定帧中呈现的图像(像素)极其重要；例如，通过在特定帧分析游戏性的图像，然后通过 CNN 模型传递它，来指导代理玩乒乓球的游戏(在健身房环境中为“pong”)。

现在让我们考虑一个样本卷积模型的架构。它由卷积层、池层和密集网络层组成。虽然还应用了其他几个层，如填充层/展平层，但基本部分保持不变。在这种情况下，图像作为输入被传递到网络，网络被分析并传递到密集网络。根据输入的深度(RGB 或灰度)，通道可以分别是 3 或 1。举例来说，卷积模型如图 [5-5](#Fig5) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig5_HTML.png](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig5_HTML.png)

图 5-5

卷积神经网络模型

根据输入的参数，CNN 可以有不同的维度。

*   **卷积 1D:** 当输入是概率数据的一维序列时应用。这种数据通常是 1D 向量的形式，例如数组。这种模型用于时间序列预测，其中输入数据是一系列概率值。

*   **卷积 2D:** 这是我们用例中最重要的维度。输入层由图像数据—像素(3 或 1 通道)组成。在这种情况下，CNN 模型应用于不具有时间属性的静态图像。基于图像的高度和宽度提取空间属性或特征。因此，对于 RGB 或灰度数据，2D 卷积的输入大小可以分别是五个单位(RGB 的高度、宽度和三个通道)或三个单位(RGB 的高度、宽度和一个通道)。在我们的用例中，我们将研究该模型，因为它将分析 RGB 格式(五个通道)的 ML 代理环境(Gridworld)的每一帧图像，并将其传递给密集网络。

*   **卷积 3D** **:** 用于分析视频信息。因此，与空间数据一起，每个像素数据都有一个相关的时间分量。时间分量定义了视频每一帧的像素变化。随着时间通道的添加，卷积 3D 模型的有效输入大小(通道)变为六个单位(高度、宽度和 RGB 的三个通道；一个时间通道)和四个单元(高度、宽度和一个用于灰度的通道；一个时间通道)分别用于 RGB 和灰度像素。

现在，让我们研究卷积网络的组成部分。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig6_HTML.png](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig6_HTML.png)

图 5-6

CNN 中的卷积层和卷积图像的维数

*   卷积层:该层与从图像中的像素提取空间特征相关。它由某些试图通过卷积提取这些特征的过滤器或内核组成。卷积是一种数学运算，其中图像 I 通过维度为 k <sub>1</sub> x k <sub>2</sub> 的滤波器(内核)K，可以表示为:

    I * k =<sub>【m】</sub><sub>【k2-1】<sub>【I(I-m，j-n)K(m，n)=<sup>k1-1】</sup></sub></sub>

    该操作类似于具有可以改变其符号的核的互相关操作，意味着 K(m，n)=k(-m，-n)。现在，如果我们认为输入是 RGB 形式的，那么通道的数量将是 5-宽度、高度和 RGB 的三个通道。因此，图像 I 的尺寸将为 H×W×C，其中 H、W 和 C 代表高度、宽度和颜色通道。对于 D 个大小的核，我们可以将总核维数设为 k <sub>1</sub> x k <sub>2</sub> x C x D。如果我们假设 D 个滤波器的偏差为 b，那么我们可以将卷积加权神经元方程构造为:

    (I * k)=<sub>【m】</sub><sub><sub><sub><sub><sup><sub>【c = 1】I(i+m，j+n) + b</sub></sup></sub></sub></sub></sub>

    This is a general convolution equation to extract the features. There are certain metrics associated with convolution, which are referred to as hyperparameters.
    *   **深度:**这表示内核或过滤器的深度，它也计算从输入图像中提取的不同特征。在 CNN 模型末端层的密集网络中，深度也表示密集模型中隐藏层的数量。

    *   **步幅:**控制空间维度(高度和宽度)周围的深度列。如果我们指定步幅值为 1，那么内核将一次移动一个像素，这可能导致提取的字段重叠。如果赋值为 2，那么它一次跳跃两个像素。通常，当步幅增加到大于 3 的值时，输出的空间维度减小。我们用符号 s 表示步幅。

    *   **填充:**填充层或零填充层用于在空间上(沿高度和宽度)填充输入图像尺寸。该特征使我们能够控制输出体积的空间大小。

    提及卷积层的输入和输出维度之间的关系是很重要的。维数为 H X W 的输入特征图和维数为 k 的权重核之间的卷积 <sub>1</sub> X k <sub>2</sub> 产生维数为:

    dim(O) = (H - k1 + 1) x (W - k2 + 1)。

    基于步幅 S 和零填充 P，还有另一个输出音量公式，由下式给出:

    dim(O)=([(H x W)–(k1 x k2)+2P]/S)+1 =([Z-K+2P]/S+1，

    其中 Z 代表输入体积，K 代表内核尺寸。

    这个值应该是一个整数，这样神经元就可以很好地提取特征。例如，如果我们取尺寸为[11，11，4] (Z = 11)的输入图像，并且我们取值为 5 (K = 5)的内核大小，并且我们不使用零填充(P = 0)后跟步长值 2 (S = 2)，则我们可以得到输出层的尺寸:

    dim(O) = ([11-5 + 2*0]/2)+1 =4

    这意味着输出维度的大小为 4 X 4。在这种情况下，我们得到的整数输出为 4，但在许多情况下，我们看到这些值的一些无效组合可能会在输出维度中产生分数结果。例如，如果 Z = 10，没有零填充(P = 0)，并且核维数为 3 (K = 3)，则不可能有 2 的步幅，([Z-K+2P]/S+1 =([10-3+0]/2)+1 = 4.5，这是一个小数值。通过这种方式，约束被应用到步幅上，并且在许多情况下，如果这些值的组合无效，就会自动抛出异常。图 [5-6](#Fig6) 显示了输入维数 Z = 3，补零 1(P = 1)，核大小 1(K = 1)，步距 2，输出维数为([Z-K+2P]/S + 1 = ([3-1+2*1]/2)+1 = 3。

这一层还有一个有趣的事实，叫做参数共享。这有助于减少输入的维数，从而减少计算。这是基于这样的假设，即如果一个特征在某个空间位置(x，y)计算是有用的，那么在不同的位置(x <sub>2</sub> ，y <sub>2</sub> 计算也应该是有用的。假设我们有一个卷积层的输出，其尺寸为[55*55*96]，深度通道为 96，那么我们可以应用相同的权重和偏置来约束每个深度切片中的神经元，而不是对所有 96 个不同的深度切片具有不同的偏置和权重。这有助于反向传播步骤，因为梯度(流经各层)将在每个单独的深度切片上相加，并更新一组权重，而不是 96 个不同的权重。

*   **汇集层** **:** 该层用于通过减少网络中的参数和计算来逐步减少空间维度。它用于卷积层之间以及卷积层的末端。该层使用最大化操作在单个深度切片或深度通道上操作，因此这被称为最大池。最常见的最大池形式是使用大小为 2 X 2 的滤镜，以 2 为步长进行应用，这将沿宽度和高度对每个深度通道进行 2 倍的缩减采样。该层的输出可以计算为:

    dim(O) = ([Z-K]/S) + 1，

    其中 Z 代表图像的高度或宽度，K 是内核的维度，S 是步幅(和前面一样)。除了最大池化，还有其他池化指标，其中最重要的是 L2 规范池化。L2 范数是一种欧几里得正则化度量，它使用平方误差函数作为正则化方程。这也称为平均池方法。

*   **展平层** **:** 该层用于压缩或展平汇集层或卷积层的输出，以产生一系列值，这些值基本上是密集神经网络的输入。正如我们已经研究过的密集网络体系结构，需要这种压缩来产生密集网络的输入层可以使用的输入序列。例如，在对尺寸为[32，32，3]的输入图像上的 64 层深度卷积 2D 层应用核大小[3，3]，跨距为 2，零填充值为 1 之后，我们得到的输出形状为:

    ([Z-K+2P]/S + 1= ([32-3 + 1]/2)+1= 16

    这意味着输出维数为[16，16，64]。现在，如果我们对这个输出序列应用展平层，有效尺寸将是 16*16*64 = [16384]。因此，这一层产生一个一维的输入序列，可以馈入密集网络。该层不影响批量大小。

*   **密集网络** **:** 这形成了传统 CNN 模型的最后一部分。这里最有趣的部分是卷积，最大池，加权神经元模型的反向传播。在我们进入这一层之前，让我们理解我们提到的卷积层中的反向传播。

    **卷积层的反向传播** **:** 在卷积层的反向传播中，我们也遵循偏微分的链式法则来更新类似于密集网络的梯度。单个权重的梯度分量可以通过下式获得:

    √c/√w〖t0〗l〗t1〖T2〗m，n〖T3〗h-k1〖T6〗I〖T7〗〖t8〗w-k2〖T9〖T10〗j

    其中 H X W 是输入维数，C 是成本函数或二次损失函数。w<sup>l</sup>T2 m，n 是指第 l <sup>层第 m <sup>第</sup>个神经元的权重。y<sup>l</sup>T10】m，n 表示 sigmoid 激活的加权神经元方程:</sup>

    y <sup>l</sup> <sub>m，n</sub>=σ(<sub>m</sub><sub>【w】<sup><sub>m，n</sub></sup></sub>

    由于在这种情况下，我们将图像而不是 1D 阵列作为输入，卷积网络的反向传播模型将沿着不同输入通道的维度以及核权重和偏差作为输入。其余的导数与一般的链式法则相似，可以重新定义为:

    √c/√w〖t0〗l〖t1〗T2〗m，n〖T3〗w〖T4〗l〖t5〗T6〖m，n〖T7〗〗m〖t8〖m〖T9〗

    我们可以根据卷积层的深度进一步扩展这个方程，只要深度通道没有耗尽，就可以执行偏导数的链式法则。在这种情况下，我们使用了乙状结肠激活；然而，在一般的卷积 2D 网络中，我们使用“Relu”或正则化线性单元作为激活函数。Relu 可以表示为:

    rel(x)=最大值(0，x)

    因此，对于 x 大于 0，这种激活是一条直线，对于 x 小于 0，这种激活在负 x 轴上。这意味着它取最大正值。

    **密集网络中的反向传播** **:** 这与我们研究的密集网络中的反向传播和梯度流类似。展平图层后，我们得到一个由权重、输入和偏差组成的压缩图层。基于问题的类型，我们可以关联不同的激活函数。例如，在二元分类问题中，我们将使用前面提到的 sigmoid 函数。对于多职业分类，我们必须使用“softmax”激活功能，这在第 [1](1.html) 章的“多武器强盗”一节中有描述。

这就完成了卷积 2D 神经网络的架构，我们将在下一节中使用 Keras 库来创建它。

## 使用 Keras 和 TensorFlow 进行深度学习

我们现在将使用 Keras/Tensorflow 框架创建密集和卷积神经网络。Keras 是一个高级 API，它使用 Tensorflow 作为创建神经网络的后端，非常易于使用。在本节中，我们将首先创建一个密集的 MLP 神经网络，然后我们将创建标准的卷积神经网络模型，这是最先进的，广泛用于计算机视觉。

### 密集神经网络

我们将使用 Keras 框架来创建它，我们需要为此创建一些库。我们在前面的章节中已经安装了 Tensorflow 和 Keras，我们将在笔记本中导入这些。打开“Intro-To-Keras-sequential . ipynb”Jupyter 笔记本，在第一部分我们将看到以下命令:

```
import tensorflow as tf
import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import Dense, Flatten
from datetime import datetime
from tensorflow import keras

```

在我们的例子中，我们将为二元分类问题创建密集网络。二元分类问题只有两种结果，在这种情况下，sigmoid 函数被用作激活函数。Keras API 有一个称为“模型”的模块，它包含了框架中模型的不同架构。顺序模型指定深度学习模型的不同层将依次放置，这意味着一层的输出将是下一层的输入，就像密集 MLP 的情况一样。Keras 中的 layers 模块指定了我们希望包含在模型中的不同类型的层；例如,“密集”指定带有隐藏层的密集 MLP 网络,“展平”指定我们在卷积神经网络中看到的展平层，卷积 2D 指定卷积层，等等。随着本章的进展，我们将探索这些层。创建深度学习网络有不同的代码编写方式，但有一定的限制。在大多数情况下，我们将使用 Keras API 并调用 Dense()等函数来创建密集的 MLP 层；但是，也有使用 Tensorflow 的 Keras 功能 API 的替代编写技术。为了使用第二种形式创建相同的密集层，我们将编写为 keras.layers.Dense()。我们将使用这些符号来表示本章中的模型。

在下一部分中，我们有一个“inp_data”方法，它为运行我们的深度学习模型创建合成数据。我们用称为数据的随机值初始化大小为[512，512]的张量，并创建一个[512，1]维张量作为包含二进制输出 1 或 0 的标记集。为了简单起见，所有这些都是随机分配的。在一般监督学习中，我们有一个与数据相关联的初始标签集，这是我们的验证数据集。基于每个时期的深度学习模型的结果，基于这些结果与验证数据集的差异来计算误差。

```
def inp_data():
  data=np.random.random((512,512))
  labels=np.random.randint(2, size=(512,1))
  return data, labels

```

下一部分包含模型构建功能——“build _ model”方法。该方法包含前面提到的 Keras.models 中的顺序模块。然后我们有了由方法“dense()”表示的密集网络。这个密集网络中有几个参数，包括:

*   **单位:**这代表输出空间的维数，在我们的例子中是二进制的；因此，我们需要一个 1D 容器(向量，数组)作为输出。

*   **input_dim:** 这表示密集网络的输入的维数。

*   **激活:**表示激活功能，如 sigmoid、softmax、relu、elu 等。

*   **use_bias:** 这指定了我们是否想要在我们的密集网络中使用 bias。

*   **bias_initializer:** 这表示网络中初始化的偏差张量。有几种初始值设定项，如 0、1 等。

*   **kernel_initializer:** 这个初始化权重张量，可以有不同的初始化器。最常见的，也是我们将要使用的，是 glorot_uniform，它是一个 Xavier 初始化器。还有一些变体，比如统一的或者普通的初始化器。

*   **偏差正则化:**这表示偏差张量的正则化。当我们想要防止深度学习中的过度拟合时使用这种方法，并通过修改步长来帮助梯度下降。

*   **kernel _ regulator:**这类似于偏差正则化，但用于内核权重张量。存在使用 L1 和 L2 正则化规范来惩罚学习率以帮助梯度下降的变体。

*   **bias_constraints:** 应用于偏差张量的约束

*   **内核约束:**应用于内核权重张量的约束

现在我们对密集方法的参数有了一个概述，我们可以使用 Keras 编写一个单一的密集神经网络模型。“model.compile”方法用于编译模型并关联神经网络的输入和输出。该部分包含:

*   **优化器:**这包括不同的梯度下降优化器，帮助寻找全局最小值，如 SGD、RMSProp、Adam、Adagrad、Adadelta 等等。

*   **损失:**这表示熵损失，根据我们的要求可以是二元的或分类的。二元损失是逻辑(sigmoid)熵损失，而分类损失是软最大熵损失。

*   **指标:**这些指标指定了对模型进行基准测试的指标，如准确性、均方误差(mse)、平均绝对误差(mae)等。

我们现在可以创建如下函数:

```
def build_model():
  model=Sequential()
  model.add(Dense(1, input_dim=512, activation="softmax",
  use_bias=True,
  bias_initializer='zeros',
  kernel_initializer='glorot_uniform',
  kernel_regularizer=None, bias_regularizer=None,
  kernel_constraint=None, bias_constraint=None))

  model.compile(optimizer='rmsprop',
  loss='binary_crossentropy',
  metrics=['accuracy'])

  return model

```

下一部分包含“train”方法，该方法调用模型并拟合用于训练的输入数据。“model.fit”方法用于拟合不同批次的训练输入数据，每个批次包含输入的特定部分，在我们的情况下，这是通过使用批次大小来指定的。这里使用的参数是:

*   **输入:**输入数据来训练模型

*   **标签:**数据中存在的用于预测和分类的标签

*   **时期:**我们想要训练模型的时期数

*   **batch_size:** 要传递给输入层的单批输入的大小。在我们的例子中，我们使用值 64。

*   **verbose:** 这表示每个时期的训练日志将被显示的模式。

这可以使用以下代码行来完成:

```
def train(model, data, labels):
  model.fit(data, labels, epochs=20, batch_size=64)

```

该段的最后一部分包含“main()”方法，该方法调用“inp_data”方法生成随机张量作为输入和标签，然后调用“build_model”方法创建序列模型，最后使用“train”方法训练模型。“model.summary”方法提供了顺序模型架构的视图。

```
if __name__=='__main__':
  data, labels=inp_data()
  model=build_model()
  model.summary()
  train(model, data, labels)

```

现在，如果我们训练这个模型，我们将看到时代，每一步训练的准确性，损失，以及相关的细节。总结一下这个模型，它看起来如图 [5-7](#Fig7) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig7_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig7_HTML.jpg)

图 5-7

包含单个密集神经网络的序列模型

现在让我们在一个张量板的帮助下想象这个训练。为此，我们将使用具有不同编码格式的相同模型。在本节中，我们将看到用“keras.layers.Dense()”替换“dense()”方法，用“keras.models.Sequential()”方法替换顺序方法。我们还使用“keras . optimizer . rms prop()”方法，它包含梯度下降的学习速率。在这种情况下，我们使用 Tensorflow 框架的 Keras API,“build _ model”方法如下所示:

```
def build_model():
  model=keras.models.Sequential()
  model.add(keras.layers.Dense(1, input_dim=512,
  activation="softmax", use_bias=True, bias_initializer="zeros",
  kernel_initializer='glorot_uniform',
  kernel_regularizer=None, bias_regularizer=None,
  kernel_constraint=None, bias_constraint=None))

  model.compile(keras.optimizers.RMSprop(learning_rate
  =1e-4), loss="binary_crossentropy", metrics=['accuracy'])
  return model

```

现在，为了集成 TensorBoard，我们必须指定日志文件，该文件以日期-时间格式保存训练数据的日志。在可视化过程中，它获取这些日志来表示训练的度量:损失、准确性等等。

```
logdir= "logs/scalars/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback=keras.callbacks.TensorBoard
(log_dir=logdir)

```

下一步是更新 train 函数中的“model.fit”方法。就像我们在前面章节中看到的，我们必须添加“回调”并将 TensorBoard 日志文件指定为实时可视化的回调资源。

```
def train(model, data, labels):
  model.fit(data, labels, epochs=200, batch_size=64,
  callbacks=[tensorboard_callback])

```

如果我们现在运行主函数，我们将在 TensorBoard 中看到训练和相应的日志。预览如图 [5-8](#Fig8) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig8_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig8_HTML.jpg)

图 5-8

张量板中稠密序列模型的可视化

我们已经用 Keras 创建了一个深度学习模型，这是创建更多样、更复杂模型的第一步，尤其是深度 RL。让我们尝试为多类分类创建一个简单的多层密集序列模型。在下一段的“build_model”方法中，我们使用“keras.models.Sequential”方法初始化一个顺序模型。然后，我们创建一个包含 256 个输出维度单元的密集网络，其中输入维度为 20 个单元和一个“Relu”激活单元。然后，我们重复一个新的密集层，它将先前密集网络的输出作为输入。dropout 层随机将输入单位设置为 0，频率为 rate，以防止在训练过程中过度拟合。因此，该层仅适用于训练模式，因为在训练期间，辍学不会冻结重量。然后我们有 64 个单位的密集层和“Relu”激活方法。最后一层是另一个密集层，10 个单位，softmax 激活多类分类。“model.compile”包含来自“keras . optimizer . Adam()”方法的 Adam optimizer，学习率为 0.0001。这种情况下的损失是“分类交叉熵”,它用准确性度量来表示 softmax 损失。下面的方法表示如下:

```
def build_model():
  model= keras.models.Sequential(name="Multi-Class Dense
         MLP Network")
  model.add(keras.layers.Dense(256, input_dim=20,
            activation="relu"))
  model.add(keras.layers.Dense(256, activation="relu"))
  model.add(keras.layers.Dropout(0.2))
  model.add(keras.layers.Dense(64, activation="relu"))
  model.add(keras.layers.Dense(10, activation="softmax"))
  model.compile(keras.optimizers.Adam(lr=1e-3,
  name="AdamOptimizer"),
  loss="categorical_crossentropy", metrics=['accuracy'])
  return model

```

当我们调用“model.summary”方法时，我们可以看到如图 [5-9](#Fig9) 所示的时序网络。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig9_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig9_HTML.jpg)

图 5-9

具有密集网络的样本多类分类模型

现在我们已经熟悉了密集神经网络，让我们来探索 Keras 中的 CNN。

### 卷积神经网络

CNN 是计算机视觉的标准网络模型，专门用于图像分类、图像生成、运动捕捉和许多其他任务。在我们探讨该网络的 Keras 实现之前，我们将研究一下我们在前面章节中提到的经过预训练的最新模型，如 Resnet50、VGG16 及其变体。我们要研究的第一个模型是 VGG-16 模型，这里的 16 是指神经网络内部的层数。

**VGG-16 型号** **:** 这是一款具有顺序架构的经典型号。以下是该模型的组成部分:

*   2 个卷积 2D 层，具有 64 个深度通道，内核(过滤器)大小为 3 x 3，填充保持不变。“相同”填充由图像宽度或高度除以步幅的上限值给出，即 dim(O)=ceil(Z/S)

*   1 个最大池层，池大小为 2 x 2，跨距值为 2

*   2 个卷积 2D 层，具有 128 个深度通道，内核(过滤器)大小为 3 x 3，填充保持不变

*   1 个最大池层，池大小为 2 x 2，跨距值为 2

*   3 个卷积 2D 层，具有 256 个深度通道，内核(过滤器)大小为 3 x 3，填充保持不变

*   1 个最大池层，池大小为 2 x 2，跨距值为 2

*   3 个卷积 2D 层，具有 512 个深度通道，内核(过滤器)大小为 3 x 3，填充保持不变

*   1 个最大池层，池大小为 2 x 2，跨距值为 2

*   3 个卷积 2D 层，具有 512 个深度通道，内核(过滤器)大小为 3 x 3，填充保持不变

*   1 个最大池层，池大小为 2 x 2，跨距值为 2

所有层的激活功能都保持为“Relu”该卷积网络广泛用于图像分类，并在展平数据后提供包含以下属性的输出:

1 个具有 4096 个单元的密集神经层

然后，这可以通过相应的激活函数作为输入传递到密集 MLP 网络，其中 sigmoid 用于二分类，softmax 用于多类分类。现在让我们将这个模型可视化，它已经在 Keras 框架内进行了预训练和构建。为此，我们将在 keras.applications 模块中导入已经构建在 Keras 框架中的 VGG-16 模型。

从 keras.applications.vgg16 导入 vgg16

“build_auto_VGG16_model()”方法介绍了如何构建这个模型。我们只需加载 VGG-16 模型，并按如下方式返回它:

```
def build_auto_VGG16_model():
    model=VGG16()
    return model

```

通过在这个模型上运行“model.summary”方法，我们可以看到这个模型的架构。由于可训练参数很大，模型的样本视图如图 [5-10](#Fig10) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig10_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig10_HTML.jpg)

图 5-10

来自 Keras 框架的 VGG-16 预构建模型

如果我们想建立自己的这个模型的变体，我们必须从 Keras 导入某些层。我们将导入卷积-2D 层、展平层、密集层、最大池层和零填充层，所有这些都是传统 CNN 的基本组件。我们还将从 Keras.optimizers 模块导入 Adam 优化器，并将使用顺序模型。

```
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Convolution2D, MaxPooling2D, Dropout, Flatten, Activation, ZeroPadding2D
from keras.optimizers import Adam

```

我们首先在“构建 _ 样本 _ VGG _ 模型()”中初始化序列模型，然后我们最初添加零填充 2D 层，以将 0 的行和列添加到输入图像张量。零填充层具有以下参数。

*   **填充:**提供一个整数或两个整数值的元组。如果提供单个整数，则意味着相同的填充应用于高度和宽度，如果提供两个整数的元组，则表示对称的高度填充和对称的重量填充。

*   **data_format:** 输入图像张量的维度，具有高度、宽度和通道数。

具有相关细节的零填充 2D 的代码如下:

```
model.add(ZeroPadding2D((1,1), input_shape=(3,224,224)))

```

然后我们有 Keras 中的卷积-2D 模型，它有以下参数。

*   **滤波器:**输出张量的维数，卷积中输出滤波器的数目

*   **内核大小:**要进行卷积的内核的大小；通常由两个整数组成的列表指定内核的高度和宽度

*   **填充:**指输入张量的填充；Valid 意味着没有填充，而 Same 指定沿高度和宽度的相等填充，以保持与输入相同的输出尺寸

*   **data_format:** 根据通道的位置，数据的格式。如果选择 channels_first 类型，则格式为(batch_size，channels，height，width)，如果选择 channels_last，则输入数据的格式为(batch_size，height，width，channels)。默认情况下，它是 Keras 的“image_data_format ”,但如果未指定，则默认为 channels_last。

*   **膨胀速率:**用于指定膨胀卷积的速率(本场景不需要)。

*   **组:**一个整数，指定沿着通道轴拆分输入的组。每组分别与滤波器进行卷积。输出是一个通道上所有滤波器的串联。

*   **激活:**要使用的激活函数，在我们的例子中是“Relu”

*   **use_bias:** 指定我们是否想要使用偏差。

*   **bias_initializer:** 这指定了 bias 的初始化，类似于密集网络。

*   **kernel_initializer:** 这个控制内核的初始化，比如我们这里的 glorot_uniform。

*   **偏差 _ 正则化:**偏差的正则化

*   **核正则化子:**核权重张量的正则化子

*   **activity _ regulator:**这是控制卷积层输出的激活函数的正则化器。

*   **内核约束:**应用于内核的约束

*   **bias_constraint:** 应用于偏差的约束。

在这种情况下，使用具有过滤器、内核大小、激活和输入数据格式的卷积-2D 层:

```
model.add(Convolution2D(64, 3, 3, activation="relu", input_shape=(3,224,224)))

```

然后，我们在代码中使用了 MaxPooling-2D 层，它通过提取池大小的最大值来缩减数据采样。以下是它的参数。

*   **pool_size:** 这表示要计算最大值的窗口大小。如果指定了单个值，则高度和宽度将应用相同的尺寸。如果指定了两个整数的元组，那么它们对应于池窗口的高度和宽度。

*   **stride:** 这可以是一个整数，也可以是两个整数的元组，指定池的步距。如果未指定，则默认为 pool_size。

*   **填充:**这个和之前类似，其中 Valid 代表没有填充，Same 代表相等的填充。

*   **数据格式:**决定数据的格式，可以是 channels_first 或 channels_last 格式。

MaxPooling 2D 图层具有以下代码:

```
model.add(MaxPooling2D((2,2), strides=(2,2)))

```

如前所述，模型的其余部分使用展平、下降和密集网络图层。要构建模型，我们必须首先将通道深度为 64 的两个卷积 2D 图层与输入数据堆叠在一起，然后应用跨度为 2 x 2 的最大池 2D 图层，如下所示:

```
model.add(Convolution2D(64, 3, 3, activation="relu", input_shape=(3,224,224)))
model.add(Convolution2D(64, 3, 3, activation="relu"))
model.add(MaxPooling2D((2,2), strides=(2,2)))
The same pattern is repeated with 2 Convolution 2D layers of depth channels 128 and a MaxPooling 2D layer.
model.add(Convolution2D(128, 3, 3, activation="relu"))
model.add(Convolution2D(128, 3, 3, activation="relu"))
model.add(MaxPooling2D((2,2), strides=(2,2)))

```

下一部分由数据上深度为 256 和 512 的三个卷积层组成，在三个卷积 2D 层之间有一个 MaxPooling 层。深度 512 的卷积 2D 图层与 MaxPooling 图层重复，如下所示:

```
model.add(Convolution2D(256, 3, 3, activation="relu"))
model.add(Convolution2D(256, 3, 3, activation="relu"))
model.add(Convolution2D(256, 3, 3, activation="relu"))
model.add(MaxPooling2D((2,2), strides=(2,2)))

model.add(Convolution2D(512, 3, 3, activation="relu"))
model.add(Convolution2D(512, 3, 3, activation="relu"))
model.add(Convolution2D(512, 3, 3, activation="relu"))
model.add(MaxPooling2D((2,2), strides=(2,2)))

model.add(Convolution2D(512, 3, 3, activation="relu"))
model.add(Convolution2D(512, 3, 3, activation="relu"))
model.add(Convolution2D(512, 3, 3, activation="relu"))
model.add(MaxPooling2D((2,2), strides=(2,2)))

```

最后一部分包含展平层，以使卷积输入张量的输出适合输入到密集网络。密集网络有 4096 个具有“Relu”和“softmax”激活功能的单元。中间的漏失层用于在训练阶段将一半的输入单元设置为 0。VGG 模型的最后一层是:

```
model.add(Flatten())
model.add(Dense(4096, activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(4096, activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(1000, activation="softmax"))

```

然后，我们使用 Adam optimizer 编译模型，以交叉熵损失和准确性作为度量。

**Resnet-50** **:** 这是另一个图像处理的基准模型，是残差网络的简称。一般来说，在复杂的顺序卷积模型中，精度会逐渐降低，并达到饱和。为了避免这种饱和精度，结合了残余块。我们考虑堆叠层在通过函数 F(X)传递输入张量 X 之后产生输出张量 Y。这是传统的顺序 CNN 模型结构。然而，如果我们要添加残差网络，而不是获得输出 F(X)，则输入张量被添加到其中，使得 Y = F(X) + X。现在在通用 CNN 模型中，由于退化，随着网络深度的增加，很难保持精度。这就是剩余方程:Y = F(X) + X 出现的原因。本质上，这意味着输入张量到输出张量的恒等变换。为了理解这一点，我们可以打个比方，如果 F(X) = 0，那么我们得到一个相同的张量输出(Y)，它与 X 相同，经过非线性激活单元。图 [5-11](#Fig11) 展示了该残留块。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig11_HTML.png](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig11_HTML.png)

图 5-11

ResNet 中的剩余框图

残差网络因此保持了准确性，即使在模型中应用了数千个卷积 2D/最大池层之后。在最差情况下，如果输入和输出张量的维度不匹配，则添加一个填充层，由下式给出:

Y= F(X，{W <sub>i</sub> }) + W <sub>i</sub> X

Resnet-50 模型有五个带有残差和卷积块的阶段。残余块也被称为身份块。每个卷积块具有三个卷积 2D 层，每个身份/残差块具有三个卷积 2D 层。该模型中有超过 2300 万个可训练参数。

我们将使用 keras.applications 模块中已经构建的 Resnet-50 模型。然后，我们将在 build_model 函数中加载 Resnet-50 模型，如下所示:

```
from keras.applications.resnet50 import ResNet50
def build_model():
  model=ResNet50()
  return model

resnet_model= build_model()
resnet_model.summary()

```

当我们总结模型时，我们将看到模型正在被下载，然后可训练参数与层一起显示，如图 [5-12](#Fig12) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig12_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig12_HTML.jpg)

图 5-12

来自 Keras 的 Resnet-50 型号

有几种不同的 VGG 和雷斯网模型，具有不同数量的隐藏和卷积层，还有其他最新的模型，如 AlexNet 和 GoogleNet。

### 用 Resnet-50 构建图像分类器模型

现在，我们已经了解了神经网络的基本原理和在 Keras 中构建模型，让我们使用 Keras 框架和 ResNet-50 模型用几行代码构建一个图像分类器模型。打开 resnet 50 Convolution networks . ipynb 笔记本，在这种情况下，我们将使用该模型让机器判断给定的图像是猫还是狗。为了首先构建这个二进制分类模型，我们必须从 Keras 导入库，即从 keras.applications 导入 Resnet-50 模型，从 keras.preprocessing 模块导入 img，img_to_array 模块。这些层帮助我们解码和预处理输入图像，使其可用于 Resnet 模型。我们还需要 numpy 来改变图像张量的形状，还需要“matplotlib”来在屏幕上显示图像。

```
from keras.applications.resnet50 import ResNet50, decode_predictions, preprocess_input
from keras.preprocessing.image import image, img_to_array
import numpy as np
import matplotlib.pyplot as plt

```

在下一阶段，我们导入数据集并解压缩。这包含了一个大型的猫和狗的图像数据库，这些图像被正确地标记，我们可以在其中使用我们的监督 Resnet 模型。

```
!wget -qq http://sds-datacrunch.aau.dk/public/dataset.zip
!unzip -qq dataset.zip

```

现在，让我们通过使用 matplotlib 以[244，244]的维度绘制数据集来查看数据集的第一幅图像。

```
img = image.load_img
("dataset/single_prediction/cat_or_dog_1.jpg", target_size = (224, 224))
plt.imshow(img)

```

运行这个程序时，我们会在屏幕上看到一只拉布拉多犬的图像，如图 [5-13](#Fig13) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig13_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig13_HTML.jpg)

图 5-13

数据集中一只狗的图像

我们将使用这张图像来验证模型是否能够正确地将其分类为一只狗。在下一阶段，我们必须将这个图像转换成张量。这是通过转换 numpy 数组中的图像，然后沿轴扩展它，使其成为输入张量来实现的:

```
img=image.img_to_array(img)
img=np.expand_dims(img, axis=0)
img=preprocess_input(img)

```

在下一步中，我们使用 Resnet-50 模型训练该图像，并且我们将从该模型获得预测。现在我们想要预测给定图像的最可能的分类，在我们的例子中是狗。这是通过使用“decode_predictions”方法来完成的，该方法陈述了模型预测的预测值，无论它是狗还是猫。

```
model=ResNet50(weights='imagenet')
model.summary()
#predict image
predictions= model.predict(img)
print("Accuracy of predictions")
print(decode_predictions(predictions, top=1)[0])

```

现在，如果我们运行这个模型，我们会立即看到 Resnet 模型以 56.7%的准确率预测图像是拉布拉多寻回犬的图像。我们已经用 Resnet-50 建立了一个二元分类模型，我们也可以创建一个多类分类模型。“卷积网络-分类. ipynb”笔记本包含一个多类分类模型，该模型使用自定义卷积-2D 神经网络(实现 VGG-16 的变体)对 CIFAR 数据集进行分类。CIFAR 是一个基准影像分类数据集，存在于 keras.datasets 模块中，广泛用于影像分析和分类。然而，模型规范和实现是为感兴趣的读者保留的，他们希望深入研究图像分析、处理、分类模型和其他计算机视觉模型。

这些神经网络模型，包括 MLP(密集)和卷积，将在我们的深度 RL 算法中广泛使用，现在我们对这些网络有了一个公平的想法，我们将研究它们。

## 深度强化学习算法

在第 [1](1.html) 章中，我们了解了 RL 中的对比以及价值和政策功能之间的关系。我们使用贝尔曼函数来更新这些值，并且还更改了策略以获得更好的价值估计。所有这些都是基于离散数据。在我们深入研究主要算法之前，我们应该对深度学习中不同类型的算法范例有一个概念。

### 论策略算法

这些算法依赖于策略性能和优化。这一系列算法试图优化策略并提供良好的性能。本节我们将通过比较 Q-learning (off-policy)和 SARSA (state，action，reward，state，action；政策上的)算法。

#### 传统 RL: Q-Learning 和 SARSA

在这种情况下，更新策略函数以获得最佳 Q 值。在离散 RL 场景中，我们读到了贝尔曼方程和 Q 学习算法，我们还提到了策略函数。更新策略函数而不是值的离散 RL 的另一个变体是 SARSA 算法。在一般 Q 学习中，我们有以下 Q 值更新等式:

Q(S，A) = Q(S，A)+α[R+ymax<sub>A</sub>Q(S<sup>**`**</sup>，A)-Q(S，A) ]

S=S <sup>**`，**T3】</sup>

其中，α是学习率，y 是折扣因子，S、A 和 R 分别代表状态、动作和奖励空间。Q(S，A)表示通过从状态 S 采取动作 A 而检索到的值。在这种学习形式中，代理使用 Q 值策略(贪婪策略)从 S 中选择动作 A。然后它接收 R(奖励)并观察下一个状态 S **`** 。因此，存在对旧状态的依赖性，这对于非策略算法是有用的。与 Q-learning 相反，我们有 SARSA 算法。SARSA 的更新公式由下式给出:

Q(S，A)= Q(S，A) + α [R + yQ(S <sup>**`**</sup> ，A**`**)–Q(S，A) ]

A=A <sup>`</sup>

S = S<sup>T1 `T3】</sup>

虽然这两个方程都来自贝尔曼方程，但根本的区别是，SARSA 基于 Q 策略从状态 S 中选择动作 A，然后它观察 R(奖励)和新状态(S <sup>**`**</sup> )，然后通过使用从 Q 导出的策略从状态 S <sup>**`**</sup> 中选择另一个动作 A <sup>**`**</sup> 。策略的更新使 SARSA 成为离散 RL 空间中的策略上算法。在 SARSA 中，代理学习最优策略，并使用相同的策略(如ε贪婪策略)来表现。对于 SARSA，更新策略与行为策略相同。

要看到这个对比，我们可以打开 Q-Learning | SARSA-frozen lake . ipynb 笔记本，看看离散空间中传统学习算法中开关策略技术的对比。除了 Q 和 SARSA 函数之外，大部分代码段都是相同的。在“Q_learning”方法中，我们看到 Q 策略试图应用贪婪最大化策略，并根据前面提到的计算出的 Q 值进行下一步。这是通过以下代码行实现的:

```
best_value, info = best_action_value(table, obs1)
Q_target = reward + GAMMA * best_value
Q_error = Q_target - table[(obs0, action)]
table[(obs0, action)] += LEARNING_RATE * Q_error

```

如图 [5-14](#Fig14) 所示，可以观察到 Q 学习的准确度(回报)步骤图。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig14_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig14_HTML.jpg)

图 5-14

Q 学习的精度步骤图

现在，对于 SARSA 算法,“SARSA”方法实现了更新策略的 on-policy update 方法:

```
target=reward + GAMMA*table[(state1, action)]
predict=table[(state0, action)]
table[(state0, action)]+= LEARNING_RATE*(target- predict)

```

在运行这一部分时，我们可以可视化奖励步骤图，并可以在离散空间中比较开策略和关策略之间的差异，如图 [5-15](#Fig15) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig15_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig15_HTML.jpg)

图 5-15

SARSA 的精度步骤图

#### 深度 RL:连续空间深度策略算法

现在，在许多复杂的环境中，可能存在与特定策略相关的值的连续分布。虽然这组算法可以扩展到离散空间，我们将考虑连续分布。对于连续空间，我们必须使用深度 RL 算法。在用于优化策略的基于策略的算法的情况下，最重要的一类算法被称为策略梯度。策略梯度方法直接对策略进行建模和优化。通常我们联系一个参数化的函数π <sub>θ</sub> (a|s)，它表示政策和政策梯度试图优化θ以获得更好的回报。策略梯度中的奖励函数通常表示为:

j(θ)=∑<sub>S</sub>d<sup>π</sup>(S)V<sup>π</sup>(S)=∑<sub>S</sub>d<sup>π</sup>(S)∑<sub>A</sub>π<sub>θ</sub>(A | S)Q<sup>π</sup>(S，A)、

其中 d <sup>π</sup> (s)表示π <sub>θ</sub> 策略的马尔可夫链的平稳分布，是一种保单上分布。这可以概括为，如果代理无限期地沿着马尔可夫链行进，则代理终止于某个状态的概率变得不变，这被称为稳定概率。在这种情况下，我们将探索大多数基于策略的算法，如普通策略梯度(VPG)、优势行动者评价(A2C)、PPO，以及异步行动者评价(A3C)及其变体，包括 ACER(非策略 A3C)和 ACKTR。然而，最重要的是我们将理解 PPO 政策，因为 ML 代理广泛使用它。

##### 政策梯度-强化

可以预期，在连续空间中，基于策略的深度 RL 比基于值的学习更有用。这是因为可以估计这些值的动作的数量是无限的；因此，基于值的方法计算量太大。在政策培训的政策梯度法中，有一个梯度上升的概念。这意味着政策改进根据以下等式发生:

Q<sup>π</sup>(s<sup>T3】`T5】| A<sup>T7】`T9)= arg max<sub>A</sub>Q<sup>π</sup>(s | A)</sup></sup>

为了最大化策略更新，我们通常需要计算策略 J(θ)的梯度，以便我们可以将θ向该策略的最高回报π <sub>θ的方向移动。</sub>这是通过使用策略梯度定理来完成的，并且是连续空间中存在的大多数深度 RL 算法的支柱。

###### 政策梯度定理

计算梯度是一个困难的问题，因为它取决于行动选择空间以及状态的静态分布，特别是政策行为π <sub>θ</sub> 。后一部分很难计算，因为不可能知道动态环境中状态的静态分布。我们将策略的梯度表示为∈<sub>θ</sub>J(θ)，其中∈<sub>θ</sub>表示参数空间θ上的梯度算子。政策梯度定理提供了对目标函数的偏导数的改造，而不涉及状态分布的导数。政策梯度可以简化如下:

∈θJ(θ)=∈θ∑S dπ(S)vπ(S)=∈θ∑S dπ(S)∑Aπθ(A | S)qπ(S，A)

α∑<sub>S</sub>d<sup>π</sup>(S)∑<sub>A</sub>∈<sub>θ</sub>π<sub>θ</sub>(A | S)Q<sup>π</sup>(S，A)、

其中α代表比例。因此，我们不是计算状态空间上的导数，而是有效地计算策略函数的导数。这个定理的推导可以从状态值函数∈<sub>θ</sub>V<sup>π</sup>(s)开始，然后扩展到包括策略函数和 Q <sup>π</sup> (s，a)状态，如下:

∈<sub>θ</sub>V<sup>π</sup>(s)=∈<sub>θ</sub>(∈<sub>A</sub>π<sub>θ</sub>(A | s)Q<sup>π</sup>(s，A))

=∑<sub>A</sub>(∈<sub>θ</sub>π<sub>θ</sub>(A | s)Q<sup>π</sup>(s，A)+π<sub>θ</sub>(A | s)∈<sub>θ</sub>Q<sup>π</sup>(s，A))利用导数的链式法则

用未来状态值扩展 Q

=∈<sub>A</sub>(∈<sub>θ</sub>π<sub>θ</sub>(A | S)Q<sup>π</sup>(S，A)+π<sub>θ</sub>(A | S)∈<sub>θ</sub>∈<sub>S，R</sub> P(s **`** ，r|s，a) (r+V <sup>π</sup>

报酬不是θ的函数

=∈<sub>A</sub>(∈<sub>θ</sub>π<sub>θ</sub>(A | S)Q<sup>π</sup>(S，A)+π<sub>θ</sub>(A | S)∈<sub>S，R</sub> P(s **`** ，r|s，A)∈<sub>θ</sub>V<sup>π【T17</sup>

作为 P(s **`** ，r|s，a) = P(s **`** |s，a)

=∈<sub>A</sub>(∈<sub>θ</sub>π<sub>θ</sub>(A | S)Q<sup>π</sup>(S，A)+π<sub>θ</sub>(A | S)∈<sub>S，R</sub> P(s **`** |s，A)∈<sub>θ</sub>V<sup>π</sup>(t

因此，我们有梯度更新策略的递归公式，如以下等式中所述:

∈<sub>θ</sub>V<sup>π</sup>(S)=∑<sub>A</sub>(∈<sub>θ</sub>π<sub>θ</sub>(A | S)Q<sup>π</sup>(S，a) + π <sub>θ</sub> (a|s) ∑ <sub>S，R</sub> P(s **`** |s，A

如果我们仔细观察，我们会发现，如果我们将∈<sub>θ</sub>V<sup>π</sup>()无限延伸，我们很容易发现，通过递归展开过程，我们可以从一个起始状态 s 过渡到任何一个状态，并将所有的访问概率相加，我们将得到∈<sub>θ</sub>V<sup>π</sup>(s)。这可以简单地用π <sub>θ</sub> (a|s) ∑ <sub>S，R</sub> P(s **`** |s，a)∈<sub>θ</sub>V<sup>π</sup>(S**`**)反复过渡到新状态 s **`、** s **`、**等来证明。这种形式可以表示为:

∈<sub>θ</sub>V<sup>π</sup>(S)=∑<sub>S</sub>∑<sub>K</sub>p<sup>π</sup>(S→x，K)φ(S)、

其中 p <sup>π</sup> (s →x，k)表示 k 步后从状态 s 到 x 的跃迁概率，φ(s)表示∈<sub>θ</sub>π<sub>θ</sub>(a | s)Q<sup>π</sup>(s，a)。

现在，将它传递到梯度上升的策略梯度函数中，我们得到:

j(θ)=<sub>v<sup>(s)</sup></sub>

=∑<sub>S</sub>∑<sub>K</sub>p<sup>π</sup>(S<sub>0</sub>→S，K)φ(S)从状态 S <sub>0</sub> 开始通过马尔可夫分布和递推

=∑<sub>S</sub>η(S)φ(S)让η(S)=∑<sub>K</sub>p<sup>π</sup>(S<sub>0</sub>→S，K)

=(∑<sub>S</sub>η(s))[∑<sub>S</sub>η(S)/(∑<sub>S</sub>η(S))]φ(S)归一化η(S)

α[∑<sub>S</sub>η(S)/(∑<sub>S</sub>η(S))]φ(S)∑<sub>S</sub>η(S)是常数

=∑<sub>S</sub>d<sup>π</sup>(S)∑<sub>A</sub>∈<sub>θ</sub>π<sub>θ</sub>(A | S)Q<sup>π</sup>(S，A)d<sup>π</sup>(S)=∑<sub>S</sub>η(S)/(∑<sub>S</sub>ζ

这就完成了策略梯度定理的证明，我们可以将等式简化如下:

∈<sub>θ</sub>J(θ)α∈<sub>S</sub>d<sup>π</sup>(S)∈<sub>A</sub>∈<sub>θ</sub>π<sub>θ</sub>(A | S)Q<sup>π</sup>(S，A)

=∑<sub>S</sub>d<sup>π</sup>(S)∑<sub>A</sub>π<sub>θ</sub>(A | S)Q<sup>π</sup>(S，A)[(∈<sub>θ</sub>π<sub>θ</sub>(A | S))/π<sub>θ</sub>(A | S)]

添加该修改以产生期望形式的通用策略梯度算法的新形式，使用 ln(x) (log <sub>e</sub> x)的导数是 1/x 的事实:

√t0〖t1〗j(θ)= e[q〖T2〗T3〗(s，a)〖T4〖t5〗ln(π〖T6〗〗〖T7〗(a | s)]]

通常，该等式表示为:

√t0〖t1〗j(ο)= e[√T2〗t〖T3〗t〖t5〗t〖T6〖T7〗ln(π〖t8〖T9〖a | s〗

其中可能有以下功能:

*   **奖励轨迹:** ∑ <sub>t</sub> r <sub>t</sub>

*   **状态-动作值:**∑<sub>t</sub>Q<sup>π</sup>(s<sub>t</sub>，a <sub>t</sub> )

*   **优势功能:**∑<sub>t</sub>A<sup>π</sup>(s<sub>t</sub>，a <sub>t</sub> )

*   **时差残差:**r<sub>t+</sub>V<sup>π</sup>(s<sub>t+1</sub>)-V<sup>π</sup>(s<sub>t</sub>)

优势函数定义为:

a<sup>(s<sub>，a<sub>= q<sup>(s<sub>)，a<sub>【t】</sub></sub></sup></sub></sub></sup>

在我们将密切关注的 VPG，我们在政策梯度方程中有优势函数，表示为:

√t0〖t1〗j(ζ)= e[√T2〗t〗T3〖T3〖T3〗ln(π〖T6〖T7〗a〖t8〗T9〗

现在，我们已经看到，最优策略试图最大化回报，梯度用于计算高维空间中的最大值。在使用政策方法的深度 RL 中，这是梯度上升步骤，本质上是随机的，并试图最大化参数θ，如下所示:

ο<sub>k+1 =</sub><sub>【k】</sub>+α】<sub>j(θ)，</sub>

其中，α是学习率。策略梯度根据回报计算优势函数估计值，然后使用我们在神经网络中讨论的二次损失函数，通过正常梯度下降(SGD/Adam)来尝试最小化价值函数中的误差:

φ<sub>k+1</sub>α∑<sub>S</sub>∑<sub>t</sub>(V<sup>π</sup>(S<sub>t</sub>)–R<sub>t</sub>)<sup>2</sup>，

其中，α表示比例，φ<sub>k+1</sub>是均方损失的误差梯度。

为了理解这种策略上的深度强化算法，我们将研究 VPG 算法的简单实现。打开 Policy Gradients.ipynb 笔记本。我们将使用 OpenAI 的 CartPole 环境，就像我们之前的大多数例子一样，并将开发算法。然后，我们将在健身房的其他可训练深度 RL 环境中使用这一点，如 Atari Games (Pong)和 MountainCar。

我们定义 PGAgent 类，并声明控制探索开发率(gamma)、学习率、状态大小、动作大小的初始变量，并初始化数组以包含奖励、动作、状态和概率的列表。这由以下几行表示:

```
class PGAgent:
  def __init__(self, state_size, action_size):
    self.state_size=state_size
    self.action_size=action_size
    self.gamma=0.99
    self.learning_rate=0.001
    self.states=[]
    self.rewards=[]
    self.labels=[]
    self.prob=[]
    self.model=self.build_model()
    self.model.summary()

```

现在让我们看看这个算法的模型部分，它负责优势的梯度上升和梯度下降，以最小化价值估计的误差。在我们的例子中，我们将使用密集的 MLP 网络。在这种情况下，我们也可以使用卷积网络。最初的两个密集使用 64 个单元，使用“Relu”激活和 glorot_uniform 作为内核初始化函数。然后我们有一个密集层，softmax 激活，self.action_size 作为输出维度。因为在扁担中，动作空间包括向左或向右移动，所以我们也可以使用乙状结肠激活。然后我们有一个带有 Adam 优化器和分类交叉熵损失的“model.compile”方法。因为这段代码是为大多数 OpenAI Gym 的环境提供一种策略梯度方法；因此，我们将 softmax 激活用于多类分类和交叉熵损失。对于 CartPole，我们可以通过使其成为二进制交叉熵并在最后的密集层中使用 sigmoid 激活来改变损失。

```
model=Sequential()
    model.add(Dense(64, input_dim=self.state_size, activation="relu", kernel_initializer="glorot_uniform"))
model.add(Dense(64, activation="relu", kernel_initializer="glorot_uniform"))
model.add(Dense(self.action_size, activation="softmax"))
    model.compile(optimizer=Adam(learning_rate=self.learning_rate), loss="categorical_crossentropy")
 return model

```

下一个方法是记忆，它填充了动作、奖励和状态的数组。在这个方法中，每当选择一个动作时，它在数组中被标记为 1。这可以被认为是一种一次性编码机制。

```
y=np.zeros([self.action_size])
y[action]=1
self.labels.append(np.array(y).astype('float32'))
self.states.append(state)
self.rewards.append(reward)

```

在“行动”方法中，我们基于优势估计(GAE)函数，使用模型来预测代理应该采取的行动。“模型.预测”方法用于预测概率。基于 GAE 策略，选择具有最高概率的动作作为下一个动作。

```
  def act(self, state):
    state=state.reshape([1, state.shape[0]])
    probs=self.model.predict(state, batch_size=1).flatten()
    self.prob.append(probs)
    action=np.random.choice(self.action_size,1, p=probs)[0]
    return action, probs

```

“折扣奖励”方法通过使用勘探开发系数γ提供折扣奖励。这是一个通用的折扣奖励政策，将在各种算法中使用。

```
def discount_rewards(self, rewards):
    discounted_rewards = np.zeros_like(rewards)
    running_sum = 0
    for t in reversed(range(len(rewards))):
        if rewards[t] != 0:
            running_add = 0
        running_add = running_add * self.gamma + rewards[t]
        discounted_rewards[t] = running_add
    return discounted_rewards

```

在下一个方法“训练”方法中，我们将训练神经网络。为此，我们将奖励数组标准化。然后，我们使用“model.train_on_batch”方法来训练神经网络，并将输入作为状态(“自我状态”)传递，将输出作为动作(“自我标签”)。

```
def train(self):
    labels=np.vstack(self.labels)
    rewards=np.vstack(self.rewards)
    rewards=self.discount_rewards(rewards)
    rewards=(rewards-np.mean(rewards))/np.std(rewards)
    labels*=-rewards
    x=np.squeeze(np.vstack([self.states]))
    y=np.squeeze(np.vstack([self.labels]))
    self.model.train_on_batch(x, y)
    self.states, self.probs, self.labels, self.rewards
    =[],[],[],[]

```

在 load_model 和 save_model 函数中，我们加载并保存模型的权重，如下所示:

```
def load_model(self, name):
    self.model.load_weights(name)

  def save_model(self, name):
    self.model.save_weights(name)

```

在“main()”方法中，我们使用“gym.make”方法从 Gym 创建环境。这种方法随着奖励也有了行动空间和观察空间。我们创建了一个 PGAgent 类的对象，并将动作大小和状态大小作为来自健身房环境的参数传入。我们对我们的模型进行 200 个时期的训练，对于每一次训练，我们观察奖励、状态和动作，并相应地将奖励添加到累积奖励中。对于“agent.train”方法代表的每一集培训，我们记录相应的行动和奖励。如果奖励为负，则重置为 0，并重新开始训练。

```
if __name__=="__main__":
  env=gym.make('CartPole-v0')

  state=env.reset()

  score=0
  episode=0
  state_size=env.observation_space.shape[0]
  action_size=env.action_space.n
  agent=PGAgent(state_size, action_size)
  j=0
  while j is not 2000:
    screen = env.render(mode='rgb_array')
    action, prob=agent.act(state)
    state_1, reward, done,_=env.step(action)
    score+=reward
    agent.memory(state, action, prob, reward)
    state=state_1
    plt.imshow(screen)
    ipythondisplay.clear_output(wait=True)
    ipythondisplay.display(plt.gcf())

    if done:
      j+=1
      agent.rewards[-1]=score
      agent.train()
      print("Episode: %d - Score: %f."%(j, score))
      score=0.0
      state=env.reset()

    env.close()

```

最后，训练完成后，我们关闭环境。我们还可以看到使用“ipythondisplay”方法时的 CartPole，我们在第一章中已经看到了。图 [5-16](#Fig16) 说明了这一点。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig16_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig16_HTML.jpg)

图 5-16

基于策略梯度算法的标杆环境训练

此代码段可用于解决健身房环境中的登山车问题，在这种情况下，汽车被困在两个山坡之间，必须向上攀爬才能到达目的地。通过改变 env.make (MountainCar-v0)，我们可以模拟这个用例的 GAE 策略梯度算法，如图 [5-17](#Fig17) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig17_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig17_HTML.jpg)

图 5-17

政策梯度中的登山车问题培训

**演员评论家算法**

现在让我们冒险进入演员评论家算法集，这是一个政策上的技术。正如我们在政策梯度中看到的，梯度函数有一个梯度上升，用于选择更好的回报，还有一个标准梯度下降或最小化函数，用于减少价值估计中的误差。在 actor critic 中，通过引入两个竞争神经网络模型并存储与特定策略相关的先前值估计来进行修改。因此演员评论家由两个神经网络模型或代理组成:

*   **评论家:**更新价值函数参数 w，通过更新状态-行动值∑ <sub>t</sub> Q <sup>π</sup> (s <sub>t</sub> ，a <sub>t</sub> 或状态值∑<sub>t</sub>V<sup>π</sup>(s<sub>t</sub>)来修改政策梯度

*   **Actor:** 依靠评论家来更新π的策略参数θ<sub>θ</sub>(a | s)

对于 critic 更新步骤，我们可以考虑用状态动作值代替优势函数的策略梯度定理和函数如下:

√t0〖t1〗j(θ)= e[√q〗T2〖T3〗(s，a)〖T4〗〖t5〗ln(π〖T6〗θ〗〖T7〗(a | s)]，

其中梯度上升步骤由表示

ο<sub>k+1 =</sub>θ<sub>k</sub>+α<sub>j(θ)</sub>

行动者可以升级其政策以获得更好的奖励。也有一个价值估计，支配评论家在决定下一个状态的价值。根据当前的政策π <sub>θ</sub> (a|s)，批评家估计行动者要采取的行动是否最有回报。因此，《演员评论》的总体架构可以形象化，如图 [5-18](#Fig18) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig18_HTML.png](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig18_HTML.png)

图 5-18

演员评论家网络架构

关于这一点，我们将理解演员评论家算法的一些变种。

**A2C 算法**

a<sup>(s<sub>，a<sub>= q<sup>(s<sub>)，a<sub>【t】</sub></sub></sup></sub></sub></sup>

A2C 算法可以在 A2C.ipynb 笔记本中找到。该算法是使用我们观察到的策略梯度算法的相同代码库构建的，并且所有功能都是相同的。A2C 算法唯一的变化是我们训练了两种不同的神经网络模型——演员和评论家。这可以在 A2Cagent 类中的“build_actor_model”和“build_critic_model”方法中观察到。actor 模型包含两个具有 64 个单元的密集 MLP 层，具有激活“Relu”(可以根据需要更改为卷积 2D 层)和内核初始化器。然后，它有一个 softmax 分布式外部密集层，以动作空间作为输出。我们最后编译了带有分类交叉熵损失和 Adam 优化器的演员模型。

```
def build_actor_model(self):
    logdir= "logs/scalars/" + datetime.now().
    strftime("%Y%m%d-%H%M%S")
    tensorboard_callback=keras.callbacks.
    TensorBoard(log_dir=logdir)

    Actor=Sequential()
    Actor.add(Dense(64, input_dim=self.state_size,
    activation='relu',
    kernel_initializer='glorot_uniform'))

    Actor.add(Dense(64, activation="relu",
    kernel_initializer='glorot_uniform'))

    Actor.add(Dense(self.action_size, activation="softmax"))

    Actor.compile(optimizer=
    Adam(learning_rate=self.learning_rate),
    loss='categorical_crossentropy')
    return Actor

```

在下一个阶段，我们有 build_critic_model 函数，它具有与 actor 模型相似的体系结构。然而，我们可以通过引入不同的密集/卷积层和/或改变激活函数来改变这种架构。

```
def build_critic_model(self):
    logdir= "logs/scalars/" + datetime.now().
    strftime("%Y%m%d-%H%M%S")

    tensorboard_callback=keras.callbacks.
    TensorBoard(log_dir=logdir)

    Critic=Sequential()
    Critic.add(Dense(64, input_dim=self.state_size,
    activation='relu',
    kernel_initializer='glorot_uniform'))

    Critic.add(Dense(64, activation="relu",
    kernel_initializer='glorot_uniform'))
    Critic.add(Dense(self.action_size, activation="softmax"))

    Critic.compile(optimizer=
    Adam(learning_rate=self.learning_rate),
    loss='categorical_crossentropy')
    return Critic

```

“记忆”方法具有与策略梯度中类似的实现。但是，在“行动”方法中，我们将使用 critic 模型来预测行动，如下所示:

```
def act(self, state):
    state=state.reshape([1, state.shape[0]])
    probs=self.Critic.predict(state, batch_size=1).flatten()
    self.prob.append(probs)
    action=np.random.choice(self.action_size,1, p=probs)[0]
    return action, probs

```

然后我们有“discount_rewards”的方法，类似于政策梯度。“训练”方法已经改变，因为在这种情况下，我们必须训练演员和评论家模型。我们接受行动(“标签”)和折扣奖励的输入，然后为演员和评论家模型指定状态和行动(自我状态，自我标签)。“train_on_batch”方法用于专门针对用户定义的批次进行训练，在这种情况下，我们也可以使用“model.fit”方法进行训练。不同之处在于，在后一种情况下，“拟合”方法会自动将采样数据转换成批次进行训练，并且还可能包括生成器函数。

```
def train(self):
    labels=np.vstack(self.labels)
    rewards=np.vstack(self.rewards)
    rewards=self.discount_rewards(rewards)
    rewards=(rewards-np.mean(rewards))/np.std(rewards)
    labels*=-rewards
    x=np.squeeze(np.vstack([self.states]))
    y=np.squeeze(np.vstack([self.labels]))
    self.Actor.train_on_batch(x, y)
    self.Critic.train_on_batch(x, y)
    self.states, self.probs, self.labels, self.rewards=[],[],[],[]

```

代码段的其余部分与策略梯度相同，因此像“load_weight”和“save_weight”这样的方法是相似的。在横竿环境中运行这个，我们可以想象训练和奖励/损失。因此，我们建立了一个 A2C 代理，它使用两个神经网络，使用策略梯度和优势值估计来预测输出动作。这也可以用来训练 OpenAI 健身房的 Acrobot 环境。Acrobot 由两个关节和两个连杆组成，其中两个连杆之间的关节欠驱动。目标是将下连杆的末端摆动到给定的高度，如图 [5-19](#Fig19) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig19_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig19_HTML.jpg)

图 5-19

使用 A2C 的 Acrobot 环境训练

**A3C** **:** 这是 A2C 算法的异步版本，主要依靠线程。这意味着有几个工作线程并行工作来更新参数θ。这种基于策略的算法的重要性在于，当多个参与者被并行训练时，A3C 代理学习价值函数，并且这些与全局参数周期性地同步。虽然这种算法在 GPU 中运行得特别好，但我们将为 CPU 实现一个非常简单的版本，使用我们拥有的相同代码库。不同并行行动者的效果意味着训练更快，因为参数更新更快，这有助于更新策略。

在 Colab 或 Jupyter 笔记本中打开 A3C.ipynb 笔记本。这段代码可以在 Cloud GPUs 上运行，这是 Colab 提供的，只需做一些代码更改。在我们的例子中，我们必须导入线程库及其相关模块，比如锁和线程。锁定和释放的概念对于进程调度(线程调度)非常重要，并且可以防止操作系统中的死锁。当一个子线程更新全局参数时，其他子线程不应读取或使用这些参数；这是通过经典的互斥锁(互斥)来完成的。这类似于操作系统理论中的“读者-写入者”问题，即除非写入者完成对同一磁盘或文件的写入，否则读者无法读取。

```
import threading
from threading import Lock, Thread

```

我们有大部分实现 A3CAgent 类的函数，类似于 A2CAgent 类。不同之处在于，这里我们将使用线程来制作 Actor 模型的多个副本。“train_thread”方法根据“n_threads”属性中指定的线程数量创建“守护程序”线程。现在，这些守护线程中的每一个都调用“thread_train”方法，该方法有一个相关的锁，用于读取和更新全局参数集。

```
def train_thread(self, n_thread):
    self.env.close()
    envs=[gym.make('CartPole-v0') for i in range(n_thread)]
    threads=(threading.Thread(target=self.thread_train(),
    daemon=True, args=(self, envs[i], i)) for i
    in    range(n_thread))
    for t in threads:
      time.sleep(1)
      t.start()

```

“thread_train”方法在每个锁阶段内调用“agent.train()”方法，训练特定的 worker actor 线程，并更新全局参数集(状态、动作、奖励)。

```
def thread_train(self):
    lock=Lock()
    lock.acquire()
    agent.train()
    lock.release()

```

这就完成了 A2C 算法的 A3C 修改。如上所述，还有其他版本更快，在 GPU 上有更好的性能，但在这种情况下，我们只是使用更小的 CPU 版本。我们也可以用这种算法训练 MountainCar，并与 A2C 的结果进行比较。为了说明，A3C 架构如图 [5-20](#Fig20) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig20_HTML.png](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig20_HTML.png)

图 5-20

A3C 架构

**使用克罗内克分解信赖域(ACKTR)** **:** 这是 Actor critic 类算法的变体，其中使用自然梯度来代替 Adam/SGD/RMSProp 优化器。它比梯度下降法更快，因为它通过使用 Fisher 信息矩阵(FIM)作为损失函数的曲率来获取损失景观。在给定 x 值的情况下，输出 y 的条件概率的概率模型的一般 FIM 表示为:

f<sub>= e<sub>(x，y)</sub>[√日志 p(y | x)；ο日志 p(y | x)；θ) <sup>T</sup></sub>

在一般分类任务中，我们通常使用对数似然的平均值(mse 误差)作为损失函数。我们可以导出 FIM 和负对数似然损失 E(θ)的 Hessian(标量场的 2 <sup>和</sup>阶偏导数矩阵)之间的关系:

h(e)(θ)=√t0]2e<sub>(x，y)</sub>[-log p(y | x)；[原件:英文]

= - E <sub>(x，y)</sub>[<sup>2</sup>日志 p(y | x)；[原件:英文]

= - E <sub>(x，y)</sub> [-((日志 p(y | x))；ο日志 p(y | x)；θ)<sup>t</sup>/p(y | x；θ)<sup>2</sup>+<sup>2</sup>p(y | x)；θ)/p(y | x)；θ)(链规则)

= f<sub>-e<sub>(x，y)</sub>[<sup>2</sup>p(y | x)；θ)/p(y | x)；[原件:英文]</sub>

梯度更新规则由下式给出:

ο<sub>k+1</sub>= o<sub>【k】</sub>–α(f<sub>)<sup>-1</sup><sub>【k】</sub>【e】【θT10】</sub>

其中，α是学习率，表示梯度算子(用于导数)。

我们将使用来自基线(稳定基线)的 ACKTR 模型来并行训练 CartPole 环境的四个实例。打开 ACKTR.ipynb 笔记本第一步，我们必须进行必要的导入，包括稳定基线、策略(MLP/MLPLSTM)、ACKTR 模型和用于可视化的 ipythondisplay。

```
import gym

from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy
from stable_baselines.common import make_vec_env
from stable_baselines import ACKTR
import matplotlib.pyplot as plt
from IPython import display as ipythondisplay
from pyvirtualdisplay import Display
display = Display(visible=0, size=(400, 300))
display.start()

```

然后，我们加载 car pole-v1 环境，并加载具有 25000 个时间步长的 MLP(密集神经网络)策略的 ACKTR 模型。

```
env = make_vec_env('CartPole-v1', n_envs=4)

model = ACKTR(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=25000)
model.save("acktr_cartpole")
model = ACKTR.load("acktr_cartpole")

obs = env.reset()

```

然后，我们有一个执行算法的循环，并观察每一步训练的状态、奖励和动作。

```
while True:
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    screen = env.render(mode='rgb_array')
    plt.imshow(screen)
    ipythondisplay.clear_output(wait=True)
    ipythondisplay.display(plt.gcf())

```

在运行该模块时，我们可以用 ACKTR 策略可视化发生在四个实例上的训练，如图 [5-21](#Fig21) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig21_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig21_HTML.jpg)

图 5-21

CartPole 中的 ACKTR 算法

我们还可以修改我们在前面案例中使用的现有代码库。我们将看到 A2C _ 阿克特类，代替“构建 _ 行动者 _ 模型”类中的亚当优化器，我们将使用“kfac”(克罗内克因子)作为自然梯度下降的优化器。这使用了 Tensorflow 的“kfac”模块，该模块尚未进行任何升级以兼容 Tensorflow 2.0。因此，可能存在误差不匹配。然而，代码保持不变，唯一的修改是包含了“kfac”模块，而不是 Adam 优化器。

**连续空间的随机演员评论家** **:** 这是演员评论家算法的另一种变体，它使用高斯分布而不是 softmax 分布。这种参数化分布有助于连续空间。为此，策略π <sub>θ</sub> (a|s))的高斯方程(分布)可以写成:

π<sub>θ</sub>(a | s))= 1/(√2σ<sub>θ</sub>(s))exp(--(a-μ<sub>θ</sub>(s)<sup>2</sup>)/2σ<sub>θ</sub>(s)<sup>2</sup>)

这是在随机连续正常 AC.ipynb 笔记本中实现的，在“build_actor_model”方法中进行简单替换，将内核初始化为“Normal”，这表示正常(高斯内核)和 sigmoid 激活单元。

```
Actor.add(Dense(64, input_dim=self.state_size, activation="relu", kernel_initializer="normal"))
Actor.add(Dense(64, activation="relu", kernel_initializer="normal"))
Actor.add(Dense(self.action_size, activation="sigmoid"))

```

这也可以通过编写自定义高斯激活单元，并使其与 Tensorflow 保持一致来实现。

我们在本节中读到的演员评价算法的变体完全是政策演员评价版本。我们将在接下来的几节中简要介绍一些不符合政策的演员评论算法(ACER)。

**最接近的策略优化**

现在我们已经探讨了演员评论家算法，我们可以了解 PPO 算法。这是基于 A3C 然而，还是有根本的区别。在该策略梯度 on-policy 算法中，基于两个策略的回报在它们之间进行比较。在进入 PPO 的细节之前，我们先来了解一下信赖域策略优化(TRPO)算法，这是 PPO 的起点。

**信任区域策略优化** **:** 为了提高 actor critic 变量的稳定性，设计了 TRPO，它通过避免基于 Kullback-Leiblar (KL)散度的某些约束来避免频繁的策略更新。KL 散度提供了变量 x 上的两个概率分布 p 和 q 之间的散度关系，由下式给出:

d<sub>KL</sub>= p(x)日志(p(x)/q(x))

一般来说，TRPO 是一种少数化-多数化(MM)算法，它提供了用于比较的策略的预期回报的上限和下限，并使用 KL 散度来设定它们的阈值。TRPO 通过与∈<sub>θ</sub>J(θ)定义的运行梯度进行比较，试图找到一个最优的策略升级规则，该运行梯度称为代理梯度，对应的策略梯度方程作为代理目标函数。TRPO 算法可以被分析为具有以下条件的优化问题:

*   在新老策略的分布上增加 KL 散度约束，问题是最大化∈<sub>θ</sub>J(θ):

    最大值<sub>j(θ)</sub>

    服从 d<sub>KL</sub>(π<sub>θold</sub>(a | s)/π<sub>θ</sub>(a | s))<∂

*   KL 散度梯度目标函数的正则化；

    max<sub>= j(π<sub>)-c<sub>【KL】</sub>(π<sub>【老】</sub>(a | s)/π</sub></sub>

这些方程取决于 c 和∂.的参数现在我们在这种情况下应用抽样，这意味着找到每一步的优势。现在，如果我们参考政策梯度定理和梯度函数，由以下等式给出:

√t0〖t1〗j(ο)= e[√T2〗t〖T3〗t〖t5〗t〖T6〖T7〗ln(π〖t8〖T9〖a | s〗

我们将重新构造策略π <sub>θ</sub> (a|s)部分，以包含旧策略和当前策略的比率，并且代替ψ<sub>t</sub>，我们将使用优势函数 A <sup>π</sup> (s <sub>t</sub> ，a <sub>t</sub> )。用于策略更新的新梯度目标函数变成:

∈<sub>θ</sub>J(θ)= E[∈<sub>t</sub>A<sup>π</sup>(s<sub>t</sub>，a <sub>t</sub> )，∈<sub>θ</sub>ln(π<sub>θ</sub>(A | s)/π<sub>θ老</sub>(A | s))]，

这可以简化为:

∈<sub>θ</sub>J(θ)= E[A<sup>π</sup>(s<sub>t</sub>，A<sub>t</sub>)(π<sub>θ</sub>(A | s)/π<sub>θold</sub>(A | s))]

现在最大化问题可以简化为:

max<sub>θ</sub>E[∑<sub>t</sub>A<sup>π</sup>(s<sub>t</sub>，a <sub>t</sub> )，∈<sub>θ</sub>ln(π<sub>θ</sub>(A | s)/π<sub>θold</sub>(A | s))]

服从 d<sub>KL</sub>(π<sub>θold</sub>(a | s)/π<sub>θ</sub>(a | s))<∂

这是 TRPO 算法背后的主要概念。有一些方法，比如自然梯度，可以使用 FIM 来加快收敛速度。策略比率用于计算一组参数θ的重要性抽样。现在我们将尝试使用这个概念来构建一个 TRPO 代理。正如我们所见，TRPO 是对演员评论方法的优化。因此，我们将理解 TRPO 的两种变体——一种是通过 OpenAI 基线，另一种是通过更新 actor critic 的相同代码库。

打开 TRPO.ipynb 笔记本。首先，我们将使用 OpenAI 的基线 TRPO 模块。为此，我们将导入我们需要的库模块。

```
import gym
from stable_baselines.common.policies import MlpPolicy
from stable_baselines import TRPO
import matplotlib.pyplot as plt
from IPython import display as ipythondisplay
from pyvirtualdisplay import Display
display = Display(visible=0, size=(400, 300))
display.start()

```

这与前一种情况相同，只是在这种情况下，我们将使用 TRPO 算法。在这种情况下，我们将使用钟摆 v0 作为我们的环境。钟摆是一个经典的 RL 环境，类似于 CartPole，其中有一个从随机位置开始的倒立摆。目标是将这个钟摆向上摆动，使其保持直立——这是一个经典的控制问题。首先，我们使用 Gym 加载环境，然后使用 MLP(密集)作为我们的神经网络架构加载 TRPO 模型。

```
env = gym.make('Pendulum-v0')
model = TRPO(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=25000)
model.save("trpo_pendulum")
del model
model = TRPO.load("trpo_pendulum")

```

在 while 循环中，我们运行算法 25000 次迭代，并观察状态、奖励和动作空间。

```
while True:
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    screen = env.render(mode='rgb_array')
    plt.imshow(screen)
    ipythondisplay.clear_output(wait=True)
    ipythondisplay.display(plt.gcf())

```

在运行代码段时，我们可以想象钟摆试图直立，如图 [5-22](#Fig22) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig22_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig22_HTML.jpg)

图 5-22

使用 TRPO 算法的钟摆环境

现在我们要修改我们之前的代码库，这是演员评论家的 A2C 版本。大部分代码段将保持不变，除了在 TRPOAgent 类的“build_actor_model”方法中，我们将添加一个新的损失函数——trpo _ loss。该损失函数计算预测结果与先前结果的 KL 偏差，并且具有管理约束的熵因子。通过取 KL 散度分布的负对数似然来测量损失，并与 Adam 优化器一起用于 actor 神经网络。

```
def trpo_loss(y_true, y_pred):
      entropy=2e-5
      old_log= k.sum(y_true)
      print(old_log)
      pred_log=k.sum(y_pred)
      print(pred_log)
      kl_divergence= k.sum(old_log* k.log(old_log/pred_log))
      prob=1e-2
      loss=-k.mean(kl_divergence +
      entropy*(-(prob*k.log(prob+1e-10))))
      return loss
Actor.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=trpo_loss)

```

因此，我们现在可以在 A2C 变体上使用 TRPO。我们也可以在 A3C 变体中加入这个损失函数，使 TRPO-A3C 代理变体。我们还可以利用超参数来估计我们在特定政策上的价值。

**最近策略优化(PPO)** **:** 使用 TRPO 概念，开发 PPO 以增加简单性和易用性。PPO 算法依赖于替代梯度目标函数的剪裁，同时保持值估计的性能。让我们看看 TRPO 以前的目标方程，它是:

∈<sub>θ</sub>J(θ)= E[A<sup>π</sup>(s<sub>t</sub>，A<sub>t</sub>)(π<sub>θ</sub>(A | s)/π<sub>θold</sub>(A | s))]

TRPO 不限制保单之间的距离π <sub>θ</sub> (a|s)，π <sub>θold</sub> (a|s)。而且在很多情况下，这可能会导致训练不稳定。PPO 使用由[(1+ ε)，(1- ε)]给出的限幅阈值，其中ε是超参数。因此，基于 PPO 的梯度函数的有效公式定义为:

√t0〖t1〗j(θ)= e[min(r)a〖T2〗T3〗(s〖T4〗t〖t5〗，a〖T6〗t〖T7〗，剪辑(r)，1+ ε，1-ε〗a〖t

其中 r(θ)=π<sub>θ</sub>(a | s)/π<sub>θold</sub>(a | s)和 clip(r(θ)，1+ ε，1- ε)将比值 r(θ)剪辑为不大于(1+ ε)且不小于(1- ε)。

这是 PPO 算法背后的概念。PPO 算法还有其他变体，如 PPO-Penalty，它惩罚 KL 发散，类似于 TRPO，只是在 TRPO 中有一个硬约束。这种剪裁以下面的方式控制新旧策略之间的差异，这由下面的优点控制。

*   如果优势 A <sup>π</sup> (s <sub>t</sub> ，a <sub>t</sub> )为正，那么如果行动变得更有可能，目标函数就会增加——即π <sub>θ</sub> (a|s)增加。然而，由于最小化项，目标可以增加多少是有限制的。一旦比率π<sub>θ</sub>(a | s)>(1+ε)π<sub>θold</sub>(a | s))，最小化操作就开始控制，从而避免新策略远离旧策略。

*   如果优势 A <sup>π</sup> (s <sub>t</sub> ，a <sub>t</sub> )为负，那么行动可能性变小，目标函数就会降低——也就是π <sub>θ</sub> (a|s)会降低。然而，由于最大化项，目标可以减少多少是有限制的。一旦比例π<sub>θ</sub>(a | s)<(1-ε)π<sub>θold</sub>(a | s))，最大化就会踢进来，避免新政策远离旧政策。

现在让我们使用来自 Open AI 的基线实现一个 PPO 代理，然后我们将探索如何修改 TRPO 源(损失函数)使其成为 PPO。

打开 PPO-Baselines.ipynb 笔记本。我们将像以前一样使用稳定的基线，并将导入所有与 PPO 相关的库和神经网络。由于我们将使用剪裁的 PPO 版本，我们必须从基线和“MlpPolicy”中导入 PPO2，这意味着我们将使用密集网络进行培训。

```
import gym
from stable_baselines.common.policies import MlpPolicy
from stable_baselines.common import make_vec_env
from stable_baselines import PPO2

```

在下一步中，我们将设置环境，在本例中，我们将使用 CartPole 环境。我们也将加载模型。这是使用下面几行代码完成的。

```
env = make_vec_env('CartPole-v1', n_envs=4)
model = PPO2(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=2000)
model.save("ppo2_cartpole")
model = PPO2.load("ppo2_cartpole")

```

在下一步中，我们在类似于前面的 CartPole 环境的四个实例上训练我们的 PPO 代理进行 2000 次迭代。

```
obs = env.reset()
while True:
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)

```

一旦完成，我们将再次想象四个小车的环境，如图 [5-23](#Fig23) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig23_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig23_HTML.jpg)

图 5-23

PPO 政策下的钢管环境培训

现在我们将实现 PPO 惩罚版本和 PPO 剪辑版本。

打开 PPO-A2C.ipynb 笔记本，因为我们将在 A2C 基线上运行 PPO，类似于 TRPO。

*   **PPO-** **罚分** **:** 这需要对 TRPO 的分布的 KL 散度进行罚分，并移除硬约束。唯一需要更改的是 PPOAgent 类中“build_actor_model”方法内的损失函数。“trpo_ppo_penalty_loss”方法使用预测策略和旧策略。然后，它发现 KL 发散类似于 TRPO 的情况。之后，它计算优势估计，并提供由比率 r(θ)给出的新旧策略的比率的最小化界限。这是 PPO 算法的一个惩罚剪辑变体。

    ```
    def trpo_ppo_penalty_loss(y_true, y_pred):
          entropy=2e-5
          clip_loss=0.2
          old_log= k.sum(y_true)
          print(old_log)
          pred_log=k.sum(y_pred)
          print(pred_log)
          r=pred_log/(old_log + 1e-9)
          kl_divergence= k.sum(old_log* k.log(old_log/pred_log))
          advantage=kl_divergence
          p1=r*advantage
          p2=k.clip(r, min_value=
          1-clip_loss, max_value=1+clip_loss)*advantage
          prob=1e-2
          loss=-k.mean(k.minimum(p1, p2) +
          entropy*(-(prob*k.log(prob+1e-10))))
          return loss

    Actor.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=trpo_ppo_penalty_loss)

    ```

*   **PPO-** **削波** **:** 这是标准的 PPO 削波策略。这是通过“trpo_ppo_clip_loss”方法实现的，并通过 Adam 优化器传递。这里，优势是通过从先前策略的估计值中减去 Q 值来计算的。然后，我们使用限幅方法来限幅受“ε”阈值限制的比值 r(θ)。

    ```
    def trpo_ppo_clip_loss(y_true, y_pred):
          entropy=2e-5
          clip_loss=0.2
          old_log= k.sum(y_true)
          print(old_log)
          pred_log=k.sum(y_pred)
          print(pred_log)
          r=pred_log/(old_log + 1e-9)
          advantage=pred_log-old_log
          p1=r*advantage
          p2=k.clip(r, min_value=
          1-clip_loss, max_value=1+clip_loss)*advantage
          prob=1e-2
          loss=-k.mean(k.minimum(p1, p2) +
          entropy*(-(prob*k.log(prob+1e-10))))
          return loss

    Actor.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=trpo_ppo_clip_loss)

    ```

因此，我们已经看到了 PPO 算法，并理解了该算法背后的基本概念，这是 ML 代理的默认训练算法。在接下来的部分中，当我们将使用 ML 代理构建更新的代理时，我们可以像前面提到的那样创建自己的 PPO 模型，或者也可以使用基线模型。在这种情况下，重要的是要提到，我们也可以使用卷积-2D 神经网络来代替密集网络，并且一些实现已经提供了来自健身房环境的 Pong Atari 游戏(2D)。这是在图像处理的帮助下控制的，其中像素被传递到卷积-2D 神经网络，然后应用策略梯度算法，如图 [5-24](#Fig24) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig24_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig24_HTML.jpg)

图 5-24

基于卷积-2D 神经网络的策略梯度 Pong Atari 博弈

Unity ML 代理有自己的 PPO 模型，我们一直用它来培训我们的代理。在下一节中，我们将探讨 deep RL 中的非策略算法，包括 DQN、DDQN、D3QN、SAC、ACER 和其他算法。

### 非策略算法

这类算法依赖于包含过去估计的缓冲区，并相应地做出决定。在这种情况下，过去数据的采样是在贝尔曼方程的帮助下完成的，利用该方程可以训练 Q 函数来满足代理和环境之间的相互作用。有一个经验重放的概念，它涉及过去状态的大量取样和优化贝尔曼函数的价值估计。正如我们提到的，Q-learning 是一种非策略技术。在深度学习环境中，我们有 DQN、DDQN、D3QN、DDPG、TD3、宏基和 SAC 作为非策略算法。在这一节中，我们将广泛研究 DQN 的变体。

**深度 Q-网络** **:** 这是 Q-learning 的一个深度 RL 变种。这使得 Q 学习在连续和离散空间上更加稳定，因为在这种情况下，Q 函数通过非线性激活来近似。非策略 DQN 算法在更新策略之前计算所有可能状态的值，而策略方法通过与目标函数的梯度进行比较来更新策略。DQN 涉及的两个基本概念是:

*   体验回放:这意味着对过去的状态、价值观和行为进行取样。

*   **训练更新的目标网络:**这涉及应用深度学习层，通过计算值估计来更新策略。

这种情况下的损失或目标函数定义为:

Y(s，a，r，s**`**)= r+y max<sub>a</sub>Q<sub>θ</sub>(s**`**，a **`** )

L(θ) = E <sub>θ</sub> [(Y(s，a，r，s**`**)-Q<sub>θ【T5(s，a)) <sup>2</sup> ，</sub>

其中 y(γ)是勘探开发系数。根据网络的类型，可以使用不同的神经网络。对于图像特定的 DQN，可以使用卷积神经网络。

在下一个案例中，我们将研究 DQN 的实现，我们在第 [1](1.html) 章中已经瞥见了。打开 CartPole-Rendering.ipynb 笔记本。像往常一样，我们必须从 keras 导入所有必要的库和模块来构建密集模型。我们还将导入 ipythondisplay for visualization algon 和 Tensorboard，用于训练阶段的可视化。我们还将有一个 deque 数据结构来存储我们的状态、动作和每个训练阶段的 rand ewards。

```
import gym
from gym import logger as gymlogger
from gym.wrappers import Monitor
gymlogger.set_level(40)
from keras.callbacks import TensorBoard
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
import numpy as np
import random
import pandas as pd
import math
import matplotlib.pyplot as plt
from collections import deque
import glob
import io
import base64
from IPython.display import HTML

from IPython import display as ipythondisplay
%load_ext tensorboard
%tensorboard --logdir logs

```

在 DeepQLearning 类中的“__init__”方法中，我们将包含构建 Q 网络所需的所有超参数。这包括探索-利用因子、学习因子、ε、α衰减、batch_size、重放记忆(deque)和其他参数。

```
    self.replay_memory= deque(maxlen=1000)
    self.env=gym.make('CartPole-v0')
    self.gamma=gamma
    self.epsilon=epsilon
    self.epsilon_min=epsilon_min
    self.log_epsilon=log_epsilon
    self.alpha=alpha
    self.alpha_decay=alpha_decay
    self.no_episodes=no_episodes
    self.no_complete=no_complete
    self.batch_size=batch_size
    self.quiet=quiet
    if env_steps is not None:
      self.env._max_episode_steps=env_steps

```

在下一步中，我们用 Keras 建立具有密集层的序列模型。致密层有 48 个单位，每个单位都有“tanh”激活。密集网络的最后一层有两个单元，表示在“sigmoid”激活下，磁极可以向左或向右移动。

```
self.model=Sequential()
    self.model.add(Dense(48, input_dim=4, activation="tanh"))
    self.model.add(Dense(48, activation="tanh"))
    self.model.add(Dense(2, activation="sigmoid"))
     self.model.compile(loss='mse', optimizer=Adam
    (lr=self.alpha,
    decay=self.alpha_decay))

```

下一步，我们有“记住”方法，它在一个容器中包含状态、奖励和动作，用于存储和采样。

```
def remember(self, state, action, reward, next_state, done):
    self.replay_memory.append((state, action, reward,
    next_state, done))

```

“choose_step”方法用于在动作的估计值大于由ε表示的某个阈值时选择特定动作。

```
def choose_step(self, state, epsilon):
    return self.env.action_space.sample() if(np.random.random()<=epsilon)
   else np.argmax(self.model.predict(state))

```

“预处理”方法用于转换(展平)状态，以便将其输入到密集网络中。

```
def preprocess_state(self, state):
    return np.reshape(state, [1, 4])

```

“get_epsilon”和“decay_epsilon”是修改 epsilon 衰减的方法，在训练步骤中用于体验重放。

```
def get_epsilon(self, t):
    return max(self.epsilon_min, min(self.epsilon,
    1.0-math.log((t+1)*self.log_epsilon)))

  def decay_epsilon(self):
    if self.epsilon>self.epsilon_min:
      self.epsilon*=self.log_epsilon

```

“replay”方法用于从内存缓冲区中以采样分布的形式收集状态、动作和奖励。然后，它使用训练好的模型来预测下一个可能的动作，并通过使用ε和γ因子来更新动作的值(奖励)。这就是政策外逻辑发挥作用的地方。体验回放允许算法从队列(内存)中采样信息，然后让模型预测与价值估计或奖励中的最高回报相对应的行动。

```
def replay(self, batch_size):
    x_batch, y_batch=[],[]
    minibatch=random.sample(
     self.replay_memory, min(len(self.replay_memory), batch_size))
    for state, action, reward, next_state, done in minibatch:
      y_target=self.model.predict(state)
      y_target[0][action]=reward if done
     else reward+self.gamma*(np.max(self.model.predict(next_state)[0]))
      x_batch.append(state[0])
      y_batch.append(y_target[0])

```

在下一个阶段，我们有“运行”方法，在这里我们控制算法的逻辑。在这种情况下，我们加载 CartPole 环境，然后在内存中记录状态、动作和奖励，并对状态进行预处理，使输入适合密集网络。deque 的内存限制为 100，我们训练模型，直到算法的平均分数超过某个阈值。每期培训有 100 集内部收集奖励和计算平均分数。

```
def run(self):
    print(self.env.action_space)
    total_scores=deque(maxlen=100)
    for i in range(self.no_episodes):
      state= self.preprocess_state(self.env.reset())
      done=False
      j=0
      while not done:
        action= self.choose_step(state, self.get_epsilon(i))
        next_state, reward, done,_=self.env.step(action)
        next_state=self.preprocess_state(next_state)
        self.remember(state, action, reward, next_state, done)
        state=next_state
        j+=1
      total_scores.append(j)
      mean_score=np.mean(total_scores)
      if mean_score >=self.no_complete and i>=100:
        if not self.quiet:
          print("Ran {} episodes.Solving after
          {} trainings".format(i, i-100))
          return i-100
      if i%100==0 and not self.quiet:
        print("Episode Completed {}.
        Mean score {}".format(i, mean_score))

      self.replay(self.batch_size)
    if not self.quiet:
      print("Not solved after {} episodes", format(i))
    return i

```

在运行该模型时，我们将看到 DQN 算法为 CartPole 环境执行价值估计非策略控制。现在，我们可以理解并联系我们在第 [1](1.html) 章的介绍部分所取得的成果。张量板可视化如图 [5-25](#Fig25) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig25_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig25_HTML.jpg)

图 5-25

利用 DQN 进行钢管环境训练

**双深度 Q-网络** **:** 在很多情况下，DQN 会导致价值回报的高估，因为我们使用相同的策略 Q <sub>θ</sub> (s，a)来预测最佳可能行动，并且还使用相同的策略 Q <sub>θ</sub> (s **`** ，a **`** )来估计回报/价值。DDQN 使用两个神经网络来解耦动作选择和价值估计阶段。有效损失或目标函数涉及两个 Q 网络，Q <sub>θ1</sub> (s，a)和 Q <sub>θ2</sub> (s，a)，如下所示:

Y <sub>1</sub> (s，a，r，s**`**)= r+y max<sub>a</sub>Q<sub>θ1</sub>(s**`**，arg max<sub>a</sub>Q<sub>θ2</sub>(s**`**，a **`** ))

Y <sub>2</sub> (s，a，r，s**`**)= r+y max<sub>a</sub>Q<sub>θ2</sub>(s**`**，arg max<sub>a</sub>Q<sub>θ1</sub>(s**`**，a **`** ))

这有助于提高训练的稳定性，因为抽样策略中涉及两个策略。第一 Q 网络(原始 Q 网络)使用来自第二 Q 网络(目标 Q 网络)的最大值估计，第二 Q 网络(目标 Q 网络)根据第一网络的返回更新其策略。

我们将探索这个网络，为此我们必须打开双深 Q 网络。ipynb 笔记本。我们将在之前的 DQN 的基础上进行构建，并在 DoubleDeepQLearning 类中的“__init__”和“replay”方法内进行更改。在“__init__”方法中，我们有两个神经网络，即“模型”和“目标模型”，它们彼此相似(就像在 DQN)，具有相同的参数。但是，这可以根据需要进行更改。

```
self.model=Sequential()
    self.model.add(Dense(48, input_dim=4, activation="tanh"))
    self.model.add(Dense(48, activation="tanh"))
    self.model.add(Dense(2, activation="sigmoid"))
    self.model.compile(loss='mse', optimizer=
    Adam(lr=self.alpha,
    decay=self.alpha_decay))
    self.target_model=Sequential()
    self.target_model.add(Dense(48, input_dim=4,
    activation="tanh"))
    self.target_model.add(Dense(48, activation="tanh"))
    self.target_model.add(Dense(2, activation="sigmoid"))
    self.target_model.compile(loss='mse', optimizer=
    Adam(lr=self.alpha,
    decay=self.alpha_decay))

```

下一个变化是在“重放”方法中，我们必须将来自队列的输入(状态、动作、奖励)分配给两个网络。我们指定目标网络的预测来代替原始网络，并将其传递给模型。程序的注释中提到了 DQN 和 DDQN 之间的对比。

```
    x_batch, y_batch=[],[]
    minibatch=random.sample(
   self.replay_memory, min(len(self.replay_memory), batch_size))
    for state, action, reward, next_state, done in minibatch:
      y_target=self.model.predict(state)
      y_next_target=self.model.predict(next_state)
      y_next_val=self.target_model.predict(next_state)
      #DQN Update
      #y_target[0][action]=reward if done
      else reward +self.gamma*(np.max
     (self.model.predict(next_state)[0]))
      #DDQN Update
      y_next_target[0][action]=reward if done
      else reward+self.gamma*(np.max(y_next_val[0]))
      x_batch.append(state[0])
      #DQN
      #y_batch.append(y_target[0])
      #DDQN
      y_batch.append(y_next_target[0])

```

这些是创建网络 DDQN 所需的唯一更改。在运行代码段时，我们可以看到横竿正在被训练，如图 [5-26](#Fig26) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig26_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig26_HTML.jpg)

图 5-26

正在接受 DDQN 培训的钢管环境

**决斗双 Q 网络** **:** 作为输出层的 D2QN 网络架构上的一个增强中的决斗网络被分成两个主要部分——价值估计 V <sup>π</sup> (s <sub>t</sub> )和优势估计 A <sup>π</sup> (s <sub>t</sub> ，a <sub>t</sub> )。它们与 Q 策略之间的关系由以下关系式给出(我们在策略梯度中研究过):

a<sup>(s<sub>，a<sub>= q<sup>(s<sub>)，a<sub>【t】</sub></sub></sup></sub></sub></sup>

实际上，在这种情况下，估计的优势总计为 0 (∑ A <sup>π</sup> (s <sub>t</sub> ，a <sub>t</sub> )π(a|s) = 0)，我们必须减去优势的平均值

[1/|A <sup>π</sup> (s <sub>t</sub> ，A<sub>t</sub>)|]∑A<sup>π</sup>(s<sub>t</sub>，a <sub>t</sub> )从值估计 V <sup>π</sup> (s <sub>t</sub> )得到 Q 值 Q <sup>π</sup> (s <sub>t</sub> ，t 这由下面的等式表示，这是决斗 DQN 的驱动力。

Q <sup>π</sup> (s <sub>t</sub> ，A<sub>t</sub>)= V<sup>π</sup>(s<sub>t</sub>)+(A<sup>π</sup>(s<sub>t</sub>，A<sub>t</sub>)–[1/| A<sup>π</sup>(s<sub>t</sub>，A<sub>t</sub>|])

为了看到这一点，让我们打开决斗双 DQN.ipynb 笔记本。我们维护 DDQN 的类似代码库，在“__init__”方法的“model”部分包含一个优势损失(名为“advantage_loss”)。在“优势损失”方法中，我们计算 Q 值相对于价值估计的优势。然后，我们计算优势的平均值，并将其相应地拟合到损失估计值中。所有这些都是在 Keras 中使用后端模块完成的。然后我们将它传递给“model.compile”方法，Adam 作为我们的优化器。

```
def advantage_loss(y_true, y_pred):
      q_val=y_pred
      v_val=y_true
      advantage=(q_val-v_val)
      adv_mean=k.mean(advantage)
      adv_factor=1/adv_mean*(k.sum(advantage))
      loss=- adv_factor*self.epsilon
      return loss
self.model.compile(loss=advantage_loss, optimizer=Adam(lr=self.alpha, decay=self.alpha_decay))

```

代码段的其余部分是相同的，我们现在在 CartPole 环境中对其进行训练，以了解其性能，如图 [5-27](#Fig27) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig27_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig27_HTML.jpg)

图 5-27

D3QN 中正在训练的钢柱环境

**演员评论家体验回放** **:** 这是我们之前看到的 A3C 算法的一个非策略变体。它包括先前状态的采样以及追溯 Q 值估计。在这方面适用的三个主要条件包括:

*   回扫 Q 值估计

*   使用偏差校正截断权重

*   应用 KL 散度边界 TRPO

追溯部分是一种偏离策略的采样技术，它使用误差项向正确的估计值策略移动。这由公式表示:

q<sup>(s<sub>，a<sub>= q<sup>(s<sub>，a<sub>【t】</sub></sub></sup></sub></sub></sup>

在哪里，

q<sup>(s<sub>，a<sub>=α】<sub>，</sub></sub></sub></sup>

其中，Q <sup>π</sup> (s <sub>t</sub> ，a <sub>t</sub> )是增量误差更新，称为时间差(TD)误差。增量更新然后被公式化为两个策略的比率((π<sub>θ</sub>(a | s)/π<sub>θold</sub>(a | s)))，带有来自先前值估计的误差项∂ <sub>t</sub> 。简化版看起来像这样:

⇼qπret(ST，at)= ytπ((πθ(a | s)/πθold(a|s))∂t)

这就是非政策抽样与通用行动者评价/TRPO 方法的政策梯度更新一起使用的地方。ACER 被认为是不符合政策的，这主要是因为从以前的值对误差估计进行了增量更新。

宏碁还有一个权重截断规则，用来减少政策梯度的方差。通常在 AC 算法中，优势估计(GAE)用以下等式表示:

a<sup>(s<sub>，a<sub>= q<sup>(s<sub>)，a<sub>【t】</sub></sub></sup></sub></sub></sup>

现在，在 ACER 中追溯，政策梯度的优势估计变为:

a<sup>(s<sub>，a<sub>= q<sup><sub>【ret】</sub>(s</sup></sub></sub></sup>

因此，政策梯度估计变为:

√t0〖t1〗j(ο)= e[√q〖T2〗〗T3〖T4〗ret〖t5〗ret〖s，a〗v〖T6〖T7〗(s〖t8〗t〗T9〗

现在，我们将构建 ACER 算法的基线版本。打开 ACER-Baselines.ipynb 笔记本。我们将像以前一样从稳定的基线导入库和模块。

```
import gym
from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy
from stable_baselines.common import make_vec_env
from stable_baselines import ACER
import matplotlib.pyplot as plt
from IPython import display as ipythondisplay
from pyvirtualdisplay import Display
display = Display(visible=0, size=(400, 300))
display.start()

```

在这种情况下，我们将实例化 CartPole 环境的四个实例，其中 ACER 代理将使用 MLP(密集)网络。

```
env = make_vec_env('CartPole-v1', n_envs=4)
model = ACER(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=25000)
model.save("acer_cartpole")
del model
model = ACER.load("acer_cartpole")

```

然后我们有 while 循环，它控制 ACER 算法并运行 25000 次迭代。它收集了每个训练时期的奖励、状态和动作，并在屏幕上显示四个侧翻的动作。

```
while True:
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    screen = env.render(mode='rgb_array')
    plt.imshow(screen)
    ipythondisplay.clear_output(wait=True)
    ipythondisplay.display(plt.gcf())

```

在运行这个模型时，我们可以看到侧手翻。我们可以将这个结果与我们之前读到的演员评论家模型的其他变体进行比较，如图 [5-28](#Fig28) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig28_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig28_HTML.jpg)

图 5-28

宏基钢管舞训练四例

**软演员评论家** **:** 这是另一种非策略算法，它依赖于基于熵的建模。它是一种随机行动者批评策略，使用熵正则化来最大化期望收益和熵之间的权衡。具有密度函数的概率变量的熵可以定义为:

H(P) = E [-log(P(x))]

在熵正则化形式中，代理人在每个时间步获得与特定时间步的策略熵成比例的额外奖励。这表示为:

π<sub>θ</sub>(a | s)= arg max<sub>π</sub>E[∑<sub>t</sub>y<sup>t</sup>(R(s<sub>t</sub>，a <sub>t</sub> ，s <sub>t+1</sub> ) + αH(π <sub>θ</sub> (。|s <sub>t</sub> )))]，

其中 R 是回报，是权衡系数。利用熵正则化，值估计和 Q 策略的关系如下:

v<sup>(s<sub>)= e【q】<sup>(s<sub>，a<sub>)+h(π|s <sub>t</sub> )</sub></sub></sup></sub></sup>

在 ML 代理中，SAC 是一种非策略算法，我们将从基线以及通过修改 TRPO-A2C/PPO-A2C 计划来研究它的实现。

打开 SAC- Baselines.ipynb 笔记本。这是没有并行化的 SAC 算法的实现。首先，我们将从基线导入必要的库、模块和网络。

```
import gym
import numpy as np

from stable_baselines.sac.policies import MlpPolicy
from stable_baselines import SAC
import matplotlib.pyplot as plt
from IPython import display as ipythondisplay
from pyvirtualdisplay import Display
display = Display(visible=0, size=(400, 300))
display.start()

```

然后，我们加载模型并声明它使用 MLP 或密集网络，尽管我们可以指定包括 LSTM 在内的其他类型的策略。我们为我们的算法创建了钟摆环境，并将其载入。

```
env = gym.make('Pendulum-v0')
model = SAC(MlpPolicy, env, verbose=1)
model.learn(total_timesteps=50000, log_interval=10)
model.save("sac_pendulum")
del model
model = SAC.load("sac_pendulum")

```

然后，我们使用 MLP 策略和 SAC 算法运行钟摆环境 50000 次迭代。在每一步，我们记录训练的参数，包括状态、行动和奖励。

```
while True:
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    screen = env.render(mode='rgb_array')
    plt.imshow(screen)
    ipythondisplay.clear_output(wait=True)
    ipythondisplay.display(plt.gcf())

```

经过大量的训练后，我们可以看到钟摆以直立的方式保持平衡，如图 [5-29](#Fig29) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig29_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig29_HTML.jpg)

图 5-29

用 SAC 进行钟摆环境训练

现在，我们还可以通过引入熵因子来修改 A2C 变体的源代码，使其成为一个软演员评论模块。为此，我们改变“构建 _ 行动者 _ 模型”方法中的损失函数。在这种情况下，我们将使用我们的 SAC 实现来训练“MountainCar”环境。在“sac_loss”方法中，我们使用熵方程计算前一步的回报以及当前步骤的熵。在应用策略更新公式之后，其涉及奖励和熵正则化形式的添加，我们可以计算负的对数似然损失。然后我们返回这个损失函数，供 Adam 优化器使用。

```
def sac_loss(y_true, y_pred):
      entropy=2e-5
      pred_reward= k.sum(y_true)
      entropy_val=k.sum(- (k.log(y_pred)))
      expectation = pred_reward + entropy_val
      prob=1e-2
      loss=-k.mean(expectation +
       entropy*(-(prob*k.log(prob+1e-10))))
      return loss
Actor.compile(optimizer=Adam(learning_rate=self.learning_rate), loss=sac_loss)

```

在训练这个算法时，我们可以看到登山车试图前往屏幕右侧的山顶。这样来回移动几次，以获得必要的动量，如图 [5-30](#Fig30) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig30_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig30_HTML.jpg)

图 5-30

带定制 SAC 的登山车训练

还有其他几种非策略算法，如深度确定性策略梯度(DDPG)及其变体，以及无模型深度 RL 算法，我们将在下一章重点讨论。

### 无模型学习:模仿学习——行为克隆

在 ML 代理的上下文中，还有一个关于模仿学习的模块，它属于行为克隆算法。具体来说，我们将探索生成对抗模仿学习(GAIL)，它使用一般对抗网络(GAN)，这是一种不同于密集或卷积网络的神经网络形式。让我们简单讨论一下盖尔。GAIL 属于无模型 RL 范式，它也使用熵，并且它从专家演示中学习成本/目标函数。GAIL 等模仿学习算法用于逆 RL。但在此之前，让我们先了解一下 GANs。

**一般对抗性网络:**这种神经网络有两个组件:生成器和鉴别器。生成器的任务是准备原始数据的虚假副本，以欺骗鉴别器，使其认为它是真实的。而鉴别器的任务是正确地识别来自发生器的错误数据。这是借助熵正则化来完成的。数学上，如果 p(x)定义样本 x 上的数据分布，并且 p(z)是来自发生器 G 的数据分布，样本为 z，则 GAN 是关于发生器 G 和鉴别器 D 的最大化-最小化函数:

最小 <sub>G</sub> 最大 <sub>D</sub> V(D，G)

V(D，G)= E<sub>x-p(x)</sub>【log(D(x))】+ E<sub>z-p(z)</sub>【log(1-D(G(z))】】

等式的第一项是样本 x 的熵分布，即原始数据。鉴别器试图将其最大化为 1。第二项是从样本 z 生成的数据，鉴别器的任务是将这部分数据减少到 0，因为它包含合成数据。生成器的任务是最大化等式的第二部分。这就是 GANs 的工作方式，它是一个介于生成器和鉴别器之间的竞争性神经网络模型。

GAN 的架构可以形象化，如图 [5-31](#Fig31) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig31_HTML.png](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig31_HTML.png)

图 5-31

用于分类的 GAN 模型架构

**一般对抗性模仿学习** **:** GAIL 是行为克隆的一个变种，它使用 GAN 来学习一个代价函数。鉴别器试图从生成的轨迹中分离专家轨迹。轨迹包括梯度朝向价值估计的适当回报的方向。最初，我们训练一个 SAC 模型作为专家模型，然后使用它作为真实的采样数据。然后，GAIL 构建自己的生成器和鉴别器网络来生成合成轨迹，并将它们与 SAC 算法生成的专家轨迹进行比较。我们将从基线开始使用 GAIL 算法。打开 GAIL-Baselines.ipynb 笔记本。在第一种情况下，我们首先运行钟摆环境，并用前面提到的 SAC 算法对其进行训练。SAC 根据 MLP 策略对其进行训练，并生成专家轨迹，用“generate_expert_traj”方法表示。

```
from stable_baselines import SAC
from stable_baselines.gail import generate_expert_traj
# Generate expert trajectories (train expert)
model = SAC('MlpPolicy', 'Pendulum-v0', verbose=1)
generate_expert_traj(model, 'expert_pendulum', n_timesteps=6000, n_episodes=10)

```

然后我们有盖尔代码段。在我们将由 SAC 策略产生的轨迹保存在一个数据集(由“ExpertDataSet”方法提供)中之后，我们然后将这个数据集用于 GAIL 来生成生成器数据和鉴别器。GAIL 算法然后在 MLP 网络上训练 1000 个时间步，其中鉴别器试图优化策略，使得值估计与 SAC 策略产生的值一致。另一方面，生成器试图修改和综合生成难以与真实 SAC 轨迹分离的相似轨迹。

```
dataset = ExpertDataset(expert_path='expert_pendulum.npz', traj_limitation=10, verbose=1)
model = GAIL('MlpPolicy', 'Pendulum-v0', dataset, verbose=1)
model.learn(total_timesteps=1000)
model.save("gail_pendulum")
del model
model = GAIL.load("gail_pendulum")
env = gym.make('Pendulum-v0')
obs = env.reset()
while True:
  action, _states = model.predict(obs)
  obs, rewards, dones, info = env.step(action)
  env.render()

```

在 GAIL 的训练中，我们可以看到不同的参数和训练的分数，如图 [5-32](#Fig32) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig32_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig32_HTML.jpg)

图 5-32

使用从 SAC 生成的轨迹训练 GAIL

我们已经在策略上和策略外模型方面介绍了深度 RL 中的大多数主要算法，包括无模型 RL (GAIL)。在下一个上下文中，我们将为 Puppo 创建一个深度学习模型，该模型将包括关节运动，并将使用 ppo 算法进行训练。

## 为 Puppo 构建一个近似策略优化代理

我们现在对为什么 PPO 算法是深度 RL 策略的最广泛使用的形式有了相当的理解，这是因为它的鲁棒性和剪裁防止了 TRPO 策略梯度的不稳定性。我们将为 Puppo 构建一个 PPO 代理，它的任务是在环境中找到棍子。但是，在这种情况下，它将使用受 PPO 策略约束的关节向量进行移动。为此，我们将使用普普统一场景。在这种情况下，关节系统用于通过驱动和施加力来训练 Puppo 到达目标。应用于关节的奖励函数可以被公式化为:

r(θ) = v.d X (- θ) + 1，

其中奖励由沿 y 轴施加的角向力θ参数化。v 表示 Puppo 的归一化速度矢量，d 表示 Puppo 朝向目标的归一化方向。在 Unity 中，这是使用联合驱动控制器脚本控制的。如果 Puppo 成功达到目标，将提供额外的 1 英镑奖励，如果没有达到，将提供 0 英镑奖励。这是代理的控制逻辑，我们将用 PPO 来训练它。

我们可以打开 Puppo 代理 Unity 文件，并观察场景中出现的组件。我们可以看到像 BehavorialParameters.cs、Decision Requester 和 Model Overrider 这样的组件脚本被添加到其中。这类似于前几章，我们将这些组件附加到代理上。但是，在这种情况下，除了所有这些组件之外，我们还将使用联合驱动控制器脚本。然后我们有一个关联的 PuppoAgent.cs 脚本，这是我们将在本节中探索的。该脚本控制代理的关节运动，并在关节运动的帮助下记录对环境的观察。图 [5-33](#Fig33) 显示了环境的预览。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig33_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig33_HTML.jpg)

图 5-33

Unity 编辑器中的 Puppo 代理

现在让我们打开 PuppoAgent.cs 脚本。像以前一样，我们包括 Unity ML 代理模块和传感器模块来记录观察结果。

```
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using Unity.MLAgents;
using System;
using System.Linq;
using Unity.MLAgents.Sensors;
using Random = UnityEngine.Random;
using Unity.MLAgentsExamples;

```

在继承自代理类的 PuppoAgent 类中，我们声明了将出现在环境中的变量。我们有变换，如目标和狗(Puppo)，我们有关节的变量。这些包括形成小腿、大腿、嘴和躯干(身体)的 11 个关节。所有这些关节都与转动力(扭矩)和相应的转速(角速度)相关联。这些用下面几行表示。

```
   public Transform target;
    public Transform dog;
    // These items should be set in the inspector
    [Header("Body Parts")]
    public Transform mouthPosition;
    public Transform body;
    public Transform leg0_upper;
    public Transform leg1_upper;
    public Transform leg2_upper;
    public Transform leg3_upper;
    public Transform leg0_lower;
    public Transform leg1_lower;
    public Transform leg2_lower;
    public Transform leg3_lower;
    public bool vectorObs;
    [Header("Body Rotation")]
    public float maxTurnSpeed;
    public ForceMode turningForceMode;
    EnvironmentParameters m_params;

```

我们还有控制关节驱动控制器脚本的变量和控制目标方向的向量。在“初始化”方法中，我们分配狗和目标变量。我们实例化了关节驱动控制器脚本的实例，并将关节变量与该实例相关联。

```
dog=GetComponent<Transform>();
target=GetComponent<Transform>();
jdController = GetComponent<JointDriveController>();
jdController.SetupBodyPart(body);
jdController.SetupBodyPart(leg0_upper);
jdController.SetupBodyPart(leg0_lower);
jdController.SetupBodyPart(leg1_upper);
jdController.SetupBodyPart(leg1_lower);
jdController.SetupBodyPart(leg2_upper);
jdController.SetupBodyPart(leg2_lower);
jdController.SetupBodyPart(leg3_upper);
jdController.SetupBodyPart(leg3_lower);

m_params = Academy.Instance.EnvironmentParameters;

```

然后我们有“收集观察值”的方法，它使用矢量传感器。为此，我们首先在“GetCurrentJointForces()”方法的帮助下获得相关的关节力。然后，我们分配传感器来收集信息，如目标和 Puppo 之间的距离。传感器还收集与身体的角速度、法向速度、法向方向以及向上矢量方向相关的信息，以控制作用在 Puppo 身体上的力。然后我们有一个循环，为所有接触地面的关节运行，并在“currentXNormalizedRot”的帮助下沿着各自的轴计算归一化的角度旋转。然后，我们添加一个传感器，记录施加在某个接头上的当前强度，用该特定接头上的最大剪切力限制进行标准化。总而言之，这种方法控制与关节、角速度、旋转和力相关的传感器信息，以及身体相对于目标的方位、距离和方向。

```
public override void CollectObservations(VectorSensor sensor)
{if(vectorObs==true)
    {
        jdController.GetCurrentJointForces();
        sensor.AddObservation(dirToTarget.normalized);
        sensor.AddObservation(body.localPosition);
        sensor.AddObservation(jdController.
        bodyPartsDict[body].rb.velocity);
        sensor.AddObservation(jdController.
        bodyPartsDict[body].rb.angularVelocity);
        sensor.AddObservation(body.forward);
         sensor.AddObservation(body.up);
foreach (var bp in jdController.bodyPartsDict.Values)
        {

                                 var rb = bp.rb;
        sensor.AddObservation(bp.groundContact.
         touchingGround ? 1 : 0);
        if(bp.rb.transform != body)
        {
            sensor.AddObservation(bp.currentXNormalizedRot);
            sensor.AddObservation(bp.currentYNormalizedRot);
            sensor.AddObservation(bp.currentZNormalizedRot);
            sensor.AddObservation(bp.currentStrength/
            jdController.maxJointForceLimit);
        }
        }}

    }

```

“旋转体”方法用于在 Puppo 上施加扭矩或角度力。它将决策过程中特定步骤的相应动作作为输入。它在 0 和“最大转速”之间提高速度然后，它在标准化的旋转方向和正向向量的方向上施加一个力，并将其乘以速度。这是在应用于身体的“AddForceAtPosition”方法的帮助下完成的。

```
void RotateBody(float act)
    {
        float speed = Mathf.Lerp(0, maxTurnSpeed
        , Mathf.Clamp(act, 0, 1));
        Vector3 rotDir = dirToTarget;
        rotDir.y = 0;
        // Adds a force on the front of the body
        jdController.bodyPartsDict[body].
        rb.AddForceAtPosition(
         rotDir.normalized * speed * Time.deltaTime
        , body.forward, turningForceMode);
        // Adds a force on the back of the body
        jdController.bodyPartsDict[body].
         rb.AddForceAtPosition(
            -rotDir.normalized * speed * Time.deltaTime
          , -body.forward, turningForceMode);
    }

```

在下一步中，我们有一个被覆盖的方法“OnActionsReceived ”,该方法将环境中的向量观测值分配给相应的关节。正如我们看到的，有 20 个来自传感器的矢量化观测值，它们是在训练阶段接收的。现在我们正在为这个模型使用密集神经网络，并且我们只使用射线传感器信息。

```
public override void OnActionReceived(float[] vectorAction)
    {

        var bpDict = jdController.bodyPartsDict;
      Debug.Log(vectorAction.Length);

        // Update joint drive target rotation
        bpDict[leg0_upper].SetJointTargetRotation(vectorAction[0], vectorAction[1], 0);
        bpDict[leg1_upper].SetJointTargetRotation(vectorAction[2], vectorAction[3], 0);
        bpDict[leg2_upper].SetJointTargetRotation(vectorAction[4], vectorAction[5], 0);
        bpDict[leg3_upper].SetJointTargetRotation(vectorAction[6], vectorAction[7], 0);
        bpDict[leg0_lower].SetJointTargetRotation(vectorAction[8], 0, 0);
        bpDict[leg1_lower].SetJointTargetRotation(vectorAction[9], 0, 0);
        bpDict[leg2_lower].SetJointTargetRotation(vectorAction[10], 0, 0);
        bpDict[leg3_lower].SetJointTargetRotation(vectorAction[11], 0, 0);

        // Update joint drive strength
        bpDict[leg0_upper].SetJointStrength(vectorAction[12]);
        bpDict[leg1_upper].SetJointStrength(vectorAction[13]);
        bpDict[leg2_upper].SetJointStrength(vectorAction[14]);
        bpDict[leg3_upper].SetJointStrength(vectorAction[15]);
        bpDict[leg0_lower].SetJointStrength(vectorAction[16]);
        bpDict[leg1_lower].SetJointStrength(vectorAction[17]);
        bpDict[leg2_lower].SetJointStrength(vectorAction[18]);
        bpDict[leg3_lower].SetJointStrength(vectorAction[19]);

        rotateBodyActionValue = vectorAction[19];
    }

```

接下来是“FixedUpdate”方法，我们控制模拟循环并跟踪决策计数器；如果计数器的值为 3，则通过学院向外部培训环境发送新的请求，以对一组新的决策进行采样。这是在通讯器模块的帮助下完成的，我们在上一章中已经看到了。在这个方法中有一个“UpdateDirToTarget”方法，它获取从关节的身体到相应的目标代理的距离。还有一个能量守恒步骤，控制 Puppo 的转折频率。每当 Puppo 转得非常快很多次时，这就增加了一个负奖励。此外，我们对 Puppo 无法达到目标的时间进行处罚。

```
void FixedUpdate()
    {
        UpdateDirToTarget();

        if (decisionCounter == 0)
        {
            decisionCounter = 3;
            RequestDecision();
        }
        else
        {
            decisionCounter--;
        }

        RotateBody(rotateBodyActionValue);

        var bodyRotationPenalty =
        -0.001f * rotateBodyActionValue;
        AddReward(bodyRotationPenalty);

        // Reward for moving towards the target
        RewardFunctionMovingTowards();
        // Penalty for time
        RewardFunctionTimePenalty();
    }

```

“OnEpisodeBegin”被覆盖的方法用于每当一集训练完成时重置环境。该方法重置关节和关节上的力，并计算从当前情节到目标的方向。

```
public override void OnEpisodeBegin(){

     if (dirToTarget != Vector3.zero)
        {
            transform.rotation
            = Quaternion.LookRotation(dirToTarget);
        }

        foreach (var bodyPart
        in jdController.bodyPartsDict.Values)
        {
            bodyPart.Reset(bodyPart);
        }
        //SetResetParameters();

}

```

“RewardFunctionMovingTowards”方法用于在 Puppo 向目标移动时奖励它，这就是我们在本节开始时提到的数学奖励函数的编写位置。同样，在达到目标后，我们可以通过分配奖励来结束培训。

```
void RewardFunctionMovingTowards()
{

   float movingTowardsDot = Vector3.Dot(
   jdController.bodyPartsDict[body].rb.velocity
    ,   dirToTarget.normalized);
   AddReward(0.01f * movingTowardsDot);
  var dist=Vector3.Distance(dog.position, target.position);
  if(dist<0.02f)
 {
    SetReward(3.0f);
    EndEpisode();
 }
}

```

这就完成了代理的整个脚本。现在我们将通过 Tensorflow 中的 communicator 使用外部大脑来训练这个代理。我们将使用通用的“trainer_config.yaml”文件，其中包含 PPO 训练算法的超参数。我们将使用默认的超参数集，这是我们在之前的培训范围中使用的。

```
default:
  trainer: ppo
  batch_size: 1024
  beta: 5.0e-3
  buffer_size: 10240
  epsilon: 0.2
  hidden_units: 128
  lambd: 0.95
  learning_rate: 3.0e-4
  learning_rate_schedule: linear
  max_steps: 5.0e5
  memory_size: 128
  normalize: false
  num_epoch: 3
  num_layers: 2
  time_horizon: 64
  sequence_length: 64
  summary_freq: 100
  use_recurrent: false
  vis_encode_type: simple
  reward_signals:
    extrinsic:
      strength: 1.0
      gamma: 0.99

```

我们现在打开 Anaconda 提示符并导航到“config”文件夹来运行“mlagents-learn”命令。写完命令后

```
mlagents-learn <path to trainer_config.yaml> --run-id=Puppoagent –train,

```

我们可以想象训练在 Anaconda 提示符下开始。还会提示我们在 Tensorflow 的帮助下运行当前 Unity 场景进行 PPO 策略的培训，如图 [5-34](#Fig34) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig34_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig34_HTML.jpg)

图 5-34

使用 Tensorflow 对 Puppo 代理进行 ppo 培训

现在让我们在 TensorBoard 中想象这种学习，我们知道开始它的步骤。我们导航到“config”文件夹，然后键入命令:

```
tensorboard –logdir=summaries

```

这将在 config 文件夹内的 summaries 文件夹中记录观察、行动和奖励。

在运行命令时，我们可以在 TensorBoard 上看到损失、估计回报、累积回报、熵和其他详细信息。一旦训练完成，我们就可以关闭训练和与外部大脑的连接。然后我们将训练好的 PPO 神经网络分配给 Puppo 代理。我们现在有一个代理，我们准备建立。在最后一步，我们构建了代理并将其保存到 environments 文件夹中。在该文件夹中，我们有一个 Puppo Unity 模拟，其中包含经过培训的 ppo 神经网络代理。图 [5-35](#Fig35) 显示了训练的张量板可视化。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig35_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig35_HTML.jpg)

图 5-35

PPO 的张量板可视化

现在让我们尝试与 Python API 进行交互。Python API 通过通信器端口 5004 控制观察、决策和动作。

### 与 Python API 接口

为此，让我们打开 Python-API-MLagents.ipynb 笔记本。在这种情况下，我们将了解 Python API 如何控制 Unity 环境之间的交互，以及如何控制决策步骤、终端步骤、代理 ID 和行为名称。

*   **Unity 环境:**控制 Unity 应用程序和训练代码之间的接口。这是通过用于外部培训的通信器完成的。

*   **代理 ID:** 在现场接受培训的代理的唯一标识符 ID

*   **行为名称:**代理接受培训的行为(PPO、SAC、GAIL)

*   **决策步骤:**包含正在接受特定行为培训的代理的观察、奖励数据。这里只包括在“env.step()”方法的最后一次调用中请求决策的代理。

*   **终端步骤:**这也控制着当前受训代理的观察和奖励数据。在这种情况下，在“env.step()”方法的最后一次调用中其情节已经完成的代理包括在此处。

*   **行为规范:**确定决策和终端步骤中观察数据的形状，并包含预期的行动步骤。

现在让我们加载 Unity 环境，在笔记本上进行实时训练和互动。为此，我们从“mlagents_envs.environment”模块导入 Unityenvironment。在下一阶段，我们创建 Unityenvironment，它采用以下参数:

*   **文件位置:**Unity 可执行文件的位置

*   **Seed:** 控制随机数生成算法，用于超参数的 RL。对于确定性学习，为代理提供类似的种子值。

*   **Worker ID:** 控制与代理交互的端口。如果我们使用并行化算法，比如 on-policy A3C，那么我们必须指定这一部分。

*   **Side_channels:** 控制 Unity 环境和 Python 环境之间信息传递的接口，与 RL 训练循环无关。这可能包括一些配置。

以下片段用于与 Puppo 环境交互:

```
from mlagents_envs.environment import UnityEnvironment
import matplotlib.pyplot as plt
import numpy as np
import sys
import tensorflow as tf
env=UnityEnvironment(file_name="G:/DeepLearning/environments/DeepLearning", seed=1, side_channels=[])

```

现在让我们与培训环境互动。Unity 中的“BaseEnv”接口具有以下属性:

*   **Reset:** 这是“env.reset”方法，重置环境。

*   **Close:** 这是“env.close()”方法(类似于 OpenAI Gym)，发送信号关闭通信通道。

*   **Step:** 这相当于“env.step()”的 Gym 实现，我们在训练循环中使用它来记录观察、动作和奖励。

*   **获取行为名称:**这由“env.get_behavior_names()”方法表示。这控制了环境中的行为名称。

*   **获取行为规范:**这由“env.get_behavior_spec()”方法表示。这控制了观察和动作空间的形状，无论它们是连续的还是多重离散的。

*   **Get States:** 这由“env.get_steps()”方法表示，它控制代理的决策和终止步骤。

*   **设置动作:**这由“env.set_actions()”方法表示。这将设置场景中所有代理的动作。对于离散情况，我们有 int32 数据类型的 2D numpy 数组，对于连续分布，我们有 float32 数据类型的 numpy 数组。第一维度控制其动作被设置的代理的数量，第二维度控制多离散空间中的离散动作的数量或连续类型中的动作的数量。

*   **为代理设置动作:**这由“env.set_action_for_agent()”方法表示。这是一个 1D numpy 数组，类似于前面控制特定代理属性的方法。

在我们的例子中，我们将通过这个 API 看到 Puppo 代理的观察空间:

```
def_behaviours= env.get_behavior_names()
print(def_behaviours)
env.step()
BehaviorSpec(observation_shapes=[(59,)], action_type=<ActionType.CONTINUOUS: 1>, action_shape=20)
steps=env.get_steps('Puppo?team=0')
print(steps)
print(steps[0])
print(behaviour_specs.action_shape)
print(behaviour_specs.observation_shapes[0])
print(behaviour_specs.count)
print(behaviour_specs)
steps=env.get_steps('Puppo?team=0')
print(steps.count)

```

在这种情况下，我们为 PPO 代理提供了一个连续的动作控制。决策步骤有以下四个属性:

*   Obs: 这表示观察空间。

*   **奖励:**表示对应步骤的奖励。

*   **代理 ID:** 代理的唯一 ID。

*   **动作掩码:**这是代理的 2D 数组，用于控制离散/多离散动作空间中的批量大小和分支。

对于终端步骤，我们有四个属性:

*   Obs: 这表示观察空间。

*   **奖励:**表示对应步骤的奖励。

*   **代理 ID:** 代理的唯一 ID。

*   **最大步骤:**最后一个模拟步骤中的批量。

我们可以通过这个 API 实时可视化环境中的不同属性，比如状态、观察和动作。图 [5-36](#Fig36) 为模拟图。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig36_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig36_HTML.jpg)

图 5-36

实时 Python 与 Puppo 代理 Unity.exe 接口

### 与侧通道接口

还有一个传递与训练步骤无关的信息的范围。这可能包括发动机配置，并且通常与学习步骤的任务无关。接口有两种侧通道:

*   **引擎配置通道:**该通道控制环境的时标、分辨率和图形，可用于在训练期间微调性能。它有两个参数。这由文档中的以下几行表示:

    ```
    from mlagents_envs.environment import UnityEnvironment
    from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel

    channel = EngineConfigurationChannel()

    env = UnityEnvironment(side_channels=[channel])

    channel.set_configuration_parameters(time_scale = 2.0)

    i = env.reset()

    ```

    *   **设置配置参数:**控制模拟/训练阶段的高度、宽度、质量、时标、目标帧速率和捕获帧速率。

    *   **Set configuration:** 它有一个参数“Config”，其中包含一组相关的参数。

        This is denoted by the following lines from the documentation:
*   **环境参数:**控制环境中的数值属性或者与代理相关的一些数值属性。它只有一个方法:这由文档中的以下代码段表示:

    ```
    from mlagents_envs.environment import UnityEnvironment
    from mlagents_envs.side_channel.environment_parameters_channel import EnvironmentParametersChannel

    channel = EnvironmentParametersChannel()

    env = UnityEnvironment(side_channels=[channel])

    channel.set_float_parameter("parameter_1", 2.0)

    i = env.reset()

    ```

    一旦在 Python 中修改了一个属性，我们就可以在 C#中访问它，如下所示:

    ```
    var envParameters = Academy.Instance.EnvironmentParameters;
    float property1 = envParameters.GetWithDefault("parameter_1", 0.0f);

    ```

    *   **设置浮点参数:**设置 Unity 环境中特定键的浮点值。

        This is denoted by the following lines of code segment from the documentation:

因此，我们现在已经认识到 Unity ML 代理如何与 Python API 接口。这非常重要，因为在下一节中，我们将使用来自基线或我们自己的实现的 PPO/SAC 算法来训练我们的环境。

### 使用基线培训 ML 代理

既然我们可以与 Python API 交互，我们就可以使用 Gym 包装器，使用 OpenAI 的基线来训练 ML 代理。gym 包装器在“UnityEnvironment”类之上提供了一个接口。打开“与 MLAgents.ipynb 连接”笔记本。首先，我们将安装“gym_unity”包装器，如下所示:

```
!pip install gym_unity

```

我们将使用“UnityToGymWrapper”与健身房环境进行交互。它具有以下参数:

*   **Unity 环境:**要包装的 Unity 环境

*   **使用视觉:**这表示在决策步骤和最终步骤中，是否将视觉观察用于环境，以代替矢量观察。

*   **Uint8_visual:** 这个控制视觉观察的输出格式为 Uint8 值(0-255)，大部分雅达利游戏都用这个(比如 Pong)。默认为范围(0.0-1.0)内的浮点值，指的是灰度图像。

*   **Flatten _ bracked:**用于将分支的离散动作空间展平为健身房离散空间，使其与健身房环境兼容。

*   **Allow _ multiple _ visual _ OBS:**这允许多次目视观察，而不是一次观察。

现在让我们在 Unity ML 代理中使用 GridWorld 环境，并使用定制的偏离策略的 DQN 从基线对其进行训练。为了简单起见，我们将使用文档中提供的实现。为此，我们必须创建一个文件名——例如“baseline-dqn-grid world-train . py”——并将其保存在我们的存储库的/env 文件夹中。然后，我们必须包括我们将使用的库、包装器和算法:

```
from mlagents_envs.environment import UnityEnvironment
from gym_unity.envs import UnityToGymWrapper

```

然后在“main”方法中，我们加载 Unity 环境，并通过使用“UnityToGymWrapper”方法将其转换为健身房环境。因为在 GridWorld 中，图像分析是使用卷积神经网络完成的，在训练 GridWorld 时，我们将使用 CNN 策略进行 DQN 决斗。我们将把 unint8_visuals 设置为“True”，因为在转换为类似 2D Atari 的环境时，我们需要输出格式为类型(0-255)。我们还将“use_visual”设置为 true，因为我们将把视觉观察作为输入。

```
unity_env = UnityEnvironment("./envs/GridWorld")
env = UnityToGymWrapper(unity_env, 0, use_visual=True, uint8_visual=True)
logger.configure('./logs') # Çhange to log in a different directory

```

下一步，我们将从基线调用“deepq.learn”方法，因为它提供了 DQN 的实现。在这种情况下，将使用以下参数。

*   Env:这将设置体育馆转换的环境。

*   cnn:这是将要使用的神经网络策略——在这种情况下，卷积 2D 神经网络。

*   学习率:这是算法的学习率。

*   total_timesteps:训练将发生的总集数

*   buffer_size:控制体验回放的缓冲深度

*   探索分数:控制探索系数

*   探索最后一集:追踪最后一集的探索

*   打印频率:在屏幕上打印日志

*   train_freq:以指定的频率对样本进行训练

*   learnin_starts:经过多少步后，权重更新发生在学习中

*   目标网络更新频率:神经网络的更新频率

*   伽马:勘探开发系数

*   prioritized_replay:如果我们将 DDQN 与优先级 replay buffer 一起使用，则使用这一选项，这意味着某些观察会优先于其他观察使用。

*   checkpoint_freq:用于控制日志频率

*   checkpoint_path:存储日志频率的路径

*   决斗:这控制了策略的性质；在这种情况下，我们将使用决斗 DQN。

下面的程序段代表了这一点:

```
act = deepq.learn(
        env,
        "cnn", # conv_only is also a good choice for GridWorld
        lr=2.5e-4,
        total_timesteps=1000000,
        buffer_size=50000,
        exploration_fraction=0.05,
        exploration_final_eps=0.1,
        print_freq=20,
        train_freq=5,
        learning_starts=20000,
        target_network_update_freq=50,
        gamma=0.99,
        prioritized_replay=False,
        checkpoint_freq=1000,
        checkpoint_path='./logs',
       # Change to save model in a different directory
        dueling=True
    )

```

然后我们保存模型，使用“main”方法对其进行训练。

```
print("Saving model to unity_model.pkl")
    act.save("unity_model.pkl")

if __name__ == '__main__':
    main()

```

然后，我们必须导航到该文件所在的文件夹位置。然后，我们必须在 Anaconda 提示符下运行以下命令。

```
python –m baseline-dqn-gridworld-train.py

```

运行该命令后，我们可以看到控制台和 GridWorld 环境中正在进行一项决斗 DQN 策略的训练。我们可以在 Unity 可执行文件运行时看到训练，如图 [5-37](#Fig37) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig37_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig37_HTML.jpg)

图 5-37

GridWorld 训练使用从基线决斗 DQN

我们还可以使用 ML 代理 PPO 方法，通过使用以下命令来训练它:

```
mlagents-learn <pah to trainer_config.yaml> --run-id=GridWorldNew –train

```

训练阶段如图 [5-38](#Fig38) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig38_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig38_HTML.jpg)

图 5-38

关于传销代理人 PPO 政策的培训

现在让我们使用来自 Gym 的基线 PPO2(剪辑)版本来训练这个环境。如前所述，我们加载 Unity GridWorld 环境，并且我们将在这个上下文中创建一个受监控的环境。这是一个使用文档中提到的基线中的 PPO2 算法进行多实例训练的示例。我们使用“UnityToGymWrapper”来转换环境，并应用“use_visuals”进行视觉观察。然后，我们使用“SubprocVecEnv”方法创建子流程，用于实例化 GridWorld 环境的多个实例。

```
from mlagents_envs.environment import UnityEnvironment
from gym_unity.envs import UnityToGymWrapper
from baselines.common.vec_env.subproc_vec_env import SubprocVecEnv
from baselines.common.vec_env.dummy_vec_env import DummyVecEnv
from baselines.bench import Monitor
from baselines import logger
import baselines.ppo2.ppo2 as ppo2

import os

try:
    from mpi4py import MPI
except ImportError:
    MPI = None

def make_unity_env(env_directory, num_env, visual, start_index=0):

    def make_env(rank, use_visual=True):
        def _thunk():
            unity_env = UnityEnvironment(env_directory)
            env = UnityToGymWrapper(unity_env,
            rank, use_visual=use_visual, uint8_visual=True)
            env = Monitor(env, logger.get_dir()
            and os.path.join(logger.get_dir(), str(rank)))
            return env
        return _thunk
    if visual:
        return SubprocVecEnv([make_env(i + start_index)
         for i in range(num_env)])
    else:
        rank = MPI.COMM_WORLD.Get_rank() if MPI else 0
        return DummyVecEnv([make_env(rank, use_visual=False)])

```

然后是来自基线的 PPO2 算法，它使用 MLP 策略或密集网络，也可以修改为使用卷积神经网络。在这种情况下，我们为多线程训练创建了四个 GridWorld 环境。我们训练了 10 万集:

```
def main():
    env = make_unity_env('./envs/GridWorld', 4, True)
    ppo2.learn(
        network="mlp",
        env=env,
        total_timesteps=100000,
        lr=1e-3,
    )

if __name__ == '__main__':
    main()

```

在使用 Python–m 命令运行时，我们可以可视化四个 GridWorld 环境实例的同步训练。我们也可以通过使用日志文件来可视化 Tensorboard 中的训练。

现在，我们可以使用我们自己的上一节中讨论的 on/off 策略算法的实现来训练我们的 ML 代理，而无需定制模型。这留给感兴趣的读者通过 Gym 界面尝试使用 Unity ML 代理的 on/off 策略算法的先前实现。在下一节中，我们将研究“ML 代理”的某些方面，这些方面在 ML 代理库中；这将包括对在用于 PPO 算法的 Unity 中的 ML 代理中实施的不同策略的简要概述，以及对基于记忆的神经网络(即 LSTM 网络)的理解。

## 了解 Unity ML 代理和基于内存的网络中的深度 RL 策略

在这种情况下，我们将浏览上一章提到的 ML 代理存储库中的某些脚本，即存储库的 Examples/SharedAssets/Scripts 目录中的 ModelOverrider 脚本。这与 Unity 中代理的行为参数和决策请求程序脚本一起使用。

### 模型覆盖脚本

此脚本是一个实用程序类，在推理模式下覆盖神经网络模型，并用于在剧集完成后在内部验证培训。这适用于代理和环境之间 1:1 的比例。首先，我们有 Barracuda 用于推理的神经网络模型文件的类型。nn“或者”。onnx”格式。然后，我们指定涉及神经网络模型路径和位置的参数，以及神经网络模型的训练步骤、扩展、情节和目录以及其他细节。

```
HashSet<string> k_SupportedExtensions = new HashSet<string>{"nn", "onnx"};
        const string k_CommandLineModelOverrideFlag = "--mlagents-override-model";
        const string k_CommandLineModelOverrideDirectoryFlag = "--mlagents-override-model-directory";
        const string k_CommandLineModelOverrideExtensionFlag = "--mlagents-override-model-extension";
        const string k_CommandLineQuitAfterEpisodesFlag = "--mlagents-quit-after-episodes";
        const string k_CommandLineQuitOnLoadFailure = "--mlagents-quit-on-load-failure";

```

然后它有控制代理的变量，一个包含资产路径和行为名称作为键-值对的字典，另一个存储行为名称和神经网络模型作为键-值对的字典。它还包含步骤数、先前步骤数、已完成剧集数的变量，以及用于检查是否在失败时加载的布尔值，还包含相同的静态变量。

```
Agent m_Agent;
Dictionary<string, string> m_BehaviorNameOverrides = new Dictionary<string, string>();
string m_BehaviorNameOverrideDirectory;
string m_OverrideExtension = "nn";
Dictionary<string, NNModel> m_CachedModels = new Dictionary<string, NNModel>();
int m_MaxEpisodes;
int m_NumSteps;
int m_PreviousNumSteps;
int m_PreviousAgentCompletedEpisodes;
bool m_QuitOnLoadFailure;
[Tooltip("Debug values to be used in place of the command line for overriding models.")]
public string debugCommandLineOverride;
static int s_PreviousAgentCompletedEpisodes;
static int s_PreviousNumSteps;

```

还有一些方法，如“TotalCompletedEpisodes”、“TotalNumSteps”、“HasOverrides”和“GetOverridenBehaviorName”，它们分别控制训练期间完成的总步骤、训练的总步骤、断言特定行为覆盖是否可能的布尔函数以及被覆盖行为的名称。

```
int TotalCompletedEpisodes
{
    get { return m_PreviousAgentCompletedEpisodes + (m_Agent == null ? 0 : m_Agent.CompletedEpisodes);  }
}

int TotalNumSteps
{
    get { return m_PreviousNumSteps + m_NumSteps; }
}

public bool HasOverrides
{
    get { return m_BehaviorNameOverrides.Count > 0
    || !string.IsNullOrEmpty(m_BehaviorNameOverrideDirectory);  }
}

public static string GetOverrideBehaviorName(string originalBehaviorName)
{
    return $"Override_{originalBehaviorName}";
}

```

“GetAssetPathFromCommand”行方法用于从命令行参数加载资产。它通过拆分参数来实现这一点。因为资产是存储的(。nn/。onnx)在键-值对格式中，拆分使得能够提取行为名称和资产类型。这是通过以下代码行完成的:

```
m_BehaviorNameOverrides.Clear();
    var maxEpisodes = 0;
    string[] commandLineArgsOverride = null;
    if (!string.IsNullOrEmpty
      (debugCommandLineOverride)
       && Application.isEditor)
    {
        commandLineArgsOverride
        =   debugCommandLineOverride.Split(' ');
    }
    var args
     = commandLineArgsOverride
     ??    Environment.GetCommandLineArgs();
    for (var i = 0; i < args.Length; i++)
    {
        if (args[i] == k_CommandLineModelOverrideFlag &&
        i < args.Length-2)
        {
            var key = args[i + 1].Trim();
            var value = args[i + 2].Trim();
            m_BehaviorNameOverrides[key] = value;
        }

```

它还包含了在拆分参数时控制扩展的条件(在写这本书的时候。在此上下文中不支持 onnx 文件)，如果模型加载失败，它还控制“加载失败时”属性。有一个条件，规定了退出前使用的训练集数。

```
   else if (args[i] == k_CommandLineModelOverrideExtension
    Flag && i < args.Length-1)
        {
            m_OverrideExtension = args
            [i + 1].Trim().ToLower();
            var isKnownExtension
           = k_SupportedExtensions.Contains
           (m_OverrideExtension);
            var isOnnx = m_OverrideExtension.Equals("onnx");
            if (!isKnownExtension || isOnnx)
            {
                Debug.LogError($"loading unsupported format
                 : {m_OverrideExtension}");
                Application.Quit(1);
#if UNITY_EDITOR
                        EditorApplication.isPlaying = false;
#endif
                    }
        }
        else if (args[i] == k_CommandLineQuit
        AfterEpisodesFlag && i < args.Length-1)
        {
            Int32.TryParse(args[i + 1], out maxEpisodes);
        }
        else if (args[i] == k_CommandLineQuitOnLoadFailure)
        {
            m_QuitOnLoadFailure = true;
        }
    }

```

“OnEnable”方法用于设置和重置参数，如已完成的剧集、代理行为以及行为是否有覆盖，并在我们重置 Unity 场景时使用。

```
void OnEnable()
{
    m_PreviousNumSteps = s_PreviousNumSteps;
    m_PreviousAgentCompletedEpisodes
    = s_PreviousAgentCompletedEpisodes;
    m_Agent = GetComponent<Agent>();
    GetAssetPathFromCommandLine();
    if (HasOverrides)
    {
        OverrideModel();
    }
}

```

“OnDisable”方法用于更新控制情节和步骤计数的静态变量。

```
void OnDisable()
{
    s_PreviousAgentCompletedEpisodes
    = Mathf.Max(s_PreviousAgentCompletedEpisodes,
     TotalCompletedEpisodes);
    s_PreviousNumSteps = Mathf.Max
    (s_PreviousNumSteps, TotalNumSteps);
}

```

然后是“固定更新”方法，该方法具有 maxSteps 属性，该属性控制训练所需的最大步数，并让代理在终止之前至少训练指定的最大步数。它还通过将训练延长几个时间步长来检查训练阶段中的任何错误。

```
void FixedUpdate()
{
    if (m_MaxEpisodes > 0)
    {
        if (TotalCompletedEpisodes >= m_MaxEpisodes
       && TotalNumSteps > m_MaxEpisodes * m_Agent.MaxStep)
        {
            Debug.Log($"ModelOverride
              reached {TotalCompletedEpisodes} episodes
             and {TotalNumSteps} steps. Exiting.");
            Application.Quit(0);
#if UNITY_EDITOR
            EditorApplication.isPlaying = false;
#endif
        }
    }
    m_NumSteps++;
}

```

“GetModelFromBehaviorName”方法用于通过行为名称中指定的参数检索神经网络模型。它获取神经网络模型资产的路径，然后使用一个键-值对来生成行为名称和神经网络模型。然后，它读取推理模式模型的字节码。它还缓存文件(资产)以供将来在推理训练期间参考。

```
public NNModel GetModelForBehaviorName(string behaviorName)
{
    if (m_CachedModels.ContainsKey(behaviorName))
    {
        return m_CachedModels[behaviorName];
    }
    string assetPath = null;
    if (m_BehaviorNameOverrides.ContainsKey(behaviorName))
    {
        assetPath = m_BehaviorNameOverrides[behaviorName];
    }
    else if(!string.IsNullOrEmpty(m_BehaviorNameOverrideDirectory))
    {
        assetPath = Path.Combine(m_BehaviorNameOverrideDirectory, $"{behaviorName}.{m_OverrideExtension}");
    }

    if (string.IsNullOrEmpty(assetPath))
    {
        Debug.Log($"No override for
        BehaviorName {behaviorName}, and no directory set.");
        return null;
    }

    byte[] model = null;
    try
    {
        model = File.ReadAllBytes(assetPath);
    }
    catch(IOException)
    {
        Debug.Log($"Couldn't load file {assetPath}
        at full path {Path.GetFullPath(assetPath)}", this);
        m_CachedModels[behaviorName] = null;
        return null;
    }

    var asset = ScriptableObject.CreateInstance<NNModel>();
    asset.modelData
    = ScriptableObject.CreateInstance<NNModelData>();
    asset.modelData.Value = model;
    asset.name = "Override - " + Path.GetFileName(assetPath);
    m_CachedModels[behaviorName] = asset;
    return asset;
}

```

“OverrideModel”方法包含神经网络模型的实际覆盖逻辑。它从附着在代理上的 BehaviorParameters 脚本中提取分配大脑的参数，然后以字节码的形式分配神经网络模型。它获取相应的行为名称，并检查该名称是否有效，是否存在于 Unity 内部的资产中。然后，在加载代理后，它分配训练好的神经网络模型进行推理。这是通过以下代码行完成的:

```
void OverrideModel()
{
    bool overrideOk = false;
    string overrideError = null;

    m_Agent.LazyInitialize();
    var bp = m_Agent.GetComponent<BehaviorParameters>();
    var behaviorName = bp.BehaviorName;

    var nnModel = GetModelForBehaviorName(behaviorName);
    if (nnModel == null)
    {
        overrideError =
            $"Didn't find a model for
            behaviorName {behaviorName}. Make " +
            $"sure the behaviorName is set correctly
            in the commandline " +
            $"and that the model file exists";
    }
    else
    {
        var modelName = nnModel != null ?
        nnModel.name : "<null>";
        Debug.Log($"Overriding behavior {behaviorName}
        for agent with model {modelName}");
        try
        {
           m_Agent.SetModel(GetOverrideBehaviorName
           (behaviorName), nnModel);
            overrideOk = true;
        }
        catch (Exception e)
        {
            overrideError = $"Exception
            calling Agent.SetModel: {e}";
        }
    }
    if (!overrideOk && m_QuitOnLoadFailure)
    {
        if(!string.IsNullOrEmpty(overrideError))
        {
            Debug.LogWarning(overrideError);
        }
        Application.Quit(1);
#if UNITY_EDITOR
        EditorApplication.isPlaying = false;
#endif
    }

}

```

这样，我们对 Unity 中的模型覆盖脚本有了一个大致的了解，这是一个重要的脚本，用于覆盖 Unity 中的推理模型。接下来，我们将简单看看 Unity 内部构建的实际神经网络。我们还将探索 ML 代理中模型脚本内部的 LSTM 模块。

在这种情况下，让我们导航到 mlagents/trainers/tf 文件夹，其中包含 Unity 中实际神经网络模型的 Tensorflow 实现。我们将研究 Models.py 脚本。

### 模型脚本:Python

这是 PPO/SAC/GAIL/Ghost 和 ML 代理中的其他训练算法使用的基本脚本。它控制神经网络的整个张量流规范以及超参数。我们将会看到这个脚本的某些部分。我们在脚本的开头提到了ε，它也可以更改。然后我们有包含编码参数的不同类。这还包含张量的编码参数(卷积网络的高度、宽度和通道)。对于所使用的不同神经网络，也有一个相关的最小分辨率，我们可以看到在这种情况下提到的“RESNET:15 ”,它指定对于基于 CNN 的模型，我们可以使用 RESNET 作为我们的神经网络。

```
EPSILON = 1e-7

class Tensor3DShape(NamedTuple):
    height: int
    width: int
    num_channels: int

class NormalizerTensors(NamedTuple):
    update_op: tf.Operation
    steps: tf.Tensor
    running_mean: tf.Tensor
    running_variance: tf.Tensor

class ModelUtils:

    MIN_RESOLUTION_FOR_ENCODER = {
        EncoderType.SIMPLE: 20,
        EncoderType.NATURE_CNN: 36,
        EncoderType.RESNET: 15,
    }
    }
}

```

然后，我们在 Tensorflow 中指定全局训练步骤，并且我们还冻结由“trainable = False”命令指定的网络中的某些权重。然后我们有方法“create_schedule”，它控制学习率张量，控制基础学习率、全局步数和最大步数。有两种类型的训练器:线性的和恒定的。

```
if schedule == ScheduleType.CONSTANT:
            parameter_rate = tf.Variable(parameter
             , trainable=False)
elif schedule == ScheduleType.LINEAR:
            parameter_rate = tf.train.polynomial_decay(
            parameter, global_step, max_step,
            min_value, power=1.0
            )
else:
            raise UnityTrainerException(f"The
            schedule {schedule} is invalid.")
return parameter_rate

```

然后，我们有方差缩放初始化器和 Swish 激活方法，用于为神经网络中的可训练权重创建初始化器。现在，我们有了通过相机从 Unity 场景中获取视觉观察并将它们转换为包含高度、宽度和通道的 3D 张量(“create_visual_input”)的方法。

```
visual_in = tf.placeholder(
        shape=[None, o_size_h, o_size_w, c_channels], dtype=tf.float32, name=name

```

如果我们不使用视觉观察，我们必须以 1D 张量(“创建向量输入”)的形式转换向量观察(射线传感器数据)。

```
vector_in = tf.placeholder(
            shape=[None, vec_obs_size], dtype=tf.float32, name=name
        )

```

有一些方法(“normalize_vector_obs”)对输入向量和视觉观察值(张量)进行归一化，这是通过除以方差的平方根来实现的。

```
normalized_state = tf.clip_by_value(
            (vector_obs - running_mean)
            / tf.sqrt(
                running_variance / (tf.cast(normalization_steps, tf.float32) + 1)
            ),
            -5,
            5,
            name="normalized_state",
        )

```

“create_normalizer”方法用于创建一个张量，该张量包含要基于当前运行均值和方差进行归一化的下一个值(用于批量归一化)。它还包含不可训练权重的标准化步骤。

```
       vec_obs_size = vector_obs.shape[1]
        steps = tf.get_variable(
            "normalization_steps",
            [],
            trainable=False,
            dtype=tf.int32,
            initializer=tf.zeros_initializer(),
        )
        running_mean = tf.get_variable(
            "running_mean",
            [vec_obs_size],
            trainable=False,
            dtype=tf.float32,
            initializer=tf.zeros_initializer(),
        )
        running_variance = tf.get_variable(
            "running_variance",
            [vec_obs_size],
            trainable=False,
            dtype=tf.float32,
            initializer=tf.ones_initializer(),
        )
        update_normalization
        = ModelUtils.create_normalizer_update(
            vector_obs, steps, running_mean, running_variance
        )
        return NormalizerTensors(
            update_normalization, steps,
             running_mean, running_variance
        )

```

下一个方法“创建向量观测值编码器”是一个重要的方法，因为它包含了向量观测值的密集神经网络(MLP)的实际实现。因为我们已经在 Keras 中研究了稠密网络，所以这里使用来自 Tensorflow 的稠密网络。我们有参数，比如隐藏层、单元和内核初始化器，在这个例子中是 variance_scaling、激活函数和其他细节。

```
with tf.variable_scope(scope):
            hidden = observation_input
            for i in range(num_layers):
                hidden = tf.layers.dense(
                    hidden,
                    h_size,
                    activation=activation,
                    reuse=reuse,
                    name=f"hidden_{i}",
                    kernel_initializer=tf.initializers.variance_scaling(1.0),
                )
return hidden

```

然后，我们有“创建 _ 视觉 _ 观察 _ 编码器”方法，它包含在 ML 代理中使用的卷积-2D 神经网络结构。这是在 GridWorld 环境中使用 PPO 策略和 cnn 作为神经网络类型进行训练时使用的方法。正如我们所研究的，在传统的 CNN 中，我们有卷积层、展平/汇集层以及密集网络。

*   **ML 代理中的卷积层** **:** 这里我们可以理解 ML 代理使用卷积-2D 神经网络，其核大小为[8，8]，步长为[4，4]，第一个卷积网络的激活为“Relu”。对于第二个 CNN，我们的核大小为[4，4]，步长为[2，2]，以“elu”(指数线性单位)作为激活函数，32 是信道深度。

*   **密集层** **:** 第二个<sup>第二个</sup>卷积层的输出被传入密集网络，该网络是使用前一个案例中的(密集神经网络)“create _ vector _ observation _ encoder”方法创建的。

下面几行代表了这一点，我们可以根据我们的需求修改这段代码，并添加我们之前研究过的池化/丢弃或其他层。

```
with tf.variable_scope(scope):
            conv1 = tf.layers.conv2d(
                image_input,
                16,
                kernel_size=[8, 8],
                strides=[4, 4],
                activation=tf.nn.elu,
                reuse=reuse,
                name="conv_1",
            )
            conv2 = tf.layers.conv2d(
                conv1,
                32,
                kernel_size=[4, 4],
                strides=[2, 2],
                activation=tf.nn.elu,
                reuse=reuse,
                name="conv_2",
            )
            hidden = tf.layers.flatten(conv2)

with tf.variable_scope(scope + "/" + "flat_encoding"):
            hidden_flat = ModelUtils.create_vector_observation_encoder(
                hidden, h_size, activation, num_layers, scope, reuse
            )
return hidden_flat

```

在下一个方法“create _ nature _ CNN _ visual _ observation _ encoder”中，有一个为 ML 代理构建的残差网络或 ResNet 类型的网络。现在，我们已经研究了 ResNet 体系结构中残余块的重要性，以便随着神经网络模型深度的增加而提高训练的准确性。这是在 Tensorflow 中通过使用 reuse=reuse 参数来完成的，该参数在上下文中有所提及。这里，我们有三个卷积-2D 神经网络模型，其中“elu”作为共同的激活函数，具有不同的核大小(分别为 8、4 和 3)和不同的步长(分别为 4、2 和 1)。在第二和第三层中，通道的深度从 32 到 64 个单位变化。然后，我们有通常的密集 MLP 网络包括从矢量观测方法。

```
with tf.variable_scope(scope):
            conv1 = tf.layers.conv2d(
                image_input,
                32,
                kernel_size=[8, 8],
                strides=[4, 4],
                activation=tf.nn.elu,
                reuse=reuse,
                name="conv_1",
            )
            conv2 = tf.layers.conv2d(
                conv1,
                64,
                kernel_size=[4, 4],
                strides=[2, 2],
                activation=tf.nn.elu,
                reuse=reuse,
                name="conv_2",
            )
            conv3 = tf.layers.conv2d(
                conv2,
                64,
                kernel_size=[3, 3],
                strides=[1, 1],
                activation=tf.nn.elu,
                reuse=reuse,
                name="conv_3",
            )
            hidden = tf.layers.flatten(conv3)

with tf.variable_scope(scope + "/" + "flat_encoding"):
            hidden_flat
            = ModelUtils.create_vector_observation_encoder(
             hidden, h_size, activation, num_layers,
             scope, reuse
            )
return hidden_flat

```

“create _ resnet _ visual _ observation _ encoder”是 ResNet 模型的另一个变体，在这种情况下，我们在通常的卷积块之间添加了具有“Relu”激活的隐藏层。同样在这种情况下，最大池层的池大小为[3，3]，步长大小为[2，2]。此外，卷积层现在应用了“相同”的填充。

```
n_channels = [16, 32, 32] # channel for each stack
n_blocks = 2 # number of residual blocks
with tf.variable_scope(scope):
            hidden = image_input
            for i, ch in enumerate(n_channels):
                hidden = tf.layers.conv2d(
                    hidden,
                    ch,
                    kernel_size=[3, 3],
                    strides=[1, 1],
                    reuse=reuse,
                    name="layer%dconv_1" % i,
                )
                hidden = tf.layers.max_pooling2d(
                    hidden, pool_size=[3, 3], strides=[2, 2], padding="same"
                )
                # create residual blocks
                for j in range(n_blocks):
                    block_input = hidden
                    hidden = tf.nn.relu(hidden)
                    hidden = tf.layers.conv2d(
                        hidden,
                        ch,
                        kernel_size=[3, 3],
                        strides=[1, 1],
                        padding="same",
                        reuse=reuse,
                        name="layer%d_%d_conv1" % (i, j),
                    )
                    hidden = tf.nn.relu(hidden)
                    hidden = tf.layers.conv2d(
                        hidden,
                        ch,
                        kernel_size=[3, 3],
                        strides=[1, 1],
                        padding="same",
                        reuse=reuse,
                        name="layer%d_%d_conv2" % (i, j),
                    )
                    hidden = tf.add(block_input, hidden)
            hidden = tf.nn.relu(hidden)
            hidden = tf.layers.flatten(hidden)

with tf.variable_scope(scope + "/" + "flat_encoding"):
            hidden_flat
            = ModelUtils.create_vector_observation_encoder(
              hidden, h_size, activation, num_layers,
              scope, reuse
            )
return hidden_flat

```

然后，我们有“break_into_branches”方法，它采用一组表示多个离散分支的连接逻辑，并将其分解为每个分支的单个张量，还有“create _ discrete _ action _ masking _ layer”方法，它在离散操作上应用一个遮罩层。它使用 softmax 激活来掩盖逻辑，并对输出使用多项式分布。

```
branch_masks = ModelUtils.break_into_branches(action_masks, action_size)
raw_probs = [
            tf.multiply(tf.nn.softmax(branches_logits[k])
           + EPSILON, branch_masks[k])
            for k in range(len(action_size))
        ]
normalized_probs = [
            tf.divide(raw_probs[k]
           , tf.reduce_sum(raw_probs[k],
            axis=1, keepdims=True))
            for k in range(len(action_size))
        ]
output = tf.concat(
            [
                tf.multinomial(tf.log(normalized_probs[k]
               + EPSILON), 1)
                for k in range(len(action_size))
            ],
            axis=1,
        )
return (
            output,
            tf.concat([normalized_probs[k]
            for k in range(len(action_size))], axis=1),
            tf.concat(
                [
                    tf.log(normalized_probs[k] + EPSILON)
                    for k in range(len(action_size))
                ],
                axis=1,
            ),
        )

```

“create_observation_streams”使用前面的方法，其中包含用于构建训练中使用的观察流的神经网络架构。下一种方法“create_recurrent_encoder”是另一种重要的方法，它使用递归神经网络或 LSTM 网络进行基于记忆的处理。我们将很快学习 LSTMs 然而，在这种情况下，这种方法创建了一个神经网络，它在存储器中存储了一部分动作/状态/奖励，并将其用于将来的参考。Tensorflow(或 Keras)的“BasicLSTMCellMethod”用于此目的。

```
s_size = input_state.get_shape().as_list()[1]
m_size = memory_in.get_shape().as_list()[1]
lstm_input_state = tf.reshape(input_state, shape=[-1, sequence_length, s_size])
memory_in = tf.reshape(memory_in[:, :], [-1, m_size])
half_point = int(m_size / 2)
with tf.variable_scope(name):
            rnn_cell
           = tf.nn.rnn_cell.BasicLSTMCell(half_point)
            lstm_vector_in = tf.nn.rnn_cell.LSTMStateTuple(
             memory_in[:, :half_point],
             memory_in[:, half_point:]
            )
            recurrent_output, lstm_state_out
             = tf.nn.dynamic_rnn(
                rnn_cell, lstm_input_state
               , initial_state=lstm_vector_in
            )

recurrent_output = tf.reshape(recurrent_output, shape=[-1, half_point])
return recurrent_output, tf.concat([lstm_state_out.c, lstm_state_out.h], axis=1)

```

该脚本是最重要的脚本，因为它管理 Unity ML 代理中存在的不同 on/off 策略算法的网络架构，我们可以修改这些超参数(如卷积-2D 神经网络中的核大小/步距或激活)，并尝试了解训练的结果。在下一节中，我们将非常简要地看一下 PPO 脚本，然后冒险进入基于 LSTM 的网络。

### Unity ML 试剂中的 PPO

在 Unity ML Agents 中，我们必须导航到 mlagents/trainers/ppo 文件夹，在里面我们可以看到有两个 python 脚本:optimizers 和 trainer。

*   **训练器 Python 脚本** **:** 这控制了在 ML 代理中 PPO 算法的实际实现。它从配置文件(“trainer_config.yaml”)中获取超参数，如批量大小、剧集、策略、学习率、gamma 和 epsilon，并检查有效性。在这种情况下，一个重要的方法是“过程轨迹”方法。这控制了使用 PPO 策略构建的初始轨迹，并获得了遵循该特定策略的价值评估(观察、奖励、状态)。它计算奖励并将其存储在缓冲区中。之后，它会计算 GAE，我们在 deep RL 的 PPO 部分提到过。它根据特定策略的价值估计和回报来计算优势。

    ```
            tmp_advantages = []
            tmp_returns = []
            for name in self.optimizer.reward_signals:
                bootstrap_value = value_next[name]

                local_rewards = agent_buffer_trajectory[
                    "{}_rewards".format(name)

    ```

```
            ].get_batch()
            local_value_estimates = agent_buffer_trajectory[
                "{}_value_estimates".format(name)
            ].get_batch()
            local_advantage = get_gae(
                rewards=local_rewards,
                value_estimates=local_value_estimates,
                value_next=bootstrap_value,
                gamma=self.optimizer.reward_signals[name].
                gamma,
                lambd=self.trainer_parameters["lambd"],
            )

            local_return = local_advantage
            + local_value_estimates
            agent_buffer_trajectory["{}_returns".
            format(name)].set(local_return)
            agent_buffer_trajectory["{}_advantage".
            format(name)].set(local_advantage)
            tmp_advantages.append(local_advantage)
            tmp_returns.append(local_return)

```

这是局部和全局计算的。“_update_policy”方法用于通过将返回与旧策略的 GAE 进行比较来更新当前策略。它在小批量的培训中更新政策。“create_policy”方法用于创建 PPO 策略，它有几个参数，如 brain 参数、seed 和“is training”(布尔值)，所有这些都是从 NNPolicy 类派生的。“add_policy”方法将当前培训策略添加到培训师。“discount_rewards”方法与我们在前面几节中所写的方法相同，提供奖励的折扣回报。方法“get_gae”返回用于更新策略的 gae 优势估计，由以下等式表示:

a<sup>(s<sub>，a<sub>= q<sup>(s<sub>)，a<sub>【t】</sub></sub></sup></sub></sub></sup>

下面几行代码表示这个公式。

value _ estimates = NP . append(value _ estimates，value_next)

delta _ t = rewards+gamma * value _ estimates[1:]-value _ estimates[:-1]

优势=折扣 _ 奖励(r=delta_t，gamma=gamma * lambd)

回报优势

*   **优化器 Python 脚本** **:** 这个脚本包含了我们在深度 RL 部分创建的 PPO 优化器的一个复杂变体。这控制了损失和学习率，并通过更新策略梯度来微调优化。它包含某些控制ε、β、最大步长、层数和其他属性的超参数。它还包含要使用的神经网络体系结构的类型——是使用递归网络、卷积网络还是密集网络。“create_cc_critic”方法表示创建一个连续受控的 critic 值网络，在循环网络的情况下，它使用我们提到的 Model.py 脚本中的“create_recurrent_encoder”方法。

```
if self.policy.use_recurrent:
            hidden_value, memory_value_out
            = ModelUtils.create_recurrent_encoder(
                hidden_stream,
                self.memory_in,
                self.policy.sequence_length_ph,
                name="lstm_value",
            )
            self.memory_out = memory_value_out

```

“create_dc_critic”表示一个离散的受控 critic 网络，如果策略是 CNN，那么它使用 Model.py 脚本中的“create _ visual _ observation _ encoder”方法。它也可以使用其他变化的视觉编码器，如 Resnet。在离散的情况下，我们取缓冲区的最高索引进行分析。

```
hidden_stream = ModelUtils.create_observation_streams(
            self.policy.visual_in,
            self.policy.processed_vector_in,
            1,
            h_size,
            num_layers,
            vis_encode_type,
        )[0]

```

“_create_losses”方法包含我们在 deep RL 的 PPO 部分看到的策略的实际剪辑。剪辑是基于旧策略的 GAE 回报完成的，由以下代码行表示:

```
r_theta = tf.exp(probs - old_probs)
p_opt_a = r_theta * advantage
p_opt_b = (
    tf.clip_by_value(r_theta, 1.0 - decay_epsilon, 1.0
    + decay_epsilon)
    * advantage
)

```

其中ε是本文中使用的超参数。

关于 PPO 的部分到此结束，我们对如何在 Unity 内部创建 PPO 算法用于 ML 代理有了一个想法。我们还知道，“trainer_config.yaml”中提到的超参数在这些脚本中用于控制训练和策略梯度优化。现在让我们简要地了解一下 LSTM，并试着去理解与之相关的超参数。

### 最大似然代理中的长短期记忆网络

**递归神经网络**

LSTMs 是递归神经网络(rnn)的修改版本。rnn 是通过隐藏层向其自己的输入端传递已处理信息的网络，因此用于在网络内部保留内存。rnn 从前面的步骤中学习信息并存储这些信息。在内部，隐藏层有一些试图保留数据的激活。在训练 RNN 的梯度下降部分，由于通过 RNN 的连续梯度流，网络对于存储来自先前时间步长的某些误差梯度是有用的，并且有助于学习过程，这在传统神经网络的情况下是不可能的。然而，在传统的 RNN 中，由于梯度更新的递归循环，出现了两个问题。

*   **消失梯度问题** **:** 这意味着随着训练长度的增加，连续梯度减小，因为非常小的误差传播到前面的层。这导致不收敛的学习曲线，并且算法在全局最小值的区域中振荡而不收敛。

*   **爆炸梯度问题:**由于训练中频繁的递归更新，模型权重可能会大量更新。这导致了训练阶段的不稳定性，因为在许多情况下，算法超出了全局最小值。

**LSTM**T2【网络

为了解决这个梯度问题，LSTMs 被设计成在训练期间稳定权重更新和梯度流，同时保持记忆。LSTM 模块的典型架构如图 [5-39](#Fig39) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig39_HTML.png](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig39_HTML.png)

图 5-39

LSM 体系结构

现在我们有了一个独立的激活单元，用“tanh”表示。这是因为“tanh”的二阶导数不会随着学习的进行而消失。一阶导数由以下公式给出:

塔恩(z)=[(1-z)]t0[2]t1]z

在 LSTM，有四个主要的门控制着信息和记忆的流动。h(t-1)控制前一个 LSTM 细胞的输出，c(t-1)控制前一个 LSTM 细胞的记忆。X(t)是当前单元格的输入。我们将从乙状结肠激活单元开始理解从左到右的门。

*   **正门:**这控制前一个 LSTM 单元的 sigmoid 激活输出[h(t-1)带权重(W)]和带一些偏置的当前输入 X(t)。

    f<sub>=σ(w<sub>【f】</sub>【h<sub>【t】</sub>【x<sub>]+b</sub></sub>

*   **输入门:**这也控制权重为 w 的前一个 h(t-1)的 sigmoid 激活输出。此外，还有一个“tanh”激活应用于带有相应偏置的输入 X(t)的前一个输出。

    I<sub>t</sub>=σ(wi[h]T2【t】-x<sub>t</sub>]+b<sub>I</sub>

    c<sub>**= tanh(WC[h]<sub>【x】<sub>]+b<sub>【c】</sub></sub></sub>**</sub>

***   **遗忘门:**它控制着我们希望 LSTM 网络遗忘的信息。这是使用 c(t-1)通过以下公式计算的:

    C<sub>t</sub>= f<sub>t</sub>* C<sub>t-1</sub>+I<sub>t</sub>* C<sub>t</sub>T10】T11 `T13】

    *   **输出门:**这包含一个 sigmoid 激活单元，它决定我们想要输出什么。随后是“tanh”激活，它箝位在(-1，1)之间，以获得当前 LSTM 含量 h(t):

    o <sub>t</sub> =σ(凡【h】<sub>【t】</sub>x<sub>]+b<sub>或</sub>)</sub>

    h(t)= o <sub>t</sub> * tanh(C <sub>t</sub>** 

 **通过这些大门，LSTM 网络试图减轻传统 RNN 的弊端。我们已经观察了 Model.py 脚本中的情况，其中我们提到了 RNNs 用例。

**LSTM 在** **ML 特工模特**

在 ML 代理中，我们已经看到了 rnn 对于训练 Model.py 脚本中的模型的重要性。这又是一个同样的例子。

```
s_size = input_state.get_shape().as_list()[1]
m_size = memory_in.get_shape().as_list()[1]
lstm_input_state = tf.reshape(input_state, shape=[-1, sequence_length, s_size])
memory_in = tf.reshape(memory_in[:, :], [-1, m_size])
half_point = int(m_size / 2)
with tf.variable_scope(name):
            rnn_cell
           = tf.nn.rnn_cell.BasicLSTMCell(half_point)
            lstm_vector_in = tf.nn.rnn_cell.LSTMStateTuple(
             memory_in[:, :half_point],
             memory_in[:, half_point:]
            )
            recurrent_output, lstm_state_out
             = tf.nn.dynamic_rnn(
                rnn_cell, lstm_input_state
               , initial_state=lstm_vector_in
            )

recurrent_output = tf.reshape(recurrent_output, shape=[-1, half_point])
return recurrent_output, tf.concat([lstm_state_out.c, lstm_state_out.h], axis=1)

```

这里重要的部分是“BasicLSTMCell”，它的单元用“m_size/2”表示。这些表示 LSTM 街区的单元数量。“LSTMStateTuple”方法用于包括在处理观察值时使用的存储器范围。在内部，这些信号通过一系列的 sigmoid 和 tanh 激活的细胞来获得电流输出。现在这个模型文件在 ML 代理中跨 SAC/GAIL 和其他算法使用。我们可以查看 SAC 的源代码，特别是了解 LSTM 网络在该算法中的使用。LSTMs 的不同属性也存在于 trainer_config.yaml 文件中，该文件用于训练 PPO 代理。我们将在下一章研究对抗性的基于记忆的网络时更深入地探讨 LSTM 网络。

## 简化的超参数优化

当谈到深度 RL 时，优化是一个重要的方面。在本章的所有章节中，我们看到有许多超参数控制着开关策略和无模型算法，包括神经网络架构。我们将考虑出现在 trainer_config.yaml 文件中的 ML 代理内的 PPO 配置。对于基于 SAC 的配置，除了某些例外，大多数参数保持不变。一些常见的超参数如下。

*   **培训师:** SAC-或 PPO-根据政策要求

*   **总结频率:**控制训练总结的频率。这也可以用 TensorBoard 来看。

*   **批量:**控制训练的批量。这应该比缓冲区大小小几倍。通常对于 PPO(连续)来说，它应该在 512 和 5120 之间，而对于 SAC(连续)来说，它应该在 128 和 1024 之间。对于离散变量，应该是 32 到 512。

*   **缓冲区大小:**这控制每个策略的观察/动作/状态的数量。它通常应该大于批量大小。在 PPO 中，这大约是 2048 到 409600，对于 SAC，这应该是大约 5000 到 1000000。

*   **隐藏单元:**全连接神经网络架构中隐藏单元的数量。通常，它应该在 32 到 512 之间。

*   **学习率:**梯度下降算法的学习率。这通常应在 1e-5 至 1e-3 (0.00001 至 0.001)的范围内。

*   **学习率计划:**它控制我们在 Models.py 脚本中看到的学习率的变化。它可以是线性的，也可以是常数。对于 PPO，学习率采用线性衰减策略，而对于 SAC，学习率保持不变。

*   **最大步数:**算法所需的最大步数；这个应该在 5e5 和 1e7 以内。

*   **归一化:**使用移动平均值和方差控制矢量观测值的归一化。

*   **Num layer** **:** 控制观察层之后有多少隐藏层。通常应介于 1 和 3 之间(CNN)。

*   **时间范围:**在保存到重放缓冲区之前，收集到的观察、状态和动作的次数。

*   **视觉编码器类型:**控制视觉观察的编码器类型。正如我们在前面几节中看到的，它可以是“cnn_natural”、“resnet”及其变体。

*   **Init_path:** 控制之前用于训练的教练的路径。先前使用的过去行为神经网络模型用该参数表示。

*   **Threaded:** 这控制了当环境被步进时，模型是否可以更新它的权重。当使用类似 PPO 的策略训练算法时，建议将其保持为 False。

### PPO 的超参数

除了标准训练参数之外，这些是在 PPO 情况下重要的某些超参数。

*   **Beta:** 这控制熵正则化的强度，以便代理可以在训练期间探索空间。该值通常介于 1e-4 和 1e-2 之间。

*   **Epsilon:** 这控制着政策偏离旧政策的速度。较小的值具有稳定的策略更新。该值通常介于 0.1 和 0.3 之间。

*   **λ:**用于计算 GAE 的正则化因子。通常，低值类似于使用当前优势值，而高值表示使用从环境接收的实际优势(高方差)。该值通常介于 0.9 和 0.95 之间。

*   **Num_epoch:** 应用梯度下降步骤前通过缓冲器的次数。降低这个值将会导致更慢和更稳定的更新。该值通常介于 3 和 10 之间。

### 通过张量板训练分析超参数

我们将研究一个名为 Tiny Agent 的 Unity 环境，这是一个赛车环境。虽然我们将在下一章探讨这一点，但当应用不同的超参数集时，显示训练中所做的权衡是很重要的。为了训练这个特定的环境，我们使用了 PPO 的默认超参数集。

```
default:
  trainer: ppo
  batch_size: 1024
  beta: 5.0e-3
  buffer_size: 10240
  epsilon: 0.2
  hidden_units: 128
  lambd: 0.95
  learning_rate: 3.0e-4
  learning_rate_schedule: linear
  max_steps: 5.0e5
  memory_size: 128
  normalize: false
  num_epoch: 3
  num_layers: 2
  time_horizon: 64
  sequence_length: 64
  summary_freq: 100
  use_recurrent: false
  vis_encode_type: simple
  reward_signals:
    extrinsic:
      strength: 1.0
      gamma: 0.99

```

为了便于训练和理解，我们修改了某些超参数的值，如 gamma、batch_size、学习速率和隐藏单元。虽然通常建议遵循本节中提到的超参数指南，但据观察，作为批量大小为 1024 的隐藏单元，256 至 512 的兼容值范围提供了具有稳定更新和适当收敛的合适学习曲线。在这种情况下，我们将分析 TensorBoard 中使用 Tiny Agent 的不同训练实例制作的图，如图 [5-40](#Fig40) 所示。

![../images/502041_1_En_5_Chapter/502041_1_En_5_Fig40_HTML.jpg](../images/502041_1_En_5_Chapter/502041_1_En_5_Fig40_HTML.jpg)

图 5-40

微型代理中超参数调谐

虽然我们可以相应地使用超参数并在 Tensorboard 中可视化培训，但重要的是要提到，从“mlp”到“cnn”再到其他网络(“mlplstm”)的政策变化也在培训过程中发挥了重要作用。当我们使用 CNN 和密集神经网络来训练 GridWorld 环境时，我们可以将这种效果可视化。在 PPO 的默认超参数中，我们可以将“use_recurrent”值改为 True，然后它将成为 PPO 策略训练的 LSTM 形式。

## 摘要

我们已经到了围绕复杂和基本概念的这一章的结尾。总结一下:

*   我们了解了从密集网络到卷积网络的神经网络结构的基本原理。我们还学习了如何使用 Keras 和 Tensorflow 编写自定义神经网络模型。

*   我们研究了卷积-2D 神经网络的不同变体及其相关组件，如激活、内核之间的关系、步长、填充以及不同的相关层，如最大池。我们还了解到 Resnet 和 VGG-16 是传统的最先进型号。

*   然后，我们深入研究了深度 RL 算法。我们理解两种范式——政策上的学习和政策外的学习。在 on-policy 中，我们看到了 Q-learning 和 SARSA 在传统 RL 空间中的对比。然后，我们深入探讨了不同策略的深度 RL 算法。

*   我们研究了策略梯度，并使用 OpenAI Gym 环境构建了相应的代理。然后，我们研究了学习的行动者批评范式，包括优势行动者批评(A2C)和异步优势行动者批评(A3C ),并使用基线和自定义代码构建了使用这些策略的代理。然后我们研究了演员评论家克罗内克因子信赖域和随机演员评论家算法。

*   我们学习了 PPO 算法及其从信任区域策略优化(TRPO)中派生出来的算法，包括编写自定义神经网络模型和使用基线。我们了解了在 PPO 中剪辑以稳定训练的重要性。

*   之后，我们学习了非策略算法，它专门涵盖了深度 Q 网络(DQN)及其变体，如决斗 DQN 和双 DQN。我们还分析了这些算法的重要性，从缓冲区中抽取过去的状态。然后，我们探讨了非政策演员评论家，如宏基。

*   然后，我们研究了软行为者批评(SAC ),我们引入了熵正则化作为稳定非策略算法的手段。

*   我们简要了解了一种依赖于行为克隆和一般敌对网络的无模型 RL 算法，称为 GAIL，并看到了它如何通过使用生成器和鉴别器来模仿不同的策略(如 SAC)。

*   然后，我们为 Puppo 创建了一个 PPO 代理，它使用关节连接和矢量化的运动方向和角度力来移动代理。然后，我们使用 PPO 学习的默认参数训练它，并使用 TensorBoard 可视化它们。

*   然后我们学习了 Python API 及其与 Unity 环境的交互。我们还将 Unity 环境转换为等效的健身房环境，并使用基线算法(DQN 和 PPO)对其进行训练。

*   然后，我们了解了 ML 代理中的一些最重要的脚本(Model Overrider 脚本和 Models.py 脚本)以及 PPO 策略中的 trainers 和 optimizers Python 文件，并了解了 Unity 如何在内部实现神经网络以及 PPO 策略。

*   然后，我们了解了基于记忆的训练中的 LSTM 网络，并简要地看到了这个版本的 RNNs 在某些策略上训练 ML 代理的重要性。

*   在本章的最后一节，我们深入研究了 Unity ML Agents 中超参数的优化。我们学习了 SAC 和 PPO 通用的某些超参数的默认值和标准值。然后我们独家看到了 PPO 的一些特征超参数。最后，我们用不同的超参数集训练了一个名为 Tiny Agent 的 Unity 环境(我们将在下一章讨论),并用 TensorBoard 观察它。

这总结了整个一章，其中有很多核心概念。在下一章中，我们将研究一些复杂的算法、课程学习和对抗性代理，并将构建一个购物车游戏。**********