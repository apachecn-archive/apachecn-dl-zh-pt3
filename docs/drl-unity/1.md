# 1.强化学习简介

强化学习(RL)是一种基于奖励和行动的学习算法范式。基于状态的学习范式不同于一般的监督和非监督学习，因为它通常不试图在未标记或标记的数据集合中找到结构推理。通用 RL 依赖于有限状态自动化和决策过程，这些过程有助于找到优化的基于奖励的学习轨迹。RL 领域非常依赖于目标搜索、随机过程和决策理论算法，这是一个活跃的研究领域。随着更高阶深度学习算法的发展，在该领域已经取得了巨大的进步，以创建能够通过使用梯度收敛技术和复杂的基于记忆的神经网络来实现目标的自学习代理。本章将重点介绍马尔可夫决策过程(MDP)的基础知识、隐马尔可夫模型(HMMs)和状态枚举的动态规划、贝尔曼迭代算法，以及价值和策略算法的详细演练。在所有这些部分中，将有相关的 python 笔记本，以便更好地理解概念，以及用 Unity(2018 . x 版)制作的模拟游戏。

RL 学院的基本方面是主体和环境。Agent 指的是一个使用学习算法逐步尝试和探索奖励的对象。代理人试图优化一个合适的路径，以达到最大化回报的目标，并在这个过程中，试图避免惩罚国家。环境是代理周围的一切；这包括状态、障碍和奖励。环境可以是静态的，也可以是动态的。如果代理在探索不同状态时有足够的缓冲存储器来保持朝向目标的正确轨迹，则静态环境中的路径收敛会更快。动态环境对代理人提出了更大的挑战，因为没有明确的轨迹。第二个用例需要足够的深度记忆网络模型，如双向长短期记忆(LSTM ),以保留在动态环境中保持静态的某些关键观察。象征性的通用强化学习可以如图 [1-1](#Fig1) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig1_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig1_HTML.jpg)

图 1-1

强化学习中主体与环境的相互作用

控制和支配代理和环境之间的交互的变量集包括{状态(S)，奖励(R)，动作(A)}。

*   状态是环境中提供的一组可能的枚举状态:{s <sub>0</sub> ，s <sub>1</sub> ，s <sub>2</sub> ，… s <sub>n</sub> }。

*   奖励是环境中特定状态下可能出现的奖励集合:{r <sub>0</sub> ，r <sub>1</sub> ，r <sub>2</sub> ，…，r <sub>n</sub> }。

*   行动是代理可以采取的使其报酬最大化的可能行动的集合:{A <sub>0</sub> ，A <sub>1</sub> ，A <sub>2</sub> ，… A <sub>n</sub> }。

## 开放式健身环境:钢管舞

为了理解这些在 RL 环境中的角色，让我们试着研究 OpenAI gym 的 CartPole 环境。OpenAI gym 包括许多用于研究和学习经典 RL 算法、机器人和深度 RL 算法的环境，这在 Unity machine learning(ML)agents Toolkit 中用作包装器。

CartPole 环境可以描述为一个经典的物理模拟系统，其中一个杆连接到 cart 的一个“未驱动”关节上。手推车可以沿着无摩擦的轨道自由移动。对系统的约束包括对小车施加+1 和-1 的力。钟摆开始直立，目标是防止它翻倒。杆保持直立的每个时间戳提供+1 的奖励。当倾斜角度大于法线 15 度时，情节终止(惩罚)。如果推车从中心线向任一方向移动超过 2.4 个单位，该集终止。图 [1-2](#Fig2) 描绘了环境。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig2_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig2_HTML.jpg)

图 1-2

来自露天体育馆的钢管环境

该环境中可能的状态、奖励和动作集包括:

*   状态:长度为 4 的数组。:[小车位置、小车速度、磁极角度、磁极尖端速度]如[`4.8000002e+00,3.4028235e+38 ,4.1887903e-01,3.4028235e+38`]

*   奖励:每个时间戳+1，柱子保持直立

*   动作:大小为 2 的整数数组:[左方向，右方向]，它控制购物车的运动方向，例如[-1，+1]

*   终止:如果小车偏离中心超过 2.4 个单位或摆锤倾斜超过 15 度

*   目标:保持钟摆或柱子直立 250 个时间步并获得超过 100 点的奖励

## 用于 ML 代理和深度学习的 Python 的安装和设置

为了可视化这个环境，需要安装 Jupyter notebook，它可以从 Anaconda 环境中安装。下载 Anaconda(Python 的推荐最新版本)，Jupyter 笔记本也会被安装。

下载 Anaconda 还会安装像 numpy、matplotlib 和 sklearn 这样的库，这些库用于一般的机器学习。像 IPython Console、Spyder、Anaconda Prompt 这样的控制台和编辑器也随 Anaconda 一起安装。Anaconda 提示符应该设置为环境路径变量。终端的预览如图 [1-3](#Fig3) 所示。

Note

Anaconda Navigator 随 Anaconda 一起安装。这是一个交互式仪表板应用程序，其中提供了下载 Jupyter notebook、Spyder、IPython 和 JupyterLab 的选项。这些应用程序也可以通过点击来启动。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig3_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig3_HTML.jpg)

图 1-3

Anaconda 导航终端

Jupyter 笔记本可以使用 pip 命令安装，如下所示:

```
pip3 install –upgrade pip
pip3 install jupyter notebook

```

要运行 Jupyter 笔记本，请打开 Anaconda 提示符或命令提示符，并运行以下命令:

```
jupyter notebook

```

或者，Google Colaboratory (Google Colab)在云上运行 Jupyter 笔记本，并保存到本地 Google drive。这也可以用于笔记本共享和协作。谷歌联合实验室如图 [1-4](#Fig4) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig4_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig4_HTML.jpg)

图 1-4

谷歌协作笔记本

首先，创建一个新的 Python3 内核笔记本，并将其命名为 CartPole environment。为了模拟和运行环境，需要安装某些库和框架。

*   安装 Gym : Gym 是 OpenAI 创建的环境集合，包含了开发 RL 算法的不同环境。

在 Anaconda 提示符下运行命令，命令提示符:

```
pip install gym

```

或者从 Jupyter 笔记本或 Google Colab 笔记本运行此命令

*   安装 Tensorflow 和 Keras: Tensorflow 是 Google 开发的开源深度学习框架，将用于在 deep RL 中创建神经网络层。Keras 是 Tensorflow 上的一个抽象(API ),包含了 Tensorflow 的所有内置功能，易于使用。这些命令如下所示:

```
!pip install gym

```

```
pip install tensorflow>=1.7
pip install keras

```

这些命令用于通过 Anaconda 提示符或命令提示符进行安装。本书后面用于 Unity ML 代理的 Tensorflow 版本是 1.7。但是，为了与 Unity ML 代理集成，也可以使用 Tensorflow 版。如果由于版本不匹配而出现问题，则可以通过查看 Unity ML 代理版本和与 Tensorflow 兼容性的文档来解决，后者可以通过使用 pip 命令重新安装。

对于 Tensorflow 和 Keras 的 Jupyter notebook 或 Colab 安装，需要以下命令:

```
!pip install tensorflow>=1.7
!pip install keras

```

Note

Tensorflow 拥有每天发布的带有版本号的夜间版本，这可以在 Tensorflow 的 Python 包索引(Pypi)页面中查看。这些版本通常被称为 tf-nightly，可能与 Unity ML 代理的兼容性不稳定。然而，官方版本推荐与 ML 代理集成，而夜间版本也可以用于深度学习。

*   安装健身房 pyvirtualdisplay 和 python opengl:这些库和框架(为 OpenGL API 构建)将用于在 Colab notebook 中渲染健身房环境。在 Windows 中本地安装 xvfb 存在问题，因此 Colab 笔记本可以用于显示健身房环境培训。Colab notebook 中的安装命令如下:

```
!apt-get install –y xvfb python-opengl > /dev/null 2>&1
!pip install gym pyvirtualdisplay > /dev/null 2>&1

```

一旦安装完成，我们就可以深入到 CartPole 环境中，并尝试获得关于环境、奖励、状态和动作的更多信息。

## 玩转 CartPole 环境进行深度强化学习

打开“Cartpole-Rendering.ipynb”笔记本。它包含设置环境的启动代码。第一部分包含导入笔记本中的库的导入语句。

```
import gym
import numpy as np
import matplotlib.pyplot as plt
from IPython import display as ipythondisplay

```

下一步是设置显示窗口的尺寸，以便在 Colab 笔记本中可视化环境。这使用 pyvirtualdisplay 库。

```
from pyvirtualdisplay import Display
display = Display(visible=0, size=(400, 300))
display.start()

```

现在，让我们使用 gym.make 命令从 Gym 加载环境，并查看状态和动作。观察状态指的是环境变量，它包含像小车速度和极点速度这样的关键因素，是一个大小为 4 的数组。动作空间是一个大小为 2 的数组，它引用二元动作(向左或向右移动)。观察空间还包含高值和低值作为问题的边界值。

```
env = gym.make("CartPole-v0")
#Action space->Agent
print(env.action_space)
#Observation Space->State and Rewards
print(env.observation_space)
print(env.observation_space.high)
print(env.observation_space.low)

```

如图 [1-5](#Fig5) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig5_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig5_HTML.jpg)

图 1-5

标杆环境下的观察与行动空间

运行后，详细信息会出现在控制台中。细节包括不同的动作空间以及观察步骤。

让我们尝试运行环境 50 次迭代，并检查累积的奖励值。这将模拟 50 次迭代的环境，并提供对代理如何利用基线 OpenAI 模型平衡自身的洞察。

```
env = gym.make("CartPole-v0")
env.reset()
prev_screen = env.render(mode='rgb_array')
plt.imshow(prev_screen)

for i in range(50):
  action = env.action_space.sample()
  #Get Rewards and Next States
  obs, reward, done, info = env.step(action)
  screen = env.render(mode='rgb_array')
  print(reward)
  plt.imshow(screen)
  ipythondisplay.clear_output(wait=True)
  ipythondisplay.display(plt.gcf())

  if done:
    break

ipythondisplay.clear_output(wait=True)
env.close()

```

最初使用 env.reset()方法重置环境。对于 50 次迭代中的每一次，env.action_space.sample()方法都试图对最有利的状态或奖励状态进行采样。采样方法可以使用表格离散 RL 算法，如 Q-learning，或连续深度 RL 算法，如 deep-Q-network(DQN)。有一个折扣因子，在每次迭代开始时调用，以折扣前一个时间戳的奖励，极点代理试图相应地找到新的奖励。env.step(action)从“记忆”或以前的动作中进行选择，并通过尽可能长时间保持直立来尝试最大化其回报。在每个动作步骤结束时，显示会发生变化，以呈现杆的新状态。如果迭代已经完成，循环最终会中断。env.close()方法关闭与健身房环境的连接。

这帮助我们理解了状态和奖励是如何影响一个代理人的。我们将进入对深度 Q 学习算法建模的深入研究的细节，以提供对 CartPole 问题的更快和最优的基于奖励的解决方案。该环境具有离散的观察状态，并且可以通过使用表格 RL 算法来解决，如基于马尔可夫的 Q 学习或 SARSA。

深度学习通过将离散状态转换为连续分布来提供更多优化，然后尝试应用高维神经网络来将损失函数收敛到全局最小值。这是通过使用诸如 DQN、双深度 Q 网络(DDQN)、决斗 DQN、行动者评论(AC)、邻近策略操作(PPO)、深度确定性策略梯度(DDPG)、信任区域策略优化(TRPO)、软行动者评论(SAC)的算法来支持的。笔记本的后一部分包含了对 CartPole 问题的深度 Q 学习实现，这将在后面的章节中解释。为了突出这段代码的某些重要方面，有一个用 Keras 制作的深度学习层，并且对于每次迭代，状态、动作和奖励的集合都存储在重放内存缓冲区中。基于缓冲存储器的先前状态和先前步骤的回报，极点代理试图在 Keras 深度学习层上优化 Q 学习函数。

## 用张量板可视化

在训练过程的每一次迭代中，损失的可视化标志着深度 Q 学习试图以直立的方式优化极点的位置并平衡动作阵列以获得更大回报的程度。这个可视化是在 TensorBoard 中实现的，可以通过在 Anaconda 提示符下键入以下行来安装。

```
pip install tensorboard

```

要在 Colab 或 Jupyter Notebook 中启动 TensorBoard 可视化，下面几行代码会有所帮助。虽然控制台会提示使用最新版本的 Tensorflow (tf>=2.2)，但对此没有硬性要求，因为它与所有 Tensorflow 版本兼容。使用 Keras 的 Tensorboard 设置也可以使用旧版本(Tensorboard)实现，如 1.12 或低至 1.2。启动 TensorBoard 的代码段在不同版本中是相同的。建议在 Colab 中导入这些库，因为在这种情况下，我们可以在运行时灵活地升级/降级不同版本的库(Tensorflow、Keras 或其他)。这也有助于解决本地安装时不同版本的兼容性问题。我们也可以在本地为 Tensorflow 1.7 版本安装 Keras 2.1.6。

```
from keras.callbacks import TensorBoard
% load_ext tensorboard
% tensorboard –-logdir log

```

TensorBoard 从 6006 端口开始。为了在日志中包含训练数据的片段，在运行时会创建一个单独的日志文件，如下所示:

```
tensorboard_callback = TensorBoard(
log_dir='./log', histogram_freq=1,
write_graph=True,
write_grads=True,
batch_size=agent.batch_size,
write_images=True)

```

为了引用 tensorboard_callbacks 来存储数据，将 callbacks =[tensor board _ callback]作为参数添加到 model.fit()方法中，如下所示:

```
self.model.fit(np.array(x_batch),np.array(y_batch),batch_size=len(x_batch),verbose=1,callbacks=[tensorboard_callback])

```

最终结果显示了一个张量板图，如图 [1-6](#Fig6) 所示。：

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig6_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig6_HTML.jpg)

图 1-6

基于深度 Q 学习的横竿问题张量板可视化

总的来说，我们对 RL 是什么以及它是如何被状态、行为和奖励控制的有了一些了解。我们已经看到了代理人在环境中的角色，以及实现回报最大化的不同途径。我们已经学会了设置 Jupyter 笔记本和 Anaconda 环境，还安装了一些关键的库和框架，它们将在开发过程中被广泛使用。有一个系统的方法来理解 OpenAI 健身房的钢管舞环境，作为一个经典的 RL 问题，以及理解环境中的状态和奖励。最后，我们开发了一个小车环境的微型模拟，该模拟将使小车直立 50 次迭代，并且还使用深度 Q 学习模型进行了可视化。细节和实现将在后面的章节和 Unity ML 代理一起深入讨论。下一部分涉及使用 Unity Engine 理解 MDP 和决策理论，并将创建相同的模拟。

## Unity 游戏引擎

Unity Engine 是一个跨平台引擎，不仅用于创建游戏，还用于模拟、视觉效果、电影摄影、建筑设计、扩展现实应用，尤其是机器学习研究。我们将集中精力理解 Unity Technologies 开发的开源机器学习框架——即 Unity ML Toolkit。在撰写本书时，最新发布的 1.0 版本有几个新的特性和扩展、代码修改和模拟，这些将在后续章节中深入讨论。该工具包建立在 OpenAI Gym 环境上，作为一个包装器，并在 Python API 和 Unity C# Engine 之间进行通信，以构建深度学习模型。尽管在最新版本中，工具包的工作方式有了根本性的改变，但是 ML 工具包的核心功能保持不变。我们将广泛使用 Tensorflow 库和 Unity ML 代理，通过自定义 C#代码对模型进行深度推理和训练，并且还将尝试通过使用基线模型来了解健身房环境中的学习情况，以获得最佳性能指标。图 [1-7](#Fig7) 显示了 ML 代理工具包中的环境预览。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig7_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig7_HTML.jpg)

图 1-7

Unity 机器学习工具包

Note

我们将使用 Unity 版本 2018.4x 和 2019 以及 Tensorflow 版本 1.7 和 ML 代理 Python API 版本 0.16.0 和 Unity ML 代理 C#版本(1.0.0)。但是，对于任何高于 2018.4.x 的 Unity 版本，功能保持不变。安装 Unity Engine 和 ML 代理的详细步骤将在后续章节中介绍。

## 马尔可夫模型和基于状态的学习

在开始使用 Unity ML Toolkit 之前，让我们了解一下基于状态的 RL 的基本原理。马尔可夫决策过程(MDP)是一种随机过程，它试图根据当前状态的概率来枚举未来状态。有限马尔可夫模型依赖于由 *q*(s，a)* 表示的当前状态的信息，其包括状态、动作对。在本节中，我们将重点介绍如何在不同决策之间生成转换状态，并基于 Unity Engine 中的这些转换创建模拟。将会有一个关于状态枚举和隐马尔可夫模型(HMM)如何帮助一个代理在一个环境中找到一个合适的轨迹来获得它的统一回报的演练。

有限 MDP 可以被认为是集合的集合:{S，A，R}，其中奖励 R 类似于状态空间 S 中奖励的任何概率分布。对于状态*S*<sub>*I*</sub>€*S*和*R*<sub>*I*</sub>*€R*的特定值，在给定先前状态和动作的特定值的情况下，这些值有可能出现在时间 *t、*

p (s <sub>i</sub> ，r <sub>i</sub> | s，A)= P<sub>R</sub>{ S<sub>t</sub>= S<sub>I</sub>，R<sub>t</sub>= R<sub>I</sub>| S<sub>t-1</sub>= S，A <sub>t-1</sub> = a}

决策过程通常涉及转移概率矩阵，该矩阵提供了特定状态向前移动到另一状态或返回到其先前状态的概率。马尔可夫模型的示意图如图 [1-8](#Fig8) 所示。：

Note

Andrey Andreyevich Markov 于 1906 年在随机过程中引入了 Markov 链的概念。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig8_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig8_HTML.jpg)

图 1-8

马尔可夫模型的状态转移图

### 马尔可夫模型中的状态概念

状态转换图提供了具有状态 S 和 P 的二元链模型。状态 S 保持在其自身状态的概率是 0.7，而进入状态 P 的概率是 0.3。同样，状态 P 到 S 的转移概率是 0.2，而 P 的自转移状态概率是 0.8。根据概率定律，相互转移概率和自转移概率之和将为 1。这允许我们生成 2 X 2 阶的转移矩阵，如图 [1-9](#Fig9) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig9_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig9_HTML.jpg)

图 1-9

状态转移矩阵

每个操作结束时的转移矩阵产生不同状态的自转移和交叉转移的不同值。这在数学上可以形象化为计算转移矩阵的幂，其中幂是我们要求模拟发生的迭代次数，如下所述:

T(t+k) = T(t) <sup>k</sup> k€ R

该公式示出了在 k 次迭代之后的转移矩阵的状态，该转移矩阵的状态由初始状态时间 k 处的转移矩阵的幂给出，假设 k 属于实数。

让我们通过用初始概率初始化单个状态 S 和 P 来扩展这个想法。如果我们认为 V 是包含两个状态的初始概率的数组，那么在模拟的 k 次迭代之后，状态 F 的最终数组可以如下获得:

F(t+k)=V(t)*T(t) <sup>k</sup>

### Python 中的马尔可夫模型

这是一个迭代的马尔可夫过程，根据转移和初始概率枚举状态。打开“Markov models . ipynb”jupy ter 笔记本，让我们试着了解一下过渡模型的实现。

```
import numpy as np
import pandas as pd

transition_mat=np.array([[0.7,0.3],
                         [0.2,0.8]])

intial_values= np.array([1.0,0.5])

#Transitioning for 3 turns
transition_mat_3= np.linalg.matrix_power(transition_mat,3)
#Transitioning for 10 turns
transition_mat_10= np.linalg.matrix_power(transition_mat,10)
#Transitioning for 35 turns
transition_mat_35= np.linalg.matrix_power(transition_mat,35)
#output estimation of the values
output_values= np.dot(intial_values,transition_mat)
print(output_values)
#output values after 3 iterations
output_values_3= np.dot(intial_values,transition_mat_3)
print(output_values_3)
#output values after 10 iterations
output_values_10= np.dot(intial_values,transition_mat_10)
print(output_values_10)
#output values after 35 iterations
output_values_35= np.dot(intial_values,transition_mat_35)
print(output_values_35)

```

我们导入了 numpy 和 pandas 库，这将帮助我们计算矩阵乘法。S 和 P 的集合初始状态分别设置为 1.0 和 0.5。转移矩阵如前所述被初始化。然后，我们分别计算 3 次、10 次和 35 次迭代的转移矩阵的值，利用每个阶段的输出，我们乘以初始概率数组。这为我们提供了每个状态的最终值。您可以更改概率值，以获得关于特定状态保持不变或转换到另一个状态的程度的更多结果。

在第二个例子中，我们提供了一个三进制转换系统如何基于初始和转换概率迁移到不同状态的可视化。可视化如图 [1-10](#Fig10) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig10_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig10_HTML.jpg)

图 1-10

马尔可夫状态的转移可视化

### 下载和安装 Unity

现在，让我们试着模拟一个基于这个统一马尔可夫状态原理的游戏。我们将使用 Unity 2018.4 版本，它也将兼容 2019 和 2020 版本。第一步是安装 Unity。从 Unity 官方网站下载 Unity Hub。Unity Hub 是一个仪表板，包含 Unity 的所有版本，包括测试版以及教程和入门包。下载安装 Unity Hub 后，我们可以选择 2018.4 以上的版本。接下来，我们继续下载和安装版本，这将需要一些时间。在 Windows 的 C: drive 中应该有足够的空间来完成下载，即使我们在一个单独的驱动器中下载。一旦安装完成，我们可以打开 Unity 并开始创建我们的模拟和场景。Unity Hub 出现如图 [1-11](#Fig11) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig11_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig11_HTML.jpg)

图 1-11

Unity Hub 和安装 Unity

下载名为 DeepLearning 的示例项目文件，其中包含本书课程的所有代码。需要下载和安装 Unity ML Toolkit 的预览包，因为文件夹中的其他项目依赖于它们。下载后，如果控制台收到与 Barracuda 引擎或 ML 代理相关的错误消息(大多与无效方法相关)，请转到:

```
Windows > Package Manager

```

在搜索栏中输入 ML agents，会出现 ML agents 预览包(1.0)选项。点击“安装”,在 Unity 中本地下载与 ML 代理相关的预览包。要进行交叉验证，请打开 Packages 文件夹并导航到“manifest”。Json”源文件。在 Visual Studio 代码或任何编辑器中打开它，并检查以下行:

```
"com.unity.ml-agents":"1.0.2-preview"

```

如果错误仍然存在，那么我们可以通过手动下载 Unity ML 代理来解决这个问题，或者在 Anaconda 提示符下使用以下命令:

```
pip install mlagents

```

或者也可以从 Unity ML Github 资源库下载。但是，安装指南将在第 [3](3.html) 章中介绍。

### 单位概率下的马尔可夫模型

打开环境文件夹，并导航到 MarkovPuppo.exe 统一场景文件。

双击运行它，您将能够看到类似图 [1-12](#Fig12) 的内容。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig12_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig12_HTML.jpg)

图 1-12

MarkovPuppo Unity 场景应用程序

这个游戏是一个模拟游戏，Puppo (Puppo，柏林联合的柯基犬)试图在马尔科夫过程中找到模拟的棍子。用预定义的概率状态初始化棒，并提供转移矩阵。对于模拟的每次迭代，具有最高自转移概率的棒被选择，而其余的被破坏。Puppo 的任务是在每次迭代中找到这些棍子，当他能够正确地到达一根棍子时，给他 6 秒钟的休息时间。由于转移概率的计算速度非常快，Puppo 采取的步骤是瞬时的。这是一个纯粹的随机分布的马尔可夫状态，其中的状态转移概率是随时计算的。让我们试着深入挖掘 C#代码，以便更好地理解它。

在 Unity 中打开 DeepLearning 项目，并导航到 Assets 文件夹。在文件夹中，尝试找到 MarkovAgent 文件夹。其中包含称为脚本、预设和场景的文件夹。在 Unity 中打开 MarkovPuppo 场景，然后按 play。我们将能够看到普普试图定位随机采样的马尔可夫棒。让我们先试着了解一下场景。

场景由左侧的场景层次和右侧的检查器细节组成，随后是项目、底部的控制台选项卡和中间的游戏视图。在层级中，找到“平台”游戏对象并点击下拉菜单。在游戏对象内部，有一个“CORGI”游戏对象。单击它在场景视图中定位，并在右侧的检查器窗口中打开细节。这是 Puppo 预置，它有一个附加的脚本叫做“马尔科夫代理”通过点击下拉菜单可以进一步探索预设，将会有几个关节和刚体组件附加，这将使物理模拟 Puppo。场景视图如图 [1-13](#Fig13) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig13_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig13_HTML.jpg)

图 1-13

Markov Puppo 场景的场景视图，包括层次和检查器

检查器窗口如图 [1-14](#Fig14) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig14_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig14_HTML.jpg)

图 1-14

检查器标签和脚本

在 Visual Studio 代码或 MonoDevelop(您选择的任何 C#编辑器)中打开 Markov 代理脚本，让我们尝试理解代码库。在代码开始时，我们必须导入某些库和框架，如 UnityEngine、System 等。

```
using System.Collections;
using System.Collections.Generic;
using UnityEngine;
using System;
using Random=UnityEngine.Random;
public class MarkovAgent : MonoBehaviour
 {
    public GameObject Puppo;
    public Transform puppo_transform;
    public GameObject bone;
    public GameObject bone1;
    public GameObject bone2;
    Transform bone_trans;
    Transform bone1_trans;
    Transform bone2_trans;
    float[][] transition_mat;
    float[] initial_val=new float[3];
    float[] result_values=new float[3];
    public float threshold;
    public int iterations;
    GameObject active_obj;
    Vector3 pos= new Vector3(-0.53f,1.11f,6.229f);

```

该脚本派生自 MonoBehaviour 基类。在里面，我们声明了游戏对象、变换和其他我们想在代码中使用的变量。游戏对象“Puppo”引用了 Puppo Corgi 代理，在检查器窗口的图 [1-14](#Fig14) 中也引用了它。游戏对象“骨骼”、“骨骼 1”和“骨骼 2”是场景中的三个棍子目标，由马尔可夫状态随机化。接下来，我们有一个转移矩阵(名为“transition_mat”，一个浮点值矩阵)，三个棒的初始概率数组(名为“initial_val”，一个大小为 3 的浮点数组)，以及一个包含每次迭代后的概率的结果概率数组(名为“result_val”，一个大小为 3 的浮点数组)。变量“迭代次数”表示模拟的迭代次数。游戏对象“active_obj”是另一个变量，它包含在每次迭代中保持活动状态的最有可能的自转换棒。最后一个变量是名为“pos”的 Vector3，它包含每次迭代后 Puppo 的种子位置。接下来，我们将讨论创建转移矩阵、初始值数组的细节，并尝试理解迭代是如何形成的。

```
void Start()
    {
        puppo_transform=GameObject.FindWithTag("agent").
        GetComponent<Transform>();
        bone=GameObject.FindWithTag("bone");
        bone1=GameObject.FindWithTag("bone1");
        bone2=GameObject.FindWithTag("bone2");
        bone_trans=bone.GetComponent<Transform>();
        bone1_trans=bone1.GetComponent<Transform>();
        bone2_trans=bone2.GetComponent<Transform>();
        transition_mat=create_mat(3);

        initial_val[0]=1.0f;
        initial_val[1]=0.2f;
        initial_val[2]=0.5f;

        transition_mat[0][0]=Random.Range(0f,1f);
        transition_mat[0][1]=Random.Range(0f,1f);
        transition_mat[0][2]=Random.Range(0f,1f);
        transition_mat[1][0]=Random.Range(0f,1f);
        transition_mat[1][1]=Random.Range(0f,1f);
        transition_mat[1][2]=Random.Range(0f,1f);
        transition_mat[2][0]=Random.Range(0f,1f);
        transition_mat[2][1]=Random.Range(0f,1f);
        transition_mat[2][2]=Random.Range(0f,1f);
        Agentreset();
        StartCoroutine(execute_markov(iterations));

    }

```

在 Unity C#脚本中，在 MonoBehaviour 下，默认情况下有两种方法。这些是名为 Start 和 Update 的 void 方法。Start 方法通常用于场景变量的初始化和不同对象的标签分配；这是在游戏开始时创建场景的预处理步骤。更新方法按帧运行，所有的决策功能和控制逻辑都在这里执行。因为这是逐帧更新的，所以如果我们执行大型复杂操作，它的计算量非常大。其他方法包括唤醒和固定更新。在开始线程执行之前调用 Awake 方法，与 Update 方法相比，Fixed Update 具有常规的统一大小的帧速率。在 Start 方法的第一部分，我们将游戏对象分配给各自的标签。标签可以在检查器窗口中创建，在每个选中的游戏对象下，如图 [1-15](#Fig15) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig15_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig15_HTML.jpg)

图 1-15

为游戏对象分配和创建标签

标签是通过“游戏对象”分配的。FindWithTag()"方法。下一步是创建转换矩阵，这是一个 C#实现，用于创建 3 X 3 阶的通用浮点矩阵。这显示在“create_mat”函数中。

```
public float[][] create_mat(int size)
    {

        float[][] result= new float[size][];
        for(int i=0;i<size;i++)
        {
            result[i]=new float[size];
        }
        return result;

    }

```

创建空矩阵后，我们给它赋值。这些值来自 Unity Engine 的随机库，它为矩阵分配随机浮点值。

初始值数组也在这个部分初始化。

“StartCoroutine”方法调用 C# Unity 中的“IEnumerator”接口。我们在协程内部传递游戏逻辑，而不是每一帧都更新游戏(使用 Update 方法)。协程按照初始化中提供的迭代次数运行，并且还控制模拟。这可以用下面的代码来解释。

```
private IEnumerator execute_markov(int iter)
{
    yield return new WaitForSeconds(0.1f);
    for(int i=0;i<iter;i++)
 {

   transition_mat[0][0]=Random.Range(0f,1f);
   transition_mat[0][1]=Random.Range(0f,1f);
   transition_mat[0][2]=Random.Range(0f,1f);
   transition_mat[1][0]=Random.Range(0f,1f);
   transition_mat[1][1]=Random.Range(0f,1f);
   transition_mat[1][2]=Random.Range(0f,1f);
   transition_mat[2][0]=Random.Range(0f,1f);
   transition_mat[2][1]=Random.Range(0f,1f);
   transition_mat[2][2]=Random.Range(0f,1f);
   mult(transition_mat,initial_val,result_values);
   tanh(result_values);
   initial_val=result_values;
   Debug.Log("Values");

```

这部分代码有一个 yield return 语句，将协程线程的控制权释放给启动线程 0.1 秒(瞬间暂停)。然后，对于模拟的每次迭代，转移矩阵被随机化，初始值和转移矩阵的乘积由“mult()”函数计算。双曲正切函数是一种非线性激活函数，用于使结果值数组中的分布呈非线性。

接下来我们有一系列 if–else 语句，它们从结果值数组中选择最大概率状态。

```
 int bone_number=maximum(result_values,threshold);
 if(bone_number==0)
 {
bone.SetActive(true);
       bone1.SetActive(false);
       bone2.SetActive(false);
      active_obj=bone;
 }
 if(bone_number==1)
 {
       bone.SetActive(false);
       bone1.SetActive(true);
       bone2.SetActive(false);
       active_obj=bone1;
 }
 if(bone_number==2)
 {
       bone.SetActive(false);
       bone1.SetActive(false);
       bone2.SetActive(true);
       active_obj=bone2;
}
Debug.Log(bone_number);

```

下一步是让 Puppo 根据之前的转换来确定哪个操纵杆被激活了。这可以通过在 Unity 引擎物理系统中使用光线投射来完成。RayCast 在用户指定的方向投射光线，并且具有控制光线保持活动的深度和时间限制的参数。光线投射要发挥作用的要求是，应该有一个碰撞器物体附着在三根棍子上。碰撞器有助于理解基于物理的游戏对象何时发生碰撞。在这种情况下，我们使用一个简单的 BoxCollider 来检测光线投射是否击中它。根据 Puppo 命中的光线投射棒，我们看到 Puppo 通过指定目标棒的变换值自动将自己传输到目标位置。

```
RaycastHit hit;
var up = puppo_transform.TransformDirection(Vector3.up);
Debug.DrawRay(puppo_transform.position,up*5,Color.red);
if(Physics.Raycast(puppo_transform.position,up,out hit))
 {
      if(hit.collider.gameObject.name=="bone")
      {
            Debug.Log("hit");
puppo_transform.position= bone_trans.position;
      }
                            if(hit.collider.gameObject.name=="bone1")
      {
puppo_transform.position= bone1_trans.position;
      }
                        if(hit.collider.gameObject.name=="bone2")
      {
puppo_transform.position= bone2_trans.position;
      }
  }

Debug.Log(puppo_transform.position);
Debug.Log("Rest");
Debug.Log(active_obj.GetComponent<Transform>().position);
puppo_transform.position=active_obj.GetComponent<Transform>().position;
Debug.Log(puppo_transform.position);
yield return new WaitForSeconds(6f);

Agentreset();

```

在一次迭代中，Puppo 到达操纵杆后，我们通过调用“yield”方法让他休息一会儿，持续 6 秒钟。一旦我们理解了代码库的全部功能，我们可以在编辑器中单击播放。我们可以根据自己的选择改变迭代的值以及脚本中初始值数组的值，以查看分布是如何变化的。调试。Console 选项卡中的日志语句提供了关于每次迭代的结果数组值的信息，以及哪个棒被激活。游戏预告如图 [1-16](#Fig16) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig16_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig16_HTML.jpg)

图 1-16

马尔科夫 Puppo 的最终博弈模拟

这是我们用 Unity 引擎创建的一个简单模拟，以随机方式模拟马尔可夫状态。在下一节中，我们将尝试理解使用 Python 和 Unity 创建路径的 hmm 和决策过程。

### 隐马尔可夫模型

hmm 是马尔可夫状态的扩展，其中一些状态是不可观察的或“隐藏的”HMM 假设如果状态 P 依赖于状态 S，则 HMM 模型应该通过观察状态 P 来了解 S。HMM 是时间离散随机过程，可以在两个状态{S <sub>n</sub> ，P <sub>n</sub> 之间简单地进行数学解释，使得:

*   S <sub>n</sub> 是马尔可夫过程状态，是“隐藏的”或不可直接观察的。

*   P(P<sub>n</sub>€P | S<sub>1</sub>= S<sub>1</sub>，…，S<sub>n</sub>= S<sub>n</sub>)= P(P<sub>n</sub>€P | S<sub>n</sub>= S<sub>n</sub>

    对于所有 n>0，s <sub>1</sub> ，…，s <sub>n</sub> ，其中 P 和 S 是状态的超集，P()是条件概率。

### 隐马尔可夫模型的概念

让我们用一个例子来理解这一点。我们假设有朋友爱丽丝和鲍勃。鲍勃只能做三件事:散步、购物和打扫卫生。鲍勃活动的选择取决于环境中的天气。Alice 知道 Bob 在某一天将要进行的活动，但是不知道任何影响 Bob 活动的天气情况。这可以被公式化为离散马尔可夫链模型，其中天气条件类似于状态。该组天气条件包括雨天和晴天条件。因此，天气状况是影响 Bob 活动的隐藏状态。该图进一步解释了这种情况。该图还显示了状态转移概率值以及自转移值。HMM 的 Python 模拟具有以下细节:

```
states = ('Rainy', 'Sunny')

observations = ('walk', 'shop', 'clean')

start_probability = {'Rainy': 0.7, 'Sunny': 0.3}

transition_probability = {
   'Rainy' : {'Rainy': 0.8, 'Sunny': 0.2},
   'Sunny' : {'Rainy': 0.1, 'Sunny': 0.9},
   }

emission_probability = {
   'Rainy' : {'walk': 0.1, 'shop': 0.4, 'clean': 0.5},
   'Sunny' : {'walk': 0.6, 'shop': 0.3, 'clean': 0.1},
   }

```

图 [1-17](#Fig17) 描述了 HMM。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig17_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig17_HTML.jpg)

图 1-17

隐马尔可夫模型环境

### 带张量流的隐马尔可夫模型

打开 MarkovModel.ipynb 笔记本，第三节有一个基于初始概率和转移矩阵的 HMM 张量流实现。

```
import tensorflow as tf
import tensorflow_probability as tfp
tf_distributions=tfp.distributions

#Generate Hidden Markov Model With Tensorflow
#Transition Probability of states
transition_mat=tf_distributions.Categorical(probs=[[0.7,0.3],
                                                  [0.2,0.8]])
#Initial Probability of states
intial_values= tf_distributions.Categorical(probs=[1.0,0.5])

#Creating a Distribution Pattern for State Observation: Mean and STD of 1st state is 2.5 and 10, respectively, and that of 2nd state is 6.5 and 7, respectively.
observation_mat= tf_distributions.Normal(loc=[2.5,10],scale=[6.5,7])

#HMM
model=tf_distributions.HiddenMarkovModel(initial_distribution=         intial_values,
transition_distribution=transition_mat,
observation_distribution=observation_mat,
num_steps=10,
allow_nan_stats=True,
name="HiddenMarkovModel")

#Mean of the Distribution of States
print(model.mean())
#Log probability of 0 enumerated states i.e 1st State
print(model.log_prob(tf.zeros(shape=[10])))
#Log probability of 1 enumerated states i.e 2nd State
print(model.log_prob(tf.zeros(shape=[10])))

```

HMM 在 Tensorflow Probability 库(名为“tensorflow_probability”)下，是 tf_distributions 类的一个方法。我们初始化这些值，并为两个初始状态中的每一个分配一个正态分布(具有平均值和标准偏差)。tf_distributions 下的 HMM 将初始概率、转移矩阵、包含正态分布的观察数组、要模拟的迭代次数以及可选参数(如模型名称和 allow_nan_stats)作为参数。在笔记本上运行这段代码，了解 10 次迭代后模拟值是如何变化的。我们还可以修改参数和概率的值来产生新的模拟。

### Unity 中的隐马尔可夫模型代理

让我们尝试使用这个原理，并生成一个在 Unity 中使用 HMM 的随机路径算法。目标是训练代理检测产生奖励或物品的路径。在每个时期的开始，代理试图确定一个基于特定对象/奖励在特定时间状态下的最高概率值的生产路径。在 Unity 中打开“HMMAgent”场景，点击播放。控制台向我们显示了立方体代理在每个时间点获得最高价值的奖励或对象所遵循的遍历顺序。对于每一集或每一个学习时期，魔方代理会获得场景中出现的任何奖励。奖励基于对象在特定时间戳中具有的最高概率的形式。假设，在时间戳 t <sub>0</sub> 中，对象 o <sub>1</sub> 的概率最高，在 t <sub>1</sub> 中，对象 o <sub>2</sub> 的概率最高，然后代理拾取 o <sub>1</sub> ，o <sub>2</sub> ，以此类推。在每一集内，有三个时间戳，并且对于每一集，可以观察到不同的序列。某些序列可以是 o <sub>2</sub> ，o <sub>1</sub> ，o <sub>3</sub> ，甚至是 o <sub>2</sub> ，o <sub>1</sub> ，o <sub>1</sub> 等重复状态。这些序列或路径由称为维特比算法的 HMM 的动态编程实现生成。模拟预览如图 [1-18](#Fig18) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig18_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig18_HTML.jpg)

图 1-18

HMMAgent Unity 场景

让我们打开 Assets 文件夹中名为“hmmviterbiagent . cs”C # script 的关联脚本。该脚本的大部分内容类似于前面的马尔可夫模型脚本，具有转移矩阵、初始值和结果值数组的共同初始化。我们另外包含了发射矩阵(名为“发射 _mat”)和观察状态(名为“观察 _ 状态”)，它们分别具有发射概率值和观察状态。

```
float[][] emission_mat;
int[] observation_states=new int[3];

```

它们的初始化如下。

```
emission_mat[0][0]=Random.Range(0f,1f);
emission_mat[0][1]=Random.Range(0f,1f);
emission_mat[0][2]=Random.Range(0f,1f);
emission_mat[1][0]=Random.Range(0f,1f);
emission_mat[1][1]=Random.Range(0f,1f);
emission_mat[1][2]=Random.Range(0f,1f);
emission_mat[2][0]=Random.Range(0f,1f);
emission_mat[2][1]=Random.Range(0f,1f);
emission_mat[2][2]=Random.Range(0f,1f);

for(int i=0;i<3;i++)
{
    observation_states[i]=i;
}

```

一旦我们按照代码进行初始化，并将标签附加到不同的目标和代理上，我们就可以检查使用概率生成路径的 Viterbi 算法。函数声明将矩阵和数组作为参数，并用生成的目标对象的索引填充路径数组。

```
public void HMMViterbi(float[][] transition_mat,float[][]
emission_mat, int[] observation_states, float[]
hidden_states, float[] initial_val,int[] path)

```

我们创建了一个最小权重算法，在该算法中，我们尝试降低状态或对象的概率，然后尝试在特定时间内找到最有可能的对象或目标(在对象中)。

```
{
    int  mini_state=0;
    float mini_weight=0f;
    float weight=0f;
    float[][]  a= create_mat(3);
    int[][] s= create_mat_int(3);

``

int i,j;
for(i=0;i<3;i++)
{

   a[i][0]=(float)(-1*Math.Log(initial_val[i])
         *(Math.Log(emission_mat[i][observation_states[0]])));

}

```

对于接下来的步骤，使用“a”矩阵的初始值，我们对每个观察状态运行循环操作，并相应地计算最小权重值。

```
for(i=1;i<3;i++)
{
       for(j=0;j<3;j++)
       {
            mini_state=0;
            mini_weight=a[0][i-1]-(float)
                      (Math.Log(transition_mat[0][j]));

             for(int k=1;k<3;k++)
             {
                  weight=a[k][i-1] - (float)
  (Math.Log(transition_mat[k][j]));

                  if(weight<mini_weight)
                  {
                      mini_weight=weight;
                      mini_state=i;

                  }

         }

```

在这之后，我们更新“a”矩阵来临时保存最小权重。我们还将具有最小值的状态存储在状态矩阵“s”中

```
     a[j][i]= mini_weight-(float)
     (Math.Log(emission_mat[j][observation_states[i]]);
     s[j][i]=mini_state;

    }

}

```

一旦我们填充了“a”和“s”矩阵的值，我们就获得了具有最高概率和最小权重的目标对象的索引。然后，我们进行回溯以生成涉及其他两个目标对象的可能的最小加权路径。

```
mini_state=0;
mini_weight=a[0][i-1];
for(int k=1;k<3;k++)
{
       if(a[k][i-1]<mini_weight)
       {
                   mini_state=i;
                   mini_weight=a[k][i-1];
       }
}
       //dp backtracking
       //int[] path= new int[3];
       path[i-1]=mini_state;
       for(int k=i-2;k>=0;k--)
       {
           path[k]=s[path[k]][k+1];
       }

      float distribution_prob=(float)Math.Exp(-mini_weight);

}

```

维特比算法是一种动态规划算法，它利用马尔可夫状态和转移发射概率来选择最小加权路径，代理可以沿着该路径收集在每个时间戳具有最高概率的目标。

要运行该函数，我们像前面一样启动一个协程，通过将初始值数组和转移矩阵相乘来计算结果值数组，并调用 HMMViterbi 函数。

```
mult(transition_mat,initial_val,result_values);
tanh(result_values);
initial_val=result_values;
Debug.Log("Start Path");
HMMViterbi(transition_mat,emission_mat,observation_states,
hidden_states,initial_val,path);

```

其余的代码类似于前面的 Markov 模型模板，其中立方体代理使用 RayCast 来检测目标对象的碰撞体，立方体的变换被更新以遵循路径。一个有趣的观察是在维特比函数被调用后路径上的更新:

```
for(int l=0;l<path.Length;l++)
{

    target_number=path[l];
     /../
   }

```

理解代码后，我们可以通过将脚本分配给多维数据集代理(名为“AgentCube_Purple”)来进一步测试 Play 模式下的代码。环境的 Unity 场景视图如图 [1-19](#Fig19) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig19_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig19_HTML.jpg)

图 1-19

脚本分配给 AgentCube_Purple 的 Unity 场景

如果我们密切关注，在检查器窗口中，我们可以根据自己的选择更改迭代次数或纪元/剧集。控制台以目标多维数据集索引的形式显示所遵循的步骤的详细信息。检查器窗口的预览如图 [1-20](#Fig20) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig20_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig20_HTML.jpg)

图 1-20

场景的检查员视图

在本节中，我们已经对马尔可夫状态、MDP 和 HMM 进行了概述。本节是理解 RL 基础的一个重要方面，因为离散 RL 的所有学习算法都是基于状态的。在本节中，我们已经理解并实现了 Python 和模拟游戏中的马尔可夫模型，并理解了基于动态编程的 HMM 及其 Unity 模拟的简要概述。下一节将重点介绍如何将奖励融入这些状态，并让代理学习如何获得这些奖励。我们将理解贝尔曼方程的核心基本原理，并探索多武装匪徒以及迭代贝尔曼方程的一些变化。

### 贝尔曼方程

现在我们已经了解了 MDP，让我们沿着这个思路延伸，引入基于奖励的迭代学习的概念。正如我们在路径跟踪模型中看到的，MDP 和 HMM 为我们提供了如何保留状态的过去信息的洞察力。当我们在马尔可夫模型的上下文中涉及奖励时，一个通用的学习算法方程就进化了。从数学上来说，我们假设奖励在集合 R 内，并试图计算奖励的期望值，在一集结束时用 E[R]表示。这可以表现为:

g<sub>t</sub>= R<sub>t+1</sub>+R<sub>t+2</sub>+…+R<sub>t</sub>，

其中 G <sub>t</sub> 是基于收到的奖励序列的时间步长 t 的预期奖励。这可以被认为是对基于动态规划的马尔可夫模型的补充，其中每个状态都有一个回报预期。一般来说，贝尔曼方程有一个相关联的贴现因子 y(γ)，它贴现在先前状态中收到的奖励。因此，一个正式的迭代贝尔曼报酬折扣方程可以被构造为:

g<sub>t</sub>= r<sub>t+1</sub>+yr<sub>t+2</sub>+，+√y<sup>【k】r<sub>t+k</sub></sup>

这是关于马尔可夫状态的贝尔曼期望回报函数，在以前的回报上有折扣因子γ。这使得代理能够选择预期奖励的最近值，并相应地选择导致更多奖励的最近状态。这可以被视为自顶向下的树，因为这是一种迭代的动态编程方法。

迭代贝尔曼方程是使用两种基本算法方法来表述的:

*   **策略函数**:由π(a|s)表示的策略迭代函数是状态和动作集之间的映射，以产生由π*(a|s)表示的最优策略，该策略获得最高的期望回报。策略功能以在每一步最大化策略值的形式被验证。

*   **价值函数**:vπ(s)表示的价值函数表示从状态 s 开始，遵循策略π时得到的价值或期望收益。

现在我们对贝尔曼方程以及什么是价值和政策函数有了一个简单的概念，让我们试着把它们想象成图 [1-21](#Fig21) 中自上而下的树。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig21_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig21_HTML.jpg)

图 1-21

贝尔曼值迭代树

所提供的贝尔曼树显示了在策略π的作用下，如何控制从 k+1 状态到 k 状态的值 vπ(s ),并列举 k+1、k+2 状态的期望回报，等等。

Note

理查德·e·贝尔曼是一位应用型美国数学家，他创立了贝尔曼方程作为一种动态规划方法，用于最优控制理论以及后来的 RL。

### Bellman 代理在 Unity 中的实现

我们将在本章的后面部分讨论更多关于价值和策略迭代的方面，现在让我们创建另一个基于奖励和贝尔曼迭代算法的模拟。打开 Assets 文件夹，导航到“BellmanAgent”文件夹。这包含一个模拟游戏，其中代理人在赛车上，代理人必须到达并收集奖励(绿色大立方体),然后到达对面的终点。场景打开后，将出现如图 [1-22](#Fig22) 所示的画面。：

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig22_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig22_HTML.jpg)

图 1-22

贝尔曼代理团结现场

让我们理解模拟是如何工作的代码。卡丁车代理有一个目标函数，使用贝尔曼方程的迭代值函数，最大化它对每一集或每一个学习时期的回报。一旦代理到达第二个绿色目标(绿色立方体)或终点(终点线)，该集终止。目前，模拟是沿着卡丁车代理的 z 轴向前-向后的一个方向，这也可以扩展到多方向的遍历。

打开“bellmanagent . cs”c#脚本。虽然初始化和变量是根据马尔可夫模型和 HMM 的先前模板来设置的，但是需要做出不同的新特征和变量。

```
float max_reward=30.0f;
public float gamma;
public float epsilon;
public GameObject TinyAgent;
public Transform tiny_transform;
public GameObject target;
public GameObject target1;
public int iter;
public GameObject target2;
public Transform target_transform;
public Transform target1_transform;
public Transform target2_transform;
float[][] states;
float[] reward=new float[9];
float[] values=new float[9];
Dictionary<float,float> state_reward= new Dictionary<float,float>();

```

变量“max_reward”是代理在一集内需要累积的奖励总值。变量“gamma”是折扣系数。接下来，我们为卡丁车代理和目标初始化游戏对象变量。我们创建了一个称为状态的浮点值矩阵，在这种情况下，它就是转换矩阵。每个州还有一个浮动的奖励数组，一个将州和奖励对应起来的字典。使用迭代 Bellman 方程更新值函数需要 float 的值数组。

以表格离散方式为贝尔曼更新创建状态的基本概念是使用轨迹(即道路)的 transform.z 位置值。轨迹可能包含绿色目标或终点线，也可能不包含。根据轨道的特定部分是否有目标或处于终点位置，我们为每个轨道部分分配奖励。这将赛道转化为一个虚拟的类似健身房的环境，包含不同的状态和奖励，特别类似于 GridWorld(健身房中的环境)。

我们用随机数初始化奖励。Range()函数如下:

```
reward[0]=Random.Range(-0.05f,0.05f);
reward[1]=Random.Range(-0.05f,0.05f);
reward[2]=Random.Range(-0.05f,0.05f);
reward[3]=Random.Range(0.05f,0.20f);
reward[4]=Random.Range(-0.05f,0.05f);
reward[5]=Random.Range(-0.05f,0.05f);
reward[6]=Random.Range(-0.05f,0.05f);
reward[7]=Random.Range(0.05f,0.2f);
reward[8]=Random.Range(-0.05f,0.05f);

```

水平轨道分为九个部分，每个部分都有相应的奖励。我们也可以手动设置奖励。我们还用键值对填充字典:位置值(Transform.position.z)和奖励(float)。

```
public void populate_state_rewards(Dictionary<float,float> state_reward,float[] reward)
{

        state_reward.Add(-32.5f,reward[0]);
        state_reward.Add(-13.8f,reward[1]);
        state_reward.Add(-4.2f,reward[2]);
        state_reward.Add(4.9f,reward[3]);
        state_reward.Add(14.57f,reward[4]);
        state_reward.Add(23.93f,reward[5]);
        state_reward.Add(33.7f,reward[6]);
        state_reward.Add(41.7f,reward[7]);
        state_reward.Add(50.8f,reward[8]);

}

```

然后，我们创建值数组，并使用随机数。距离函数。转换矩阵也被创建，其中对角元素具有相同的概率 0.04，并且所有其他元素都是用“随机”创建的。范围”功能。start 方法与之前类似，包含所有的状态、游戏对象的初始化和标签的链接。

一旦初始化完成，让我们对贝尔曼方程的代码形成一个深刻的理解。

```
public int calculate_Bellman(float gamma,float[][] states,float[] reward,Dictionary<float,float> state_reward,float[] values)
{
    float[] new_values=new float[9];
        new_values=values;
    values=mult(states,values,new_values);
    for(int i=0;i<9;i++)
    {
        values[i]= reward[i]+ values[i]*gamma;
    }

    float max_values= maxi(values);
    int max_index=maxi_index(values,max_values);
    return max_index;

}

```

该函数将参数作为转换矩阵、奖励、字典以及值数组。它计算转换矩阵或状态与值数组的乘积。然后应用伽玛贴现因子。最后，添加当前状态的奖励以生成新值。在操作完成后，我们获取具有最高值的最有价值的元素或状态，并从值数组中检索其索引。这个指数将帮助我们映射哪个奖励值被触发。根据该奖励值，我们可以通过查找字典来导航到特定的轨道部分(位置)。

```
public  float take_action(Dictionary<float,float> state_reward,int max_index)
{   float action=0f;
    float max_reward=collect_rewards(max_index);
    foreach(KeyValuePair<float,float> i in state_reward)
        {
                 if(i.Value==max_reward)
                 {
                       action=i.Key;
                       break;
                  }
        }

    Debug.Log(action);
    return action;
}

```

此功能有助于查找在游戏迭代过程中获得的特定奖励，并自动返回被触发轨道部分的位置。

让我们将所有这些放在一个协程函数中，它是代理的控制逻辑。在 IEnumerator 函数中，我们设置了一个 for 循环，它按照我们想要的历元数运行。

```
Debug.Log("Start Epoch");
        float reward_now=0f;
        while(reward_now<max_reward)
         {
            Debug.Log("Start Episode");
            int max_index=calculate_Bellman(gamma,
states,reward,state_reward,values);
            Debug.Log(max_index);
            reward_now+=collect_rewards(max_index);
            Debug.Log("Rewards");
            Debug.Log(reward_now);
            float action_step=take_action(state_reward,
max_index);
            Debug.Log(action_step);

```

只要收集的奖励少于初始化步骤中指定的总预期奖励，这段代码就会运行。在这段代码中，我们执行了 Bellman 迭代值操作，获得了为该实例触发的特定奖励值，还检索了相应轨道部分的变换位置值。奖励也相应增加。

```
if(action_step==target_transform.position.z )
            {
                Debug.Log("Reached first target");
                tiny_transform.position=new Vector3(0f,0f,
target_transform.position.z);
                target.SetActive(false);
                yield return new WaitForSeconds(1f);

            }
if(action_step==target1_transform.position.z )
            {
                Debug.Log("Reached Second target");
                tiny_transform.position=new Vector3(0f,0f,
target1_transform.position.z);
                target1.SetActive(false);
                yield return new WaitForSeconds(1f);

            }
tiny_transform.position=new Vector3(0f,0f,action_step);
yield return new WaitForSeconds(1f);

```

if-else 语句指定并检查卡丁车代理采取的行动是否将他引向绿色目标或终点线。如果代理到达终点或目标，那么在下一次迭代开始之前，我们给他 1 秒钟的短暂休息。只要回报少于总预期回报，这个迭代就继续。

```
if(reward_now==max_reward)
             {
                break;
             }
reset();
yield return new WaitForSeconds(2f);
  }
  Debug.Log("End Episode");

reset();
 }
Debug.Log("End Epoch")
}

```

如果满足条件且累积的奖励等于总奖励，我们将终止该实例，并让代理休息 2 秒钟，然后进入下一集培训。

当我们对代码库感到满意时，我们可以对奖励函数进行试验和修改，并为不同的情况添加更多的离散奖励。此外，环境也可以沿着垂直轨道扩展，这也将在转移矩阵和值数组上添加更多的状态。代理的运动将在 x 轴和 z 轴上被控制。我们可以通过获取字典中垂直放置的轨道部分的 x 值来修改脚本以适应更多的状态。如图 [1-23](#Fig23) 所示设置好环境后，我们可以点击编辑器中的播放按钮。请注意代理如何快速导航到每次迭代的最高奖励状态，并由于值数组中的更新而继续这样做。值数组保留包含最高奖励的状态或轨道部分的信息，并且还帮助代理在连续的时间戳中遵循该路径。这可以实现为保留具有最高值的状态的缓冲存储器。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig23_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig23_HTML.jpg)

图 1-23

行李员代理统一场景与检查员窗口

在本节中，我们学习了贝尔曼方程，这是一个迭代动态规划方程。在贝尔曼方程的帮助下，我们已经学习了一些将奖励与马尔可夫状态和决策过程联系起来的重要概念。这一节还简要地谈到了通过遵循特定的策略来实现回报最大化或价值最大化的价值和策略迭代技术。我们创建并模拟了一个 Unity 游戏，其中卡丁车代理必须应用贝尔曼值函数来导航到奖励最高的赛道部分。到目前为止，我们已经了解了离散表格环境和价值/策略优化算法(如 Bellman 方程)在 RL 中是如何有用的。在下一节中，我们将讨论创建另一个基于多武装匪徒的模拟游戏。

### 在 Unity 中创建多臂 Bandit 强化学习代理

多武装匪徒(MAB)是 RL 算法模拟的最简单形式。这种技巧不涉及学习选择最大回报状态，而是依赖于一种反馈机制。k 武装土匪问题可以描述为:一个土匪代理人处在一个有 k 个不同的房子可以抢劫的环境中(行动空间)。根据每一步选择房子的决定，强盗会得到奖励。通常，奖励是一个稳定的概率分布，可以是负的(当警察在房子里时)或正的(当房子里没有警察时)。强盗代理的目标是最大化 n 个时间步的正回报。

这种基本的 RL 环境缺乏马尔可夫状态和贝尔曼优化技术，因为在数学上它只是奖励和行动的函数。重新定义一下，这可以解释为:

q(A)= E[R<sub>t</sub>| A<sub>t</sub>= A]，

其中 Q(a)表示行动 a 后的预期回报。

由于土匪的行动主要依赖于奖励的结果，因此有几项政策已经实施。图 [1-24](#Fig24) 展示了多兵种土匪环境的预览。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig24_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig24_HTML.jpg)

图 1-24

多臂土匪与环境

### 多股武装匪徒的策略

有几个班迪特算法的变化，以最大限度地提高每一步收集的奖励。这些通常被称为“行动-价值”功能。

*   **贪婪算法**:暴力应用涉及使用贪婪利用算法。这包括为每次迭代存储动作的 Q 值(值函数),并选择 Q 值最大的动作。这是一种非开发策略，因为在许多情况下，在相同的分支/位置，回报可能不是最高的。从数学上讲，这可以被视为:

Q <sub>t</sub> (a) = ∑ 1 <sub>(at=a)</sub> 。(R <sub>i</sub> ) / ∑ 1 <sub>(at=a)</sub> ，

选择最大 Q 值:

Argmax <sub>a</sub> Q <sub>t</sub> (a)

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig25_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig25_HTML.jpg)

图 1-25

ε-贪婪算法

*   **Epsilon-Greedy 算法**:在这种算法中，bandit 基于某些概率值来利用和探索新的状态，表示为 bye。土匪以概率 e 探索一个新状态，并以概率 1-e 利用当前最佳奖励状态，如图 [1-25](#Fig25) 所示。

*   **衰变ε-贪婪算法**:这是之前剥削-探索算法的一个变种。这里，对数衰减因子可能与先前状态的利用相关联。

*   **非平稳加权算法**:如果土匪获得的奖励随时间变化，那么就是一个非平稳问题。在这种情况下，我们对之前的 Q 值和当前的奖励进行加权平均。这可以表现为:

q<sub>n+1</sub>= q<sub>+α【r<sub>-q<sub>，</sub></sub></sub>

其中，α是平均加权因子。

*   **置信上限(UCB)算法**:这是不同奖励的探索-开采的统计分布。该算法通过测量先前行动的回报的预期方差来探索。这是一种乐观价值方法，可以减少获得高回报的特定行为的方差。在数学上，它可以用以下等式来描述:

a<sub>t</sub>= argmax[q<sub>t</sub>(a)+c(ln(t)/n<sub>t</sub>(a))，

其中 Nt(a)表示动作 a 在时间 t 之前被选择的次数，ln(t)是 t 的自然对数。

*   **梯度班迪特算法**:这是一种数值偏好算法，考虑了特定状态 a 相对于其他状态的偏好。一个动作相对于另一个动作的相对偏好被记录，并且可以通过包括 softmax 函数被采样成概率分布。这在一般深度学习中是一个非常重要的函数，并将在我们的深度 RL 算法中使用。

软体最大值(z)= e<sup>z</sup>/(e<sup>z</sup>)

*   **Thompson 采样**:这是一种贝叶斯采样技术，比 Epsilon-Greedy 算法更复杂，基于行动和回报的后验概率分布。

### 基于 UCB 算法的联合多兵种土匪模拟

在本节中，我们将尝试将ε贪婪 UCB 与 softmax 梯度优化相结合。我们将尝试探索 Unity 模拟中不同的算法方法，然后我们可以尝试创建一个基于 Python 的环境。

打开场景“MAB-团结”这个场景由三个立方体组成，每个立方体都有不同的奖励概率。在编辑器中选择游戏时，我们看到强盗选择不同的奖励(即立方体),并且总是试图优化它的奖励。这是一个纯粹的基于行为奖励的、非文本的反馈土匪，因此我们可以用ε贪婪算法控制探索开发。默认情况下，模拟在ε-贪婪模式下运行，并且还有一个激活 UCB 算法的选项。在控制台选项卡中，我们可以看到预期的奖励。环境预览如图 [1-26](#Fig26) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig26_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig26_HTML.jpg)

图 1-26

Unity 中的多臂土匪场景

现在我们已经获得了一些用 C# MonoBehaviour 为 Unity 编写代码的经验，让我们打开“代理”脚本。第一个包含变量 setup 这里应该注意的是，如果我们想要私有变量在 Inspector 窗口中呈现，我们必须在每个私有变量上提到[SerializeField]属性。

```
[SerializeField]
private int k = 3;
[SerializeField]
private float lr = 0.1f;
[SerializeField]
private float exp_rate = 0.3f;
    [SerializeField]
    private bool ucb = true;
    [SerializeField]
    private float c = 2f;
    [SerializeField]
    private float time_lag = 0.1f;
    List<int> actions = new List<int>();
    public float total_reward = 0;
    List<float> avg_reward = new List<float>();
    List<float> true_val = new List<float>();
    System.Random rnd = new System.Random();
    private float time = 0.0f;
    List<float> values = new List<float>();
    List<float> action_times = new List<float>();
    List<float> confidence_int = new List<float>();
    // Start is called before the first frame update
    [SerializeField]
    private GameObject g0, g1, g2;

```

初始化涉及变量，例如用于ε-贪婪算法的ε、用于 UCB 的布尔触发器、以及奖励、值(Q 值)和动作时间(其表示动作“a”重复到什么程度)的浮动数组。置信区间是一个浮动列表，有助于 UCB 算法。动作列表包含应用程序运行期间相关多维数据集的索引。start 方法包含所有变量和列表的初始化，它调用游戏控制逻辑的协程。

让我们探讨一下 chooseaction 方法，它包含了ε-贪婪 UCB 算法的实现。

```
int action = 0;
if ((float)Random.Range(0.0f, 1.0f) <= exp_rate)
{
            int idx = rnd.Next(actions.Count);
            action = idx;

}

```

这部分代码段随机选择浮点值，检查是否小于剥削率。然后，它试图用“rnd”探索新的立方体。Next()"方法。这是一种基于样本随机分布的探索策略。

如果未选择此选项，则控制权将传递给 else 语句。如果我们想使用 UCB 算法，我们必须检查布尔变量为真。UCB 电码如下:

```
if (ucb)
            {
                if (time == 0f)
                {
                    action = rnd.Next(actions.Count);

                }
                else
                {
    for (int i = 0; i < k; i++)
                    {
                        float x = (float)values[i];
  float r = c *  (Mathf.Sqrt(Mathf.Log((float)time) /
(action_times[i] + 0.1f)));
                        x += r;
                        confidence_int[i] = (float)x;

                    }
                    float max = 9999f;
                    action = 0;
                    for (int j = 0; j < k; j++)
                    {
                        if (confidence_int[j] > max)
                        {
                            max = confidence_int[j];
                            action = j;
                        }
                    }

                }
            }

```

UCB 电码分为两种说法。如果没有为 UCB 提供时间段，那么它是一个随机探索算法。如果考虑过去动作的时间段，则控制转到“else”语句。然后，UCB 使用 UCB 公式计算所有可能动作的 Q 值。在获得每个 Q 值后，它们被填入置信区间列表。然后，代码段的后面部分从这个置信区间列表中选择最大 Q 值，并返回那个特定的立方体。

接下来，我们进入“采取行动”功能，并尝试通过添加学习率来更新 Q 值和奖励值，以帮助 UCB 算法。该算法还计算每一步的相对奖励以及平均奖励。

```
time++;
action_times[action] += 1;
float reward = Random.Range(0f, 1.0f) + true_val[action];
values[action] += lr * (reward - values[action]);
total_reward += reward;
avg_reward.Add(total_reward / time);

```

“Play”方法链接了上述两个函数，并针对 Inspector 窗口中提供的迭代运行它们。具有最高置信区间值的特定动作的 Q 值被选择并传递给“takeaction”方法。

```
public void play(int n)
    {
        for (int y = 0; y < n; y++)
        {
            int act = chooseaction(exp_rate,
                                  c, values, action_times,
actions, time, ucb);
  takeaction(total_reward,
  avg_reward, action_times,
  time, act, true_val);
        }
    }

```

让我们进入 IEnumerator 代码段，在这里调用所有函数并选择适当的立方体。“Play”命令被调用 900 次，每次迭代，我们都会在控制台窗口中获得平均奖励和总奖励的详细信息。然后基于立方体的选择，每次迭代的最高值的立方体被设置为活动的，其余的被停用。这表示哪个立方体在哪个迭代中被触发，并提供最大的 Q 值。

```
yield return new WaitForSeconds(time_lag);

        play(900);
        //Debug.Log("Total REward " + total_reward);
        /*for (int t = 0; t < k; t++)
        {
            Debug.Log("True Values " + true_val[t]);
            Debug.Log("Estimated values " + values[t]);

        }
      */
        float thresh = 0.9f;
        for (int y = 0; y < avg_reward.Count; y++)
        {
            if (avg_reward[y] > thresh)
            {
                Debug.Log("select " + y + y % 3);
                if (y % 3 == 0)
                {
                    g0.SetActive(true);
                    g1.SetActive(false);
                    g2.SetActive(false);
                }
                else if (y % 3 == 1)
                {
                    g0.SetActive(false);
                    g1.SetActive(true);
                    g0.SetActive(false);
                }
                else if (y % 3 == 2)
                {
                    g0.SetActive(false);
                    g1.SetActive(false);
                    g2.SetActive(true);
                }
            }
            yield return new WaitForSeconds(time_lag);
        }

```

因此，我们实现了一个基于ε贪婪 UCB 的多臂土匪模拟。我们可以在游戏过程中通过在 Epsilon-Greedy 和 UCB 算法之间切换来玩这些设置，看看探索-开发细节是如何受到影响的。一旦我们理解了代码，我们就可以点击 Unity 编辑器中的 Play 来观看模拟运行。经过多次迭代后，我们将观察到一些立方体被重复选择——这是强盗利用的典型例子。然后，我们可以更改ε和学习率值，或者更改 UCB 算法以进行更多探索。图 [1-27](#Fig27) 是模拟的预览。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig27_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig27_HTML.jpg)

图 1-27

多臂土匪督察窗口

### 用ε-贪婪算法和梯度 Bandit 算法构建多臂 Bandit

一旦我们验证了我们的算法，我们可以使用 Jupyter Notebook 或 Colab Notebook 在 Python 中实现一个更简单的 Epsilon-Greedy 环境。在 Colab 或 Notebook 中打开“多臂 Bandit.ipynb”笔记本。代码非常简单。我们有一个经典的 10 臂 bandit 的实现，其中我们有每个臂的概率的初始地面真实分布(在我们的例子中是房子)。我们还初始化了武器的概率和代理的概率。其余代码表示一个 for 循环，它运行“num_epochs”变量中提供的时期数。如果ε的值大于随机值，那么我们在 10 个不同的臂中探索；否则我们就剥削。一旦对于特定情节获得了奖励，我们通过使用由通用等式给出的增量方法来更新 Q 函数:

q<sub>n+1</sub>=【r】<sub>I</sub>/n，

其中第 n+1 次迭代的 Q 值是在过去 n 集上收集的先前奖励 R <sub>i</sub> 的平均值。

该程序几乎是不言自明的，并在注释中包含了所有的细节，以生成武器的可能分布和采样。

Note

代码和场景是使用 Unity 2018.3 版构建的，但兼容 Unity 的所有版本，包括 2020 测试版。

```
import numpy as np
np.set_printoptions(2)
initial_values = np.random.rand(10)
number_of_arms = np.zeros(10)
agents_prob = np.zeros(10)
reward_count = np.zeros(10)
num_epochs = 4000
e = 0.33

```

初始化后，我们进入 for 循环，如下所示:

```
for _ in range(num_epochs):

    # Either choose a greedy action or explore
    if e > np.random.uniform():
        which_arm_to_pull = np.random.randint(0,10)
    else:
        which_arm_to_pull = np.argmax(agents_prob)

    # now pull the rewarding arm
    if initial_values[which_arm_to_pull]
    >  np.random.uniform():
        reward = 1
    else:
        reward = 0

```

ε贪婪抽样技术相应地产生奖励。然后，我们更新土匪必须拉的武器数量，并更新奖励计数如下:

```
# now update the lever count and expected value
number_of_arms[which_arm_to_pull] += 1
reward_count[which_arm_to_pull] =reward_count[which_arm_to_pull] + reward

```

然后应用增量方法来生成第 n+1 次迭代的 Q 值，如下所示:

```
#Incremental Approach to Update value of Q
#Q(t+1)=Q(t) + (1/n(R(n)-q(n)))

agents_prob[which_arm_to_pull] =agents_prob[which_arm_to_pull] + (1/number_of_arms[which_arm_to_pull]) *
   (reward -agents_prob[which_arm_to_pull])

```

最后，我们根据获得的最佳奖励、Q 值和选择的 arm 打印每个时期的结果。运行后，输出应如图 [1-28](#Fig28) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig28_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig28_HTML.jpg)

图 1-28

多臂强盗 Epsilon-贪婪的蟒蛇

在本节中，我们了解了 RL 算法的最简单形式，称为多臂土匪。我们学习了在 MAB 勘探和开采的不同优化技术，以在每个行动步骤中获得更大的 Q 值或回报。与复杂的马尔可夫模型、HMM 和贝尔曼方程不同，这里没有状态的概念，这就是为什么它是 RL 的轻量级模型。然后，我们使用 Epsilon-Greedy 和 UCB 算法技术在 Unity 中创建了一个 MAB 模拟环境，并可视化了该过程每个阶段的奖励。在下一节中，我们将考虑介绍性 RL 的最后步骤:价值和策略迭代。本节在贝尔曼方程中初始化，并将从那里继续。

### 价值和策略迭代

自从本章前面介绍了贝尔曼方程之后，我们已经简要地了解了价值和策略迭代函数。策略迭代可以被认为是从基本策略π(s | a)优化策略π*(s | a)，这将导致与该策略相关联的价值函数的增加。简单的值更新函数在数学上可以简化为:

V(s<sub>t</sub>)= V(s<sub>t</sub>)+α[G<sub>t</sub>-V(s<sub>t</sub>)]，

其中，α是学习中使用的恒定步长，V(s <sub>t</sub> )是在状态 t 获得的值，G <sub>t</sub> 是在贝尔曼方程部分引入的期望回报 R <sub>t</sub> 。如果我们将 G <sub>t</sub> 的值替换为贝尔曼方程中引入的迭代折现因子 y(γ),我们可以将价值方程重新构造如下:

v(s<sub>t</sub>= v(s<sub>t</sub>)+α[r<sub>t+1</sub>+yv(s<sub>t+1</sub>)-v(s<sub>t</sub>)]

这种针对特定策略的价值优化方法被称为时间差异学习。特别地，这个等式表示 TD(0)算法，因为我们只关心当前状态之前的一个状态(即，步骤 t+1)。

值迭代机制也有一种确定的方法，通过评估 t+1 和 t 状态的值来定义在学习过程的每个连续步骤之后产生的误差项∂。这可以用简化的公式进行数学表示:

√t0= r<sub>+yv(s<sub>)-v(s<sub>，</sub></sub></sub>

其中，∂是时间戳 t 的误差

现在我们已经有了值迭代技术的数学概念，我们可以更好地了解 Unity 中的“BellmanAgent，cs”脚本发生了什么。值函数的结果由策略函数控制，策略函数也会更新。基于策略函数，代理必须最大化价值函数，如果策略不能做到这一点，那么策略也会得到更新。这可以用下面的等式来简单解释:

π*(s，a) = argmax V <sub>π</sub> (s，a)，

在这里，符号有其原始意义。让我们试着想象一下策略迭代如何帮助获得图 [1-29](#Fig29) 中的最大值函数。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig29_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig29_HTML.jpg)

图 1-29

价值策略迭代函数

现在让我们理解一个涉及价值和政策迭代的基本概念，称为 Q-learning。Q-learning 算法是一种非策略时间差异学习算法，它使用 Q 函数作为值函数，并试图最大化该值。它可以根据广义值迭代方程简单地写成，用 Q(s，a)代替 V(s)如下:

Q(s <sub>t</sub> 、a<sub>t</sub>= q(s<sub>t、</sub>a<sub>)+α[r<sub>t+1</sub>和 maxQ(s</sub>

Q-learning 算法是一种表格离散算法，它使用值和策略迭代等式，并形成一般 RL 中的基线模型。让我们首先通过理解一个表格健身房环境来理解 Q-learning 和策略更新是如何工作的。

Note

Q-learning 是克里斯·沃特金斯在 1989 年提出的。

### 利用出租车健身房环境实施 Q-Learning 策略

打开“策略迭代函数和 Q Learning.ipynb”笔记本，让我们试着了解一下环境。在本笔记本中，我们使用了 OpenAI Gym 环境集中的出租车环境(2D ),出租车在黄色方框的帮助下显示。该环境可以安装在笔记本电脑中，如下所示:

```
#Policy Iteration by using Q-Learning
import numpy as np
import random
from IPython.display import clear_output
import gym
#Initialize the Taxi Gym Environment
enviroment = gym.make("Taxi-v3").env
enviroment.render()
#collect the details of observation and action space
print('Number of states: {}'.format(enviroment.observation_space.n))
print('Number of actions: {}'.format(enviroment.action_space.n))

```

在运行笔记本或 Colab 时，我们将看到观察空间和行动空间的细节，就像在 CartPole 环境中一样，如图 [1-30](#Fig30) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig30_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig30_HTML.jpg)

图 1-30

Taxi-v3 健身房环境

笔记本的下一部分包含使用基于观察状态和奖励的 Q-learning 进行训练的逐步代码。我们将变量初始化为

```
alpha = 0.1
gamma = 0.6
epsilon = 0.1
q_table = np.zeros([enviroment.observation_space.n, enviroment.action_space.n])
num_of_epochs = 100000,

```

其中“alpha”是学习率，“gamma”是折扣因子，“epsilon”用于探索-开发策略(在 MAB 算法中使用)，“q_table”是包含观察和动作空间的矩阵。

然后，我们对我们希望用 Q-learning 训练的时期数运行 for 循环，如下所示:

```
for episode in range(0, num_of_epochs):
    # Reset the enviroment
    state = enviroment.reset()

    # Initialize variables
    reward = 0
    terminated = False

```

我们在每个纪元开始前重置环境。下一个部分包含不同的开发-Q 学习的探索策略和通过应用 Q 学习方程更新“q_table”。在学习过程的每个阶段，从体育馆环境接收状态、奖励和动作空间。

```
while not terminated:
# Similar to epsilon-greedy algorithm for exploration-#exploitaion tradeoff
        if random.uniform(0, 1) < epsilon:
            action = enviroment.action_space.sample()
        else:
            action = np.argmax(q_table[state])

        # Return the current state, reward and action
        next_state, reward,
   terminated, info = enviroment.step(action)

   #Compute the q value of the state
   from the tabular   environment
        q_value = q_table[state, action]
        #Get the maximum Q value
        max_value = np.max(q_table[next_state])
        #update the Q-value based on the Q-learning Equation
   new_q_value = (1 - alpha) * q_value + alpha *
   (reward    + gamma * max_value)

        # Update Q-table
        q_table[state, action] = new_q_value
        state = next_state

```

每行中的注释描述了后续步骤中正在执行的操作。训练完成后，Q 表包含最有回报状态的值。这可以用更新后的滑行环境来表示，如图 [1-31](#Fig31) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig31_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig31_HTML.jpg)

图 1-31

Q-learning 中的出租车健身房环境培训

下一段包含模型评估的代码。对于 Q-learning agent 的每个时期，出租车都会根据 Q-table 获得一个奖励。如果奖励是负的，那么我们加上一个惩罚值，这意味着出租车与环境中的另一个物体发生了碰撞。

```
state = enviroment.reset()
epochs = 0
penalties = 0
reward = 0

terminated = False
#Run epochs
while not terminated:
    action = np.argmax(q_table[state])
    state, reward, terminated, info = enviroment.step(action)
    #If reward is negative,penalty
    if reward == -10:
        penalties += 1

    epochs += 1

total_penalties += penalties
total_epochs += epochs

```

在运行片段时，我们得到惩罚值和每集的次数，如图 [1-32](#Fig32) 所示。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig32_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig32_HTML.jpg)

图 1-32

出租车健身房环境评价

### 在统一中学习

现在让我们在 Unity 中为 Q-learning 创建一个类似的模板。打开名为“Q-Learning”的场景，然后单击“播放”。圆柱形对象是代理，它试图通过避开红色平面并仅使用绿色平面从上到下移动。这些平面是状态样本，或观察空间，就像在健身房环境中一样。绿色的飞机有相关的奖励，而红色的飞机有负奖励。代理使用迭代 Q 学习算法作为策略来更新价值函数并最大化其回报。场景应该出现如图 [1-33](#Fig33) 所示的东西。

![../images/502041_1_En_1_Chapter/502041_1_En_1_Fig33_HTML.jpg](../images/502041_1_En_1_Chapter/502041_1_En_1_Fig33_HTML.jpg)

图 1-33

Unity 中的 q-学习场景

打开附件中的脚本“QLearning.cs”，让我们简单地浏览一下重要的函数。由于大多数状态初始化、标签分配和变量初始化与前面的项目相似，我们将在本节中跳过这些。在这种情况下，最重要的函数是“训练”函数，它包含 Q 学习算法的核心功能，以及通过选择每个时间戳中最高值的状态(平面)来更新 Q 表。

```
for (int k = 0; k < max_epoch; k++)
        {
            int current_state = rnd.Next(0, reward.Length);
            while (true)
            {
                Debug.Log("training started");

                int next_state = GetProbNextState(current_state,
                                            transition_mat);
                List<int> possiblenextsteps = GetPossibleStates(next_state,
                                            transition_mat);
                Debug.Log("Current" + current_state);
                Debug.Log("Next" + next_state);
               //choose among the best probable
               state which gives the max reward

                float maxq = float.MinValue;

```

在循环内部，我们从给定的状态中随机选择一个初始状态，并在 Q 表中查询“GetProbNextState”方法给出的下一个状态。该函数实际上从状态列表中返回该特定时期或情节中的最大值状态。

在接下来的步骤中，我们将使用这些信息，并通过使用 Q-learning 等式来更新 Q-table，如下所示:

```
for (int j = 0; j < possiblenextsteps.Count; j++)
                {
                    Debug.Log(possiblenextsteps[j]);

                    int n_s = possiblenextsteps[j];
                    float qs = quality_mat[next_state][n_s];
                    //update q value matrix
                    based on maximum value
                    if (qs > maxq)
                    {
                        maxq = qs;
                    }
                }
                // update q matrix with
                //Q-learning algorithmic formula
                quality_mat[current_state][next_state] =
                ((1 - learning_rate) *   quality_mat[current_state][next_state])
                + ((learning_rate) * (reward[current_state][next_state] + gamma * maxq));

                current_state = next_state;
                if (current_state == goal)
                {
                    //Agent.transform.position
                    //= green_33.transform.position;
                    //StartCoroutine(move_33());
                    Debug.Log("Reached");

                    break;

```

Note

代码和场景是使用 Unity 2018.3 版构建的，但兼容 Unity 的所有版本，包括 2020 测试版。

基于 Q-policy 抽象出状态后，代码选择被触发的平面的位置或变换。代理会自动切换到变换后的位置。代码段的其余部分是环境的模板代码和控制整个工作流逻辑的协程“train”方法。一旦我们对代码库的理解感到满意，我们就可以运行场景并模拟它。这个模拟的主要思想是理解 Q 学习在奖励、状态和行动的离散环境中的重要性。

## 摘要

我们已经学完了第一章，下面是我们目前所学内容的总结:

*   我们试图理解强化学习(RL)的基础，以及状态、动作和奖励的包含。RL 是一个不同的范例，它包括一个代理与环境的交互，以最大化其目标。

*   为了理解状态、奖励和行动的概念，我们安装了一个健身房环境。Gym 是 OpenAI 开发的一个 RL 环境，用于深度 RL 和机器人方面的研究。

*   我们学习了如何设置 Anaconda 和 Jupyter 笔记本，以及如何在 Colab 和 Jupyter 中为 RL 初始化 Python 内核。

*   我们试图理解 CartPole 的逻辑问题，并试图用 Python 将其建模为 RL 环境。我们看到了环境的观察和行动空间，也深入了解了 Tensorflow、Tensorboard 和其他深度学习库和框架。

*   我们学习了马尔可夫过程、有限决策理论、隐马尔可夫模型(hmm)以及状态的概率分布如何帮助学习。

*   我们学习了如何从 Unity Hub 安装 Unity Engine。

*   我们开发了一个使用 Puppo (Unity Berlin)的模拟，该模拟基于使用转移矩阵概率的马尔可夫模型，另一个模拟使用用于 HMMs 的立方体代理。

*   我们学习了如何创建 Unity C#脚本，并学习了 Unity 中的协程和内部方法，如 Start 和 Update。

*   通过引入贝尔曼方程，我们将马尔可夫模型的知识扩展到基于奖励的学习中。我们开发了一个卡丁车比赛模拟，卡丁车代理商必须根据回报和价值最大化来决定选择哪个赛道。我们学习了价值和策略迭代算法。

*   我们设计并学习了多臂土匪问题，以及不同的策略如ε贪婪、上置信限和梯度土匪如何影响土匪的决策。我们了解到 bandit 问题是 RL 算法的最简单形式，没有状态的概念，并在 Unity 中创建了 Bandit 模拟。

*   最后一节依赖于对价值和策略迭代算法以及如何基于某个策略最大化特定价值函数的深入讨论。我们学习了时间差分和 Q 学习，这是经典 RL 最著名的基线学习算法。我们创建了一个健身房出租车环境来理解 Python 中的 Q 学习算法，然后使用 Q 学习策略创建了一个基于奖励最大化的寻路代理的仿真。

*   在所有这些部分中，我们尝试使用 Python 和 C#并行开发算法，以便在核心级别更好地理解这些算法是如何表示的。

*   为了更好地理解概念，我们使用了 Tensorflow 库，也在 Jupyter 笔记本或 Colab 中创建了 bandit 问题。

这本书的第一章到此结束。本章构成了 RL 概念的基础，我们试图深入了解 RLc 的发展是如何发生的，从最初的马尔可夫模型到贝尔曼方程，再到价值和策略迭代以最大化回报。

所有这些概念都属于离散经典 RL，当我们试图用 Unity ML Agents Toolkit 探索深度 RL 时，将广泛需要这些概念，Unity ML Agents Toolkit 将离散动作空间转换为深度学习算法的多离散或连续空间。在下一章中，我们将研究导航网格，寻路人工智能，以及 Unity 中更多的自动寻路和启发式寻路算法。