

# 十、玩围棋游戏

游戏一直是人类文化的一种现象，在这里人们表现出智慧、互动和竞争。但是游戏也是逻辑、人工智能、计算机科学、语言学、生物学，以及最近越来越多的社会科学和心理学的重要理论范式。游戏，尤其是策略游戏，为强化学习算法提供了一个理想的特权测试环境，因为它们可以充当真实问题的模型。在这一章中，我们将学习如何使用强化学习算法来解决博弈论中的一个问题。

以下是涵盖的主题:

*   博弈论的基本概念
*   博弈论的实际应用
*   AlphaGo DeepMind 项目
*   蒙特卡罗树搜索
*   卷积网络

在本章结束时，读者将学习博弈论的基本概念，AlphaGo DeepMind 项目如何工作，**蒙特卡罗树搜索** ( **MCTS** )如何执行深度搜索——直到达到最终状态——以及卷积网络如何引导树搜索过程。



# 博弈论

博弈论是一门数学科学，研究和分析一个主体在与其他竞争主体冲突或战略互动的情况下的个人决策，旨在实现每个主体的最大利润。在这种情况下，根据反馈机制，通过模型寻求竞争性和/或合作性解决方案，一个人的决定可以影响其他人取得的结果，反之亦然。

博弈论可以追溯到 1654 年布莱士·帕斯卡和皮耶·德·费玛之间的一封通信，内容是计算赌博的概率。

博弈论这个词最早是在 20 世纪 20 年代由 mile Borel 使用的。Borel 解决了两人零和游戏的*问题，并试图找到一个被称为约翰·冯·诺依曼零和游戏解决方案的概念。*

现代博弈论的诞生可以与约翰·冯·诺依曼和奥斯卡·莫根施特恩在 1944 年出版的《博弈论与经济行为》一书*(下图是该书的封面)相吻合，尽管其他作者(如恩斯特·策梅洛、阿曼德·博雷尔和约翰·冯·诺依曼本人)也写过博弈论:*

![](Images/fcf6bd7e-49ac-45b3-916e-97f8872a9f14.png)

这两位学者的想法可以被非正式地描述为试图在人与人之间的互动涉及赢得或分割某种资源的情况下用数学方法描述人类行为。

后来研究博弈论，特别是非合作博弈的最著名的学者是数学家约翰·福布斯·纳什，朗·霍华德的《美丽心灵》就是为他而写的(下图是一张电影海报):

![](Images/0736b555-775d-4bc5-96e4-8fff278a9244.png)

八个诺贝尔经济学奖授予了研究博弈论的学者。约翰·梅纳德·史密斯也获得了克拉福德奖，他是一位杰出的生物学家和遗传学家，长期在苏塞克斯大学担任教授，以表彰他在这一领域的贡献。



# 基本概念

博弈论的主要目标是胜利:每个人都必须意识到游戏规则，意识到每一步行动的后果。一个人打算采取的一系列行动被称为策略。根据所有参与者采用的策略，每个人都可以根据适当的度量单位获得收益。奖励可以是正的、负的或零。如果每个玩家的支出都有其他玩家相应的损失，那么这个游戏就叫做恒和游戏。特别地，两个玩家之间的零和游戏代表了一个玩家向另一个玩家支付报酬的情况。遵循的策略对所有玩家都是满意的；否则，有必要计算并最大化玩家的数学希望或期望值，这是可能奖励的加权平均值，每个奖励针对事件的相应概率进行加权。

在一场比赛中，有一个或多个竞争者试图赢得比赛，即最大化他们的奖金。奖金是由一个规则来定义的，这个规则根据参赛者的行为来定量确定他们的奖励；这个功能被称为支付功能。每个玩家可以采取有限到无限数量的行动或决定来决定一个策略。每一个策略都以采用它的玩家的结果为特征，这可以是一种奖励(积极的/消极的)。游戏的结果完全由他们的策略顺序和其他玩家采取的策略决定。

我们如何描述每个玩家的游戏结果？如果你用回报来衡量一个策略的后果，那么每个策略都可以配一个值:负值会表示给对手一个支付，也就是惩罚；正值表示中奖，即领取了一份奖品。一般玩家与其策略相关联的收益或损失以及所有剩余玩家在给定时刻采取的策略由支付函数指示的货币值来表示。

一个游戏者做出的决定自然会与其他游戏者做出的决定相冲突或相一致，并从这种情况中衍生出各种类型的游戏。

表示两个参与者、两个公司或两个个人之间交互的有用工具是复式决策矩阵或表格。该决策表用于显示两个玩家进行的游戏的策略和奖金。

因此，决策矩阵是一种表示，通过它我们可以将玩家之间互动的所有可能结果进行分类，并为每个玩家分配在每种情况下竞争的奖励值。另一种表现形式涉及到每个决策或行动的执行顺序。每个游戏的这种特征可以通过树形图来描述，该图代表了从初始状态到最终状态的竞争者的游戏的每一种可能的组合，其中奖金被分配，如下图所示:

![](Images/f8441c0b-7e03-412f-8bd4-82e7583f7258.png)

要描述一个战略形势，需要四个基本要素:

*   **玩家**:游戏中的决策者(有谁参与？)
*   **动作**:玩家可以选择的可能动作，或者移动(他们能做什么？)
*   **策略**:玩家的行动计划(他们要做什么？)
*   **赢款**:玩家获得的可能收益(赚了多少？)

因此，策略是一个完整的或有计划的，或决策性的决定，它规定了玩家在任何可能的情况下必须如何行动。

作为一个完整的应急计划，策略通常定义了玩家在游戏中可能无法实现的情况下必须选择的行动。



# 游戏类型

游戏可以根据不同的范例进行分类:

*   合作
*   对称
*   总和
*   定序



# 合作博弈

当局中人的利益并不直接对立，而是存在利益的共同性时，就出现了合作博弈。玩家追求一个共同的目标，至少在游戏持续期间是这样；他们中的一些人可能倾向于联合以提高他们的收益。担保是由有约束力的协议提供的。共同兴趣的数学表示是什么？在一个联盟或同盟中个人利益联盟的概念是由基本游戏的定义来表达的，而一般联盟的价值是由一个称为特征函数的函数来衡量的。

相比之下，在非合作游戏中，也称为竞争游戏，玩家不能达成有约束力的协议(即使是通过监管)，不管他们的目标是什么。这一类别回答了约翰·纳西用他的纳什均衡给出的解决方案，由于其广泛的适用性，这可能是整个理论中最著名的概念。非合作博弈中采用的理性行为准则是个体的，称为最大策略。



# 对称游戏

在一个对称的游戏中，采用一种特定策略所获得的利润只取决于所采用的其他策略，而不是由采用这些策略的人来决定。如果玩家的身份可以在不改变收益的情况下改变，那么这个游戏就是对称的。

相比之下，在非对称博弈中，双方都没有相同的一系列策略。然而，有可能一个博弈对双方都有相同的策略，但它是不对称的。



# 零和游戏

零和博弈是常数为零的恒和博弈的特例。零和游戏模拟了所有冲突的情况，其中两个玩家的对比是完全的:一个玩家的胜利正好与另一个玩家的失败重合。换句话说，根据使用的策略，两个竞争者的赢款总和总是零。例如在国际象棋中，它意味着唯一三种可能的结果是胜利、失败和平局(奖励:+1、-1 和 0)。



# 顺序博弈

在顺序博弈中，后续的玩家会保留一些之前行为的知识。这并不意味着他们知道之前玩家的每一个动作。例如，玩家可能知道前一个玩家没有执行某个动作，而他们不知道第一个玩家实际执行了其他哪些可用动作。



# 博弈论应用

博弈论一直引起学者们的兴趣，因为它在实践领域和人类工作的所有领域都很有用:

*   **哲学**:分析了博弈论，因为它提供了一种方式来阐明一些哲学家的逻辑困难，比如康德、卢梭、霍布斯以及其他社会和政治理论家。
*   经济:商业世界中的许多投机行为可以用博弈论的方法来建模。一个著名的例子是寡头垄断价格的设定和囚徒困境之间的相似性。
*   生物学:虽然大自然通常被认为是残酷的，但许多不同物种之间还是有合作的。这种共存的原因可以用博弈论来建模。
*   人工智能:人类能够根据他们收到的环境刺激做出决定。取而代之的是，机器只有在根据许多条件编制了决策表的情况下才能做出决策。这种限制可以通过人工智能来克服，人工智能可以赋予机器从它们的创造者那里做出新的计划外决定的能力。这将要求程序能够根据观察到的刺激和经验生成新的回报矩阵。

在接下来的小节中，我们将考察一些应用博弈论规则的实际案例。



# 囚徒困境

囚徒困境是阿尔伯特·塔克在 20 世纪 50 年代作为博弈论问题提出的完全信息博弈。这个困境可以描述如下。两个人被指控犯罪。调查人员逮捕了他们两人，并把他们关在两个不同的牢房里，阻止他们相互交流。他们每个人都有两个选择:合作或不合作。还向他们解释了以下内容:

*   如果两名通敌者中只有一人指控另一人，通敌者可免于处罚，但另一人将被判处三年监禁
*   如果两人互相指控，两人都被判两年
*   如果两人都不合作，两人都会被判一年徒刑，因为他们已经犯了非法武器港罪

这一困境同样可以描述冷战期间美国和苏联(两个囚犯)在 20 世纪 50 年代的军备竞赛。

这个博弈可以用下面的决策矩阵来描述:

| A / B | 承认 | 保持沉默 |
| 承认 | (2,2) | (0,3) |
| 保持沉默 | (3,0) | (1,1) |

这个非合作博弈的最佳策略是(保持沉默，保持沉默)，因为我们不知道对方会选择做什么。对于这两个人中的每一个人，他们的目标都是最小化他们自己的信念。

保持沉默策略严格受坦白策略支配。通过剔除严格劣势策略，我们到达了纳什均衡，两个囚犯合作并被监禁两年。两者最好的结果当然是不合作(一年监禁而不是两年)，但这并不是一种平衡。

假设两人承诺万一被捕不合作。他们现在被关在两个不同的牢房里，不知道另一个人是否会信守承诺；如果一名囚犯不遵守承诺，而另一名遵守，那么第一名囚犯将被释放。因此，有一个两难的问题:合作还是不合作。博弈论告诉我们，只有一个平衡(保持沉默，保持沉默)。

如果我们认为美国和苏联是两个囚犯，而忏悔是原子军备(相反，否认就等于单方面裁军)，这一困境描述了在冷战时期，对这两个国家来说，减缓军备竞赛是不可避免的，尽管这一最终结果对两个超级大国中的任何一个都不是最佳的(对整个世界而言)。



# 猎鹿

猎鹿是一种游戏，由让-雅克·卢梭首次提出，其场景是两个男人可以在一次狩猎旅行中选择是试图抓住一只鹿还是一只野兔。他们的决定必须在不知道另一方的决定的情况下做出，并考虑到要捕获一只鹿，必须双方都决定选择后者作为目标，而野兔只需要一个人的承诺。该游戏还规定，野兔是一种不如鹿令人满意的奖励，它是一顿更好的饭，即使是在合作的两个猎人之间分享。因为每个猎人都忽略了对方的决定会是什么，所以这是非合作博弈。

这个博弈可以用下面的决策矩阵来描述:

| A \ B | 全是男人的 | 野兔 |
| 全是男人的 | (3,3) | (0,2) |
| 野兔 | (2,0) | (1,1) |

使用纯策略，纳什均衡的唯一解是通过射野兔找到的。这样，你就避免了一无所获的风险，如果你朝牡鹿开枪，而对方朝野兔开枪，就会出现这种结果。在混合策略的情况下，纳什的均衡是射向雄鹿的几率为 75%，射向野兔的几率为 25%。



# 小鸡游戏

吃鸡游戏是一个零和博弈的游戏。信息完整，同时有两个玩家参与。经典的例子是基于 1955 年詹姆斯·迪恩执导的电影《无缘无故的反叛》的挑战，在这部电影中，两个家伙同时发动汽车驶向悬崖，进行汽车比赛。如果两人在到达之前都突然转向，他们都会给同伴留下不好的印象；如果一个突然转向，另一个继续走一段更长的路，第一个会成为鸡的形象，而第二个会赢得同伴的尊重。如果两个都继续上路，他们都会死。

正如在囚徒困境中，两者的合作是一个不稳定的均衡，即使在短期内也不成立，即在一次性博弈的情况下。看看收益矩阵:

| A \ B | 转向 | 直的 |
| 转向 | (0,0) | (-1,1) |
| 直的 | (1,-1) | (-10,-10) |

在这个博弈中，两个玩家都没有优势策略，有两种潜在的平衡:(直行)和(直行，直行)。这个博弈被称为非协调博弈，因为双方采取相反的策略比另一方更好。当然，这两个玩家中的每一个都偏爱某种特定的平衡。游戏的解决方案来自于两个玩家之一的可信声明，即不愿意不惜任何代价转向。另一名玩家将被迫首先转弯，以避免驶下悬崖。



# 围棋比赛

围棋是两个人玩的战略棋类游戏。围棋起源于中国，在中国至少玩了 2500 年；它在东亚非常流行，但近年来已经蔓延到世界其他地区。尽管规则简单，但从战略上讲，这是一个非常复杂的游戏；一句韩国谚语说，没有一局围棋会下两次，如果你认为在一个 19 × 19 的围棋子上有 2.08 × 10 ^(170) 种不同的可能位置，这是可能的。

围棋是由两名棋手在一个 19 × 19 的格子内交替将黑白棋子(称为棋子)放置在一个由黑线组成的格子(称为围棋子)的空交叉点上。游戏的目的是控制一个比对手控制的更大的区域；为了这个目的，玩家们试图排列他们的石头，使他们不能被捕获，同时雕刻出对手不能入侵而不被捕获的领土。下图显示了一个 19 x 19 的 goban:

![](Images/5bd4fc11-12c4-437a-b349-335a0d5807c9.png)

除了五子棋的大小和开始的位置之外，规则已经保持了几个世纪，所以它可以被认为是仍然在玩的最古老的游戏。



# 游戏的基本规则

两个玩家，黑人和白人，轮流把他们颜色的石头放在棋盘上画的格子的空(交叉)点上。通常尼禄先行动；在差点游戏的情况下，当两个玩家中的一个比另一个强得多时，较弱的一方拿走黑色并在 goban 上有两个或更多差点石，Bianco 先移动。官方网格由 19 × 19 行组成，但规则适用于所有网格；一旦被使用，石头不能被移动到不同的地方。

水平或垂直相邻的相同颜色的石头形成一个组，其自由度是组成它的石头的自由度的总和，并且不能随后被分割，形成一个更大的石头。只有通过画在棋盘上的线相互连接的石头才构成一组；对角相邻的石头没有连接。可以通过在靠近它们的交叉点上玩其他石头来扩大组，或者通过在与两个或更多相同颜色的组相邻的交叉点上玩石头来将组连接在一起。

大多数规则不允许玩家使用石头，这样他的一个团队就没有自由了，这是一种自杀，但有一个例外:如果新的石头占领了一个或多个对立的石头，这些石头将首先被移除，剩下的石头至少有一个自由。据说这条规定禁止自杀。

玩家不能将游戏返回到对手之前的位置；这个规则被称为 ko 规则，用来防止相同动作的无限重复。

玩家可以通过，而不是玩石头；这通常发生在玩家认为他们没有其他有用的棋可走的时候。当双方球员连续传球时，比赛结束，计算得分。



# 评分规则

有两种计分方法来决定一场比赛的胜负；这两种方法只是偶尔会导致不同的结果，并且每种计数方法都有优点和缺点。第一种方法(称为日本计数)计算控制领土，是日本和韩国使用的方法，很可能是中国最初使用的方法；第二种方法(称为中国计数)，计算被占领的面积，是中国使用的一种方法，据信始于 15 世纪。

在基于面积的计算中，玩家的分数是棋盘上与他们的颜色相同的石头的数量加上被它们包围的空的交叉点。基于区域的分数要求玩家保留被捕获的石头，称为囚犯，他们在游戏结束时将死亡的石头添加到囚犯中。分数等于被玩家的石头包围的空交叉点的数量加上被俘虏的石头的数量。



# AlphaGo 项目

AlphaGo 是谷歌 DeepMind 开发的围棋游戏软件。这是第一个能够在没有障碍的情况下，在标准尺寸的围棋子(19 × 19)上击败人类冠军的软件。

DeepMind 是一家由 Alphabet 控股的英国人工智能公司。它成立于 2011 年，名为 DeepMind Technologies，于 2014 年被谷歌收购。

据 DeepMind 的研究员大卫·西尔弗(David Silver)称，AlphaGo 项目于 2014 年启动，旨在研究深度神经网络如何应用于围棋比赛。

AlphaGo 代表了对现有围棋程序的重大改进。与其他软件进行的 500 多场比赛，包括疯狂的石头和禅宗——AlphaGo(在单台计算机上运行)赢得了除一场比赛之外的所有比赛——并进行一系列类似的比赛，但打开 alpha go 集群赢得了所有 500 场比赛，并赢得了在单台计算机上进行的 77%的比赛。

分布式版本使用了 1，202 个 CPU 和 176 个 GPU，大约是单个计算机硬件的 25 倍。以下截图显示了项目网站:

![](Images/1f6ccab0-6c69-4cbd-ad08-69850bfec226.png)

2015 年 10 月，AlphaGo 以 5 比 0 击败欧洲冠军范辉，成为第一个能够在没有差点的情况下，在标准尺寸的 goban 上击败人类大师的软件。直到 2016 年 1 月 27 日^日才发布公告，与《自然》杂志上发表的一篇描述该软件所用算法的文章相吻合。

2016 年 3 月 9 日^日至 15 日^日之间，AlphaGo 与世界上最强围棋选手之一的韩国棋手 Lee Sedol 进行了一次会面。这场比赛分五场进行，分别于 3 月 9 日^日，10 日^日，12 日^日，13 日^日，15 日^日在首尔四季酒店举行，并进行直播。AlphaGo 运行在谷歌的云平台上，谷歌的服务器位于美国，该软件的移动由 DeepMind 团队成员兼围棋业余选手 Aja Huang 在 goban 上报告。会议采用中国规则，Komi 7.5 分，反思时间 2 小时，byoyomi 60 秒。所用的计算能力和和范辉见面时用的差不多。

在那次会议上，李·塞多尔在国际围棋锦标赛中赢得了世界第二多的胜利。尽管没有官方的国际排名，一些消息来源认为李·塞多尔是当时世界上第四好的选手。AlphaGo 并没有特别针对李开复的打法进行配置。

AlphaGo 因弃赛赢了 Lee Sedol 的前三局。他赢了第四局，AlphaGo 走了一百八十步。软件最后赢了最后一局放弃。

电脑在游戏中击败人类已经不是第一次了。1996 年 2 月 10 日^日，IBM 的超级计算机“深蓝”第一次单场击败了现在的国际象棋世界冠军；传奇的加里·卡斯帕罗夫赢了三场比赛，平了两场，最后四比二的结果是他的历史性方式，因为那是国际象棋世界冠军与计算机的最后一次胜利。

一年后，深蓝重返挑战赛，这次卡斯帕罗夫无事可做。卡斯帕罗夫怒不可遏，但在这种愤怒中有一种挫败感，即人类被一台机器超越了，不仅在体力上，而且在智力上。

几年后，回想起那次失败，卡斯帕罗夫说，从那时起，计算机已经发展到这样一个程度，即使我们今天在手机上拥有的最后一个国际象棋应用也能打败现在的国际象棋冠军。这是内存和计算能力的问题；毕竟，它足以加载成千上万的游戏和所有可能的变种到计算机上，它赢了。现在你也不再需要它了。

AlphaGo 的不同之处在于，深蓝是由设定了所有可能策略的人类训练的。在 AlphaGo 的例子中，机器人已经在没有任何人类干预的情况下进行了自我学习。它的秘密在于它的“人性”；这个人工智能系统以人脑的神经网络为模型。由于这一特点，AlphaGo 的玩法与人类相似，并在与自己进行了数百万场比赛后，在历史性的训练有素的挑战中脱颖而出。



# AlphaGo 算法

AlphaGo 背后的算法由两部分组成:

*   树形搜索过程
*   指导树搜索过程的卷积网络

总共训练了两种不同类型的三个卷积网络:两个策略网络和一个值网络。

AlphaGo 的算法结合了机器学习和树搜索技术，以及广泛的游戏和人类学习阶段。它在两个深度神经网络(价值网络和政策网络)的指导下，使用 MCTS 来选择行动。在被发送到神经网络之前，输入在预处理阶段被分析以提取一些特征(例如，动作对一系列共同模式的坚持)。

在训练的第一阶段，神经网络基于人类游戏进行监督学习，试图使用历史游戏中约 30，000，000 步的数据库来模仿它。一旦达到一定的玩耍量，学习就会通过与自身的其他实例玩耍来强化它。

该软件被编程为如果胜利概率低于某个阈值，则放弃游戏，例如，在 2016 年 3 月与 Lee Sedol 的会议中，该阈值被设置为 20%。



# 蒙特卡罗树搜索

MCTS 是一种启发式搜索算法，用于某些类型的决策过程，尤其是那些在博弈论中使用的过程。传统的人工智能算法非常强大，但它们在执行时需要大量内存。2006 年，Kocsis 和 Szepesvari 为了改进经典算法，引入了 MCTS。与极小极大相比，MCTS 方法的主要区别在于状态树的扩展并不完全发生。

极大极小，在决策理论中，是一种最小化最大(minimax)可能损失的方法；或者，最大化最小增益(马希民)。这是在博弈论中发现的，在两个玩家零和博弈的情况下，在交替移动(移动)和同时移动的情况下，随后扩展到更复杂的游戏和存在不确定性时支持的决策。

极大极小方法扩展整棵树，然后选择最佳移动，而 MCTS 方法执行深度搜索，直到到达最终状态。此时，它评估当前状态，为该节点赋值，并将其传播到所有路径，直到根节点。这个过程重复几次，以增量和非对称的方式构建一个树，每次迭代选择最紧急的节点进行扩展。

这种选择是基于一种政策，该政策旨在平衡尚未访问的部分的探索和似乎有希望的节点的开发。根据可用的时间或内存，树的构造会任意停止。另一个优点是它不必计算中间状态的值。仅考虑最终状态的情况需要较少的领域知识。Kocsis 和 Szepesvari 已经证明，蒙特卡罗树搜索，有足够的时间和内存，收敛到极小极大的最优结果。

MCTS 方法的基本算法在于迭代地构建搜索树，直到达到预先建立的时间或内存量。每个节点代表域中的一个状态，它的每个子节点都可能是下一个状态。该过程可分为四个阶段:

*   **选择**:从根节点 R 开始，我们递归地选择一个最佳子节点，直到到达 L 节点
*   **展开**:如果 L 不是终端节点(即游戏没有结束)，则生成一个或多个子节点，选择一个 C
*   **模拟**:模拟新 C 节点上的一次执行，产生一个结果
*   **反向传播**:模拟的结果通过选择的节点反向传播(直到根节点)，更新它们的统计数据

下图显示了 MCTS 步骤:

![](Images/240c28c9-8dbe-404b-8180-990763f51c86.png)

对于选择和扩展，使用树策略，它决定考虑哪个节点。相反，对于模拟，使用默认策略，该策略实现了域的知识，并产生了对应于最终叶子的状态值的估计。

我们看到的程序可以应用于任何位置有有限步数和有限长度的博弈。对于每个位置，确定所有可能的移动； *k* 随机游戏玩到最后，记录分数。导致最佳分数的移动被选择。

在应用这一过程时，主要问题产生于子节点的选择。为了在具有高平均胜率的招式之后的深度变体的开发和具有很少模拟的招式的探索之间保持某种平衡，一些措施是必要的。树木 ( **UCT** )的**置信上限用于平衡游戏中的开发和探索。下面的公式用于选择下一个要扩展的节点:**

![](Images/3c48077b-d398-4f4b-aee9-447ebee5898d.png)

在此公式中:

*   *N* 是当前节点(即父节点)被选择的次数
*   *n[j]是孩子被选中的次数*
*   *C* 是要设置的正参数
*   *X[j]是 *j* 节点累加值的平均值*

该算法平衡了收集的奖励(利用)的使用和很少访问的节点的探索。假设在初始采样中，随机分量权重很大，则在有效估计这些值之前，必须访问节点一定次数。



# 卷积网络

本质上，**卷积神经网络**(**CNN**)是人工神经网络。其实就像后者一样，CNN 是由神经元通过加权分支(权重)相互连接而成的；网的训练参数再次是权重和偏差。

在 CNN 中，神经元之间的连接模式受到了动物世界中视觉皮层结构的启发。大脑这一部分(视觉皮层)中存在的单个神经元在一个狭窄的观察区域(称为感受野)中对某些刺激做出反应。不同神经元的感受野部分重叠，因此它们一起覆盖整个视野。单个神经元对其感受野中发生的刺激的响应可以通过卷积运算在数学上近似。

与神经网络的训练相关的一切，即前向/后向传播和更新权重，也适用于这种情况；此外，整个 CNN 总是使用可微分成本的单一函数。然而，CNN 做出特定的假设，即它们的输入具有精确的数据结构，例如图像，这允许它们在其架构中采用特定的属性，以便更好地处理这样的数据。

正常的神经网络以完全连接的架构分层，其中每层的每个神经元都连接到前一层的所有神经元(不包括偏置神经元)，一般来说，随着输入数据大小的增加，神经网络不能很好地扩展。

我们举一个实际的例子:假设我们要分析一个图像来检测物体。首先，让我们看看图像是如何处理的。我们知道，在一幅图像的编码中，它被分成一个个小方块的网格，每个小方块代表一个像素。在这一点上，为了对彩色图像进行编码，为每个正方形识别一定数量的阴影、不同的颜色等级，并通过适当的比特序列对每一个进行编码就足够了。下图显示了一个简单的图像编码:

![](Images/5d91e8e3-7c06-4c85-95a5-361622e64b2e.png)

网格中方块的数量决定了图像的分辨率。例如，一幅 1600 像素宽、800 像素高(1600 x 800)的图像包含(乘以)1280000 个像素(或 120 万个像素)。对此，我们必须添加三个颜色通道，最终获得 *1600 x 800 x 3 = 3，840，000* 。因此，在第一个隐藏层中完全连接的每个神经元将有 3，840，000 个权重。这只是针对单个神经元，但考虑到整个网络，事情肯定会变得不可收拾。

CNN 被设计成直接在由像素表示的图像中识别视觉模式，并且不需要或非常有限地需要预处理。他们能够识别极其多变的图案，例如代表真实世界的手绘文字和图像。

典型地，在分类的情况下，CNN 由几个交替的卷积和子采样级(汇集)组成，后面是一个或多个完全连接的最终级。下图显示了一个经典的图像处理管道:

![](Images/a96fdb33-b2b3-4dc8-8ea4-5299d44e7a2a.png)

为了解决现实世界中的问题，可以根据需要经常组合和堆叠这些步骤。可以有两层、三层或多层卷积。您可以输入所有想要减少数据大小的池。

为了探索 CNN 的结构，我们将使用一个实际的例子:从作为输入层的图像开始，将有一定系列的卷积层，穿插有**整流线性单元** ( **ReLU** )层，并且在必要时，来自标准化和汇集层，最后将有最后系列的 FC 层，在输出层之前。下图是 CNN 架构的一个示例:

![](Images/02e3612b-ee81-49fd-9128-1897b3aa8c72.png)

基本思想是从一幅大图像开始，一步一步地不断减少数据，直到得到一个结果。卷积通道越多，神经网络就越能够理解和处理复杂的函数。

在 AlphaGo 架构中，实现了两个神经网络:

*   **策略网络**:决定下一步要采取的行动
*   **价值网**:提供当前位置游戏的赢家

AlphaGo 使用前面的两个网络将搜索树的巨大复杂性减少到一个小的、可管理的搜索空间。通过这种方式，暴力算法中每一步所需的数百种不同的移动被策略网络建议的一些最佳可能的移动所取代。

此外，价值网络降低了搜索的深度。在每个位置，价值网络试图预测哪个玩家会赢，而不是遍历底部的树来评估它。因此，它返回一个值，该值量化了可能的网络所建议的移动有多好。



# 摘要

在这一章中，你学习了博弈论的一般概念。博弈论是一门数学科学，研究和分析一个主体在与其他竞争主体冲突或战略互动的情况下的个人决策，旨在实现每个主体的最大利润。博弈论的主要目标是胜利；每个人都必须了解游戏规则，了解每一步行动的后果。一个人打算采取的一系列行动被称为策略。我们已经实时分析了理论预测的不同类型的游戏和各种实际应用:囚徒困境、猎鹿和吃鸡游戏。

然后我们探索了 AlphaGo DeepMind 项目。围棋是两个人玩的战略棋类游戏。AlphaGo 是谷歌 DeepMind 开发的围棋游戏软件。这是第一个能够在没有障碍的情况下，在标准尺寸的围棋子(19 × 19)上击败人类冠军的软件。AlphaGo 的算法结合了机器学习和树搜索技术，以及广泛的游戏和人类学习阶段。

然后我们看了看 MCTS。MCTS 是一种启发式搜索算法，用于某些类型的决策过程，尤其是那些在博弈论中使用的过程。它执行深度搜索，直到到达最终状态。

最后，我们讨论了卷积网络。CNN 是由神经元通过加权分支(权重)相互连接而成的人工神经网络；网络的训练参数是权重和偏差。这些网络用于指导树搜索过程。