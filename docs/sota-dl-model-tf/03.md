# 3.TensorFlow数据集

我们通过代码示例讨论和演示TensorFlow数据集的许多方面来介绍TensorFlow数据集。虽然 TensorFlow 数据集不是 ML 模型，但我们包含了这一章，因为我们在本书的许多章节中都用到它们。这些数据集由 TensorFlow 团队创建，旨在为实践 ML 实验提供多样化的数据集。

当我们开始使用 TensorFlow 时，我们不熟悉 TensorFlow 数据集的机制。我们主要处理数字数据。所以我们不得不花相当多的时间去熟悉它们。我们相信这一章会帮助你轻松地浏览这些数据集。如果您已经有了使用 TensorFlow 数据集的经验，您可能不需要完成本章中的示例。

章节的笔记本位于以下 URL:

[T2`https://github.com/paperd/deep-learning-models`](https://github.com/paperd/deep-learning-models)

## TensorFlow数据集简介

TensorFlow Datasets (TFDSs)提供了一个数据集集合，可用于 TensorFlow 或其他机器语言框架(如 Jax、Apache Spark、Accord)。网)。所有的 TFDSs 都以 tf.data.Datasets 的形式公开，这使得我们可以使用 tf.data API 轻松构建高性能的输入管道。确定性地下载和准备 TFDSs 也非常容易，这使它们成为数据集构建器。**数据集构建器**是一个利用一组方法为 ML 模型准备数据的对象。

不要混淆 TFDSs 和 tf.data，tf.data API 允许我们构建高效的数据管道。TFDSs 是 tf.data 的高级包装。

有关 TFDSs 的精彩概述，请阅读

[T2`www.tensorflow.org/datasets/overview`](http://www.tensorflow.org/datasets/overview)

## 导入 TensorFlow 库

导入库并将其别名为 **tf** :

```py
import tensorflow as tf

```

## GPU 硬件加速器

为了方便起见，我们重复这些指令来启用 GPU:

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

验证 GPU 是否处于活动状态:

```py
tf.__version__, tf.test.gpu_device_name()

```

如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

Note

如果出现错误**名称‘TF’未定义**，请重新执行代码以导入 TensorFlow 库！

## 可用数据集

所有数据集生成器都是 tfds.core.DatasetBuilder 类的子类。因此每个 TFDS 对象都被定义为一个 tfds.core.DatasetBuilder 对象。要获取可用 TFDS 生成器的列表，请使用 tfds.list_builders():

```py
import tensorflow_datasets as tfds

tfds.list_builders()

```

让我们看看目前有多少可用的:

```py
len(tfds.list_builders())

```

哇！有很多 TFDSs 可供深度学习实践！

要了解有关 TFDSs 的更多信息，请阅读

[T2`www.tensorflow.org/datasets/catalog/overview`](http://www.tensorflow.org/datasets/catalog/overview)

## 加载数据集

所有构建器都包含一个 *tfds.core.DatasetInfo* 对象，其中包含数据集的元数据。元数据可以通过 *tfds.load* API 或者*tfds . core . dataset builder*API 访问。tfds.load API 是 tfds.core.DatasetBuilder 的一个薄薄的包装器，所以它更容易使用。根据我们的研究，这是下载 TFDS 的首选方法。我们使用任何一个 API 都会得到相同的输出。

使用 tfds 加载数据集，Load:

```py
ds, info = tfds.load('mnist', split='train',
                     shuffle_files=True,
                     with_info=True,
                     try_gcs=True)
ds

```

tfds.load API 下载数据并将其保存为 TFRecord 文件。然后加载 TFRecord 文件并创建一个 TF . data . dataset。*TF record 格式*是一种用于存储二进制记录序列的简单格式。因此，变量 *ds* 包含一个由 MNIST 数据组成的 tf.data.Dataset。数据被混洗，元数据集被返回，数据集从 Google Cloud Service (GCS)中检索。 *MNIST* 数据库是一个手写数字的大型数据库，广泛用于 ML 领域的训练和测试。

tfds.load API 可用的常见参数包括

*   *分割*–分割数据(例如，“训练”、“训练”、“测试”、“训练[80%]”等)。).

*   *shuffle _ files*–控制是否在每个历元之间重排文件(TFDSs 将大数据集存储在多个较小的文件中)。

*   *as _ supervised*–如果为真，tf.data.Dataset 具有二元组结构(输入，标签)。如果为 False，则 tf.data.Dataset 具有字典结构。

*   *data _ dir*–数据集保存的位置(默认为~/tensorflow_datasets/)。

*   *with _ info = True*–返回包含数据集元数据的 tfds.core.DatasetInfo。

*   *download = False*–禁用下载。

文档中没有列出 *try_gcs* 参数。它告诉加载器从 GCS 中检索数据集。

## TFDS 元数据

要访问所有元数据，只需显示 *info* 对象的内容:

```py
info

```

从元数据中，我们看到 MNIST 包含 60，000 个用于训练的 28 × 28 特征图像和 10，000 个用于测试的 28 × 28 特征图像。训练集和测试集都有相应的标量标签。

## 迭代数据集

### 作为字典

默认情况下，tf.data.Dataset 包含一个 tf.Tensors 字典。让我们看一个来自数据集的示例:

```py
ds = ds.take(1)
ds

```

使用 *take()* 从 tf.data.Dataset 中获取 *n* 个示例。这个例子的形式是 *{'image': tf。张量，“标签”:tf。张量}* 。简单地说，每个例子包含一个图像张量和一个标签张量。图像张量是 28 × 28 矩阵，标签张量是标量。

图像数据的数据类型是 tf.uint8，它是 8 位无符号整数。一个 *uint8* 数据类型包含范围从 0 到 255 的整数。与所有无符号数一样，这些值必须是非负的。Uint8s 多用于图形。注意，颜色(代表颜色的像素)总是非负的。

标签数据的数据类型是 tf.int64，它是 64 位有符号整数。一个 *int64* 数据类型包含范围从负 9，223，372，036，854，775，808 到正 9，223，372，036，854，775，807 的有符号整数。

显示有关示例的信息:

```py
for example in ds:
  print ('keys:', list(example.keys()))
  image = example['image']
  label = example['label']
  print ('shapes:', image.shape, label)

```

需要一个循环来从数据集中提取图像和标签信息。

### 作为元组

通过使用 *as_supervised=True* ，tf.data.Dataset 包含要素和标注的元组:

```py
ds = tfds.load('mnist', split='train', as_supervised=True,
               try_gcs=True)
ds = ds.take(1)

for image, label in ds:
  print (image.shape, label)

```

在 iterate 语句中，只需包含变量来保存要素和标签。

### 作为 NumPy 数组

*tfds.as_numpy()* 函数将 tf.data.Dataset 转换为可迭代的 numpy 数组:

```py
ds = tfds.load('mnist', split='train', as_supervised=True,
               try_gcs=True)
ds = ds.take(1)

for image, label in tfds.as_numpy(ds):
  print (type(image), type(label), label)
  print (image.shape)

```

我们加载训练数据作为特征图像元组。然后我们举一个例子，把它转换成一个 NumPy 数组。

方便的是，我们可以将整个数据集作为 NumPy 数组加载(如果内存允许的话):

```py
image_train, label_train = tfds.as_numpy(
    tfds.load('mnist', split='train',
              batch_size=-1, as_supervised=True,
              try_gcs=True))

type(image_train), image_train.shape

```

通过使用 *batch_size=-1* ，整个数据集在单个批处理中加载。然后，该批处理被转换为 NumPy 数组。

Note

请注意，您的数据集可以容纳在内存中，并且所有示例在定型前都具有相同的形状。

由于数据集由 NumPy 数组组成，我们可以用普通的 Python 操作来检查它。获取示例的数量:

```py
len(list(image_train))

```

正如所料，在训练集中有 60，000 个特征图像元组。

检查第一个示例:

```py
image_train[0].shape, label_train[0]

```

图像张量的形状为 28 × 28 × 1。“1”尺寸意味着图像是灰度的。标签是代表图像类别的标量。

检查几个例子:

```py
for row in range(3):
  print (image_train[row].shape, label_train[row])

```

Tip

如果你喜欢使用 NumPy 数组而不是 tf。张量对象，在内存允许的情况下，一次性加载数据集。

## 形象化

我们可以方便地想象 TFDS 物体的图像。

### tfds.as_dataframe

可视化图像数据的一种方法是将 tf.data.Dataset 对象转换为 pandas。DataFrame 对象。加载数据集，举四个例子，并显示:

```py
ds, info = tfds.load('mnist', split='train', with_info=True,
                     try_gcs=True)

tfds.as_dataframe(ds.take(4), info)

```

我们加载了数据集及其元数据( *with_info=True* )以支持显示。

### 举个例子

我们还可以可视化从数据集中提取的示例。举四个例子，从每个张量中挤出“1”维，将挤出的张量加到一个数组中。我们需要挤出“1”维，因为 *imshow()* 函数需要一个二维矩阵作为输入:

```py
import matplotlib.pyplot as plt

images = []
for example in ds.take(4):
  img = tf.squeeze(example['image'])
  images.append(img)

```

可视化如清单 [3-1](#PC17) 所示的图像。

```py
rows, cols = 2, 2
plt.figure(figsize=(10, 10))
for i in range(rows*cols):
  plt.subplot(rows, cols, i + 1)
  plt.imshow(images[i], cmap='bone')
  plt.axis('off')

Listing 3-1Visualize Four Images from MNIST

```

### tfds.show_examples

另一种可视化图像的方法是使用 *show_examples()* :

```py
fig = tfds.show_examples(ds, info)

```

Note

show_examples 仅支持影像数据集。

几幅图像整齐地显示出来。在每个图像的下面是标签名称，它是一个字符串和一个在括号中的整数。例如，4 的图像下面有 4(4 ),因为它的标签是字符串“4”和整数“4”。

## 装载时尚-MNIST

加载时尚-MNIST 作为 TFDS 对象。*时尚-MNIST* 是 Zalando 文章图像的数据集，由 60，000 个样本的训练集和 10，000 个样本的测试集组成。Zalando 是一家在线时尚公司，利用人工智能(AI)来改善客户体验。该数据集旨在作为原始 MNIST 数据集的直接替代，用于 ML 算法的基准测试。原始的 MNIST 数据集作为实践数据集正在被逐步淘汰，因为它很容易从一个非常简单的神经网络中获得优异的性能。推荐时尚 MNIST，因为它更具挑战性。

加载数据集:

```py
fashion, fashion_info = tfds.load(
    'fashion_mnist',
    split='train',
    with_info=True,
    shuffle_files=True,
    as_supervised=True,
    try_gcs=True)

```

我们只加载训练数据。元数据在*信息*对象中可用。我们还混洗数据集，以确保每个数据点在模型上创建一个*独立变化*，而不会受到之前相同点的影响。通过设置 *as_supervised=True* ，返回的 tf.data.Dataset 具有表示为(input，label)的二元组结构。通过设置 *try_gcs=True* ，数据集直接从谷歌云服务(gcs)流出。

检查一个示例:

```py
for image, label in fashion.take(1):
  print (image.shape, label)

```

特遣部队。张量由一个图像和一个标签组成。图像形状为 28 × 28 × 1。“1”尺寸意味着图像是灰度的。标签是代表图像类别的标量值。

### [计]元数据

检查数据集:

```py
fashion

```

正如所料，数据集中的图像是 28 × 28 × 1 张量，标签是标量。

显示元数据:

```py
fashion_info

```

我们看到了大量关于数据集的信息。一个很重要的是数据如何拆分。方便的是，时尚 MNIST 的数据已经被分成了训练集和测试集。所以我们不必手动分割数据集！

访问 tfds.features.FeatureDict:

```py
fashion_info.features

```

特征字典包含有关图像和标签的信息。这种信息通常被称为元数据。以下代码片段提供了元数据。

获取类的数量:

```py
num_classes = fashion_info.features['label'].num_classes
num_classes

```

所以我们有十个分类标签。我们将类的数量放在一个变量中，以便在模型中使用。

获取类名:

```py
classes = fashion_info.features['label'].names
classes

```

我们看到每个类别标签的名称。将类标签放在变量中对于可视化很有用。

获取形状信息:

```py
print (fashion_info.features.shape)
print (fashion_info.features.dtype)
print (fashion_info.features['image'].shape)
print (fashion_info.features['image'].dtype)

```

我们看到特征的形状和数据类型。

#### 显示分割信息

访问 tfds.core.SplitDict:

```py
fashion_info.splits

```

获取可用拆分:

```py
list(fashion_info.splits.keys())

```

获取有关列车拆分的可用信息:

```py
print (fashion_info.splits['train'].num_examples)
print (fashion_info.splits['train'].filenames)
print (fashion_info.splits['train'].num_shards)

```

### 设想

在继续下一步之前，最好先将数据集可视化！在任何深度学习实验之前，我们都会探索元数据并可视化示例，以便在进行任何分析之前了解我们将会使用什么。想法是*了解你的数据*！

展示一些例子:

```py
fig = tfds.show_examples(fashion, fashion_info)

```

在每个图像的下面是标签名称，它是一个字符串和一个在括号中的整数。例如，一件外套的图像下面有 Coat(4 ),因为它的标签是字符串形式的“外套”和整数形式的(4)。

显示数据帧中的示例:

```py
tfds.as_dataframe(fashion.take(4), info)

```

举一些例子，想象一下:

```py
images, labels = [], []
for image, label in fashion.take(4):
  img = tf.squeeze(image)
  images.append(img), labels.append(label)

```

可视化如清单 [3-2](#PC33) 所示的图像。

```py
rows, cols = 2, 2
plt.figure(figsize=(10, 10))
for i in range(rows*cols):
  plt.subplot(rows, cols, i + 1)
  plt.imshow(images[i], cmap='bone')
  t = classes[labels[i]] + ' (' +\
      str(labels[i].numpy()) + ')'
  plt.title(t)
  plt.axis('off')

Listing 3-2Visualize Four Images from Fashion-MNIST

```

## 切片 API

所有构建器数据集都公开了定义为拆分的各种数据子集(例如[训练、测试])。当构造一个 tf.data.Dataset 时，我们可以指定希望对哪个(哪些)拆分进行切片。我们还可以检索分割的切片以及它们的组合。在本节中，我们将提供一些如何分割和切片 TFDS 的示例。了解如何手动将数据集拆分为定型集和测试集是一个好主意，以防遇到未预拆分的数据。了解如何对数据分割进行切片也是一个好主意(例如，训练、测试)。

### 切片指令

我们在创建的 tfds.load 对象中指定切片指令。指令或者以字符串的形式提供，或者通过 ReadInstruction API 提供。对于简单的情况，字符串更加紧凑，可读性更好，而 ReadInstruction API 提供了更多的选项，并且可能更容易与可变切片参数一起使用。将数据作为字符串加载意味着数据集是一个 Python 字符串。将数据作为 ReadInstruction 对象加载意味着数据集是一个 Python 对象。

Note

由于碎片是并行读取的，因此不能保证子分割之间的顺序一致。因此，阅读测试[0:100]后接测试[100:200]可能会以不同于阅读测试[:200]的顺序生成示例。

#### 字符串形式的指令

让我们看一些 tfds.load 示例:

装载整个列车组:

```py
fashion_train = tfds.load('fashion_mnist', split='train',
                          try_gcs=True)
fashion_train

```

在这种情况下，我们装载(或切片)从时尚-MNIST 分离的列车。所以 slice 只是这个上下文中 load 的另一个词。

将完整的“训练”分割和完整的“测试”分割作为两个不同的数据集加载:

```py
train_ds, test_ds = tfds.load('fashion_mnist',
                              split=['train', 'test'],
                              try_gcs=True)
train_ds, test_ds

```

加载交错在一起的完整“训练”和“测试”分割:

```py
train_test_ds = tfds.load('fashion_mnist', split='train+test',
                          try_gcs=True)
train_test_ds

```

从记录 100(包括)到记录 200(不包括)加载“列车”分割的切片:

```py
train_100_200_ds = tfds.load('fashion_mnist',
                             split='train[100:200]',
                             try_gcs=True)

```

在这种情况下，我们从训练集中加载一个片段。所以在这个上下文中，slice 和 load 有不同的含义。

从“火车”分割装载前 25%的切片:

```py
train_25pct_ds = tfds.load('fashion_mnist',
                           split='train[:25%]',
                           try_gcs=True)

```

从“序列”的前 10%到“序列”的后 80%加载切片:

```py
train_10_80pct_ds = tfds.load(
    'fashion_mnist', try_gcs=True,
    split='train[:10%]+train[-80%:]')

```

执行清单 [3-3](#PC40) 所示的十倍交叉验证。

```py
test_cv = tfds.load('fashion_mnist', try_gcs=True,
                    split=[f'train[{k}%:{k+10}%]'
                    for k in range(0, 100, 10)])
train_cv = tfds.load('fashion_mnist', try_gcs=True,
                      split=[f'train[:{k}%]+train[{k+10}%:]'
                      for k in range(0, 100, 10)])

Listing 3-3Tenfold Cross-Validation

```

**交叉验证**是一种评估预测模型的技术，通过将原始样本划分为训练模型的训练集和评估模型的测试集。*十倍交叉验证*是该技术的常见实现。

交叉验证过程从随机打乱数据集开始。下一步是将数据集分成 *k* 组。对于每个 k 组，将该组作为维持或测试数据集。将剩余的组作为训练数据集。通过在训练集上拟合模型并在测试集上对其进行评估，继续正常操作。数据样本中的每个观察值都被分配到一个单独的组中，并且在整个过程中必须留在该组中。因此，每个样本都有机会用于维持组 *1* 次，并用于训练模型*k–1*次。

对于我们的十倍交叉验证，这里有一些文档:

*   *验证数据集为 10%:

*   [0%:10%], [10%:20%], …, [90%:100%]

*   *训练数据集是互补的 90%:

*   [10%:100%](对应的验证集为[0%:10%])

*   [0%:10%] + [20%:100%](对于[10%:20%]的验证集)

*   [0%:90%](对于[90%:100%]的验证集)。

#### ReadInstruction API 的说明

我们展示了 ReadInstruction API 的等效指令，从加载完整的训练集开始:

```py
train_ds = tfds.load('fashion_mnist', try_gcs=True,
                     split=tfds.core.ReadInstruction('train'))

```

加载相同的数据，但是加载到对象中，而不是字符串中。

将完整的“训练”分割和完整的“测试”分割作为两个不同的数据集加载:

```py
train_ds, test_ds = tfds.load(
    'fashion_mnist', try_gcs=True,
    split=[tfds.core.ReadInstruction('train'),
           tfds.core.ReadInstruction('test')])

```

加载交错在一起的完整“训练”和“测试”分割:

```py
ri = tfds.core.ReadInstruction('train')\
     + tfds.core.ReadInstruction('test')
train_test_ds = tfds.load('fashion_mnist',
                          split=ri, try_gcs=True)

```

从“列车”分割的记录 100(包括)到记录 200(不包括)加载切片:

```py
train_100_200_ds = tfds.load(
    'fashion_mnist',
    split=tfds.core.ReadInstruction(
        'train', from_=100, to=200,
        unit='abs'), try_gcs=True)

```

加载“列车”分割的前 25%的切片:

```py
train_25_pct_ds = tfds.load(
    'fashion_mnist', try_gcs=True,
    split=tfds.core.ReadInstruction(
        'train', to=25, unit='%'))

```

从列车的前 10%到列车的后 80%装载切片:

```py
ri = (tfds.core.ReadInstruction('train', to=10, unit='%') +
      tfds.core.ReadInstruction('train', from_=-80, unit='%'))
train_10_80pct_ds = tfds.load('fashion_mnist',
                              split=ri, try_gcs=True)

```

执行清单 [3-4](#PC47) 所示的十倍交叉验证。

```py
tests = tfds.load('fashion_mnist', split=
    [tfds.core.ReadInstruction('train', from_=k,
                               to=k+10, unit='%')
     for k in range(0, 100, 10)], try_gcs=True)
trains = tfds.load('fashion_mnist', split=
    [tfds.core.ReadInstruction('train', to=k, unit='%') +
     tfds.core.ReadInstruction('train', from_=k+10, unit='%')
     for k in range(0, 100, 10)], try_gcs=True)

Listing 3-4Tenfold Cross-Validation with ReadInstruction

```

对于我们的十倍交叉验证，这里有一些文档:

*   *验证数据集将分别为 10%: [0%:10%]，[10%:20%]，…，[90%:100%]。

*   *训练数据集各占互补的 90%:

*   [10%:100%](对应的验证集为[0%:10%])

*   [0%:10%] + [20%:100%](对于[10%:20%]的验证集)

*   [0%:90%](对于验证集[90%:100%])

## 性能提示

本节面向那些有兴趣了解有关提高 TensorFlow 性能的更多信息的人。我们展示了三个技巧，但还有更多。要深入研究，请阅读以下网址:

[T2`www.tensorflow.org/guide/data_performance`](http://www.tensorflow.org/guide/data_performance)

[T2`www.tensorflow.org/datasets/performances`](https://www.tensorflow.org/datasets/performances)

### 自动缓存

默认情况下，TFDSs 自动缓存满足以下约束的数据集:

*   *数据集总大小(所有拆分)已定义且< 250 MB。

*   * shuffle_files 被禁用或只读取一个碎片。

所以不要摆弄自动缓存，除非你想改变默认设置。

### 基准数据集

使用 *tfds.core.benchmark(ds)* 对任何 tf.data.Dataset 对象进行基准测试。

我们可以在一个步骤中加载和预处理数据集:

```py
ds = tfds.load('fashion_mnist', split='train',
               try_gcs=True).batch(32).prefetch(1)

```

请注意，我们只是从数据集中抓取了一批。

对数据集进行基准测试:

```py
tfds.core.benchmark(ds, batch_size=32)

```

确保将 *batch_size* 参数初始化为相同的值！

再次运行基准测试:

```py
tfds.core.benchmark(ds, batch_size=32)

```

由于自动缓存，第二次迭代基准测试要快得多！

### 重新加载 TFDS 对象

我们第一次加载 TFDS 对象时，会花费相当多的时间，尤其是数据集很大的时候。下次我们加载 TFDS 对象时，速度会快很多，因为它已经在内存中了！如果您想看看实际情况，请创建一个新的笔记本并加载“时尚-MNIST·TFDS”对象。第二次加载它，你会发现它只需要很少的时间。

## 负荷时尚-MNIST 作为一个单一张量

我们不需要加载一个 TFDS 作为TensorFlow张量。我们可以把它作为一个 NumPy tf 来加载。张量，如果数据集适合内存的话。推荐的方法是将整个数据集作为单个张量(或 NumPy 数组)加载。可以通过设置 *batch_size=-1* 将所有示例作为一个 tf.Tensor 进行批处理。

将整个数据集作为单个 tf 加载。张量并将其转换为 NumPy 数组:

```py
(img_train, label_train), (img_test, label_test) = tfds.as_numpy(
    tfds.load(
        'fashion_mnist', try_gcs=True, as_supervised=True,
        split=['train', 'test'], batch_size=-1))

```

Note

才能操纵一个 tf。张量就像一个 NumPy 数组，把它作为一个 TFDS 作为单个张量加载。

显示形状:

```py
img_train.shape, label_train.shape

```

训练数据图像具有形状(60000，28，28，1)，标签具有形状(60000，)。

### TensorFlow消耗的就绪数据

缩放要素影像并将数据转换为 tf.data.Dataset 对象:

```py
train = img_train / 255.0
test = img_test / 255.0

train_ds = tf.data.Dataset.from_tensor_slices(
    (train, label_train))
test_ds = tf.data.Dataset.from_tensor_slices(
    (test, label_test))

```

由于单个张量是一个 NumPy 张量，我们可以简单地通过除以 255.0 来缩放图像，就像我们使用 NumPy 数组一样。

### 构建输入管道

为训练和测试数据设置参数并构建输入管道:

```py
BATCH_SIZE = 128
SHUFFLE_SIZE = 5000

train_f = train_ds.shuffle(SHUFFLE_SIZE).batch(BATCH_SIZE)
train_fm = train_f.cache().prefetch(1)

test_f = test_ds.batch(BATCH_SIZE)
test_fm = test_f.cache().prefetch(1)

```

批量大小和混洗大小是基于反复试验来设置的。我们建议您尝试不同的批量大小(如 32、64)，以了解它们对学习成绩的影响。洗牌的大小似乎没有批量大小对学习成绩的影响大。所以我们建议你在实验中保持这个常数。

### 建立模型

获取模型的输入形状:

```py
img_shape = img_train.shape[1:]
img_shape

```

导入必需的库:

```py
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.losses import SparseCategoricalCrossentropy
import numpy as np

```

清除以前的模型并为可复制性生成种子:

```py
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

创建模型:

```py
model = Sequential([
  Flatten(input_shape=img_shape),
  Dense(128, activation='relu'),
  Dropout(0.4),
  Dense(num_classes, activation=None)
])

```

### 编译和训练模型

使用 SparseCategoricalCrossentropy 编译(from_logits=True):

```py
model.compile(optimizer='adam',
  loss=SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

```

*from_logits=True* 属性通知损失函数由模型生成的输出值未被归一化。也就是说，没有将 softmax 函数应用于输出层神经元来产生概率分布。除非另有规定，否则 softmax 激活会自动应用于输出值。但是我们明确指出不应用任何激活。

训练模型:

```py
epochs = 10
history = model.fit(train_fm, epochs=epochs,
                    verbose=1, validation_data=test_fm)

```

## 将 Beans 作为 tf.data.Dataset 加载

让我们加载一个不同的数据集作为 tf.data.Dataset 对象，而不是作为单个 tf。张量，就像我们对时尚 MNIST 做的那样。除非您更喜欢使用 NumPy 张量，否则这是使用 TFDS 的首选方式。我们坚信，使用各种 TFDS 示例有助于您更好地理解如何使用这种类型的数据。beans 数据集是一个很好的数据集，因为它不包含太多的示例，并且只有三个类。

*Beans* 是一个数据集，包含使用智能手机相机在田间拍摄的豆子图像。它由三个类组成。两个是疾病类，一个是健康类。描述的疾病是角斑病和豆锈病。乌干达国家作物资源研究所的专家对数据进行了注释，数据由 Makerere AI 研究实验室收集。

示例被预先分成测试集、训练集和验证集。测试集和训练集分别包含 128 个和 1，034 个示例。验证集包含 133 个示例。当然，如果愿意，您可以手动以不同方式分割数据集。

加载数据集:

```py
beans, beans_info = tfds.load(
    'beans', with_info=True, as_supervised=True,
    try_gcs=True)

```

检查数据:

```py
beans

```

我们看到示例被分成包含 500 × 500 × 3 特征图像的测试、训练和验证集。这样我们就不用自己拆分数据了。

对于深度学习实验，数据集通常分为训练集和测试集。在行业中，建议将数据分为训练集、验证集和测试集。通常，训练数据用于学习，验证数据用于调整，测试数据用于推广。但是，我们可以使用测试数据进行调整，使用验证数据进行推广。

将数据分成三组对于工业目的来说更好，因为测试组从未被学习模型接触过。所以它可以更有把握地用于推广。专业的数据科学家擅长调整验证集中的模型。大多数在线教程只使用培训和测试分割，因为重点是学习而不是在行业中的应用。

### [计]元数据

检查信息对象:

```py
beans_info

```

检查形状:

```py
beans['train'], beans['test'], beans['validation']

```

检查分割:

```py
beans_info.splits

```

从元数据中，我们可以获得类标签和类的数量:

```py
class_labels = beans_info.features['label'].names
num_classes = beans_info.features['label'].num_classes
class_labels, num_classes

```

### 设想

我们可以用几种方式想象 TFDS。

使用 show_examples 方法显示示例:

```py
fig = tfds.show_examples(beans['train'], beans_info)

```

TFDS 包括一个显示一些示例的方法。标签以字符串形式显示，类名和数值显示在图像下方。

我们还可以将示例显示为数据帧:

```py
tfds.as_dataframe(beans['train'].take(4), info)

```

最后，我们可以手动显示示例。首先构建一个网格来显示多个示例。继续从训练集中选择图像:

```py
num = 30
images, labels = [], []
for feature, label in beans['train'].take(num):
  images.append(tf.squeeze(feature.numpy()))
  labels.append(label.numpy())

```

创建一个函数来显示示例网格，如清单 [3-5](#PC70) 所示。

```py
def display_grid(feature, target, n_rows, n_cols, cl):
  plt.figure(figsize=(n_cols * 1.5, n_rows * 1.5))
  for row in range(n_rows):
    for col in range(n_cols):
      index = n_cols * row + col
      plt.subplot(n_rows, n_cols, index + 1)
      plt.imshow(feature[index], cmap='twilight',
                 interpolation='nearest')
      plt.axis('off')
      t = ' ('  + str(target[index]) + ')'
      plt.title(cl[target[index]] + t, fontsize=7.5)
  plt.subplots_adjust(wspace=0.2, hspace=0.5)

Listing 3-5Function That Plots a Grid of Examples

```

绘制网格:

```py
rows, cols = 5, 6
display_grid(images, labels, rows, cols, class_labels)

```

显示第一个健康的 bean，如清单 [3-6](#PC72) 所示。

```py
for img, lbl in beans['train'].take(30):
  if lbl.numpy() == 2:
    plt.imshow(img)
    plt.axis('off')
    print (class_labels[lbl.numpy()], end=' ')
    print (lbl.numpy())
    break

Listing 3-6Visualization of the First Healthy Bean

```

### 检查形状

虽然我们从元数据中知道图像都是相同的形状，但是手动检查:

```py
for i, example in enumerate(beans['train'].take(5)):
  print('Image {} shape: {} label: {}'.\
        format(i+1, example[0].shape, example[1]))

```

就像可视化示例是一个好主意一样，手动检查几个示例的形状也是一个好主意。我们展示五个例子。只需更改 take 方法中的数字(参数值)即可改变示例的数量。

### 重新格式化图像

我们不必调整图像的大小，因为它们都是相同的形状。然而，这些图像非常大。为了提高训练性能，我们将图像调整为较小的尺寸。

创建一个调整图像大小和缩放图像的函数:

```py
IMAGE_RES = 224

def format_image(image, label):
  image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES))/255.0
  return image, label

```

我们将图像调整到相同的形状，因为学习模型需要相同大小的图像。我们鼓励您尝试图像大小，但不要让图像太大，因为较大的图像会消耗更多的内存。

### 为性能配置数据集

使用缓冲预取和缓存来提高 I/O 性能是一种很好的做法。因此，我们在构建输入管道时应用了这两种技术。

预取与训练步骤的预处理和模型执行重叠。当模型正在执行训练步骤 *s* 时，输入管道正在读取步骤 *s+1* 的数据。这样做可以最大限度地减少训练过程的步骤时间(相对于总和)以及提取数据所需的时间。在训练期间，应用 *tf.data.Dataset.prefetch* 转换来重叠数据预处理和模型执行。

*tf.data.Dataset.cache* 转换将数据集缓存在内存或本地存储中。使用这种转换可以避免在每个时期执行一些操作(如文件打开和数据读取)。具体来说，tf.data.Dataset.cache 转换会在第一个时期将图像从磁盘中加载出来后，将它们保留在内存中。因此，数据集不会成为训练过程中的瓶颈。如果数据集太大而不适合内存，请使用此操作创建一个高性能的磁盘缓存。

构建输入管道:

```py
BATCH_SIZE = 32
SHUFFLE_SIZE = 500

train_batches = beans['train'].shuffle(SHUFFLE_SIZE).\
  map(format_image).batch(BATCH_SIZE).cache().prefetch(1)

validation_batches = beans['test'].\
  map(format_image).batch(BATCH_SIZE).cache().prefetch(1)

```

### 建立模型

获取输入形状:

```py
for img, lbl in train_batches.take(1):
  in_shape = img.shape[1:]
in_shape

```

输入形状如预期的那样是 TensorShape([224，224，3])。现在，我们将输入形状作为学习模型中使用的变量。

导入不在内存中的库:

```py
from tensorflow.keras.layers import Conv2D, MaxPooling2D

```

清除模型并生成种子:

```py
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

创建一个多层 CNN，如清单 [3-7](#PC79) 所示。

```py
model = Sequential([
  Conv2D(32, (3, 3), activation = 'relu',
         input_shape=in_shape, strides=1,
         kernel_regularizer='l1_l2'),
  MaxPooling2D(2, 2),
  Conv2D(64, (3, 3), activation='relu'),
  MaxPooling2D(2, 2),
  Conv2D(128, (3, 3), activation='relu'),
  MaxPooling2D(2),
  Conv2D(128, (3, 3), activation='relu'),
  MaxPooling2D(2, 2),
  Flatten(),
  Dense(512, activation='relu'),
  Dense(num_classes, activation='sigmoid')
])

Listing 3-7Multilayered CNN

```

由于前馈网络在处理大图像时表现不佳，我们需要一个 CNN。该模型包括二维空间数据的四个卷积层。一个**卷积**是对一个输入(在我们的例子中是一个图像)简单应用一个过滤器，导致一个激活。对输入重复应用相同的过滤器会产生一个激活图，称为特征图。一个*特征图*指示在诸如图像的输入中检测到的特征的位置和强度。

每个卷积图层都是一个 MaxPooling2D 图层，用于对 2D 空间数据执行最大池化操作。 **Max pooling** 通过在由沿特征轴的每个维度的池大小定义的窗口上取最大值，从卷积层对输入表示进行下采样。窗口在每个维度上以步长移动。

**汇集**包括选择一个汇集操作，就像一个应用于特征地图的过滤器。汇集操作或过滤器的大小小于要素地图的大小。具体来说，它几乎总是以 2 像素的步长应用 2 × 2 像素。一个**步距**是在输入矩阵(或图像)上移动的像素数。当跨度为 1 时，我们一次移动一个像素的过滤器。当步幅为 2 时，我们一次移动过滤器 2 个像素，以此类推。

### 编译和训练模型

使用 SparseCategoricalCrossentropy 编译(from_logits=True):

```py
loss = tf.keras.losses.\
       SparseCategoricalCrossentropy(from_logits=True)

model.compile(optimizer='adam',
              loss=loss,
              metrics=['accuracy'])

```

我们使用这个损失函数，因为我们的模型的输出层中的激活是“sigmoid”。我们使用这个激活，因为它可以很好地处理数据集。尝试不同的激活功能。

训练模型:

```py
epochs = 10
history = model.fit(
    train_batches, epochs=epochs,
    verbose=1, validation_data=validation_batches)

```

### 预测

因为我们使用测试数据进行调优，所以基于*验证数据集*进行预测(因为它从未被模型看到)。为验证集构建输入管道，以便为预测做好准备:

```py
validate = beans['validation'].\
  map(format_image).batch(BATCH_SIZE).cache().prefetch(1)

```

进行预测:

```py
predictions = model.predict(validate)

```

该变量包含每个示例的预测。每个预测是一个包含十个预测的数组。所以我们需要一个额外的步骤来得到实际的预测。

获取第一个示例的实际预测值:

```py
first_prediction = tf.math.argmax(predictions[0])
class_labels[first_prediction.numpy()]

```

使用 tf.math.argmax API 从预测数组中获取实际预测。

获得多个预测，如清单 [3-8](#PC85) 所示。

```py
p = []
for row in range(8):
  pred = tf.math.argmax(predictions[row])
  p.append(pred.numpy())
  print ('class:', '(' + str(pred.numpy()) + ')', end=' ')
  print (class_labels[pred.numpy()])

Listing 3-8Get Multiple Predictions

```

我们得到八个预测。更改范围参数值以获得您想要的多或少。

了解模型的准确性:

```py
for i, (_, label) in enumerate(beans['validation'].take(8)):
  if label.numpy() == p[i]:
    print ('correct')
  else:
    print ('incorrect', end=' ')
    print ('actual:', label.numpy(), 'predicted:', p[i])

```

对于前八个预测，我们可以看到模型的表现有多好。我们可以通过获取整个验证集并计算平均值来获得总体准确性。

## 摘要

我们通过示例学习了如何使用 TFDSs。我们还对时尚 MNIST 和 beans 数据进行了培训，以便使用 TFDSs 进行实践。