# 12.快速风格转换

**神经风格转移(NST)** 是一种计算机视觉技术，它获取两幅图像——一幅内容图像和一幅风格参考图像——并将它们混合在一起，这样得到的输出图像保留了内容图像的核心元素，但看起来像是以风格参考图像的风格绘制的。NST 网络的输出图像称为仿作。**模仿作品**是一种视觉艺术、文学、戏剧或音乐作品，模仿一位或多位其他艺术家的作品风格(或特征)。与模仿不同，仿作是赞美而不是嘲笑它模仿的作品。

在 NST 的许多应用中，内容图像是照片，风格参考图像是绘画。但是这不是一个要求。NST 的奇妙之处在于，我们可以将一幅著名画家的作品混合起来，这样输出的图像看起来就像是以参考图像风格绘制的内容图像。所以我们可以训练 NST 网络自动创建新的艺术效果图！

Note

在数据科学术语中，术语网络和模型是可以互换的(至少在使用神经网络模型时)。

该技术通过优化输出图像以匹配内容图像的内容统计和样式参考图像的样式统计来实现。使用卷积网络从图像中提取统计数据。其思想是训练网络，使基本输入图像与内容和风格参考图像相匹配。NST 通过使用反向传播最小化内容和样式参考距离(或损失)来变换基本输入图像，以创建与内容图像的内容和样式参考图像的样式相匹配的图像。

为了将内容和样式表示提取到仿品中，神经网络包括中间层。中间层表示特征地图，随着我们深入网络以定义各个图像的内容和风格的表示，这些特征地图变得越来越高序。在训练期间，网络试图将基本输入图像匹配到中间层的相应样式和内容目标表示。

对于执行图像分类的网络，它必须理解图像。因此，它将原始图像作为输入像素，并通过将原始图像像素转换为图像中存在的复杂特征的转换来构建内部表示。网络的*中间层*执行转换，允许模型从图像的原始输入像素中提取有意义的特征。因此中间层能够精确地描述输入图像的内容和风格。

尽管结果令人惊讶，但 NST 的运行速度很慢，因为它将任务视为一个优化问题，需要数百甚至数千次迭代才能在单个图像上执行样式转换！为了解决这种低效率，深度学习研究人员开发了一种被称为快速(神经)风格转移的技术。**快速风格转换**使用深度神经网络，但训练一个独立的模型，在*单*前馈通道中转换图像！因此，训练有素的快速风格转移模型可以通过网络仅用一次迭代(或时期)而不是数百或数千次来风格化任何图像。

## 为什么风格转移很重要？

不是每个人都是天生的艺术家。但随着风格转移等技术的进步，几乎任何人都可以享受创作和分享艺术杰作带来的快乐。

有了风格转移的变革力量，艺术家可以很容易地把他们的创造性审美借给别人。没有内在艺术能力的人可以创造新的和创新的艺术风格，与原创杰作共存。所以风格转移使人们能够培养自己的创造力！创造性的转变可能会导致新的艺术表现，否则可能永远不会被创造出来。

## 任意神经艺术风格化

虽然快速风格转移网络很快，但它们仅限于预选的几个风格，因为必须为每个风格图像训练单独的神经网络。*任意神经艺术风格化* (ANAS)通过使用风格网络和变形网络减轻了这种限制。*风格网络*学习如何将图像分解成代表其风格的 100 维向量(或风格向量)。*变换网络*学习如何从样式矢量和原始内容图像产生最终的风格化图像。

Note

样式向量也被称为样式瓶颈向量。

ANAS 网络是快速网络的最新化身。ANAS 被认为优于 NST 和快速风格转换，因为它们能够实时进行任意风格转换。所以它们比 NST 更快，比快速风格转换更灵活，因为它们自动适应任意的新风格。

ANAS 添加了一个*自适应实例标准化* (AdaIN)层，它将内容特征的均值和方差与样式特征的均值和方差对齐。ANAS 实现了与最快的现有方法相当的速度，而没有预定义风格集的限制。ANAS 还允许灵活的用户控制，如内容风格的权衡，风格插值，颜色和空间控制，只有一个单一的前馈神经网络传递！

章节的笔记本位于以下 URL:

[T2`https://github.com/paperd/deep-learning-models`](https://github.com/paperd/deep-learning-models)

由于 ANAS 是最快和最好的 NST 实现，我们用一个端到端的代码实验来演示它。我们使用来自 TensorFlow Lite 模块的预训练转移模型进行了第二次 ANAS 实验。

**TensorFlow Lite** 是一套帮助开发人员在移动、嵌入式和物联网设备上运行 TensorFlow 模型的工具。它支持基于设备的 ML 推断，具有低延迟和较小的二进制大小。**物联网**设备是一种附有传感器的设备，可以在互联网的帮助下将数据从一个物体传输到另一个物体或传输给人。*低延迟*描述了一种经过优化的计算机网络，能够以最小的延迟(或延迟)处理大量的数据消息。此类网络旨在支持需要对快速变化的数据进行近实时访问的操作。

通过导入主 TensorFlow 库并实例化 GPU，开始设置 Colab 生态系统。

## 导入 TensorFlow 库

导入库并将其别名为 **tf** :

```py
import tensorflow as tf

```

将 TensorFlow 库别名为 tf 是常见的做法。

## GPU 硬件加速器

为了方便起见，我们在 Colab 笔记本中包含了启用 GPU 的步骤:

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

验证 GPU 是否处于活动状态:

```py
tf.__version__, tf.test.gpu_device_name()

```

如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

Note

如果得到错误**名称****'****TF****'****未定义**，重新执行代码导入 TensorFlow 库！

## ANAS 实验

我们使用*任意图像风格化-v1-256* 网络进行实验。该网络是用于快速任意图像风格转换的预训练模型。该网络不要求调整图像的大小，但更喜欢样式参考图像约为 256 像素，因为它是在 256 × 256 像素的图像上训练的。但是内容图像可以是任意大小。

Note

我们发现尺寸大于或小于 256 × 256 像素的样式参考图像不能提供非常引人注目的模仿。因此，我们强烈建议将样式参考图像调整到首选大小。

### 导入必需的库

导入:

```py
from matplotlib import gridspec
import matplotlib.pylab as plt
import numpy as np
import tensorflow_hub as hub
from PIL import Image

```

模块指定放置子情节的网格的几何形状。需要设置网格的行数和列数。 *TensorFlow Hub* 是一个预先训练好的机器学习模型的仓库，只需几行代码就可以在任何地方进行调整和部署。*图像*模块用于表示一个 Python 图像库(PIL)图像。 *PIL* 模块提供了许多工厂功能，包括从文件中加载图像和创建新图像的功能。

### 从 Google Drive 获取图片

我们从 Google Drive 上获得这个实验的图片。其他选项包括(但不限于)从本地驱动器上传图像或从维基共享下载图像。

Note

确保所有图片都在 Google Drive 的 *Colab Notebooks* 目录中。

安装 Google Drive:

```py
from google.colab import drive
drive.mount('/content/gdrive')

```

单击 URL。选择一个 Google Gmail 帐户。点击*允许*按钮。将授权码复制并粘贴到文本框中，然后按下键盘上的 *Enter* 键。

加载并显示样式参考图像:

```py
img_path = 'gdrive/My Drive/Colab Notebooimg/serene.jpeg'
style = Image.open(img_path)
plt.axis('off')
_ = plt.imshow(style)

```

图片可通过 Apress 网站或 my GitHub 获得。

获取图像类型:

```py
type(style)

```

该图像是 PIL 图像。

获取样式参考图像的形状:

```py
w, h = style.size
w, h

```

*size* 方法返回 PIL 图像的形状。

加载并显示内容图像:

```py
img_path = 'gdrive/My Drive/Colab Notebooimg/'\
  'humming_bird.jpeg'
content  = Image.open(img_path)
plt.axis('off')
_ = plt.imshow(content)

```

获取图像类型:

```py
type(content)

```

获取内容图像的形状:

```py
w, h = content.size
w, h

```

### 预处理图像

由于两幅图像都是 PIL 图像，我们将它们转换为张量，以便预训练的模型可以使用它们。风格参考图像的推荐尺寸是 256 × 256，因为这是我们用于该实验的预训练风格传递网络所期望的尺寸。内容图像可以是任何大小。

将样式参考图像转换为 NumPy 数组并缩放它:

```py
style_array = tf.keras.preprocessing.image.img_to_array(
    style) / 255.
style_array.shape

```

将样式参考图像的大小调整为样式传输网络所期望的大小:

```py
style_img = tf.image.resize(style_array, (256, 256))
style_img.shape

```

瞧。样式参考图像已准备好供样式传送网络使用。

将内容图像转换为 NumPy 数组并缩放它:

```py
content_img = tf.keras.preprocessing.image.img_to_array(
    content) / 255.
content_img.shape

```

因为内容图像可以是任何大小，所以它现在可以被样式传输网络消费。

### 显示处理过的图像

创建显示功能:

```py
def display_one(img):
  plt.imshow(img)
  plt.axis('off')
  plt.show()

```

显示处理后的样式参考图像:

```py
display_one(style_img)

```

显示处理后的内容图像:

```py
display_one(content_img)

```

### 准备图像批次

尽管处理后的内容和样式参考图像可由样式传输网络使用，但它期望每个图像作为单独的一批。所以每一批一定是形状*【batch _ size，image_height，image_width，3】*的 4D 张量。由于内容和样式参考图像目前是形状为*【image _ height，image_width，3】*的 3D 张量，我们添加了一个批量维度“1”来满足要求。网络还要求图像作为TensorFlow张量。所以我们把它们转换成TensorFlow张量。

图像的输入和输出值应该在范围[0，1]内。我们已经通过缩放两幅图像满足了这一要求。内容和样式参考图像的形状不必匹配。所以我们在这方面很好。

Note

模仿(输出图像)形状改编自内容图像形状。

将批次维度添加到两个图像:

```py
style_image = np.expand_dims(style_img, axis=0)
content_image = np.expand_dims(content_img, axis=0)
style_image.shape, content_image.shape

```

将 NumPy 图像转换为TensorFlow张量:

```py
style_tensor = tf.convert_to_tensor(style_image)
content_tensor = tf.convert_to_tensor(content_image)

```

### 加载模型

如前所述，我们使用*任意图像风格化-v1-256* 网络来创建一个仿作。早期的 NST 模型限于预选的少数几种风格，因为必须为每种风格图像训练单独的神经网络。任意样式转换通过包括样式网络和转换器网络来减轻这种限制。

*风格网络*学习如何将一幅图像分解成代表其风格的 100 维向量(或风格瓶颈向量)。 *transformer network* 学习如何从风格瓶颈矢量和原始内容图像中产生最终的风格化图像。任意风格网络是最先进的(在撰写本文时)，因为它们比原始 NST 网络更快，比快速风格传输网络更灵活。

加载预训练的 ANAS 网络:

```py
p1 = 'https://tfhub.dev/google/magenta/'
p2 = 'arbitrary-image-stylization-v1-256/2'
URL = p1 + p2

hub_handle = URL
hub_module = hub.load(hub_handle)

```

从预先训练好的 ANAS 网络构建中心模块。

### 给模型喂食

演示任意风格转换:

```py
outputs = hub_module(content_tensor, style_tensor)
pastiche = outputs[0]
pastiche.shape

```

中枢模块的签名接受经处理的内容图像和经处理的样式参考图像，以通过学习如何混合这两个张量来创建仿作。*签名*是训练模型所需的过程语法。

用 GPU 训练很快！但是用 CPU 训练中枢模块签名需要一些时间。

### 探索仿制品

探索图像形状:

```py
pastiche_numpy = tf.squeeze(pastiche).numpy()
pastiche_numpy.shape, content_img.shape

```

将仿作转换为 NumPy 以便于探索。形状与内容图像不完全相同，但非常接近。

探索风格化图像张量的切片:

```py
pastiche_numpy[0][0]

```

仿品具有与内容和样式参考图像相同的像素特征。

提取矩阵组件:

```py
m = pastiche_numpy
r, c, channels = m.shape[0], m.shape[1], m.shape[2]
r, c, channels

```

仿作阵列是由 184 行、280 列和 3 个通道组成的 3D 矩阵。

获取矩阵中的像素数:

```py
pixels = r * c
pixels

```

每个 RGB 通道有 51，520 个像素。 **RGB** 是指三种色调的光(红、绿、蓝)，可以混合在一起产生不同的颜色。组合红、绿、蓝光是在电视、电脑显示器和智能手机等屏幕上产生彩色图像的标准方法。 *RGB 颜色模型*是一个加法模型，因为三种颜色的光束被加在一起(光谱波长乘以光谱波长)以创建最终的色谱。

检查 RGB 通道像素是否被缩放:

```py
red = m[m[:, :, 0] < 1, 0] < 1
green = m[m[:, :, 1] < 1, 0] < 1
blue = m[m[:, :, 2] < 1, 0] < 1
print (len(red), len(green), len(blue))
print (red, green, blue)

```

该算法检查每个通道中的像素是否小于 1。我们显示每个经过算法修改的通道的长度，以查看它们是否包含预期的像素数。到目前为止一切都没问题。

由于没有显示所有真值表值，我们还需要一个步骤来验证缩放是否按预期工作:

```py
all(red), all(green), all(blue)

```

如果 iterable 中的所有项目都为真，函数 *all()* 返回真；否则，它返回 False。因此所有像素都被缩放。一个**可迭代的**是一个可以被迭代的对象。Python 列表是 iterable 的一个例子。

但是我们可以检查是否所有矩阵像素都在*一个*步骤中被缩放:

```py
truth = np.where((m < 1), True, False)
truth.all()

```

Note

我们演示了多步骤的过程，以提供风格化图像的引擎盖下一瞥。

### 设想

创建一个可视化函数，如清单 [12-1](#PC28) 所示。

```py
def show_n(images, titles=('',)):
  n = len(images)
  image_sizes = [image.shape[1] for image in images]
  w = (image_sizes[0] * 6) // 320
  plt.figure(figsize=(w  * n, w))
  gs = gridspec.GridSpec(1, n, width_ratios=image_sizes)
  for i in range(n):
    plt.subplot(gs[i])
    plt.imshow(images[i][0], aspect='equal')
    plt.axis('off')
    plt.title(titles[i] if len(titles) > i else '')
  plt.show()

Listing 12-1Visualization Function

```

将原始内容、样式参考和风格化图像可视化:

```py
show_n([content_image, style_image, pastiche],
       titles=['Original content image', 'Style image',
               'Pastiche'])

```

创建一个函数来可视化新的风格化图像:

```py
def display_pastiche(img, size):
  plt.figure(figsize = size)
  plt.imshow(tf.squeeze(img))
  plt.axis('off')
  plt.show()

```

视觉化:

```py
f_size = (10, 15)
display_pastiche(pastiche, f_size)

```

### 多幅图像的图像风格化

创建样式引用和内容图像列表。从每个列表中选择一个来创建一个仿制品。

#### 华盖创意

创建一个从 Google Drive 抓取图像的功能:

```py
def get_image(img_path):
  return Image.open(img_path)

```

获取样式图像，如清单 [12-2](#PC33) 所示。

```py
d = 'gdrive/My Drive/Colab Notebooimg/dali.jpg'
dali = get_image(d)
v = 'gdrive/My Drive/Colab Notebooimg/van-gogh.jpg'
van_gogh = get_image(v)
m = 'gdrive/My Drive/Colab Notebooimg/modern.jpg'
modern = get_image(m)
e = 'gdrive/My Drive/Colab Notebooimg/escher.jpeg'
escher = get_image(e)
pic = 'gdrive/My Drive/Colab Notebooimg/picasso.jpg'
picasso = get_image(pic)
p = 'gdrive/My Drive/Colab Notebooimg/pollock.jpg'
pollock = get_image(p)
mon = 'gdrive/My Drive/Colab Notebooimg/monet.jpg'
monet = get_image(mon)

Listing 12-2Get Style Reference Images

```

显示样式参考图像形状:

```py
dali.size, van_gogh.size, modern.size, escher.size,\
picasso.size, pollock.size, monet.size

```

获取内容图像:

```py
t = 'gdrive/My Drive/Colab Notebooimg/teddy.jpeg'
teddy = get_image(t)
e = 'gdrive/My Drive/Colab Notebooimg/einstein.jpg'
einstein = get_image(e)
g = 'gdrive/My Drive/Colab Notebooimg/gem.jpeg'
gem = get_image(g)

```

显示内容图像形状:

```py
teddy.size, einstein.size, gem.size

```

#### 处理图像

创建一个如清单 [12-3](#PC37) 所示的预处理函数。

```py
def preprocess(img, style=True):
  img_array = tf.keras.preprocessing.image.img_to_array(
      img) / 255.
  if style:
    img_array = tf.image.resize(img_array, (256, 256))
  return\
  tf.convert_to_tensor(np.expand_dims(img_array, axis=0))

Listing 12-3Preprocessing Function

```

该函数将 PIL 图像转换为 NumPy 数组。然后，它对数组进行缩放、调整大小，并将其转换为具有适当维数的TensorFlow张量。

如清单 [12-4](#PC38) 所示处理样式参考图像。

```py
dali_style = preprocess(dali)
van_gogh_style = preprocess(van_gogh)
modern_style = preprocess(modern)
escher_style = preprocess(escher)
picasso_style = preprocess(picasso)
pollock_style = preprocess(pollock)
monet_style = preprocess(monet)

dali_style.shape, van_gogh_style.shape, modern_style.shape,\
escher_style.shape, picasso_style.shape, pollock_style.shape,\
monet_style.shape

Listing 12-4Process the Style Reference Images

```

处理内容图像:

```py
einstein_content = preprocess(einstein, False)
teddy_content = preprocess(teddy, False)
gem_content = preprocess(gem, False)
einstein_content.shape, teddy_content.shape, gem_content.shape

```

#### 可视化处理过的图像

在列表中放置样式参考和内容图像:

```py
styles = [dali_style, van_gogh_style, modern_style,
          escher_style, picasso_style, pollock_style,
          monet_style]
contents = [einstein_content, teddy_content, gem_content]

```

创建一个显示张量的函数，如清单 [12-5](#PC41) 所示。

```py
def display_tensors(imgs, r, c):
  _, axs = plt.subplots(r, c, figsize=(12, 12))
  axs = axs.flatten()
  for img, ax in zip(imgs, axs):
    ax.imshow(tf.squeeze(img))
    ax.axis('off')
  plt.show()

Listing 12-5Function to Display Tensors

```

显示样式张量:

```py
rows, cols = 1, 7
display_tensors(styles, rows, cols)

```

显示内容张量:

```py
rows, cols = 1, 3
display_tensors(contents, rows, cols)

```

#### 创建参考词典

创建一个字典来表示样式张量:

```py
style_names = {'dali' : styles[0],
               'van_gogh' : styles[1],
               'modern' : styles[2],
               'escher' : styles[3],
               'picasso' : styles[4],
               'pollock' : styles[5],
               'monet' : styles[6]}

```

创建一个字典来表示内容张量:

```py
content_names = {'einstein' : contents[0],
                 'teddy' : contents[1],
                 'gem' : contents[2]}

```

#### 创造一个仿制品

创建一个函数来创建一个仿作:

```py
def create(c, s):
  content_im = content_names[c]
  style_im = style_names[s]
  outputs = hub_module(content_im, style_im)
  return content_im, style_im, outputs[0]

```

创造一个仿制品:

```py
content, style = 'einstein', 'dali'
content_im, style_im, sim = create(content, style)

f_size = (7, 9)
display_pastiche(sim, f_size)

```

Tip

尝试内容和风格参考图像，创建自己的仿作。当然，你也可以根据自己的图片进行仿制！

创建可视化列表:

```py
imgs = [content_im, style_im, sim]

```

可视化内容、风格和模仿:

```py
display_tensors(imgs, 1, 3)

```

试试泰迪·罗斯福的:

```py
content, style = 'teddy', 'picasso'
content_im, style_im, sim = create(content, style)

f_size = (8, 10)
display_pastiche(sim, f_size)

```

可视化内容、风格和模仿:

```py
imgs = [content_im, style_im, sim]
display_tensors(imgs, 1, 3)

```

试一个带宝石的:

```py
content, style = 'gem', 'escher'
content_im, style_im, sim = create(content, style)

f_size = (8, 10)
display_pastiche(sim, f_size)

```

可视化内容、风格和模仿:

```py
imgs = [content_im, style_im, sim]
display_tensors(imgs, 1, 3)

```

## TensorFlow Lite 实验

*TensorFlow Lite* 是一个开源的深度学习框架，可以在设备上运行 TensorFlow 模型。**设备门户** (ODPs)允许手机用户轻松浏览、购买和使用移动内容和服务。ODP 平台使运营商能够在大型服务组合中提供一致的品牌化设备体验。TensorFlow Lite 提供了一个 ODP 平台，可以在他们的手机上试验和部署深度学习实验。

要在您的设备上开始使用 TensorFlow Lite，请阅读

[T2`www.tensorflow.org/lite/examples`](http://www.tensorflow.org/lite/examples)

有关优秀的 TensorFlow Lite 风格传输示例，请阅读

[T2`www.tensorflow.org/lite/examples/style_transfer/overview`](http://www.tensorflow.org/lite/examples/style_transfer/overview)

但是 TensorFlow Lite 不一定要部署在设备上。它可以在个人电脑上运行。我们在 PC 上的 Colab 笔记本中运行这个实验有三个原因。首先，我们知道我们的 PC 有足够的内存来运行完整的 TensorFlow 实验。因此，运行 TensorFlow Lite 实验不会有任何问题！拥有一个真正的键盘和一个大屏幕也很好。其次，我们不知道您使用的是哪种类型的设备(例如，Android、iOS 等。).第三，TensorFlow Lite 模块预装在 Colab！

如果您想在设备上开发，我们绝对推荐 TensorFlow Lite，因为它已经过优化，可以在各种设备上运行，包括手机、嵌入式 Linux 设备和微控制器。因此 TensorFlow Lite 比 TensorFlow 具有更好的设备性能和更小的二进制文件大小。

### 预训练 TensorFlow Lite 模型的架构

内容图像与我们在第一个实验中已经处理过的图像完全相同。与第一个实验一样，风格图像在被输入到风格转换模型之前被转换(或成为瓶颈)成 *100 维风格瓶颈向量*。但是与第一个实验不同，我们通过模型手动将样式图像处理成可消费的形式。

TensorFlow Lite 艺术风格转换模型包括两个子模型——风格预测模型和风格转换模型。*风格预测模型*是一个预训练的基于 MobileNet-v2 的神经网络，它获取输入风格参考图像并将其转换为 100 维风格瓶颈向量。*风格转换模型*是一个神经网络，它将风格瓶颈向量应用于内容图像以创建仿作。

### 裁剪图像

从图像中去除不想要的噪声。

导入显示所需的库:

```py
import matplotlib as mpl

mpl.rcParams['figure.figsize'] = (12,12)
mpl.rcParams['axes.grid'] = False

```

mpl 参数设置笔记本其余部分所有图像的显示尺寸。

获取内容图像:

```py
content_cd = content_names['einstein']
content_cd.shape

```

样式转换模型需要大小为 384 × 384 的内容图像:

```py
dim = [384, 384]
content_lte = tf.image.resize(content_cd, dim)
content_lte.shape

```

集中裁剪内容图像:

```py
content_lite = tf.image.resize_with_crop_or_pad(
    content_lte, dim[0], dim[1])
content_lite.shape

```

裁剪是图像最基本的数据扩充过程之一。这个想法是从图像的外围去除不想要的或不相关的噪声，改变其纵横比或改善其整体构成。

获取样式图像:

```py
style_lte = style_names['modern']
style_lte.shape

```

居中裁剪样式参考图像:

```py
dim = [256, 256]
style_lite = tf.image.resize_with_crop_or_pad(
    style_lte, dim[0], dim[1])
style_lite.shape

```

#### 显示裁剪的图像

创建一个显示函数，如清单 [12-6](#PC60) 所示。

```py
def imshow(image, title=None):
  if len(image.shape) > 3:
    image = tf.squeeze(image, axis=0)
  plt.axis('off')
  plt.imshow(image)
  if title:
    plt.title(title)

Listing 12-6Display Function for Cropped Images

```

显示裁剪的图像:

```py
plt.subplot(1, 2, 1)
imshow(content_lite, 'Content Image')

plt.subplot(1, 2, 2)

imshow(style_lite, 'Style Image')

```

### 风格化图像

我们将裁剪后的风格图像输入到风格预测模型中，以创建风格瓶颈向量。然后，我们将裁剪后的内容图像和新创建的样式瓶颈向量输入到样式转换模型中，以创建一个仿作。

#### 创建样式预测模型

为风格预测模型创建一个函数，如清单 [12-7](#PC62) 所示。

```py
def run_style_predict(processed_style_image):
  # load the model
  interpreter = tf.lite.Interpreter(
      model_path=style_predict_path)
  # set model input
  interpreter.allocate_tensors()
  input_details = interpreter.get_input_details()
  interpreter.set_tensor(
      input_details[0]["index"], processed_style_image)
  # calculate style bottleneck
  interpreter.invoke()
  style_bottleneck = interpreter.tensor(
      interpreter.get_output_details()[0]["index"])()
  return style_bottleneck

Listing 12-7Function to Create the Style Prediction Model

```

该函数创建一个样式预测模型来对裁剪后的样式图像运行样式预测( *style_lite* )。该函数接受经过处理的样式参考图像。然后，它将样式预测路径加载到解释器对象中( *tf.lite.Interpreter* )。解释器是一个 TensorFlow Lite 预训练的基于 MobileNet-v2 的神经网络。解释器使用 style_predict_path 创建一个 tflite 预测样式。然后设置解释器模型的输入。该函数继续计算风格瓶颈。该函数通过使用瓶颈计算和 tflite 预测风格来将风格图像转换成 100 维风格瓶颈向量而结束。简单地说，该函数接受一个样式参考图像，并将其转换为一个 100 维的样式瓶颈向量。

设置样式预测路径:

```py
tflite_predict = 'style_predict.tflite'
p1 = 'https://tfhub.dev/google/lite-model/magenta/'
p2 = 'arbitrary-image-stylization-v1-256/'
p3 = 'int8/prediction/1?lite-format=tflite'
URL = p1 + p2 + p3

style_predict_path = tf.keras.utils.get_file(
    tflite_predict, URL)

```

将裁剪后的样式图像转换为 100 维的样式瓶颈向量:

```py
style_bottleneck = run_style_predict(style_lite)
print('style bottleneck vector shape:',
      style_bottleneck.shape)

```

#### 创建样式转换模型

为样式转换模型创建一个函数，如清单 [12-8](#PC65) 所示。

```py
def run_style_transform(
    style_bottleneck, processed_content_image):
  # load the model
  interpreter = tf.lite.Interpreter(
      model_path=style_transform_path)
  # set model input
  input_details = interpreter.get_input_details()
  interpreter.allocate_tensors()
  # set content and style bottleneck
  interpreter.set_tensor(
      input_details[0]["index"], processed_content_image)
  interpreter.set_tensor(
      input_details[1]["index"], style_bottleneck)
  interpreter.invoke()
  # return the transformed content image
  return interpreter.tensor(
      interpreter.get_output_details()[0]["index"])()

Listing 12-8Function to Create the Style Transform Model

```

该函数创建样式转换模型，该模型将样式瓶颈向量应用于内容图像以创建仿作。该函数接受一个样式瓶颈向量和一个经过处理的内容图像。然后，它将样式转换路径加载到解释器对象(tf.lite.Interpreter)中。解释器使用 style_transform_path 创建一个 tflite 转换样式。然后设置解释器模型的输入。该功能继续为解释器设置已处理的内容图像和样式瓶颈向量。该功能通过返回转换后的内容图像而结束。

设置样式转换路径:

```py
tflite_transform= 'style_transform.tflite'
p1 = 'https://tfhub.dev/google/lite-model/magenta/'
p2 = 'arbitrary-image-stylization-v1-256/'
p3 = 'int8/transfer/1?lite-format=tflite'
URL = p1 + p2 + p3

style_transform_path = tf.keras.utils.get_file(
    tflite_transform, URL)

```

### 创作仿作

利用风格瓶颈将内容图像风格化:

```py
stylized_image = run_style_transform(
    style_bottleneck, content_lite)

```

确保风格化图像(或仿作)是 TensorFlow Lite 预训练模型预期的大小:

```py
pastiche = tf.image.resize(stylized_image, [384, 384])
pastiche.shape

```

想象一下这个仿制品:

```py
imshow(pastiche, 'Pastiche')

```

### 风格融合

通过将内容图像的风格融合到风格化的输出中，我们使模仿看起来更像内容图像。

#### 准备内容图像

通过风格预测模型重塑内容图像以供消费:

```py
dim = [256, 256]
content_blend = tf.image.resize_with_crop_or_pad(
    content_lite, dim[0], dim[1])
content_blend.shape

```

将整形后的内容图像转换成 100 维风格瓶颈向量:

```py
style_bottleneck_content = run_style_predict(
    content_blend)
style_bottleneck_content.shape

```

内容图像现在是一个 100 维的样式瓶颈向量。

#### 融合风格瓶颈向量

将样式瓶颈向量与内容样式瓶颈向量(*style _ through _ content*)混合。

定义内容混合比率(在 0 和 1 之间):

```py
content_blending_ratio = 0.5

```

将内容图像混合到仿品中的范围是从 0%到 100%。要从内容图像中不提取样式，请指定 0%。百分之零意味着混合仿品与仿品相同。要从内容图像中提取所有样式，请指定 100%。我们将混合比例设置为 50%，以从内容图像中提取一半的样式。

从样式瓶颈和内容样式瓶颈向量中获得混合样式瓶颈向量:

```py
style_bottleneck_blended =\
  content_blending_ratio * style_bottleneck_content +\
  (1 - content_blending_ratio) * style_bottleneck

```

使用样式瓶颈使内容图像风格化:

```py
stylized_image_blended = run_style_transform(
    style_bottleneck_blended, content_lite)

```

想象一下这个仿制品:

```py
imshow(stylized_image_blended, 'Blended Stylized Image')

```

### 保存仿作

将仿作保存到本地驱动器。

创建一个函数将张量转换成 PIL 图像，如清单 [12-9](#PC76) 所示。

```py
def tensor_to_image(tensor):
  tensor = tensor * 255
  tensor = np.array(tensor, dtype=np.uint8)
  if np.ndim(tensor) > 3:
    assert tensor.shape[0] == 1
    tensor = tensor[0]
  return Image.fromarray(tensor)

Listing 12-9Function to Convert a Tensor to a PIL Image

```

该函数放大缩放后的张量，并将其转换为 NumPy 数组。然后，它剥离“1”维度，并返回一个 PIL 图像。

将文件保存到本地驱动器:

```py
from google.colab import files

fn = 'patiche.jpg'
tensor_to_image(stylized_image_blended).save(fn)
files.download(fn)

```

从 google.colab 库中导入*文件*模块。对 PIL 图像调用函数和保存方法。使用文件模块中的下载方法将 PIL 映像下载到本地驱动器。

## 摘要

第一个实验通过端到端代码实验演示了 NST 的 ANAS 实现。第二个实验使用 TensorFlow Lite 模块中的预训练传输模型演示了 NST 的 ANAS 实施。