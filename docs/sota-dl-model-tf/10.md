# 十、生成对抗网络

生成建模是一种无监督的学习技术，涉及自动发现和学习输入数据中的规律(或模式)，以便经过训练的模型可以生成新的示例，这些示例很可能是从原始数据集中提取的。一种流行的生成模型是生成对抗网络。**生成对抗网络** (GANs)是生成模型，它创建类似于训练数据的新数据实例。

GANs 将该问题描述为一个有监督的学习问题，有两个子模型，即一个生成器模型和一个鉴别器模型。*生成器*模型被训练以生成新的示例。*鉴别器*模型试图将例子分为真实的(来自领域)或虚假的(生成的)。该域表示来自原始训练集的图像。这两个模型在一个零和对抗游戏中一起训练，直到鉴别器模型被愚弄大约一半的时间。训练的结果是生成似是而非的例子。

甘在图像到图像的翻译任务中擅长使用真实的例子，比如把夏天的照片翻译成冬天的照片，或者把白天翻译成夜晚的照片。他们还成功地生成了逼真的物体、场景和人物的照片，甚至人类都无法分辨这些照片是假的。甘斯可以创造出看起来像人脸照片的图像，即使这些人脸并不属于任何真实的人。

我们用一个代码示例演示了一个 GAN。我们还用代码示例演示了深度卷积 gan(dcgan)。GAN 使用前馈网络进行学习，而深度卷积 GAN 使用卷积网络进行学习。

章节的笔记本位于以下 URL:

[T2`https://github.com/paperd/deep-learning-models`](https://github.com/paperd/deep-learning-models)

我们从 GAN 实验开始。我们继续进行两个深度卷积 GAN (DCGAN)实验。GAN 从输入图像生成传真图像。DCGAN 做同样的事情，但是产生更真实的传真。

通过导入主 TensorFlow 库并实例化 GPU，开始设置 Colab 生态系统。

## 导入 TensorFlow 库

导入库并将其别名为 **tf** :

```py
import tensorflow as tf

```

## GPU 硬件加速器

为了方便起见，我们在 Colab 笔记本中包含了启用 GPU 的步骤:

1.  点击左上角菜单中的*运行时*

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

验证 GPU 是否处于活动状态:

```py
tf.__version__, tf.test.gpu_device_name()

```

如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

Note

如果得到错误**名称****'****TF****'****未定义**，重新执行代码导入 TensorFlow 库！

## 甘实验

通过将学习产生目标输出的生成器与学习从生成器的输出中辨别真实数据的鉴别器配对，GAN 可以在它创建的新数据实例中产生高度真实的数据。生成器试图愚弄鉴别器，鉴别器试图避免被愚弄。

在训练期间，生成器和鉴别器具有相反的目标。鉴别者试图区分假图像和真图像。发生器试图产生看起来足够真实的图像来欺骗鉴别器。

我们从一个简单的前馈神经网络开始，向您展示如何在一个简单的数据集上进行训练。

### 加载数据

将时尚 MNIST 加载为 NumPy 数组:

```py
import tensorflow_datasets as tfds

x_train_img, _ = tfds.as_numpy(
    tfds.load('fashion_mnist', split='train',
              batch_size=-1, as_supervised=True,
              try_gcs=True, shuffle_files=True))

```

由于我们的实验是*无监督*，所以只需要训练图像。

获取示例数量:

```py
len(x_train_img)

```

### 规模

秤示例:

```py
import numpy as np

images = x_train_img.astype(np.float32) / 255
images.shape

```

验证缩放比例:

```py
x_train_img[0][0], images[0][0]

```

缩放按预期工作。

### 构建 GAN

获取输入形状:

```py
in_shape = images.shape[1:]
in_shape

```

清除和播种:

```py
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

导入库:

```py
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten,\
  Reshape

```

按照清单 [10-1](#PC10) 所示制造发电机。

```py
codings_size = 30

generator = Sequential([
  Dense(32, activation='selu', input_shape=[codings_size]),
  Dense(64, activation='selu'),
  Dense(128, activation='selu'),
  Dense(28 * 28, activation='sigmoid'),
  Reshape(in_shape)
])

Listing 10-1The Generator

```

正如我们所知，一个*生成器*被用来从问题域中生成新的看似合理的例子。它将随机分布作为输入(通常是高斯分布)，并输出一些数据(通常是图像)。随机输入可以被认为是要生成的图像的潜在表示(或编码)。因此，发生器与可变自编码器中的解码器具有相同的功能，并且可以以相同的方式用于生成新图像。然而，他们接受的训练非常不同。

生成器通过整合来自鉴别器的反馈来学习创建假数据。它学习使鉴别器将其输出分类为真实的。

发生器训练要求发生器和鉴别器之间的集成比鉴别器训练要求的更紧密。训练发电机的 GAN 部分包括随机输入、发电机网络、鉴频器输出和发电机损耗。生成器网络将随机输入转换为数据实例。鉴别器网络对产生的数据进行分类。发电机损耗惩罚发电机未能欺骗鉴别器。

现在，构建如清单 [10-2](#PC11) 所示的鉴别器。

```py
discriminator = Sequential([
  Flatten(input_shape=in_shape),
  Dense(128, activation='selu'),
  Dense(64, activation='selu'),
  Dense(32, activation='selu'),
  Dense(1, activation='sigmoid')
])

Listing 10-2The Discriminator

```

我们还知道，一个*鉴别器*用于将示例分类为真实(来自域)或虚假(从生成器生成)。它将来自生成器的假图像或来自训练集的真实图像作为输入，并猜测输入图像是假的还是真实的。鉴别器是一个常规的二元分类器(真实或伪造的图像)。它以图像作为输入，以包含单个单元的密集层结束。

创建模型:

```py
gan = Sequential([generator, discriminator])

```

### 编译鉴别器模型

由于鉴别器自然是二进制分类器(假图像或真图像)，我们自然使用二进制交叉熵损失。由于生成器只通过 GAN 模型训练，所以我们*不需要*来编译。GAN 模型也是二元分类器，因此它可以使用相同的损失函数。重要的是，鉴别器不应该在第二阶段被训练。所以我们在编译 GAN 模型之前把它设为不可训练。

```py
discriminator.compile(
    loss='binary_crossentropy', optimizer='rmsprop')
discriminator.trainable = False
gan.compile(loss='binary_crossentropy', optimizer='rmsprop')

```

输入鉴别器的训练数据有两个来源——真实数据和虚假数据。对于我们的实验，真实的数据实例是时尚 MNIST 图像。鉴别器在训练中使用这些实例作为正面例子。生成器会创建虚假的数据实例。鉴别器在训练中使用这些实例作为反面例子。

在鉴别器训练期间，发电机不训练。它的权重保持不变，同时为鉴别器提供训练样本。

鉴别器连接到两个损失函数——发生器和鉴别器。在鉴频器训练期间，鉴频器忽略发电机损耗，仅使用鉴频器损耗。鉴别器对来自生成器的真实数据和虚假数据进行分类。鉴别器损失惩罚将真实实例误分类为假实例或将假实例误分类为真实实例的鉴别器。鉴频器通过鉴频器网络从鉴频器损耗反向传播来更新其权重。

### 构建输入流水线

建立批量为 32 的流水线:

```py
batch_size = 32

dataset = tf.data.Dataset.from_tensor_slices(
    images).shuffle(1000)
dataset = dataset.batch(
    batch_size, drop_remainder=True).prefetch(1)

```

### 为训练创建自定义循环

由于训练循环是不寻常的，我们不能使用常规的拟合方法。相反，我们创建了一个自定义循环，它接受数据集来遍历图像。训练是不寻常的，因为它不是连续的。在阶段 1 中，鉴别器在由发生器提供的真实和虚假图像上被训练。生成器根据鉴频器在阶段 2 中学习到的内容进行训练。所以在训练过程中有反馈回路。

创建一个训练循环函数，如清单 [10-3](#PC15) 所示。

```py
def train_gan(gan, dataset, batch_size,
              codings_size, n_epochs=50):
  generator, discriminator = gan.layers
  for epoch in range(n_epochs):
    print('Epoch {}/{}'.format(epoch + 1, n_epochs))
    for X_batch in dataset:
      # phase 1 - training the discriminator
      noise = tf.random.normal(
          shape=[batch_size, codings_size])
      generated_images = generator(noise)
      X_fake_and_real = tf.concat(
          [generated_images, X_batch], axis=0)
      y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)
      discriminator.trainable = True
      discriminator.train_on_batch(X_fake_and_real, y1)
      # phase 2 - training the generator
      noise = tf.random.normal(
          shape=[batch_size, codings_size])
      y2 = tf.constant([[1.]] * batch_size)
      discriminator.trainable = False
      gan.train_on_batch(noise, y2)
    plot_multiple_images(generated_images, 8)
    plt.show()

Listing 10-3Custom Training Loop Function

```

每个训练迭代分为两个阶段。第一阶段训练鉴别器。从训练集中采样一批真实图像，由生成器产生等量的伪图像。伪图像的标签设置为 0，真实图像的标签设置为 1。使用二进制交叉熵损失在标记的批次上训练鉴别器一步。重要的是，反向传播仅在此阶段优化鉴别器的权重。第二阶段训练发电机。发生器产生另一批假图像，鉴别器区分假图像和真图像。没有真实图像被添加到批处理中，并且所有标签都被设定为 1 以指示它们是真实的(尽管它们不是真实的)。这个想法是，发生器产生的图像被鉴别器错误地认为是真实的！至关重要的是，鉴别器的权重在此步骤中被冻结，因此反向传播仅影响生成器的权重。

从技术上讲，阶段 1 将高斯噪声输入到发生器中，以产生假图像。目标 *y1* 对于假图像设置为 0，对于真实图像设置为 1。然后在该批次上训练鉴别器。第二阶段向 GAN 输入一些高斯噪声。发生器从制造假图像开始。然后，鉴别者试图猜测图像是假的还是真的。这个想法是让鉴别者相信假图像是真的，所以目标 *y2* 被设置为 1。

人类可以很容易地忽略噪声，但 ML 算法却很难。人类察觉不到的微小像素变化会极大地改变神经网络做出准确预测的能力。研究表明，*噪声*和*高斯模糊*在测试模型上表现出近乎即时的平滑。**高斯模糊**(也称为高斯平滑)是通过高斯函数模糊图像的结果。在 ML 中被广泛用于降低图像噪声和减少细节。

有关添加噪声的优秀资源，请阅读

[T2`https://blog.roboflow.com/why-to-add-noise-to-images-for-machine-learning/`](https://blog.roboflow.com/why-to-add-noise-to-images-for-machine-learning/)

创建一个绘图函数，如清单 [10-4](#PC16) 所示。

```py
import matplotlib.pyplot as plt

def plot_multiple_images(images, n_cols=None):
  n_cols = n_cols or len(images)
  n_rows = (len(images) - 1) // n_cols + 1
  if images.shape[-1] == 1:
    images = np.squeeze(images, axis=-1)
  plt.figure(figsize=(n_cols, n_rows))
  for index, image in enumerate(images):
    plt.subplot(n_rows, n_cols, index + 1)
    plt.imshow(image, cmap='binary')
    plt.axis('off')

Listing 10-4Plotting Function for Generated Images

```

使用未经训练的生成器生成图像:

```py
tf.random.set_seed(0)
np.random.seed(0)

noise = tf.random.normal(shape=[batch_size, codings_size])
generated_images = generator(noise)
plot_multiple_images(generated_images, 8)

```

正如所料，生成的图像并不令人印象深刻。当然，我们还有待训练模型！

### 训练 GAN

训练 GAN 几个时期:

```py
n = 5

train_gan(gan, dataset, batch_size, codings_size, n_epochs=n)

```

生成的图像比完全没有训练要好，但仍然不会给人留下太深刻的印象！我们训练了 50 个时代，进步甚微。也就是说，图像质量停止提高。我们在这里只训练几个纪元，因为这需要相当多的时间。你可以随意训练更多的纪元，但是你可能需要午休，因为训练会花费很多时间。

Note

如果图像看起来像斑点，重启 Colab 笔记本运行时并重新运行实验。

从经过训练的 GAN 生成图像:

```py
np.random.seed(0)
tf.random.set_seed(0)

noise = tf.random.normal(shape=[batch_size, codings_size])
generated_images = generator(noise)
plot_multiple_images(generated_images, 8)

```

添加一些高斯噪声，使用生成器模型生成一些图像。因为批次大小为 32，所以生成了 32 个图像。我们添加高斯噪声来补偿用高斯噪声训练 GAN。

## 用小图像进行 DCGAN 实验

使用具有 GANs 的 CNN 架构可以产生比具有 GANs 的简单前馈网络好得多的结果。一个**深度卷积生成对抗网络** (DCGAN)是一个以卷积神经网络作为其生成器和鉴别器的 GAN。我们用 CNN 代替前馈网络作为发生器和鉴别器。在许多情况下，DCGAN 架构在图像处理任务方面实现了卓越的性能。

### 创建生成器

导入库:

```py
from tensorflow.keras.layers import BatchNormalization,\
  Conv2D, Conv2DTranspose, LeakyReLU, Dropout

```

如清单 [10-5](#PC21) 所示创建发电机。

```py
codings_size = 100

dc_generator = Sequential([
  Dense(7 * 7 * 128, input_shape=[codings_size]),
  Reshape([7, 7, 128]),
  BatchNormalization(),
  Conv2DTranspose(
      64, kernel_size=5, strides=2, padding='SAME',
      activation='selu'),
  BatchNormalization(),
  Conv2DTranspose(
      1, kernel_size=5, strides=2, padding='SAME',
      activation='tanh'),
])

Listing 10-5DCGAN Generator

```

首先，将投射到 128 维的小张量(7 × 7 像素)馈入发生器，产生 6272 维空间。乘以 7 × 7 × 128 得到维度空间。目标是增加张量图像大小以匹配 28 × 28 的时尚 MNIST 图像，并将深度减少到 1 以匹配通道大小。我们通过反复试验创建了这个网络。尝试初始张量大小，但要知道这样做会改变发生器和鉴别器的神经元计算。

具体来说，生成器接受大小为 100 的编码，并将它们投影到 6，272 个维度(7 × 7 × 128)。然后，生成器将投影整形为一个 7 × 7 × 128 的张量，该张量被批量归一化，并被馈送到一个步长为 2 的转置卷积层。步幅将张量向上采样到 14 × 14，因为它使 7 × 7 的维度加倍。这层也将张量的深度从 128 减少到 64。结果再次被批量归一化，并被馈送到另一个步长为 2 的转置卷积层。步幅将其上采样至 28 × 28，因为它是 14 × 14 维度的两倍。该层还将张量的深度从 64 减少到 1。输出张量具有形状(28，28，1)，这是目标，因为时尚-MNIST 图像具有 28 × 28 × 1 的形状。由于最后一层使用 *tanh* 激活，输出在范围–1 和 1 之间被整形。

双曲正切激活函数也称为双曲正切函数。它与 sigmoid 激活函数非常相似，甚至具有相同的 S 形。但是该函数将任何实数值作为输入，并输出-1 到 1 范围内的值。我们使用 tanh 代替 sigmoid 作为输出层，因为它在这种情况下表现更好。

使用未经训练的生成器生成图像:

```py
tf.random.set_seed(0)
np.random.seed(0)

noise = tf.random.normal(shape=[batch_size, codings_size])
generated_images = dc_generator(noise)
plot_multiple_images(generated_images, 8)

```

### 创建鉴别器

创建如清单 [10-6](#PC23) 所示的鉴别器。

```py
dc_discriminator = Sequential([
  Conv2D(64, kernel_size=5, strides=2, padding='SAME',
         activation=LeakyReLU(0.2),
         input_shape=[28, 28, 1]),
  Dropout(0.4),
  Conv2D(128, kernel_size=5, strides=2, padding='SAME',
         activation=LeakyReLU(0.2)),
  Dropout(0.4),
  Flatten(),
  Dense(1, activation='sigmoid')
])

Listing 10-6DCGAN Discriminator

```

鉴别器看起来像用于二进制分类的常规 CNN(最终的密集层是 1)，除了它使用步长来下采样而不是最大池层。

### 创建 DCGAN

从 DCGAN 发生器和鉴别器创建 DCGAN:

```py
dcgan = Sequential([dc_generator, dc_discriminator])

```

### 编译鉴别器模型

由于鉴别器自然是二进制分类器(假图像或真图像)，我们自然使用二进制交叉熵损失。由于生成器只通过 DCGAN 模型训练，所以我们*不需要*来编译它。DCGAN 模型也是二元分类器，因此它可以使用相同的损失函数。重要的是，鉴别器不应该在第二阶段被训练。所以我们在编译 DCGAN 模型之前让它不可训练。

编译 DCGAN:

```py
dc_discriminator.compile(
    loss='binary_crossentropy', optimizer='rmsprop')
dc_discriminator.trainable = False
dcgan.compile(loss='binary_crossentropy', optimizer='rmsprop')

```

### 使再成形

由于发生器最后一层中的 tanh 激活，输出范围从–1 到 1，因此将训练集重新调整到相同的范围:

```py
images_dcgan = tf.reshape(
    images, [-1, 28, 28, 1]) * 2\. - 1.

```

### 构建输入流水线

建立批量为 32 的流水线:

```py
batch_size = 32
dataset = tf.data.Dataset.from_tensor_slices(images_dcgan)
dataset = dataset.shuffle(1000)
dataset = dataset.batch(
    batch_size, drop_remainder=True).prefetch(1)

```

### 火车

清除和播种:

```py
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

训练几个时代:

```py
n = 5

train_gan(
    dcgan, dataset, batch_size, codings_size, n_epochs=n)

```

哇！好多了！

### 用经过训练的生成器生成图像

使用经过训练的 DC 发生器制作图像:

```py
tf.random.set_seed(0)
np.random.seed(0)

noise = tf.random.normal(shape=[batch_size, codings_size])
generated_images = dc_generator(noise)
plot_multiple_images(generated_images, 8)

```

## 用大图像进行 DCGAN 实验

一个非常简单的深卷积甘工作与时尚-MNIST。但是这个数据集中的图像很小而且是灰度的。让我们来看看深度卷积 GAN 与*石头剪刀布*数据集的配合有多好，该数据集包含玩石头剪刀布游戏的手的大型彩色图像。

### 检查元数据

加载数据集以检查其元数据:

```py
rps, info = tfds.load('rock_paper_scissors', with_info=True,
                      split='train', try_gcs=True)

```

检查元数据对象:

```py
info

```

获取类别标签和类别数量:

```py
num_classes = info.features['label'].num_classes
classes = info.features['label'].names
classes, num_classes

```

想象一些例子:

```py
fig = tfds.show_examples(rps, info)

```

### 加载训练数据

将训练和测试图像作为 NumPy 数组加载:

```py
(x_train_img, _), (x_test_img, _) = tfds.as_numpy(
    tfds.load('rock_paper_scissors', split=['train','test'],
              batch_size=-1, as_supervised=True,
              try_gcs=True))

```

检查形状:

```py
for element in range(10):
  print (x_train_img.shape)

```

数据集包含 2，520 个 300 × 300 × 3 的图像。由于图像大小相同，我们不必调整它们的大小。然而，我们调整图像大小的另一个原因将在本章后面解释。

### 篡改数据

将数据集缩小到 2 的幂，以保证批次相等:

```py
x_train_rps = x_train_img[:2048]
len(x_train_rps)

```

当我们试图用原始数据集大小训练时，我们得到了一个错误。所以我们创建了相同的批次来消除错误。

创建一个函数来重新格式化图像:

```py
IMAGE_RES = 256

def format_image(image):
  image = tf.image.resize(image, (IMAGE_RES, IMAGE_RES))/255.0
  return image

```

虽然我们不必调整图像的大小，但这样做是为了更容易创建一个在网络的每一层都有适当数量神经元的模型。我们在本章前面提到，所有图像都是相同的 300 × 300 像素大小。但是为了方便起见，我们将大小调整为 2 的幂。

生成器首先将相对较小的图像投影到具有较大深度尺寸的多个维度上，然后逐渐增大图像尺寸并减小深度尺寸，直到输出我们想要的图像尺寸。所以用能被 2 整除和相乘的值更容易。当模型中的神经元数量以 2 的幂为基础时，深度学习模型的性能也更好。

### 构建输入流水线

从特征图像创建张量:

```py
train_slice = tf.data.Dataset.from_tensor_slices(x_train_rps)

```

转换图像以获得最佳性能:

```py
BATCH_SIZE = 32
SHUFFLE_SIZE = 500

train_rps = (train_slice.
             shuffle(SHUFFLE_SIZE).
             map(format_image).
             batch(BATCH_SIZE).
             cache().
             prefetch(1))
train_rps

```

如清单 [10-7](#PC41) 所示，可视化一批中的一些例子。

```py
plt.figure(figsize=(10, 10))
for images in train_rps.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i])
    plt.axis('off')

Listing 10-7Visualize

Examples from a Batch

```

我们不显示标签，因为我们不需要它们来进行无人监管的实验。

### 建立模型

清除和播种:

```py
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

如清单 [10-8](#PC43) 所示创建发电机。

```py
codings_size = 100

gencolor = Sequential([
  Dense(32 * 32 * 256, input_shape=[codings_size]),
  Reshape([32, 32, 256]),
  BatchNormalization(),
  Conv2DTranspose(128, kernel_size=5, strides=2, padding='SAME',
                  activation='selu'),
  BatchNormalization(),
  Conv2DTranspose(64, kernel_size=5, strides=2, padding='SAME',
                  activation='selu'),
  BatchNormalization(),
  Conv2DTranspose(3, kernel_size=5, strides=2, padding='SAME',
                  activation='tanh'),
])

Listing 10-8The Generator

```

神经元的数量基于 2 的幂，只有一个例外。最后一层包含三个神经元来表示类的数量。

因为我们想要生成大的彩色图像，所以首先要给生成器一个更大的(32 × 32)张量投影到 256 维。所以得到的维度空间是 262，144。用 32 乘以 32 再乘以 256 得到这个值。

生成器将投影整形为一个 32 × 32 × 256 的张量，该张量被批量归一化并被馈送到一个步长为 2 的转置卷积层。步幅将张量向上采样到 64 × 64，因为它使 32 × 32 的维度加倍。该层还将张量的深度从 256 减少到 128。结果再次被批量归一化，并被馈送到另一个步长为 2 的转置卷积层。步幅将其向上采样至 128 × 128，因为它是 64 × 64 维度的两倍。这层也将张量的深度从 128 减少到 64。结果再次被批量归一化，并被馈送到另一个步长为 2 的转置卷积层。步幅将其上采样至 256 × 256，因为它是 128 × 128 尺寸的两倍。该层还将张量的深度从 64 减少到 3。

输出张量有形状(256，256，3)，这是我们的目标，因为我们希望它们是原来的 256 × 256 × 3 的形状。由于最后一层使用 tanh 激活，输出在范围–1 和 1 之间被整形。

Note

希望你能明白为什么我们用 2 的幂来给模型分层。我们从一个大的维度空间开始我们的生成器，然后一层一层地，我们将张量维度还原到我们的数据集中图像的原始形状。从数学上来说，上采样和将张量减少 2 是非常容易的。

检查:

```py
gencolor.summary()

```

创建如清单 [10-9](#PC45) 所示的鉴别器。

```py
discolor = Sequential([
  Conv2D(64, kernel_size=5, strides=2, padding='SAME',
         activation=LeakyReLU(0.2),
         input_shape=[256, 256, 3]),
  Dropout(0.4),
  Conv2D(128, kernel_size=5, strides=2, padding='SAME',
         activation=LeakyReLU(0.2)),
  Dropout(0.4),
  Flatten(),
  Dense(1, activation='sigmoid')
])

Listing 10-9The Discriminator

```

鉴频器接受 256 × 256 × 3 图像，将张量下采样至 128 × 128，并将深度从 32 增加至 64。张量被馈送到转置卷积层，转置卷积层将张量下采样到 64 × 64，并将深度增加到 128。结果被展平到一个 524，288 维的空间(64 × 64 × 128)。由于我们进行的是二元分类，最终的密集层只包含*一个*神经元。

生成器对张量进行上采样，以激活高维空间中的神经元。鉴别器从数据中将张量下采样回原始图像大小，以进行有效的特征分类。

检查:

```py
discolor.summary()

```

创建 DCGAN:

```py
dcgan_color = Sequential([gencolor, discolor])

```

### 编译鉴别器模型

由于鉴别器自然是二进制分类器(假图像或真图像)，我们自然使用二进制交叉熵损失。由于生成器只通过 DCGAN 模型训练，所以我们*不需要*来编译它。DCGAN 模型也是二元分类器，因此它可以使用相同的损失函数。重要的是，鉴别器不应该在第二阶段被训练。因此，在编译 DCGAN 模型之前，我们将其设为不可训练:

```py
discolor.compile(
    loss='binary_crossentropy', optimizer='rmsprop')
discolor.trainable = False
dcgan_color.compile(
    loss='binary_crossentropy', optimizer='rmsprop')

```

### 重新调节

创建一个函数来重新缩放:

```py
def rescale(image):
  image = tf.math.multiply(image, image * 2\. -1)
  return image

```

由于发生器最后一层中的 tanh 激活，输出范围从–1 到 1，请将训练集重新调整到相同的范围。

重新缩放图像:

```py
train_color = train_rps.map(rescale)

```

### 训练模型并生成图像

创建一个函数来绘制图像，如清单 [10-10](#PC51) 所示。

```py
def plot_color(images, n_cols=None):
  n_cols = n_cols or len(images)
  n_rows = (len(images) - 1) // n_cols + 1
  images = np.clip(images, 0, 1)
  if images.shape[-1] == 1:
    images = np.squeeze(images, axis=-1)
  plt.figure(figsize=(n_cols, n_rows))
  for index, image in enumerate(images):
    plt.subplot(n_rows, n_cols, index + 1)
    plt.imshow(image, cmap='binary')
    plt.axis('off')

Listing 10-10Plotting Function for Generated Images

```

创建一个训练循环函数来训练 DCGAN，如清单 [10-11](#PC52) 所示。

```py
def train_gan(
    gan, dataset, batch_size, codings_size, n_epochs=50):
  generator, discriminator = gan.layers
  for epoch in range(n_epochs):
    print('Epoch {}/{}'.format(epoch + 1, n_epochs))
    for X_batch in dataset:
      # phase 1 - training the discriminator
      noise = tf.random.normal(
          shape=[batch_size, codings_size])
      generated_images = generator(noise)
      X_fake_and_real = tf.concat(
          [generated_images, X_batch], axis=0)
      y1 = tf.constant(
          [[0.]] * batch_size + [[1.]] * batch_size)
      discriminator.trainable = True
      discriminator.train_on_batch(X_fake_and_real, y1)
      # phase 2 - training the generator
      noise = tf.random.normal(
          shape=[batch_size, codings_size])
      y2 = tf.constant([[1.]] * batch_size)
      discriminator.trainable = False
      gan.train_on_batch(noise, y2)
    plot_color(generated_images, 8)
    plt.show()

Listing 10-11Training Loop Function

```

训练 DCGAN:

```py
train_gan(
    dcgan_color, train_color, BATCH_SIZE, codings_size, 10)

```

仅仅经过几个时期，我们可以看到生成的图像开始代表石头、布、剪刀手。但是我们建立的生成器和鉴别器模型非常简单。为了生成逼真的图像，我们需要创建一个更加健壮的模型。

## 摘要

在这一章中，我们建立了一个带有前馈网络的 GAN，并生成了普通的传真。有了 DCGAN，生成的传真对时尚 MNIST 来说更加真实。对于石头剪子布来说，传真并不好。我们可以尝试为更多的纪元训练模型，但这需要很多时间，并且可能会占用更多的内存，超出您的计算机的处理能力。