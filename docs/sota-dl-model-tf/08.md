# 8.堆叠自动编码器

前七章集中于监督学习算法。**监督学习**是 ML 的一个子类，它使用*标记的*数据集来训练算法，以对数据进行分类并准确预测结果。剩下的章节集中在无监督学习算法。**无监督学习**使用 ML 算法来分析和聚类*未标记的*数据集。这种算法发现隐藏的模式或数据分组*，无需人工干预*。

**自动编码器**是人工神经网络，在没有任何监督的情况下学习输入数据的密集表示。习得的密集表征通常被称为潜在表征(或编码)。编码用于重建原始输出。

编码通常具有比输入数据低得多的维数，这使得自动编码器对于维数减少是有用的。自动编码器还可以用于特征提取、深度神经网络的无监督预训练和生成模型。作为生成模型，它们可以随机生成与训练数据非常相似的新数据。

如果自动编码器只是从输入数据产生相同的输出，那么训练自动编码器似乎是违反直觉的。但是我们展示了一个训练有素的自动编码器如何从新的数据中重建图像！我们还展示了它们是如何有效的降维机制。

自动编码器由编码器组件、代码组件和解码器组件组成。编码器压缩输入并产生代码。然后，解码器从代码中重建原始输入。

章节的笔记本位于以下 URL:

[T2`https://github.com/paperd/deep-learning-models`](https://github.com/paperd/deep-learning-models)

我们提出了四个堆叠自动编码器实验。我们从一个基本的堆叠编码器实验开始。我们继续做捆绑重物的实验。我们以去噪和简单的调整实验结束。

我们从简单的堆栈编码器开始，因为它是最基本的自动编码器。随后的每个实验都增加了复杂性，包括捆绑权重、去噪和调整自动编码器。

通过导入主 TensorFlow 库并实例化 GPU，开始设置 Colab 生态系统。

## 导入 TensorFlow 库

导入库并将其别名为 **tf** :

```py
import tensorflow as tf

```

将 TensorFlow 库别名为 tf 是常见的做法。

## GPU 硬件加速器

为了方便起见，我们在 Colab 笔记本中包含了启用 GPU 的步骤:

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

验证 GPU 是否处于活动状态:

```py
tf.__version__, tf.test.gpu_device_name()

```

如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

Note

如果得到错误**名称****'****TF****'****未定义**，重新执行代码导入 TensorFlow 库！

## 基本堆叠编码器实验

*堆叠编码器*有多个隐藏层。该架构通常关于中心隐藏层是对称的，该隐藏层被称为*编码层*。在本实验中，我们将向您展示如何创建和训练堆栈编码器。

### 加载数据

将时尚 MNIST 加载为 NumPy 数组:

```py
import tensorflow_datasets as tfds

(x_train_img, _), (x_test_img, _) = tfds.as_numpy(
    tfds.load('fashion_mnist', split=['train','test'],
              batch_size=-1, as_supervised=True,
              try_gcs=True))

```

请注意，我们没有从时尚 MNIST 加载类别标签，因为自动编码器是无监督的模型！也就是他们*不需要贴标签的数据来学习！*

### 缩放数据

通过将数据集除以代表图像的像素数进行缩放:

```py
import numpy as np

x_train, x_test = x_train_img.astype(np.float32) / 255.,\
                  x_test_img.astype(np.float32) / 255.

```

深度学习实验建议使用缩放数据，因为它通常会在训练过程中平滑优化。

### 构建堆叠编码器模型

清除和播种:

```py
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

获取输入形状:

```py
in_shape = x_train.shape[1:]
in_shape

```

导入库:

```py
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten,\
  Reshape

```

自动编码器分为编码器和解码器。编码器将示例编码成更小更密集的表示。解码器获取这些密集表示，并尝试将输出重建回原始输入。

创建编码器:

```py
stacked_encoder = Sequential([
  Flatten(input_shape=in_shape),
  Dense(128, activation='relu'),
  Dense(64, activation='relu'),
  Dense(32, activation='relu')
])

```

编码器接受 28 × 28 像素的灰度图像，将其展平，使每个图像都表示为大小为 784 的向量，并通过三个大小递减的密集层(128 单位到 64 单位到 32 单位)来处理这些向量。32 单元层是编码层(中央隐藏层)。对于每个输入图像，编码器输出大小为 32 的矢量。

创建解码器:

```py
stacked_decoder = Sequential([
  Dense(64, activation='relu'),
  Dense(128, activation='relu'),
  Dense(28 * 28, activation='sigmoid'),
  Reshape(in_shape)
])

```

解码器接受来自编码器的大小为 32 的编码，并通过三个递增大小的密集层(64 个单元到 128 个单元到 784 个单元)对其进行处理。然后，它将最终向量重新整形为 28 × 28 数组，使解码器的输出与编码器的输入具有相同的形状。

创建堆叠式自动编码器:

```py
stacked_ae = Sequential([stacked_encoder, stacked_decoder])

```

### 编译和训练

为精确度指标创建一个函数:

```py
def rounded_accuracy(y_true, y_pred):
    return tf.keras.metrics.binary_accuracy(tf.round(y_true),
                                            tf.round(y_pred))

```

重建是一个二元问题。要么输出与输入匹配，要么不匹配。重建损失对产生不同于输入的输出的网络不利。所以二进制精度对于这个实验来说是理想的。

编译:

```py
opt = tf.keras.optimizers.SGD(lr=1.5)

stacked_ae.compile(
    loss='binary_crossentropy',
    optimizer=opt, metrics=[rounded_accuracy])

```

火车:

```py
sae_history = stacked_ae.fit(
    x_train, x_train, epochs=10,
    validation_data=(x_test, x_test))

```

### 可视化性能

导入绘图库:

```py
import matplotlib.pyplot as plt

```

创建一个绘图函数，如清单 [8-1](#PC15) 所示。

```py
def viz_history(training_history):
  loss = training_history.history['loss']
  val_loss = training_history.history['val_loss']
  accuracy = training_history.history['rounded_accuracy']
  val_accuracy = training_history.history['val_rounded_accuracy']
  plt.figure(figsize=(14, 4))
  plt.subplot(1, 2, 1)
  plt.title('Loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.plot(loss, label='Training set')
  plt.plot(val_loss, label='Test set', linestyle='--')
  plt.legend()
  plt.grid(linestyle='--', linewidth=1, alpha=0.5)
  plt.subplot(1, 2, 2)
  plt.title('Accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.plot(accuracy, label='Training set')
  plt.plot(val_accuracy, label='Test set', linestyle='--')
  plt.legend()
  plt.grid(linestyle='--', linewidth=1, alpha=0.5)
  plt.show()

Listing 8-1Training Performance Visualization Function

```

视觉化:

```py
viz_history(sae_history)

```

### 想象重建

创建一个函数来绘制灰度为 28 × 28 的图像:

```py
def plot_image(image):
    plt.imshow(image, cmap='binary')
    plt.axis('off')

```

创建一个函数来可视化原始图像和重建，如清单 [8-2](#PC18) 所示。

```py
def show_reconstructions(model, images, n_images):
  reconstructions = model.predict(images[:n_images])
  reconstructions = tf.squeeze(reconstructions)
  fig = plt.figure(figsize=(n_images * 1.5, 3))
  for image_index in range(n_images):
    plt.subplot(2, n_images, 1 + image_index)
    plot_image(images[image_index])
    plt.subplot(2, n_images, 1 + n_images + image_index)
    plot_image(reconstructions[image_index])

Listing 8-2Visualization Function for Reconstructions

```

该函数接受经过训练的模型、一组*测试*图像和一个表示一批图像的数字。该函数计算一批*测试*图像的预测。然后为了可视化的目的，它从预测中挤出 *1* 维度。请记住，我们的自动编码器模型是在训练数据( *x_train* )上训练的，重建是基于来自时装-MNIST 数据集的测试数据( *x_test* )。还要注意，重建是在没有任何标记数据的情况下创建的*。*

检查测试数据的维度:

```py
x_test.shape

```

形状是(10000，28，28，1)。所以测试数据集中有 10000 张 28 × 28 × 1 的图像。

要使用 imshow()函数进行可视化，请从张量中移除尺寸为“1”的维度:

```py
x_test_imgs = tf.squeeze(x_test)
x_test_imgs.shape

```

形状现在是 TensorShape([10000，28，28])，这是 imshow()函数对图像的期望。

视觉化:

```py
show_reconstructions(stacked_ae, x_test_imgs, 6)

```

基于来自训练模型的预测，从测试图像生成重建图像。自动编码器能够学习如何将图像分解成小数据比特。小比特的数据提供了图像的表示。自动编码器学习如何从这些表示中重建原始图像。当然，重建与原始图像并不完全相同，因为我们使用了简单的堆叠式自动编码器。

### 故障

让我们分解清单 [8-2](#PC18) 中的函数，看看它是如何工作的。

从测试集中抓取第一个图像，作为一个批次:

```py
img = x_test[:1]

```

由于预测方法的计算是成批进行的，因此将第一幅图像作为一批图像来抓取。

基于第一批图像进行预测:

```py
reconstruction = stacked_ae.predict(img)

```

删除“1”维:

```py
reconstruction = tf.squeeze(reconstruction)

```

绘制重建图:

```py
plot_image(reconstruction)

```

绘制实际图像:

```py
plot_image(tf.squeeze(x_test[0]))

```

从图像中挤出“1”维进行绘图。

### 降维

自动编码器对降维很有用。**降维**是通过获取一组主变量，减少深度学习实验中考虑的随机变量(或输入特征)的数量的过程。主变量是数据集中最重要的特征。降维通常用于深度学习，因为大量的输入特征会导致 ML 算法的性能不佳。

为了执行维度缩减，*我们需要标签*。因此，从测试数据集中加载标签:

```py
test = tfds.as_numpy(
    tfds.load('fashion_mnist', split=['test'],
              batch_size=-1, as_supervised=True,
              try_gcs=True))

```

从测试数据集中切片测试标签:

```py
y_test = test[0][1]

```

使用编码器降低维数，如清单 [8-3](#PC29) 所示。

```py
from sklearn.manifold import TSNE

np.random.seed(0)
x_test_compressed = stacked_encoder.predict(x_test_imgs)
tsne = TSNE()
x_test_2D = tsne.fit_transform(x_test_compressed)
x_test_2D = (x_test_2D - x_test_2D.min()) /\
  (x_test_2D.max() - x_test_2D.min())

Listing 8-3Dimensionality Reduction Algorithm

```

使用 scikit-learn 实现的 t-SNE(t-分布式随机邻居嵌入)算法来降低 2D 的维数以进行可视化。该算法将维数降低到 32。 *t-SNE* 算法是一种统计方法，通过在二维或三维地图中给出每个数据点的位置来可视化高维数据。

视觉化:

```py
plt.scatter(x_test_2D[:, 0], x_test_2D[:, 1],
            c=y_test, s=10, cmap='tab10')
plt.axis('off')
plt.show()

```

每个类别用不同的颜色表示。

创建一个更漂亮的可视化效果，如清单 [8-4](#PC31) 所示。

```py
import matplotlib as mpl

plt.figure(figsize=(10, 8))
cmap = plt.cm.tab10
plt.scatter(x_test_2D[:, 0], x_test_2D[:, 1],
            c=y_test, s=10, cmap=cmap)
image_positions = np.array([[1., 1.]])
for index, position in enumerate(x_test_2D):
    dist = np.sum((position - image_positions) ** 2, axis=1)
    if np.min(dist) > 0.02: # if far enough from other images
        image_positions = np.r_[image_positions, [position]]
        imagebox = mpl.offsetbox.AnnotationBbox(
            mpl.offsetbox.OffsetImage(x_test_imgs[index],
                                      cmap='binary'),
            position, bboxprops={
                'edgecolor': cmap(y_test[index]), 'lw': 2})
        plt.gca().add_artist(imagebox)
plt.axis('off')
plt.show()

Listing 8-4Pretty Dimensionality Reduction Visualization

```

通过这种可视化，我们可以在降维的帮助下看到提取的类别标签。

## 捆绑重物实验

当自动编码器完全对称时，我们可以将解码器层的权重与编码器层的权重联系起来。*整齐对称*是指编码器和解码器具有对称的层结构。因此，我们将模型中的权重数量减半，这加快了训练速度并减少了过度拟合。

具有绑定权重的自动编码器的解码器权重是编码器权重的转置，这是参数共享的一种形式。我们通过参数共享来减少参数的数量。

### 定义自定义层

要绑定编码器和解码器的权重，使用编码器权重的转置作为解码器权重，如清单 [8-5](#PC32) 所示。

```py
class DenseTranspose(tf.keras.layers.Layer):
  def __init__(self, dense, activation=None, **kwargs):
    self.dense = dense
    self.activation = tf.keras.activations.get(activation)
    super().__init__(**kwargs)
  def build(self, batch_input_shape):
    self.biases = self.add_weight(
        name='bias', shape=[self.dense.input_shape[-1]],
        initializer='zeros')
    super().build(batch_input_shape)
  def call(self, inputs):
    z = tf.matmul(
        inputs, self.dense.weights[0], transpose_b=True)
    return self.activation(z + self.biases)

Listing 8-5Custom Layer Class

```

该类从一个模型和一个激活函数(如果包含在一个层中)接受一个层，并转置数据。很多时候，我们必须对输入 ML 算法的数据进行预处理。原因是数据可能存储为行，但 ML 算法期望输入为列，反之亦然。所以换位在 ML 活动中是非常有用的操作。

### 构建捆绑权重模型

清除和播种:

```py
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

为方便起见，为模型创建三个密集层:

```py
dense_1 = Dense(128, activation='relu')
dense_2 = Dense(64, activation='relu')
dense_3 = Dense(32, activation='relu')

```

我们可以选择创建这些变量在编码器中使用。当然，如果您愿意，可以将层语法直接包含在模型中。

构建编码器:

```py
tied_encoder = Sequential([
  Flatten(input_shape=in_shape),
  dense_1,
  dense_2,
  dense_3
])

```

构建解码器:

```py
tied_decoder = Sequential([
  DenseTranspose(dense_3, activation='relu'),
  DenseTranspose(dense_2, activation='relu'),
  DenseTranspose(dense_1, activation='sigmoid'),
  Reshape([28, 28])
])

```

由于 autoencoder 是完全对称的，我们可以将 DenseTranspose 类映射到编码器的层上。密集转置层与密集层类似，但输入矩阵是按列优先顺序存储的，而不是按行优先顺序存储的，也就是说，它是普通密集矩阵的转置。

构建捆绑砝码自动编码器:

```py
tied_ae = Sequential([tied_encoder, tied_decoder])

```

### 编译和训练

编译:

```py
tied_ae.compile(loss='binary_crossentropy',
                optimizer=opt, metrics=[rounded_accuracy])

```

火车:

```py
tied_history = tied_ae.fit(
    x_train, x_train, epochs=10,
    validation_data=(x_test, x_test))

```

### 可视化培训表现

视觉化:

```py
viz_history(tied_history)

```

### 可视化重建

显示基于训练模型预测的测试图像重建:

```py
show_reconstructions(tied_ae, x_test_imgs, 6)
plt.show()

```

## 去噪实验

自动编码器也可以被训练来去除图像中的噪声。其思想是将*随机噪声*添加到输入中，并训练以恢复原始的无噪声输入。这听起来有些违反直觉，但是很管用！

图像中固有的噪声会带来麻烦，因为算法可能认为噪声是一种应该学习的模式。但是添加随机噪声会阻止网络记忆训练样本，因为它们一直在变化。

随机噪声允许我们在存在已知数量噪声的情况下测试算法的鲁棒性和性能。也就是说，我们知道我们添加了什么噪声。通过向输入数据添加噪声，我们获得了更多数据，我们的深度神经网络可以为这些数据进行训练。但是在有噪声的数据上训练意味着模型在有噪声的数据上推广。

### 建立去噪模型

清除和播种:

```py
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

导入高斯噪声库:

```py
from tensorflow.keras.layers import GaussianNoise

```

该库使我们能够应用附加的零中心高斯噪声。**高斯噪声**(以卡尔·弗里德里希·高斯命名)是概率密度函数等于正态分布(也称高斯分布)的统计噪声。因此，添加的噪声值是高斯分布的(或正态分布的)。

我们添加高斯噪声，因为它是信息论中使用的基本噪声模型，用来模拟自然界中自然发生的许多随机过程的影响。而高斯噪声是正态分布的噪声，所以符合很多自然现象。

将纯高斯噪声直接添加到编码器中:

```py
gaussian_encoder = Sequential([
  Flatten(input_shape=in_shape),
  GaussianNoise(0.2),
  dense_1,
  dense_2,
  dense_3
])

```

构建解码器:

```py
gaussian_decoder = Sequential([
  DenseTranspose(dense_3, activation='relu'),
  DenseTranspose(dense_2, activation='relu'),
  DenseTranspose(dense_1, activation='sigmoid'),
  Reshape([28, 28])
])

```

为了获得更好的性能，请将解码器层的权重与编码器层的权重联系起来。

构建去噪自动编码器:

```py
gaussian_ae = Sequential([gaussian_encoder, gaussian_decoder])

```

### 编译和训练

编译:

```py
gaussian_ae.compile(
    loss='binary_crossentropy',
    optimizer=opt, metrics=[rounded_accuracy])

```

火车:

```py
gae_history = gaussian_ae.fit(
    x_train, x_train, epochs=10,
    validation_data=(x_test, x_test))

```

### 可视化培训表现

视觉化:

```py
viz_history(gae_history)

```

### 可视化重建

向测试图像添加相同数量的高斯噪声:

```py
noise = GaussianNoise(0.2)
show_reconstructions(gaussian_ae, noise(x_test_imgs), 6)
plt.show()

```

## 辍学实验

**Dropout** 是一种有助于防止过度拟合的正则化技术。随机退出*在训练过程中关闭*一些神经元，将一个神经网络变成一个神经网络集合。其思想是并行近似训练大量具有不同架构的神经网络。

下降的效果使图层看起来像并且被视为与前一图层具有不同数量的结点和连通性的图层。因此，训练期间对层的每一次更新都是用配置层的*不同视图*执行的。

然而，任何数量的丢失都意味着信息的丢失！通过将一个层的丢失概率设置为 0.5，我们会在每个时期(或训练迭代)丢失该层一半的信息！在大多数情况下，建议使用 Dropout，但不是在每个层都使用，并且不应该设置为高于 0.5。

丢弃使得训练过程变得嘈杂，这迫使层内的节点在概率上或多或少地承担输入的责任。通过丢弃一个神经元，该神经元连同其所有输入和输出连接一起被暂时从网络中移除。

在神经网络中，每一层都实现了丢失。它可用于大多数类型的图层，如密集全连接图层、卷积图层和递归图层。

可以在任一或所有隐藏层上实现下降。超参数指定了图层输出丢失的概率。隐藏层的一个常见值是概率 0.5，对于大范围的网络和任务来说，这似乎接近最佳值。对于输入图层，最佳值必须更接近 0 而不是 0.5。

该实验展示了构建和训练一个丢失自动编码器是多么容易。唯一的变化是在编码器中增加了一个下降值。

### 建立辍学模型

清除和播种:

```py
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

为退出导入资源库:

```py
from tensorflow.keras.layers import Dropout

```

构建压差为 0.5 的编码器:

```py
dropout_encoder = Sequential([
  Flatten(input_shape=in_shape),
  Dropout(0.5),
  dense_1,
  dense_2,
  dense_3
])

```

使用捆绑权重构建解码器以获得更好的性能:

```py
dropout_decoder = Sequential([
  DenseTranspose(dense_3, activation='relu'),
  DenseTranspose(dense_2, activation='relu'),
  DenseTranspose(dense_1, activation='sigmoid'),
  Reshape([28, 28])
])

```

构建压差自动编码器:

```py
dropout_ae = Sequential([dropout_encoder, dropout_decoder])

```

### 编译和训练

编译:

```py
dropout_ae.compile(
    loss='binary_crossentropy',
    optimizer=opt, metrics=[rounded_accuracy])

```

火车:

```py
drop_history = dropout_ae.fit(
    x_train, x_train, epochs=10,
    validation_data=(x_test, x_test))

```

### 可视化培训表现

视觉化:

```py
viz_history(drop_history)

```

### 可视化重建

向测试图像添加相同数量的下降噪声:

```py
dropout = Dropout(0.5)

show_reconstructions(dropout_ae, dropout(x_test_imgs), 6)
plt.show()

```

## 摘要

在这一章中，我们演示了基本的堆栈式自动编码器，并做了一些调整。在下一章，我们将介绍更强大的自动编码器。