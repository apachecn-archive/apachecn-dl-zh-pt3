# 6.使用 TensorFlow Hub 进行简单迁移学习

**迁移学习**是通过微调之前训练好的神经网络来创建新的学习模型的过程。我们不是从头开始训练网络，而是下载一个预先训练好的开源学习模型，并根据我们自己的目的进行微调。一个*预先训练好的模型*是由其他人创造出来解决类似问题的模型。我们可以使用其中的一个，而不是构建我们自己的模型。一个很大的优势是，专家已经精心制作了一个预训练模型，因此我们可以确信它在高水平上执行(在大多数情况下)。另一个好处是，我们不必有大量的数据来使用预先训练好的模型。

在迁移学习中，机器利用从以前的任务中获得的知识来提高对另一个任务的概括。例如，在训练分类器来预测图像是否包含食物时，我们可以使用它在训练期间获得的知识来识别饮料。所以迁移学习可以节省时间，在大多数情况下提供更好的神经网络性能，并且不需要大量数据。

*TensorFlow Hub* 是一个经过训练的机器学习模型的储存库，可以很容易地集成到深度学习实验中。我们用 TensorFlow Hub 代码示例演示简单的迁移学习。

图像分类模型可以有数百万个参数。从零开始训练它们，需要大量带标签的训练数据和大量的计算能力。有了迁移学习，我们就不用从头开始训练了！我们可以从一个已经在相关任务中训练过的模型中取出一部分，并在新的模型中重用它。由于我们使用预训练模型的权重，训练时间大大减少！所以我们不需要训练一个巨大的神经网络，因为它已经被训练过了！训练时间也减少了，因为我们在数据集上使用了预先训练的模型。

章节的笔记本位于以下 URL:

[T2`https://github.com/paperd/deep-learning-models`](https://github.com/paperd/deep-learning-models)

## 迁移学习的预训练模型

如果我们没有足够的训练数据，重用预训练模型的较低层通常是个好主意。重用预训练模型的较低层通常被称为迁移学习。我们只训练较低的层次，其他的保持不变。*较低层*指的是模型的一般(或与问题无关)特征。较高层指的是模型的特定(或依赖于问题的)特征。预训练模型的最终层也称为最终层。

这个想法是为了我们自己的目的，通过微调以前训练过的模型来创建新的深度学习模型。只需导入预训练模型的训练权重，并更改模型的最终层(或几层)。然后，我们在自己的数据集上重新训练这些层。我们节省了训练时间，获得了令人尊敬的表现。

我们用例子演示了如何使用预先训练好的模型。我们从使用 MobileNet-v2 的实验开始，以使用 Inception-v3 的实验结束。每个预训练模型都在本章的相应章节中进行了解释。

## 导入 TensorFlow 库

导入库并将其别名为 **tf** :

```py
import tensorflow as tf

```

## GPU 硬件加速器

为了方便起见，我们提供了在 Colab 笔记本中启用 GPU 的步骤:

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

验证 GPU 是否处于活动状态:

```py
tf.__version__, tf.test.gpu_device_name()

```

如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

Note

如果得到错误**名称****'****TF****'****未定义**，重新执行代码导入 TensorFlow 库！

## TensorFlow Hub

我们使用来自 TensorFlow Hub 的预训练 tensor flow saved 模型对花卉数据进行建模，以进行图像特征提取。一个 *SavedModel* 是包含序列化签名和运行它们所需的状态的目录，包括变量值和词汇表。 *TensorFlow Hub* 是一个预先训练好的 TensorFlow 模型库。预训练模型是在非常大的通用数据集上训练的。

有关 TensorFlow Hub 的更多信息，请阅读

[T2`https://tfhub.dev/`](https://tfhub.dev/)

有关 TensorFlow Hub 的教程，请阅读

[T2`www.tensorflow.org/hub/tutorials`](http://www.tensorflow.org/hub/tutorials)

我们使用两个预训练的 TensorFlow Hub 模型进行迁移学习。我们从 MobileNet-v2 预训练模型开始。然后，我们使用 Inception-v3 预训练模型，并比较两者之间的结果。

## MobileNet-v2

*MobileNet-v2* 是一个 53 层深的卷积神经网络。该网络的预训练版本在来自 ImageNet 数据库的 140 万幅图像和 1000 类网络图像上进行训练。该模型对 224 × 224 像素的图像进行操作。默认的训练批次大小是 1024，这意味着每次迭代对这些图像中的 1024 个进行操作。

ImageNet 项目是一个大型视觉数据库，设计用于视觉对象识别软件研究。该项目已经对超过 1400 万张图像进行了手工注释，以指示所拍摄的对象。

## Flowers MobileNet-v2 实验

第一个实验使用 MobileNet-v2 预训练模型对花卉进行分类。

### 将花作为 TFDS 对象加载

加载 TFDS 对象时，定型集拆分为 75%，验证集拆分为 15%，测试集拆分为 10%:

```py
import tensorflow_datasets as tfds

(test, valid, train), info = tfds.load(
    'tf_flowers', as_supervised=True,
    split = ['train[:10%]', 'train[10%:25%]', 'train[25%:]'],
    with_info=True, try_gcs=True)

```

### 探索元数据

用 info 对象显示一般信息:

```py
info

```

显示数据拆分中的示例数量:

```py
num_train_img = info.splits['train[25%:]'].num_examples
num_valid_img = info.splits['train[10%:25%]'].num_examples
num_test_img = info.splits['train[:10%]'].num_examples
print ('train images:', num_train_img)
print ('valid images:', num_valid_img)
print ('test images:', num_test_img)

```

我们希望显示每个实验的信息，以确保我们按照预期分割数据。

如清单 [6-1](#PC6) 所示，手动计算数据分割中的示例数。

```py
num_train_examples = 0
num_valid_examples = 0
num_test_examples = 0

for example in train:
  num_train_examples += 1

for example in valid:
  num_valid_examples += 1

for example in test:
  num_test_examples += 1

print('Total Number of Training Images: {}'\
      .format(num_train_examples))
print('Total Number of Validation Images: {}'\
      .format(num_valid_examples))
print('Total Number of Testing Images: {}'\
      .format(num_test_examples))

Listing 6-1Manually Verify Examples in Each Split

```

清单 [6-1](#PC6) 中的代码是可选的。我们只想向您展示如何手动检查学习集的大小。

获取标签和类别数量:

```py
class_labels = info.features['label'].names
num_classes = info.features['label'].num_classes
class_labels, num_classes

```

### 显示图像和形状

显示数据集中的一些图像:

```py
fig = tfds.show_examples(train, info)

```

显示图像形状:

```py
for i, example in enumerate(train.take(5)):
  print('Image {} shape: {} label: {}'\
        .format(i+1, example[0].shape,
                example[1]))

```

flowers 数据集中的图像大小不尽相同。因此，我们必须将图像调整到标准大小，以便TensorFlow模型可以使用它们。

### 构建输入管道

创建一个函数来重新格式化图像:

```py
def format_image(image, label):
  image = tf.image.resize(image, (224, 224)) /255.0
  return image, label

```

该函数调整图像的大小和比例。MobileNet-v2 预期的分辨率是(224，224)。该函数将一个“图像”和一个“标签”作为参数，并以所需的形式返回新的“图像”和相应的“标签”。

将功能映射到培训、验证和测试示例。并应用其他转换:

```py
BATCH_SIZE = 367

train_batches = train.shuffle(num_train_img//4).\
  map(format_image).batch(BATCH_SIZE).prefetch(1)

validation_batches = valid.map(format_image).\
  batch(BATCH_SIZE).prefetch(1)

test_batches = test.map(format_image).\
  batch(BATCH_SIZE).prefetch(1)

```

幸运的是，我们发现 367 的批量非常有效！我们尝试了默认的批量大小 1024，并根据该批量大小显示标签。令人惊讶的是，我们得到了一个类似于*我们的索引超出了 367* 大小的界限的错误。因此，我们将批量大小更改为 367，并注意到该模型在提高验证准确性方面比批量大小为 1024 的模型表现得更好。此外，培训时间也减少了。

### 创建特征向量

**特征**是被观察现象的一个单独的可测量的属性或特征。**特征向量**是包含关于对象的多个元素的向量。特征由特征向量表示。将特征向量组合在一起创建了特征空间。**特征空间**是与特征向量相关联的向量空间。一个特征可以仅代表单个像素或整个图像。在我们的例子中，特征代表由像素的特征向量描述的整个图像。

**特征** **提取**是从数据集中收集初始特征子集的过程。提取的特征应该包含来自输入数据的相关信息。特征提取的目标是通过使用缩减的表示(或子集)而不是完整的初始数据来执行期望的任务。对于模式识别、分类和回归中的有效算法来说，选择信息丰富的、有区别的和独立的特征是至关重要的一步。

使用预先训练的模型创建特征提取器:

```py
import tensorflow_hub as hub

piece1 = 'https://tfhub.dev/google/tf2-preview/'
piece2 = 'mobilenet_v2/feature_vector/4'
URL = piece1 + piece2
feature_extractor_mn = hub.KerasLayer(
    URL, input_shape=(224, 224, 3))

```

使用 MobileNet-v2 特征向量模型创建特征提取器。特征提取器是来自 TensorFlow Hub 的部分模型(没有最终分类层)。

### 冻结预训练模型

冻结特征提取器层中的变量，使得训练仅修改最终的分类器层:

```py
feature_extractor_mn.trainable = False

```

不冻结预训练层的结果会破坏它们在未来训练回合中包含的信息。

### 创建分类标题

创建分类头以利用数据集的预训练模型。分类头由一个简单的顺序模型组成，该模型包括预训练模型和新的分类层。

导入库:

```py
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

```

清除和播种:

```py
import numpy as np

tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

创建分类标题:

```py
mobile_model = tf.keras.Sequential([
  feature_extractor_mn,
  Dropout(0.5),
  Dense(num_classes)])

```

一个*分类头*只是一个容器，它保存着预先训练好的模型和我们可能希望添加的任何附加内容。请注意，我们刚刚创建的模型的第一层是特征提取器。还要注意的是，这个模型**非常**简单，因为我们利用了 MobileNet-v2 中预先训练好的权重！

Note

对于我们的实验，分类头是我们创建的新模型，它包含预训练模型(在这种情况下，预训练模型是一个特征提取器)、丢弃层(以减少过拟合)和密集层(以实现分类)。为了实现预先训练的模型，需要一种机制，它被称为分类头。

### 编译和训练模型

编译:

```py
from tensorflow.keras.losses import SparseCategoricalCrossentropy

mobile_model.compile(
  optimizer='adam',
  loss=SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

```

火车:

```py
EPOCHS = 6

history = mobile_model.fit(
    train_batches, epochs=EPOCHS,
    validation_data=validation_batches)

```

虽然我们的模型非常简单，没有调整，但我们只用六个历元就获得了相当好的准确性，因为 MobileNet-v2 是由专家经过长时间精心设计的，然后在大规模 ImageNet 数据集上进行训练。还有，训练时间很合理。

Note

对于像 MobileNet-v2 这样的超大型模型，如果我们不使用其预训练的权重，训练时间将会很长。

### 可视化性能

图验证精度和损失如清单 [6-2](#PC19) 所示。

```py
import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(EPOCHS)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

Listing 6-2Visual Accuracy and Loss Performance

```

我们发现令人惊讶的是，我们可以从未经训练的数据集上的预训练模型中获得令人尊敬的结果。

### 根据测试数据进行预测

我们根据测试数据进行预测，因为模型**从未**见过它！

根据测试数据预测:

```py
predictions = mobile_model.predict(test_batches)

```

显示类别标签:

```py
class_labels

```

#### 检查第一个预测

我们检查来自数据集的预测(或推断),向您展示从*预测*方法返回的原始值。由于推理是分类学习的主要目标，我们发现向您展示在引擎盖下正在发生的事情(可以这么说)是有价值的。

获取第一个预测数组:

```py
predictions[0]

```

返回的数组是原始预测。

使用 *np.argmax()* 函数获得预测值，即第一幅图像的预测数组中概率最高的值:

```py
predicted_id = np.argmax(predictions[0])
predicted_id

```

将标签转换为其类名:

```py
class_labels[predicted_id]

```

从第一批中获取标签:

```py
for img, lbl in test_batches.take(1):
  print (lbl)

```

标签的数量与批量相匹配。

获取第一个标签:

```py
class_labels[lbl[0].numpy()]

```

如果第一个标签与第一幅图像的预测相匹配，则预测是正确的！

#### 检查第一批预测

或者，我们可以将 *test_batches* 转换成一个迭代器:

```py
image_batch, label_batch = next(iter(test_batches))

images = image_batch.numpy()
labels = label_batch.numpy()

class_labels[labels[0]]

```

从迭代器中获取第一批，将图像和标签转换为 NumPy，并显示第一个标签。

显示第一批标签:

```py
labels

```

将该批标签转换为命名标签:

```py
named_labels = [class_labels[labels[i]]
                for i, lbl in enumerate(range(BATCH_SIZE))]
named_labels

```

从第一批中获取预测:

```py
predicted_batch = [np.argmax(predictions[i])
                   for i, _ in enumerate(range(BATCH_SIZE))]
predicted_batch

```

将预测转换为命名预测:

```py
named_pred = [class_labels[predicted_batch[i]]
              for i, lbl in enumerate(range(BATCH_SIZE))]
named_pred

```

### 绘图预测

可视化显示了第一批测试的实际图像。如果预测正确，标题为蓝色。如果没有，标题为红色。如果预测不正确，预测将与括号中的实际标签一起显示。

显示如清单 [6-3](#PC32) 所示的预测。

```py
plt.figure(figsize=(20,20))
for n in range(30):
  plt.subplot(6,5,n+1)
  plt.subplots_adjust(hspace = 0.3)
  plt.imshow(images[n])
  color = 'blue' if labels[n] == predicted_batch[n] else 'red'
  if labels[n] != predicted_batch[n]:
    t = named_pred[n].title() +\
        ' (' +named_labels[n].title() + ')'
  else:
    t = named_pred[n].title()
  plt.title(t, color=color)
  plt.axis('off')
  st = 'Model predictions (blue: correct, red: incorrect)'
_ = plt.suptitle(st)

Listing 6-3Prediction Plot

```

## 花盗梦空间-v3 实验

对于第二个实验，我们使用 Inception-v3 预训练模型来分类花。 *Inception-v3* 是一个 48 层深度的卷积神经网络，用于图像识别，在 ImageNet 数据集上已显示达到 78.1%以上的准确率。预训练的模型对 299 × 299 的图像进行操作。默认的训练批次大小是 1024，这意味着每次迭代对这些图像中的 1024 个进行操作。

有关云 TPU 上的 Inception-v3 的高级指南，请阅读

[T2`https://cloud.google.com/tpu/docs/inception-v3-advanced`](https://cloud.google.com/tpu/docs/inception-v3-advanced)

### 构建输入管道

重新创建重新格式化图像的功能:

```py
def format_image(image, label):
  image = tf.image.resize(image, (299, 299)) / 255.0
  return image, label

```

该函数调整图像的大小和比例。Inception-v3 期望的分辨率是(299，299)。该函数将一个“图像”和一个“标签”作为参数，并以所需的形式返回新的“图像”和相应的“标签”。

为 Inception-v3 构建输入管道:

```py
BATCH_SIZE = 367

train_im = train.shuffle(num_train_img//4).\
  map(format_image).batch(BATCH_SIZE).prefetch(1)

validation_im = valid.map(format_image).\
  batch(BATCH_SIZE).prefetch(1)

test_im = test.map(format_image).\
  batch(BATCH_SIZE).prefetch(1)

```

将功能映射到培训、验证和测试示例。并应用其他变换。

创建特征提取器:

```py
piece1 = 'https://tfhub.dev/google/tf2-preview/'
piece2 = 'inception_v3/feature_vector/4'
URL = piece1 + piece2
feature_extractor_im = hub.KerasLayer(URL,
  input_shape=(299, 299, 3),
  trainable=False)

```

冻结预训练模型:

```py
feature_extractor_im.trainable = False

```

### 建立模型

清除和播种:

```py
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

创建模型:

```py
inception_model = tf.keras.Sequential([
  feature_extractor_im,
  Dropout(0.5),
  Dense(num_classes)])

```

我们只是替换 Inception-v3 特征提取器来获得其预训练的权重！

### 编译和训练

编译:

```py
inception_model.compile(
  optimizer='adam',
  loss=SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

```

火车:

```py
EPOCHS = 6

history = inception_model.fit(
    train_im, epochs=EPOCHS,
    validation_data=validation_im)

```

### 可视化性能

如清单 [6-4](#PC41) 所示，可视化训练损失和准确性。

```py
import matplotlib.pyplot as plt

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(EPOCHS)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

Listing 6-4Training Performance

```

Note

我们并不真正关心比较 Inception 和 MobileNet 之间的模型性能，因为结果总是与您想要建模的数据联系在一起。也就是说，在一个数据集上可能效果更好，但在另一个数据集上可能效果不佳。这就是我们向您展示如何实现这两种模型的原因。

### 预言

进行预测:

```py
im_predictions = inception_model.predict(test_im)

```

获取一批预测并将它们转换为命名预测:

```py
im_pred_batch = [np.argmax(im_predictions[i])
                 for i, _ in enumerate(range(BATCH_SIZE))]
im_named_pred = [class_labels[im_pred_batch[i]]
                 for i, lbl in enumerate(range(BATCH_SIZE))]

```

从测试集中获取第一批图像和标签:

```py
im_image_batch, im_label_batch = next(iter(test_im))

im_images = im_image_batch.numpy()
im_labels = im_label_batch.numpy()

```

将标签转换为命名标签:

```py
im_named_labels = [class_labels[im_labels[i]]
                   for i, lbl in enumerate(range(BATCH_SIZE))]

```

### 绘图模型预测

创建一个绘图函数，如清单 [6-5](#PC46) 所示。

```py
def plot_pred(images, labels, named_labels, named_pred):
  plt.figure(figsize=(20,20))
  for n in range(30):
    plt.subplot(6,5,n+1)
    plt.subplots_adjust(hspace = 0.3)
    plt.imshow(images[n])
    color = 'blue' if named_labels[n] == named_pred[n] else 'red'
    if named_labels[n] != named_pred[n]:
      t = named_pred[n].title() +\
      ' (' +named_labels[n].title() + ')'
    else:
      t = named_pred[n].title()
    plt.title(t, color=color)
    plt.axis('off')
    st = 'Model predictions (blue: correct, red: incorrect)'
    _ = plt.suptitle(st)

Listing 6-5Plot Function

```

在函数中包装绘图逻辑。

调用函数:

```py
plot_pred(im_images, im_labels, im_named_labels, im_named_pred)

```

## 摘要

MobileNet-v2 和 Inception-v3 的性能与 flowers 数据集非常相似。两个模型都没有过度拟合。两个模型的准确率都在 80%以上。当然，我们只训练了六个纪元。但我们无法预测预训练模型在给定数据集上的效果如何。预训练模型的想法是为了节省您的时间和精力。这就是为什么有这么多可用的预训练模型。我们推测在不久的将来会有许多新的模型被创造出来。