# 11.渐进增长的生成对抗网络

gan 在生成清晰的合成图像方面很有效，但其大小被限制在 64 × 64 像素左右。**渐进式生长 GAN** 是 GAN 的扩展，它使训练生成器模型能够生成高达约 1024 × 1024 像素的大型高质量图像(在撰写本文时)。这种方法已被证明在生成高质量的逼真的合成人脸方面是有效的。

渐进式增长的 GANs 的关键创新是由生成器输出的图像大小的增量增加。通过在训练开始时生成小图像，并逐渐向生成器和鉴别器模型添加卷积层，生成分辨率越来越精细的越来越大的图像。

该技术以低分辨率矢量作为输入开始。它继续通过添加新层来逐步增加生成器和鉴别器，因此模型可以在训练过程中不断学习细节。该技术还可以加速和稳定训练，同时生成前所未有质量的图像。

## 潜在空间学习

**潜在空间**是压缩数据的表示，其中相似的数据点在空间上更加靠近。潜在空间对于学习数据特征和寻找用于分析的更简单的数据表示是有用的。创造一个潜在空间的想法是压缩现实，这样矢量数学就能工作。潜在空间也可以称为*潜在向量空间*。

当我们观察我们的世界时，我们看到一个巨大的像素景观(或*观察像素空间*)。但是我们怎么能希望从如此庞大的数据中学习呢？一种解决方案是创建潜在空间，将观察到的像素空间压缩成可管理的像素图像。例如，为了教一个模型学习人脸，我们从拍摄(或使用现有的)人脸照片开始。然后，我们将这些图片转换成一组像素图像。所以每张脸都由一组像素代表。现在我们有了一个潜在的表示，我们可以对图像像素应用微积分和矢量算术来教授学习模型人脸的本质(至少从像素图像的角度来看)。实质上，我们实验的潜在空间是从实际人脸的观察像素空间中挑选出来的压缩像素空间表示。

在观察到的像素空间中，任何两个图像之间可能没有直接的相似性。但是将一个像素空间映射到一个潜在空间会将图像压缩得更近，这样我们就可以更容易地了解这些图像。

GAN 架构中的生成模型学习映射潜在空间中的点以生成图像。它将潜在空间中的一个点作为输入，并应用矢量算法来生成新的图像。还可以在潜在空间中的两点之间的线性路径上创建一系列点，以创建多个生成的图像。在实践中，生成模型有效地使用其潜在空间表示在其潜在空间中的点之间进行插值，目的是从其生成的图像中获得有意义和有针对性的效果。但是潜在空间只有在应用于被训练的生成模型时才有意义。也就是每个学习实验都有自己的潜在空间。

章节的笔记本位于以下 URL:

[T2`https://github.com/paperd/deep-learning-models`](https://github.com/paperd/deep-learning-models)

我们提出了两个渐进生长 GAN 实验。第一个实验从预先训练的模型生成图像。第二个实验创建了一个定制的训练循环来从初始生成的图像中学习目标图像。通过导入主 TensorFlow 库并实例化 GPU，开始设置 Colab 生态系统。

## 导入 TensorFlow 库

导入库并将其别名为 **tf** :

```py
import tensorflow as tf

```

将 TensorFlow 库别名为 tf 是常见的做法。

## GPU 硬件加速器

为了方便起见，我们在 Colab 笔记本中包含了启用 GPU 的步骤:

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

验证 GPU 是否处于活动状态:

```py
tf.__version__, tf.test.gpu_device_name()

```

如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

Note

如果得到错误**名称****'****TF****'****未定义**，重新执行代码导入 TensorFlow 库！

## 创造实验环境

这两个实验都需要用于图像和动画显示的包、库和函数。

### 安装用于创建动画的软件包

安装 imageio、scikit-image 和 TensorFlow docs 包，以便为实验创建动画:

```py
!pip -q install imageio
!pip -q install scikit-image
!pip install -q git+https://github.com/tensorflow/docs

```

### 安装库

在标准日志记录之上安装日志记录模块:

```py
from absl import logging

```

日志模块来自 Abseil Python 包，该包为构建 Python 应用提供了各种库。该模块定义了允许设计者为应用和库构建灵活的事件记录系统的函数和类。日志 API 的主要好处是所有 Python 模块都可以参与日志记录。因此，我们的应用日志可以包含自己的消息，这些消息与来自第三方模块的消息集成在一起。

安装图像处理库:

```py
import imageio
import PIL.Image
import matplotlib.pyplot as plt
from IPython import display
from skimage import transform

```

*imageio* 库提供了一个简单的界面来读取和写入各种图像数据，包括动画图像、体积数据和科学格式。 *PIL。Image* 模块用于表示 Python 图像库(PIL)的图像。该库增加了对打开、操作和保存许多不同图像文件格式的支持。 *plt* 库用于显示图像。*显示*模块是 IPython 中显示工具的公共 API。 *IPython* 是一个用 Python 构建的交互式 shell。*变换*模块用于图像处理。

导入其他必需的库:

```py
import numpy as np
import tensorflow_hub as hub
from tensorflow_docs.vis import embed
import time

```

NumPy 是一个 Python 库，用于处理标量、数组和矩阵。 *hub* 模块允许访问 TensorFlow Hub，这是一个经过训练的机器学习模型的存储库。*嵌入*模块用于在笔记本中嵌入动画。 *time* 库提供了许多用代码表示时间的方法，比如对象、数字和字符串。

## 创建图像显示功能

创建一个函数来显示 PIL 图像:

```py
def display_image(image):
  image = tf.constant(image)
  image = tf.image.convert_image_dtype(image, tf.uint8)
  return PIL.Image.fromarray(image.numpy())

```

创建显示图像的函数:

```py
def show_image(image):
  plt.imshow(image)
  plt.axis('off')
  plt.show()

```

*imshow()* 函数用于将数据显示为图像。

创建一个函数来显示动画，如清单 [11-1](#PC9) 所示。

```py
def animate(images):
  images = np.array(images)
  converted_images = np.clip(
      images * 255, 0, 255).astype(np.uint8)
  imageio.mimsave('./animation.gif', converted_images)
  return embed.embed_file('./animation.gif')

Listing 11-1Function to Display Animation

```

## 创造潜在的空间维度

潜在空间对于学习数据特征和寻找用于分析的更简单的数据表示是有用的。人类理解广泛的主题和属于这些主题的事件。潜在空间旨在通过定量的空间表示为计算机模型提供类似的理解。因此，潜在空间实际上只是一个特定训练实验的压缩现实的可测量的空间表示。

将数据集压缩到潜在空间有助于模型更好地理解观察到的数据，因为模型处理的变化比处理整个数据集要小得多。因此，与必须从实际观察到的像素空间学习相比，模型从更小的空间学习。

高维度和低维度这两个术语有助于定义我们希望我们的潜在空间学习和表现的特征的具体或一般程度。*高维度*潜在空间对输入数据的更具体特征敏感，但在没有足够的训练数据时有时会导致过拟合。*低维*潜在空间旨在捕捉学习和表示输入数据所需的最重要的特征(或方面)。

设置 512 的高维潜在空间，因为我们用于该实验的预训练模型是在该潜在空间上训练的:

```py
latent_dim = 512

```

预训练的模型从 512 维的潜在空间映射到图像。如果我们事先不知道正在使用的模块，我们可以从*module . structured _ input _ signature*方法中检索潜在维度。我们将在本章后面介绍如何做到这一点。

## 设置错误日志的详细程度

要查看日志记录错误:

```py
logging.set_verbosity(logging.ERROR)

```

## 图像生成实验

我们使用 progan-128 预训练模型来生成人脸的真实图像。 *progan-128* 模型是在 CelebA(CelebA faces 属性数据集)上训练的渐进 gan，用于 128 × 128 像素图像。它从一个 512 维的潜在空间映射到图像。在训练期间，从正态分布中对潜在空间向量进行采样。

该模块采用一个张量(Tensor(tf.float32，shape=[？，512])，其表示一批潜在向量作为输入，并输出张量(Tensor(tf.float32，shape=[？128，128，3])，其代表一批 RGB 图像。原始模型在 GPU 上训练 636，801 个步骤，批量大小为 16。

*CelebA* 是一个大规模的人脸属性数据集，包含超过 20 万张名人图片。每个图像有 40 个属性注释。该数据集中的图像覆盖了较大的姿态变化和背景混乱。西里巴也有很大的多样性，大量，丰富的注释包括

*   * 10，177 张独特的名人照片

*   * 202，599 张人脸图像

*   * 5 个地标位置

*   *每个图像 40 个二进制属性注释

CelebA 可以用作计算机视觉任务的训练和测试集，例如人脸属性识别、人脸检测、界标(或面部部分)定位以及人脸编辑和合成。

### 创建一个函数来插值一个超球面

如清单 [11-2](#PC12) 所示，创建一个函数来插值潜在空间中的向量之间的空间。

```py
def interpolate_hypersphere(v1, v2, num_steps):
  v1_norm = tf.norm(v1)
  v2_norm = tf.norm(v2)
  v2_normalized = v2 * (v1_norm / v2_norm)
  vectors = []
  for step in range(num_steps):
    interpolated =\
      v1 + (v2_normalized - v1) *\
      step / (num_steps - 1)
    interpolated_norm = tf.norm(interpolated)
    interpolated_normalized =\
      interpolated * (v1_norm / interpolated_norm)
    vectors.append(interpolated_normalized)
  return tf.stack(vectors)

Listing 11-2Function to Interpolate the Hypersphere

```

该函数在两个随机初始化的向量之间启动潜在空间插值。它在非零且不在通过原点的直线上的向量之间进行插值，并将归一化的插值向量返回给调用环境。**图像** **插值**是从一系列图像中生成中间图像的过程。

该函数从创建两个欧几里得赋范向量 v1 和 v2 开始。然后将 v2 归一化，使其具有与 v1 相同的范数。它继续在超球面(或潜在空间)上的两个向量之间进行插值，以基于插值步骤的数量产生一组向量。

一个**超球**是一个球体的四维模拟。虽然球体存在于三维空间中，但它的表面是二维的。类似地，超球体有一个弯曲到 4D 空间的三维表面。我们的宇宙可能是超球面的超曲面。

### 加载预训练模型

设置全局随机种子以保持再现性:

```py
tf.random.set_seed(7)

```

设置种子值。你想用多少就用多少。但是为了再现性，在所有实验中使用相同的数字。

加载预训练的渐进式 GAN (progan-128):

```py
hub_model = hub.load(
    'https://tfhub.dev/google/progan-128/1')\
    .signatures['default']

```

获取输出形状:

```py
hub_model.output_shapes

```

渐进式 GAN (progan-128)在 CelebA 上使用 128 × 128 × 3 像素的图像尺寸进行训练。

获取潜在空间的尺寸:

```py
hub_model.structured_input_signature

```

progan-128 模型从 512 维潜在空间映射到图像。在训练期间，从正态分布中采样潜在空间向量。

### 生成并显示图像

可以从潜在空间中的随机点生成新的图像。首先在潜在空间中创建一个随机的法向量。继续将载体送入 progan-128。预训练模型识别潜在空间中与随机向量最接近的向量，并从最接近的向量生成图像。在算法上，progan-128 通过最小化真实分布和生成分布之间的总距离来识别最接近的潜在向量。

创建一个函数来寻找潜在空间中最近的向量:

```py
def get_module_space_image():
  vector = tf.random.normal([1, latent_dim])
  image = hub_model(vector)['default'][0]
  return image

```

该函数创建一个介于 1 和 512(我们潜在空间的大小)之间的随机法向量。然后，它使用 progan-128 从最接近我们创建的随机潜在向量的潜在向量生成图像。

显示生成的图像:

```py
generated_image = get_module_space_image()
display_image(generated_image)

```

还不错！预训练模型根据从潜在空间中提取的向量生成相对真实的图像。

### 创建一个函数来生成多个图像

该函数创建两个随机向量，在潜在空间中插入它们之间的空间，并使用 progan-128 生成如清单 [11-3](#PC19) 所示的图像。

```py
def interpolate_between_vectors(steps):
  v1 = tf.random.normal([latent_dim])
  v2 = tf.random.normal([latent_dim])
  vectors = interpolate_hypersphere(v1, v2, steps)
  interpolated_images = hub_model(vectors)['default']
  return interpolated_images

Listing 11-3Function to Generate Interpolated Images

```

该函数从 512 维潜在空间创建两个随机正态变量。然后在 v1 和 v2 之间用 *n* 步插值创建一个张量。它通过使用 progan-128 生成基于张量的 *n* 幅图像而结束。因为张量由 n 步插值组成，所以生成的图像沿着由 v1 和 v2 之间的潜在空间中的最近向量限定的插值线。

### 制作动画

基于 n 个生成的图像创建动画:

```py
interpolation_steps = 100
interpolated_images = interpolate_between_vectors(
    interpolation_steps)
animate(interpolated_images)

```

相当惊人！在 progan-128 的帮助下，我们基于与我们创建的两个随机向量最接近的潜在空间中的两个向量之间的插值来创建动画。步骤的数量对动画制作过程有很大的影响！步数越高，在潜在空间的向量之间创建的插值图像越多。因此，试验一下步数，看看会发生什么。但是不要使用太大的 n 大小，因为你可能会耗尽内存！

Tip

保持插值步骤相对较低，以避免内存崩溃！虽然 Colab 是一项云服务，但免费版本在笔记本电脑的内存分配量上非常有限。

### 显示插值图像向量

显示插值图像向量以解构图像生成过程。

获得由随机向量 v1 和 v2 界定的潜在空间中的内插图像的数量:

```py
num_imgs = len(interpolated_images)
num_imgs

```

我们将插值步长设置为 100，因此我们有 100 个插值图像。

显示初始插值图像:

```py
show_image(interpolated_images[0])

```

显示最终的插值图像:

```py
show_image(interpolated_images[num_imgs - 1])

```

因此，动画从初始图像开始，并变形为最终图像，因为插值向量沿着由随机向量 v1 和 v2 界定的潜在空间中的线。

创建一个函数来显示从潜在空间生成的图像向量，如清单 [11-4](#PC24) 所示。

```py
def generated_images(images, cols, rows):
  columns, rows = cols, rows
  ax = []
  fig = plt.figure(figsize=(20, 20))
  for i in range(columns*rows):
    img = images[i].numpy()
    ax.append(fig.add_subplot(rows, columns, i+1))
    plt.imshow(img)
    plt.axis('off')

Listing 11-4Function to Display Generated Image

```

显示插值图像:

```py
generated_images(interpolated_images, 10, 10)

```

看到模型如何基于潜在空间的插值向量创建一系列从初始图像到最终图像的变形是非常有趣的！因此，学习模型可以在潜在空间中两点之间的线性路径上创建一系列向量，正如我们在动画中演示的那样。

### 从单个矢量显示多个图像

创建一个函数来返回潜在向量和图像:

```py
def get_vector():
  vector = tf.random.normal([1, latent_dim])
  image = hub_model(vector)['default'][0]
  return vector, image

```

该函数返回一个随机正常潜在向量和一个由 progan-128 从随机潜在向量生成的图像。每次执行该函数时，图像都是不同的，因为每个潜在向量都是从一个随机向量生成的，被送入 progan-128 并生成。

显示:

```py
for _ in range(2):
  latent_vector, image = get_vector()
  print (latent_vector[0][0:3])
  show_image(image)

```

对于每个循环周期，显示每个潜在向量的切片及其对应的图像。这两幅图像是不相似的，并且没有关系，因为我们为每个循环周期创建了一个接近于随机向量的潜在向量。但是向量之间没有路径，因为我们没有在两个随机向量围成的线性路径上创建一系列潜向量！对于每个循环周期，我们只从潜在空间内插一个向量。

显示几幅图像，如清单 [11-5](#PC28) 所示。

```py
rows, cols = 2, 2
plt.figure(figsize=(10, 10))
for i in range(rows*cols):
  plt.subplot(rows, cols, i + 1)
  plt.imshow(get_module_space_image())
  plt.axis('off')

Listing 11-5Display Several Images Based on a Single Random Vector

```

同样，图像是不同的，没有关系，因为该函数每次被调用时都会创建一个接近随机潜在向量的潜在向量！

### 从上传的图像创建目标潜在向量

正如我们已经演示过的，我们可以创建一个随机的法线潜向量，并从中生成一个新的图像。或者，我们可以从上传的图像向量创建一个向量，并从中生成一个新的图像。

导入必需的库:

```py
from google.colab import files

```

创建一个从本地驱动器获取上传图像的函数:

```py
def upload_image():

  uploaded = files.upload()
  image = imageio.imread(uploaded[list(uploaded.keys())[0]])
  return transform.resize(image, [128, 128])

```

该功能使用 *google.colab* 库中的 *files.upload()* 将文件上传到笔记本。它使用 *imageio* 库中的 *imread()* 从本地驱动器读取图像。该函数使用 *skimage* 库中的 *transform.resize()* 以 progan-128 预期的形式返回一个调整大小的图像。

Note

确保您的本地驱动器上有要上传的图像。为了方便起见，我们建议使用网站上包含的本章图片之一。只需将图像复制到您的本地驱动器。

从本地驱动器获取图像:

```py
local_image = upload_image()

```

点击*选择文件*选择您想要从本地驱动器加载的图像。

显示图像:

```py
display_image(local_image)

```

基于本地图像向量创建生成的图像:

```py
vector = tf.dtypes.cast(local_image, tf.float32)
generated_image = hub_model(vector)['default'][0]
display_image(generated_image)

```

不是创建随机的正常潜在向量来生成图像，而是从上传的图像创建向量来生成图像。我们确实需要将局部图像张量转换成浮点张量。继续在 progan-128 的帮助下从浮动张量生成图像。以显示图像结束。progan-128 模型根据从 CelebA 学到的知识生成新图像。所以不管上传什么图像，生成的图像都像人脸，因为 progan-128 是在人脸上学习的！尽管深度学习令人惊叹，但它仍然局限于它所训练的内容(至少目前如此)。

## 自定义循环学习实验

监督学习实验的目标是为新出现的输入数据预测正确的标签。该模型学习如何以非常精确的方式将输入特征与预测结果联系起来。虽然 gan 是无监督的实验，但是我们可以通过在两个随机生成的向量所限定的线性路径上创建一系列潜在向量来创建一个*伪监督*实验。我们将生成的向量中的一个识别为特征向量，另一个识别为预测(或目标)图像。一个**潜在向量**是从在训练期间不能被访问或操纵的潜在空间中提取的。这个想法类似于前馈神经网络不能操纵隐藏层输出的值。

我们通过定义特征向量和目标图像之间的损失函数来训练模型，并在特征向量上使用梯度下降来寻找使损失最小化的变量值。训练从定义特征图像和目标图像之间的损失函数开始。定制的训练循环使得梯度下降算法和损失函数能够找到使特征向量和目标之间的损失最小化的变量值。

### 创建特征向量

为再现性生成种子并创建特征向量:

```py
seed_value = 777
tf.random.set_seed(seed_value)
feature_vector = tf.random.normal([1, latent_dim])

```

更改种子以生成不同的图像。小心使用相同的种子值，以保持实验之间的可重复性。

Note

我们为这个实验设置了不同的随机种子。我们可以保留在第一个实验中设置的全局随机种子。但是我们决定在两个实验中分开种子。

特征向量可以被认为是初始向量，因为模型通过梯度步骤学习如何从初始潜在向量前进到最终目标图像。

验证特征向量是从潜在空间中提取的:

```py
feature_vector.shape

```

特征向量从 512 维的潜在空间中提取，如形状 1 × 512 所示。所以特征向量是具有 512 个元素的一维向量。

### 显示来自特征向量的图像

使用 progan-128 显示来自特征向量的图像:

```py
display_image(hub_model(feature_vector)['default'][0])

```

### 创建目标图像

从潜在空间创建目标图像:

```py
target_image = get_module_space_image()

```

验证其形状:

```py
target_image.shape

```

目标张量的形状是 128 × 128 × 3，这是 progan-128 所期望的形状。

显示目标图像:

```py
display_image(target_image)

```

### 创建一个函数来寻找最近的潜在向量

该函数定义了特征向量和目标图像之间的损失函数。它使用梯度下降来寻找使损失最小化的变量值，如清单 [11-6](#PC40) 所示。

```py
def find_closest_latent_vector(
    initial_vector, target_image,
    num_optimization_steps,
    steps_per_image, loss_alg):
  images = []
  losses = []
  vector = tf.Variable(initial_vector)
  optimizer = tf.optimizers.Adam(learning_rate=0.01)
  loss_fn = loss_alg
  for step in range(num_optimization_steps):
    if (step % 100)==0:
      print()
    print('.', end='')
    with tf.GradientTape() as tape:
      image = hub_model(vector.read_value())['default'][0]
      if (step % steps_per_image) == 0:
        images.append(image.numpy())
      target_image_difference = loss_fn(
          image, target_image[:,:,:3])
      regularizer = tf.abs(tf.norm(vector) - np.sqrt(latent_dim))
      loss = target_image_difference + regularizer
      losses.append(loss.numpy())
    grads = tape.gradient(loss, [vector])
    optimizer.apply_gradients(zip(grads, [vector]))
  return images, losses

Listing 11-6Custom Training Loop Function

```

该函数接受特征向量、步数、每幅图像的步数和损失算法。然后它初始化变量，包括优化器和损失函数。它还将特征向量转换成 tf.Variable. A *tf。变量*是一个张量，其值可以在训练过程中改变。

该函数继续自定义训练循环。对于自定义循环学习，TensorFlow 提供了 *tf。用于自动微分的 GradientTape* API。TensorFlow 记录了在 tf 上下文中执行的相关操作。将磁带梯度转移到虚拟磁带上。TensorFlow 随后使用虚拟磁带，通过反向模式微分来计算记录计算的梯度。

**自动微分** (AD)是一套对计算机程序指定的函数的导数进行数值计算的技术。我们使用 AD 来计算计算相对于某些输入(例如，tf)的梯度。变量)。AD 实现了两种不同的模式。*正向模式微分*由内向外遍历链式法则。*逆向模式微分*由外向内遍历链式法则。自动微分对于实现 ML 算法是有用的，例如用于训练神经网络的反向传播。

有关自动微分的优秀资源，请阅读

[T2`https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation`](https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation)

为了自动区分，TensorFlow 需要记住在向前传递期间什么操作以什么顺序发生。然后，在反向过程中，TensorFlow 以*的逆序*遍历这个操作列表来计算梯度。

梯度带使用预先训练的模型来生成图像，找到特征图像和目标图像之间的空间，使用正则化来获得更真实的图像，计算损失，应用梯度下降算法，并优化梯度。一旦训练完成，该函数返回生成图像的数组和训练期间计算的损失数组。

由于潜在向量是从正态分布中采样的，如果我们将潜在向量的长度正则化为该分布中所有向量的平均长度，我们可以获得更真实的图像。我们在函数的梯度带部分用*正则变量*实现正则化。

### 创建损失函数

创建损失函数算法:

```py
reduction = tf.keras.losses.Reduction.SUM
mae_loss_algorithm = tf.losses.MeanAbsoluteError(reduction)

```

对损失函数算法使用平均绝对误差(MAE)缩减。 **MAE** 衡量两个连续变量之间的差异。

一般来说，*TF . loss . mean absoluteerror*API 计算标签和预测之间的绝对差的平均值。在我们的实验中，它计算潜在向量和实际目标之间的平均绝对差。

### 训练模型

清除以前的模型会话:

```py
tf.keras.backend.clear_session()

```

Tip

在笔记本中运行多个训练会话时，最好在使用 TF . keras . back end . clear _ session API 训练模型之前清除以前的模型会话。

调用培训功能:

```py
num_optimization_steps = 200
steps_per_image = 5
mae_images, mae_loss = find_closest_latent_vector(
    feature_vector, target_image, num_optimization_steps,
    steps_per_image, mae_loss_algorithm)

```

调整优化步骤和每个图像的步骤，以查看对可视化的影响。

### 培训损失

地块损失:

```py
plt.plot(mae_loss)
fig = plt.ylim([0, max(plt.ylim())])

```

用 MAE 减少量计算最终损失:

```py
MAE_loss = mae_loss[num_optimization_steps - 1]
MAE_loss

```

### 有生命的

根据训练生成的图像制作动画:

```py
animate(np.stack(mae_images))

```

### 将学习到的图像与目标进行比较

获取生成图像的数量:

```py
len(mae_images)

```

获取图像类型:

```py
type(mae_images[0])

```

创建一个函数来显示学习到的图像，如清单 [11-7](#PC49) 所示。

```py
def closest_latent_images(faces, rows, cols):
  fig = plt.figure(1, (20., 40.))
  for i in range(40):
    plt.subplot(10, 4, i+1)
    plt.imshow(faces[i])
    plt.axis('off')

Listing 11-7Function to Display Generated Images

```

显示特征和目标之间最近的潜像:

```py
closest_latent_images(mae_images, 10, 4)

```

在训练循环的每一步中，该函数通过利用来自 progan-128 的预训练权重来生成新图像。然后，它计算新生成的图像和目标之间的损失。渐渐地，通过梯度下降和损失最小化技术，图像变得越来越接近目标。太神奇了！

将第一个生成的图像与目标进行对比:

```py
display_image(np.concatenate(
    [mae_images[0], target_image], axis=1))

```

展示模型的表现:

```py
display_image(np.concatenate(
    [mae_images[-1], target_image], axis=1))

```

抓取' mae_images[-1]'以显示最终生成的图像。

要获得更大的图像，请使用其他显示功能:

```py
show_image(np.concatenate(
    [mae_images[-1], target_image], axis=1))

```

一点都不差！

### 尝试不同的损失函数算法

使用 MSE 代替 MAE:

```py
reduction = tf.keras.losses.Reduction.SUM
mse_loss_algorithm = tf.losses.MeanSquaredError(reduction)

```

**均方误差** (MSE)测量估计值和目标值之间误差的平方平均值(均方差)。*TF . losses . mean squarederror*API 计算标签和预测之间误差的均方值。

清除以前的模型会话:

```py
tf.keras.backend.clear_session()

```

使用 MSE 缩减来训练模型:

```py
num_optimization_steps = 200
steps_per_image = 5
mse_images, mse_loss = find_closest_latent_vector(
    feature_vector, target_image, num_optimization_steps,
    steps_per_image, mse_loss_algorithm)

```

地块损失:

```py
plt.plot(mse_loss)
fig = plt.ylim([0, max(plt.ylim())])

```

用 MSE 减少量计算最终损失:

```py
MSE_loss = mse_loss[num_optimization_steps - 1]
MSE_loss

```

MSE 损失要低得多。

显示动画:

```py
animate(np.stack(mse_images))

```

将最终生成的图像与目标图像进行比较:

```py
display_image(np.concatenate(
    [mse_images[-1], target_image], axis=1))

```

与 MAE 减少相比:

```py
display_image(np.concatenate(
    [mae_images[-1], target_image], axis=1))

```

### 从上传的图像创建目标

使用 progan-128 从潜在向量创建目标图像，而不是使用 progan-128 从上传的图像向量创建目标图像。

生成种子并从潜在空间创建初始特征向量:

```py
seed_value = 0
tf.random.set_seed(seed_value)
feature_vector = tf.random.normal([1, latent_dim])

```

请注意，我们使用了不同的种子值。当然，您可以使用您希望的任何种子值。但是为了再现性，在实验中总是使用相同的种子值。

从本地驱动器抓取图像并显示:

```py
uploaded_image = upload_image()
display_image(uploaded_image)

```

将上传的图像转换为 float32 以供 TensorFlow 使用:

```py
uploaded_vector = tf.dtypes.cast(uploaded_image, tf.float32)
display_image(uploaded_vector)

```

Note

上传的张量*不是*目标图像，因为它不是从潜在空间中绘制的。目标图像是根据上传的向量从 progan-128 创建的！为了方便起见，我们建议使用网站上包含的本章图片之一。只需将图像复制到您的本地驱动器。

使用 progan-128 从我们刚刚从上传的图像创建的向量中生成目标图像:

```py
uploaded_target = hub_model(uploaded_vector)['default'][0]
display_image(uploaded_target)

```

所以上传的图像不是目标图像。它只是 progan-128 用来创建目标的一个向量(一旦我们将图像转换为浮点张量)。

创建一个减少 MSE 的损失算法:

```py
reduction = tf.keras.losses.Reduction.SUM
loss_algorithm = tf.losses.MeanSquaredError(reduction)

```

我们使用 MSE 缩减，但是如果你愿意，你可以用 MAE 缩减来代替。

清除以前的模型会话:

```py
tf.keras.backend.clear_session()

```

火车:

```py
num_optimization_steps = 300
steps_per_image = 5
mse_images, mse_loss = find_closest_latent_vector(
    feature_vector, uploaded_target, num_optimization_steps,
    steps_per_image, loss_algorithm)

```

我们在更多的优化步骤上训练该模型，以生成目标图像的更好的复制品。

计算最终损失:

```py
MSE_loss = mse_loss[num_optimization_steps - 1]
MSE_loss

```

动画:

```py
animate(np.stack(mse_images))

```

将最终生成的图像与目标图像进行比较:

```py
display_image(np.concatenate(
    [mse_images[-1], uploaded_target], axis=1))

```

还不错。我们可以增加优化步骤的数量，以生成更真实的图像。但是要小心，因为将步数设置得太高可能会损害可用的 RAM。

### 从 Google Drive 映像创建一个目标

不要从本地驱动器抓取图像，而是从 Google Drive 抓取。我们创建了一个新的特征向量，但是如果你愿意，你也可以使用我们为上传图片练习创建的特征向量。

生成种子并从潜在空间创建初始特征向量:

```py
seed_value = 0
tf.random.set_seed(seed_value)
feature_vector = tf.random.normal([1, latent_dim])

```

安装 Google Drive:

```py
from google.colab import drive
drive.mount('/content/gdrive')

```

点击 URL，选择一个 Gmail 帐户，复制授权码，将其粘贴到文本框中，然后点击键盘上的 *Enter* 按钮。

获取并显示图像:

```py
from PIL import Image

p1 = 'gdrive/My Drive/Colab Notebooks/'
p2 = 'images/honest_abe.jpeg'
path = p1 + p2
img_path = path
gdrive_image = Image.open(img_path)
plt.axis('off')
_ = plt.imshow(gdrive_image)

```

在 Google Drive 上创建图片路径。用路径打开图像并显示。确保图像在 *Colab 笔记本*目录中！

Note

您希望加载到 Colab 笔记本中的任何图像(或文件)必须位于 Google Drive 上的“Colab 笔记本”目录(或“Colab 笔记本”目录下的子目录)中。首次保存 Colab 笔记本时，会自动创建“Colab 笔记本”目录。所有的 Colab 笔记本都保存到“Colab 笔记本”目录中。为了方便起见，我们建议使用网站上包含的本章图片之一。只需将图片复制到你的 Google Drive 中的“Colabs Notebook”目录下。

创建一个函数，将 Google Drive 图像转换为 TensorFlow 可消耗张量:

```py
def reformat(img, size):
  img = tf.keras.preprocessing.image.img_to_array(img) / 255.
  img = tf.image.resize(img, size)
  return img

```

Keras 实用程序将 PIL 图像实例转换为 NumPy 数组，并将其大小调整为 progan-128 预期的大小，即 128 × 128 × 3。

调用函数:

```py
img_size = (128, 128)
gdrive_vector = reformat(gdrive_image, img_size)
gdrive_vector.shape

```

显示图像:

```py
display_image(gdrive_vector)

```

Note

上传的张量*不是*目标图像，因为它不是从潜在空间中绘制的。目标图像是根据上传的张量从 progan-128 创建的！

使用 progan-128 从我们刚刚从 Google Drive 图像创建的向量中生成目标图像:

```py
gdrive_target = hub_model(gdrive_vector)['default'][0]
display_image(gdrive_target)

```

同样，从 Google Drive 加载的图像不是目标图像。它只是一个基于 Google Drive 图像的向量(一旦我们将图像转换为 NumPy 数组), progan-128 使用它来创建目标图像。

创建一个减少 MSE 的损失算法:

```py
reduction = tf.keras.losses.Reduction.SUM
loss_algorithm = tf.losses.MeanSquaredError(reduction)

```

我们使用 MSE 减少，但是 MAE 减少也应该起作用。

清除以前的模型会话:

```py
tf.keras.backend.clear_session()

```

火车:

```py
num_optimization_steps = 300
steps_per_image = 5
mse_images, mse_loss = find_closest_latent_vector(
    feature_vector, gdrive_target, num_optimization_steps,
    steps_per_image, loss_algorithm)

```

我们在更多的优化步骤上训练该模型，以生成目标图像的更好的复制品。

计算最终损失:

```py
MSE_loss = mse_loss[num_optimization_steps - 1]
MSE_loss

```

动画:

```py
animate(np.stack(mse_images))

```

将最终生成的图像与目标图像进行比较:

```py
display_image(np.concatenate(
    [mse_images[-1], gdrive_target], axis=1))

```

### 从维基共享图像创建一个目标

Wikimedia Commons 是一个免费使用的图像、声音、其他媒体和 JSON 文件的在线存储库。这是维基媒体基金会的一个项目。Wikimedia Commons 只接受不受版权限制的免费内容，如图像和其他媒体文件，版权限制会阻止它们被任何人在任何时间出于任何目的使用。

生成种子并创建特征向量:

```py
seed_value = 0
tf.random.set_seed(seed_value)
feature_vector = tf.random.normal([1, latent_dim])

```

抓取图像:

```py
p1 = 'http://upload.wikimedia.org/wikipedia/commons/'
p2 = 'd/de/Wikipedia_Logo_1.0.png'
URL = p1 + p2
im = imageio.imread(URL)
im.shape

```

导入将图像张量转换为 NumPy 数组所需的库:

```py
from keras.preprocessing.image import img_to_array

```

将图像转换为 NumPy 数组并显示:

```py
img_array = img_to_array(im)
print(img_array.dtype)
print(img_array.shape)
plt.imshow(tf.squeeze(img_array))
fig = plt.axis('off')

```

Keras 实用程序将 PIL 图像实例转换为 NumPy 数组，以便我们可以显示图像。

为 progan-128 消费调整图像大小:

```py
wiki_vector = tf.image.resize(img_array, (128, 128))
plt.imshow(tf.squeeze(wiki_vector))
fig = plt.axis('off')

```

将图像调整到 progan-128 的预期大小并显示。

创建目标:

```py
wiki_target = hub_model(wiki_vector)['default'][0]
display_image(wiki_target)

```

创建一个减少 MSE 的损失算法:

```py
reduction = tf.keras.losses.Reduction.SUM
loss_algorithm = tf.losses.MeanSquaredError(reduction)

```

清除以前的模型会话:

```py
tf.keras.backend.clear_session()

```

火车:

```py
num_optimization_steps = 300
steps_per_image = 5
mse_images, mse_loss = find_closest_latent_vector(
    feature_vector, wiki_target, num_optimization_steps,
    steps_per_image, loss_algorithm)

```

计算最终损失:

```py
MSE_loss = mse_loss[num_optimization_steps - 1]
MSE_loss

```

动画:

```py
animate(np.stack(mse_images))

```

比较:

```py
display_image(np.concatenate(
    [mse_images[-1], wiki_target], axis=1))

```

还不错。

虽然我们可以生成非常逼真的图像和目标图像的复制品，但需要更强大的模型和训练算法来始终如一地生成与实际图像无法区分的图像。当然，还需要更多的计算资源！

## 潜在向量和图像阵列

progan-128 模块从大小为(1，512)的潜在向量或大小为 128 × 128 × 3 的浮点向量生成新图像。progan-128 接受的潜在向量是大小为 512 的一维向量。progan-128 接受的浮点向量是 128 × 128 × 3 像素向量。

progan-128 模块是一个基于渐进 gan 的 TensorFlow 重新实现的图像生成器。它从一个 512 维的潜在空间映射到图像。在训练期间，从正态分布中采样潜在空间向量。

根据文档，progan-128 接受数据类型为 float32、形状为(？, 512).progan-128 的输入张量代表一批潜在向量。progan-128 的输出是一个形状为(？128，128，3)，其代表一批 RGB 图像。我们还可以从图像数组中生成一个新的图像，这不包括在文档中。

要查看 progan-128 文档，请阅读

[T2`https://tfhub.dev/google/progan-128/1`](https://tfhub.dev/google/progan-128/1)

### 从潜在向量生成新图像

从潜在空间创建一个随机法向量:

```py
random_normal_latent_vector = tf.random.normal([1, latent_dim])
random_normal_latent_vector.shape

```

tf.random.normal API 从正态分布中输出随机值。因此，新的向量由基于我们的潜在空间的正态分布中随机抽取的 512 个值组成。

将张量转换为 NumPy 以进行检查:

```py
rnlv = random_normal_latent_vector.numpy()
len(rnlv[0])

```

检查 NumPy 数组中的一些元素:

```py
for i, element in enumerate(rnlv[0]):
  if i < 5:
    print (element)
  else: break

```

新向量中的每个元素代表一个潜在维度(或潜在变量)，它不能被直接观察到，但可以假设存在。由于潜在维度的存在，它们可以用来解释观察变量的变化模式。在我们的实验中，观察变量代表 CelebA 图像。因此，我们可以向 progan-128 提供新的向量来生成新的图像。

使用 progan-128 从潜在空间创建浮点输出张量:

```py
float_output_tensor = hub_model(
    random_normal_latent_vector)['default'][0]
float_output_tensor.shape

```

将浮点输出张量显示为图像:

```py
display_image(generated_image)

```

所以 progan-128 从一个潜在向量生成一个 128 × 128 × 3 的图像。

### 从图像向量生成新图像

从 Google Drive 获取图片:

```py
p1 = 'gdrive/My Drive/Colab Notebooks/'
p2 = 'images/honest_abe.jpeg'
path = p1 + p2
img_path = path
abe_image = Image.open(img_path)
plt.axis('off')
_ = plt.imshow(abe_image)

```

将 JPEG 图像转换为适当数据类型和大小的图像矢量:

```py
img_size = (128, 128)
abe_vector = reformat(abe_image, img_size)
abe_vector.shape

```

显示向量的切片:

```py
abe_vector[0][0].numpy()

```

向量绝对不是来自潜空间！

从我们刚刚创建的矢量生成一个新图像:

```py
image_from_abe_vector = hub_model(abe_vector)['default'][0]
display_image(image_from_abe_vector)

```

所以 progan-128 从一个不是从潜在空间中绘制的特征图像向量生成一个 128 × 128 × 3 的图像！

## 摘要

我们以详细和逐步的方式演示了两个渐进生长 GAN 实验。我们还验证了 progan-128 也可以从不是从潜在空间中提取的特征向量中生成真实的图像。