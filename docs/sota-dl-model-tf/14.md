# 14.强化学习导论

**强化学习** (RL)是机器学习的一个领域，专注于教会智能代理如何在一个环境中采取行动，以最大化累积奖励。RL 中的**累积奖励**是所有奖励的总和，作为训练步数的函数。

我们通过使用奖励和惩罚来训练机器学习(ML)模型。当代理做出正确的决策时，我们会奖励它一个正分。用一个错误的决定，我们用一个负分来惩罚它。从这些响应中，模型学习如何在特定情况(或环境)下做出反应。因此，RL 背后的思想是，代理通过与环境交互并接受执行行为的奖励(和惩罚)来从环境中学习。

从与环境的互动中学习来自我们的自然经验。想象你是一个住在客厅的孩子。你看到一个壁炉，你走近它。很温暖，很积极，你感觉很好。你知道火是一件积极的事情。但是当你触摸到火时。哎哟！它会烫伤你的手。从你们的互动中，你了解到当你离火足够远时，火是积极的，因为它产生温暖。但是靠得太近就会燃烧。所以人类通过与环境的互动来学习。

RL 只是一种从行动中学习的计算方法。如果这个场景是一个 RL 实验，代理人在火边取暖会得到+1 的正奖励，但被火烧伤会得到-1 的负奖励。

RL 是与监督学习和非监督学习并列的三种基本 ML 范式之一。监督学习模型在指导下从标记的数据集学习。无监督学习模型在没有指导的情况下从未标记的数据中学习。当代理与环境交互时，RL 模型通过反复试验来学习，执行动作，如果动作正确则得到奖励，如果动作不正确则受到惩罚。

强化学习的三个组成部分是代理、环境和动作。一个*环境*是一个需要解决的问题。一个*代理*是一个与环境交互来解决问题的算法。*动作*是一个智能体与环境的交互。

简而言之，代理接收来自环境的观察并采取行动。环境根据代理人采取的行动，使用奖励或惩罚作为积极或消极行为的信号。因此，代理人通过与环境的反复试验来学习如何解决问题。

虽然设计者制定了奖励政策，但他们没有给学习模型任何关于如何解决问题的提示或建议。该模型通过代理人和环境之间的试错互动，自己学习如何最大化回报。

**奖励策略**是一组规则，它最大化给定强化学习环境的奖励功能。所以一个*奖励函数*规定了设计者希望代理完成什么。

## 强化学习的挑战

RL 的主要挑战是*创造环境*。创造一个有效的环境有三个问题。

首先，环境高度依赖于要解决的问题。所以这是具体的语境。(问题领域的)上下文驱动每个强化学习环境的设计，以解决一组特定的任务。简单地说，必须为每个新的 RL 任务设计一个新的环境。例如，自动驾驶汽车的环境不能转移到自动飞行无人机的环境。相比之下，监督学习分类模型可以被其他分类任务重用。

第二，奖励政策是有限的和结构化的。国际象棋、围棋和雅达利游戏的环境相对简单。无论这些游戏看起来有多复杂，它们的规则都是结构化的和有限的。对于自动驾驶汽车来说，即使是极其复杂的环境也是有限和结构化的。虽然这样的环境必须处理许多未知，但它的奖励政策是由设计师创造的。即使是这个星球上最杰出的设计师也无法创造出一个无限的奖励政策。例如，自动驾驶汽车可能被设计为在各种条件下工作良好，但它可能无法在与设计不同的情况下工作。自动驾驶汽车在特定路线上行驶时可能会工作，但如果施工需要绕道，但绕道标志尚未张贴，该怎么办？

第三，在安全问题上，出错的空间几乎为零。想到了医疗保健行业的 RL 实验。这种模型必须在许多阶段进行测试和调整，然后才能进行哪怕是简单的测试。将模型从测试阶段转移到现实世界是真正工作的开始。

缩放和调整控制代理的神经网络(或其他 ML 模型)是另一个挑战。与网络交流的唯一方式是通过它的奖惩系统。这个单一的管道带来了灾难性遗忘(或灾难性干扰)的可能性。**灾难性遗忘**是神经网络在学习新信息时，完全突然忘记以前学习过的信息的倾向。因此，当神经网络获得新信息时，一些旧信息会从网络中删除。灾难性遗忘的发生是因为当学习新信息时，神经网络的许多权重(存储信息的位置)会发生变化，这使得先前的知识不太可能保持完整。在顺序学习过程中，进入神经网络的新输入可以消除原始输入权重。

另一个挑战是达到局部最优。一个**局部最优**是一个问题在一个可能的解决方案的小邻域内的最佳解决方案。相比之下，一个**全局最优**是当*每个*可能的解都被考虑时的最优解。当目标是学习如何移动(走、跑和跳)时，局部优化应该是步行。在这种情况下，代理以次优(局部最优)的方式执行任务，而不是以(全局)最优的方式。

最后一个挑战是找到有才华的设计师。一个有能力的数据科学家可能不难找到，但平均年薪可以超过 15 万美元。此外，可能很难找到一个在创建特定类型的环境方面有经验的人，因为环境是上下文特定的。

章节的笔记本位于以下 URL:

[T2`https://github.com/paperd/deep-learning-models`](https://github.com/paperd/deep-learning-models)

我们用一个使用非常简单的 RL 环境的代码实验来演示强化学习。通过导入主 TensorFlow 库并实例化 GPU，开始设置 Colab 生态系统。

## 导入 TensorFlow 库

导入库并将其别名为 **tf** :

```py
import tensorflow as tf

```

将 TensorFlow 库别名为 tf 是常见的做法。

## GPU 硬件加速器

为了方便起见，我们提供了在 Colab 笔记本中启用 GPU 的步骤:

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

验证 GPU 是否处于活动状态:

```py
tf.__version__, tf.test.gpu_device_name()

```

如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

Note

如果得到错误`NAME ‘TF’ IS NOT DEFINED`，重新执行代码导入 TensorFlow 库！

## 强化学习实验

我们用手推车杆子演示了一个简单的 RL 实验。 *Cart-Pole* 是一种游戏，其中一根杆子通过一个未驱动的关节连接到一辆沿无摩擦轨道移动的车上。这个游戏的目标是保持杆子垂直竖立。起始状态在–0.05 和 0.05 之间随机初始化。起始状态包括小车位置、小车速度、磁极角度和磁极速度。磁极速度是在磁极尖端测量的。车杆游戏是在 2D 空间。所以大车只能左右移动来平衡杆子。

实验的目标是教代理如何用 RL 模型平衡推车上的杆子。我们可以创建自己的环境来训练代理，但我们不必这样做，因为解决这个简单的极点平衡问题的环境已经存在。我们使用开放健身房的环境。OpenAI Gym 是一个工具包，它提供了各种各样的模拟环境，包括 Atari 游戏、棋盘游戏和(2D 和 3D)物理模拟。

### 在 Colab 上安装和配置 OpenAI Gym

Python 包(OpenAI Gym)的大部分需求已经在 Colab 上实现了。但是我们仍然需要安装依赖项:

```py
!pip install gym
!apt-get install python-opengl -y
!apt install xvfb -y

```

我们需要 gym、opengl 和 xvfb 依赖项来为 OpenAI Gym 启用 Python 库，为可视化启用图形库，为渲染图形启用显示服务器。 *Gym* 是 RL 任务的开源接口。它支持教导代理从走路到玩游戏，如乒乓球或弹球。

*OpenGL* 是一个多平台支持的图形库，包括 Windows、Linux 和 MacOS。 *Xvfb* (X 虚拟帧缓冲区)是实现 X11 显示服务器协议的显示服务器。它运行在内存中，不需要物理显示器。

我们还需要安装依赖项来显示从环境中呈现的输出:

```py
!pip install pyvirtualdisplay
!pip install piglet

```

*pyvirtualdisplay* 库支持虚拟显示。*小猪*库为游戏和其他多媒体应用的创建提供了一个面向对象的 API。

### 导入库

导入并激活虚拟显示库:

```py
import pyvirtualdisplay

display = pyvirtualdisplay.Display(
    visible=0, size=(1400, 900)).start()

```

我们使用 1400 × 900 的虚拟显示器。我们选择的尺寸是任意的。请随意试验显示器的尺寸。

导入*健身房*库:

```py
import gym

```

`gym`库是为测试和开发 RL 算法而设计的环境集合。它让我们不必自己创造复杂的环境。

### 创造环境

创建*推车杆*环境:

```py
env = gym.make('CartPole-v1')

```

推车杆环境是一个 2D 模拟，它向左或向右加速推车，以平衡放在它上面的杆。一根杆子通过一个非驱动关节连接到一辆沿无摩擦轨道移动的小车上。通过对推车施加+1 或-1 的力来控制系统。钟摆(或杆)开始直立，目标是防止它翻倒。

初始化环境:

```py
env.seed(0)
obs = env.reset()
obs

```

用*复位*方法初始化环境。初始化后，该方法返回一个观察值。

观察结果因环境而异。在这种情况下，一个观察值是一个 1D NumPy 数组，由四个浮点数组成，分别代表小车的水平位置、速度、极点角度(0 =垂直)和角速度。任何正数表示磁极角度和角速度向右*移动*。任何负数表示向左*移动*。对于水平位置，负数表示杆子向左倾斜的*，正数表示向右倾斜*。对于速度来说，正数表示小车正在加速，负数表示小车正在减速。

初始呈现状态(种子为 0)产生 1D NumPy 数组:

数组([-0.04456399，0.04653909，0.01326909，-0.02099827])

所以极点的初始状态不是完全水平的(obs[0]略负)，它的速度是缓慢增加的(obs[1]略正)，极点稍微向右倾斜(obs[2]略正)，角速度是向左的(obs[3]略负)。

一个环境可以通过调用它的 *render* 方法来可视化，我们可以选择渲染模式(渲染选项取决于环境)。以初始状态显示渲染环境:

```py
env.render()

```

在我们的例子中，env.render()命令显示 *True* ，这是初始状态，因为我们刚刚对它进行了初始化。

在 RL 中，我们必须将环境渲染(初始化)到其初始状态，作为训练的前驱。原因是我们想让环境在初始状态下不给代理任何线索。我们希望代理学习如何从头开始解决任务。

### 显示环境中的渲染

设置 mode='rgb_array '以获取 NumPy 数组形式的环境图像:

```py
img = env.render(mode='rgb_array')
img.shape

```

图像形状是从 Cart-Pole 环境中呈现的。

创建一个函数，从清单 [14-1](#PC11) 中配置的环境中显示购物车上杆子位置的渲染图像。

```py
def plot_environment(env, figsize=(5,4)):
  plt.figure(figsize=figsize)
  img = env.render(mode='rgb_array')
  plt.imshow(img)
  plt.axis('off')
  return img

Listing 14-1Function to Display the Rendered Environment

```

显示:

```py
import matplotlib.pyplot as plt

plot_environment(env)
plt.show()

```

我们看到环境在初始化状态下的渲染。所以杆子在推车上几乎是垂直的，但是稍微向右倾斜。

### 显示操作

让我们看看如何与我们创造的环境互动。代理需要从动作空间中选择一个动作。一个**动作空间**是一个代理可以采取的一组可能的动作。

询问环境可能采取的行动:

```py
env.action_space

```

*Discrete(2)* 表示 Cart-Pole 环境的可能动作是整数 0 和 1。向左加速为 0，向右加速为 1。所以环境的动作空间有两种可能的动作，这意味着智能体可以向左或向右加速。当然，其他环境可能有额外的离散动作或其他种类的动作，如连续动作。

Note

车杆环境是可以创建的最简单的环境！真实世界的强化学习环境有巨大的动作空间，包含许多可能的动作。

重置环境，通过观察杆子的角度来了解它是如何倾斜的:

```py
env.seed(0)
obs = env.reset()
indx = 2
obs[indx]

```

在 *obs* 数组中的第三个位置(索引为 2)是极点的角度。如果该值小于 0，则极点向左倾斜。如果大于 0，它向右倾斜。价值刚刚超过零。所以极点稍微向右倾斜，因为 obs[2]是> 0。

Note

我们不必再次重置环境，因为我们在上一节中已经这样做了。但是，在实验开始时，应该重置一个环境，以确保代理从零开始学习如何解决任务(没有线索)。所以我们再次这样做只是为了灌输一个好的学习习惯。

我们已经知道，Cart-Pole 环境只有两个动作，左(0)和右(1)。让我们通过设置 *action=1* 来向右加速购物车:

```py
action = 1
obs, reward, done, info = env.step(action)
print ('obs array:', obs)
print ('reward:', reward)
print ('done:', done)
print ('info:', info)

```

*步骤*方法执行给定的动作并返回四个值。 *obs* 是新的观察。手推车现在向右移动，因为 obs[1] > 0。极点仍然向右倾斜，因为 obs[2] > 0，但是它的角速度现在是负的，因为 obs[3] < 0。所以在下一步之后，它可能会向左倾斜。

在这个最简单的环境中，*奖励*在每一步总是 1.0。所以我们的目标是尽可能长时间地播放这一集。当剧集结束时， *done* 值为真。如果杆子倾斜太多或者离开屏幕，或者我们赢了比赛，这一集就结束了。*信息*值提供额外的信息。在这种情况下，没有额外的信息。一旦我们使用完一个环境，调用 *close* 方法来释放资源。

Note

没有必要释放资源，因为我们的实验非常简单。但是对于更复杂的 RL 任务，在任务完成时关闭环境是一个好主意，因为它们往往会消耗大量的计算机资源。

环境告诉代理每一个新的观察，奖励，游戏何时结束，以及它在最后一步得到的信息。显示极点位置:

```py
plot_environment(env)
plt.show()

```

杆子仍然向右倾斜。

显示代理在上一步中收到的奖励:

```py
reward

```

当然，回报是 1.0，因为在这个最简单的实验中，它总是 1.0。

测试游戏是否结束:

```py
done

```

既然值为*假*，游戏就没有结束。如果输出为*真*，则任务完成，游戏结束。

一个**插曲**是从环境被重置的时刻到它被完成的一系列步骤。在一集结束时(即，当 step 方法返回 done=True 时)，在继续使用它之前重置环境。

要在单集结束时自动复位:

```py
if done:
  obs = env.reset()
else:
  print ('game is not over!')

```

Note

我们向您展示前面的代码只是为了提供信息。我们向您展示在 RL 实验的上下文中何时使用 env.reset()。

### 简单神经网络奖励策略

怎样才能让杆子保持直立？我们需要制定奖励政策。在行动中，奖励政策只是代理人在每一步选择行动的策略。代理可以使用所有过去的动作和观察来决定要做什么。

让我们创建一个神经网络，它将观察结果作为输入，并输出对每个观察结果采取的行动。为了选择一个动作，我们的网络估计每个动作的概率，并基于估计的概率随机选择一个动作。在车杆环境中，只有两种可能的动作(左和右)。所以我们只需要一个输出神经元，输出动作 0 的概率 *p* (左)和动作 1 的概率*1*–*p*(右)。

清除以前的模型并生成种子:

```py
import numpy as np

tf.keras.backend.clear_session()
tf.random.set_seed(0)
np.random.seed(0)

```

确定观察空间:

```py
obs_space = env.observation_space.shape
obs_space

```

*观察空间*是奖励政策的另一个术语。观察空间(如前面观察数组中所示)是一个 1D NumPy 数组，由四个浮点数组成，分别代表小车的水平位置、速度、极点角度(0 =垂直)和角速度。因此，观测空间包含在一个具有 4 个元素的 1D NumPy 数组中。

Note

术语策略、奖励策略、观察阵列和观察空间是可以互换的。

设置策略网络的输入数量:

```py
n_inputs = env.observation_space.shape[0]
n_inputs

```

创建策略网络:

```py
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential([
  Dense(5, activation='elu', input_shape=[n_inputs]),
  Dense(1, activation='sigmoid')
])

```

策略网络是一个简单的*序列*神经网络。输入的数量是观察空间的大小，在我们的例子中是 4。我们在第一层只包括五个神经元，因为这是一个如此简单的问题。我们只需要输出一个概率(向左走的概率)。因此，我们在输出层使用 sigmoid 激活来生成单个输出神经元作为 logit。我们只需要输出一个概率 p，因为我们可以通过 1–p 获得正确的概率。如果我们有两个以上的可能操作，我们仍然会对每个操作使用一个输出神经元，并在输出层中替换 softmax 激活。

在这个特定的环境中，可以安全地忽略过去的操作和观察，因为每个观察都包含环境的完整状态。如果存在某种隐藏状态，我们可能需要考虑过去的行为和观察，以尝试推断环境的隐藏状态。例如，如果环境只显示了车的位置而没有显示它的速度，为了估计当前的速度，我们不仅要考虑当前的观察，还要考虑先前的观察。另一个例子是，如果观察值有噪声，我们可能希望使用过去的一些观察值来估计最可能的当前状态。我们的问题非常简单，因为当前的观察是无噪声的，并且包含了环境的全部状态。

为什么我们根据策略网络给出的概率选择一个随机动作，而不是只选择概率最高的动作？因为这种方法让代理在探索新动作和利用已知有效的动作之间找到了正确的平衡。

### 模型预测

创建一个运行模型播放一集并返回帧的函数，这样我们就可以显示一个动画，如清单 [14-2](#PC24) 所示。

```py
def render_policy_net(model, n_max_steps=200, seed=0):
  frames = []
  env = gym.make('CartPole-v1')
  env.seed(seed)
  np.random.seed(seed)
  obs = env.reset()
  for step in range(n_max_steps):
    frames.append(env.render(mode='rgb_array'))
    left_proba = model.predict(obs.reshape(1, -1))
    action = int(np.random.rand() > left_proba)
    obs, reward, done, info = env.step(action)
    if done:
      break
  env.close()
  return frames

Listing 14-2Function to Return the Frames from One Episode

```

建立车杆环境并重置它。创建一个循环来运行多个步骤，直到剧集结束。通过将环境渲染的可视化添加到*帧*列表开始每个步骤。继续根据模型做出行动预测。接下来，根据预测建立一个行动。根据动作执行*步骤*方法。继续循环，直到剧集结束。最后返回帧列表。

创建函数来显示帧的动画，如清单 [14-3](#PC25) 所示。

```py
import matplotlib.animation as animation
import matplotlib as mpl

def update_scene(num, frames, patch):
  patch.set_data(frames[num])
  return patch,

def plot_animation(frames, repeat=False, interval=40):
  fig = plt.figure()
  patch = plt.imshow(frames[0])
  plt.axis('off')
  anim = animation.FuncAnimation(
      fig, update_scene, fargs=(frames, patch), blit=True,
      frames=len(frames), repeat=repeat, interval=interval)
  plt.close()
  return anim

Listing 14-3Functions to Animate Frames

```

*plot_animation* 函数通过反复调用 *update_scene* 函数来创建动画。plot_animation 函数接受帧列表。该功能继续从帧列表中提取图像(补丁)。然后它调用 update_scene 来设置面片的 x 和 y 坐标。动画根据每个帧图像的面片坐标成批返回。

基于策略网络创建框架列表:

```py
frames = render_policy_net(model)

```

### 有生命的

创建动画:

```py
anim = plot_animation(frames, interval=100)

```

试验一下*间隔*参数，看看它对动画的影响。我们将间隔大小设置为 100 只是因为我们喜欢这个结果。

Note

增加间隔只会延长动画运行的秒数。

渲染并显示动画。我们展示了实现这一点的两种方法。第一种方法使用 HTML 库来显示 HTML 元素:

```py
from IPython.display import HTML

method1 = HTML(anim.to_html5_video())
method1

```

动画用 to_html5_video 方法渲染成 *html5 video* ，用 html 模块显示。

第二种方法使用运行时配置库:

```py
from matplotlib import rc

method2 = rc('animation', html='html5')

```

要实现第二种方法，只需运行动画对象:

```py
anim

```

啊！杆子向左倒了！原因是我们还没有实施奖励政策。

### 实施基本奖励政策

在上一节中，我们只是让策略网络在*没有与代理*交互的情况下运行随机预测。如果杆子向左倾斜，代理人无法知道这是一个坏的还是好的动作。我们需要开发一个具有奖励政策的环境，代理可以与之交互，以学习如何平衡推车上的杆子。

创建一个多样化的环境空间，如清单 [14-4](#PC31) 所示。

```py
n_environments = 50
n_iterations = 5000

envs = [gym.make(
    'CartPole-v1') for _ in range(n_environments)]
for index, env in enumerate(envs):
  env.seed(index)
np.random.seed(0)
observations = [env.reset() for env in envs]
optimizer = tf.keras.optimizers.RMSprop()
loss_fn = tf.keras.losses.binary_crossentropy

for iteration in range(n_iterations):
  target_probas = np.array(
      [([1.] if obs[2] < 0 else [0.])
      for obs in observations])
  with tf.GradientTape() as tape:
    left_probas = model(np.array(observations))
    loss = tf.reduce_mean(
        loss_fn(target_probas, left_probas))
    print('\rIteration: {}, Loss: {:.3f}'.\
          format(iteration, loss.numpy()), end='')
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(
        zip(grads, model.trainable_variables))
    actions = (np.random.rand(n_environments, 1) >\
               left_probas.numpy()).astype(np.int32)
  for env_index, env in enumerate(envs):
    obs, reward, done, info = env.step(
        actions[env_index][0])
    observations[env_index] = obs if not done else env.reset()

for env in envs:
  env.close()

Listing 14-4Environment Space for the Experiment

```

我们创建了一个由 50 个不同环境并行组成的环境空间，以便在网络的每一步提供多样化的培训批次。我们训练网络 5000 次迭代。当然，您可以调整环境和迭代的数量，但是要知道可用的计算机资源。

我们使用 RMSprop 优化器，因为它似乎工作得很好。您可以尝试不同的优化器，看看代理学习任务的效果如何。我们对损失函数使用二元交叉熵，因为我们只有两个离散的可能动作(左和右)。迭代循环中的第一行检查极点的角度。如果角度< 0，目标动作为左(proba(left) = 1。).否则，目标动作为右(proba(left) = 0)。

**优化器**是用来改变一个神经网络的属性(如权重和学习率)以减少损失的算法(或方法)。所以优化者通过最小化损失函数来解决优化问题。

有关 TensorFlow 优化器的完整列表，请阅读

[T2`www.tensorflow.org/api_docs/python/tf/keras/optimizers`](http://www.tensorflow.org/api_docs/python/tf/keras/optimizers)

梯度带部分记录了代理在训练期间的动作。确定杆的倾斜度。代理人采取措施稳定推车上的杆子。这些行动随后被反馈到 50 个环境中，这些环境决定了给予代理人的奖励。这个过程重复 5000 次。

最后，环境被重置以释放资源。我们使用定制的训练循环进行训练，因此我们可以轻松地使用每个训练步骤的预测来改进环境。

为动画创建帧:

```py
frames = render_policy_net(model)

```

我们现在有了基于代理在培训中所学内容的框架。我们使用帧来创建动画。

动画:

```py
anim = plot_animation(frames, repeat=True, interval=100)
anim

```

瞧啊。动画验证了我们的 RL 模型是有效的。也就是说，我们教代理如何平衡推车上的杆子。但是我们能做得更好吗？

### 增强策略梯度算法

我们还没有展示 RL 的真正突破。在前面的部分中，代理从奖励政策中学习。但是代理能自己学习更好的政策吗？

我们可以使用强化策略梯度算法来自动进行代理学习。*政策梯度*通过遵循更高回报的梯度来优化政策参数。

#### 政策梯度如何发挥作用？

要使用策略梯度，让神经网络策略玩几次游戏。在每一步，计算梯度，使选择的行动更有可能。但是不要在这一点上应用渐变。

运行几集后，计算每一步每个行动的优势和折扣因子。*折扣系数*是通过根据行动后所有奖励的总和评估行动来计算的。如果一个行动的优势是积极的，这个行动可能是好的。所以**现在**应用渐变，使动作在未来更有可能被选择。如果它是负的，应用相反的梯度，使行动不太可能被选中。最后，计算所有合成梯度向量的平均值，并使用它来执行*梯度下降*步骤。所有合成梯度向量的平均值通过将每个梯度(或相反梯度)的平均值乘以其作用优势来计算。最终结果是创建一个有效的策略梯度下降算法，该算法最小化损失函数。

#### 用策略梯度训练模型

首先创建函数来播放一个步骤，播放多集，折扣奖励，并正常化折扣奖励。

创建一个函数，如清单 [14-5](#PC34) 所示执行一个步骤。

```py
def play_one_step(env, obs, model, loss_fn):
  with tf.GradientTape() as tape:
    left_proba = model(obs[np.newaxis])
    action = (tf.random.uniform([1, 1]) > left_proba)
    y_target = tf.constant(
        [[1.]]) - tf.cast(action, tf.float32)
    loss = tf.reduce_mean(loss_fn(
        y_target, left_proba))
  grads = tape.gradient(loss, model.trainable_variables)
  obs, reward, done, info = env.step(
      int(action[0, 0].numpy()))
  return obs, reward, done, grads

Listing 14-5Function to Play a Single Step

```

在 *GradientTape* 块中，调用带有单个观察值的模型。重塑观察，使其成为包含单个实例的批处理(模型需要一个批处理)。通过对 0 和 1 之间的随机浮点数进行采样，并检查它是否大于向左的概率，来获得向左的概率。如果概率为 left_proba，则*动作*为假，如果概率为 1–left _ proba，则为真。用适当的概率将此布尔值(真或假)转换为数字 0(左)或 1(右)。然后我们定义向左(1-动作)或向右(动作)的目标概率。如果动作为 0(左)，目标向左概率为 1。如果动作为 1(右)，目标概率为 0。

继续计算损失，并使用胶带计算与模型可训练变量相关的损失梯度。稍后根据行动的好坏来调整渐变。最后播放选中的动作并返回新的观察，奖励，剧集是否结束，渐变。

创建一个函数播放多集，返回每集每步的奖励和渐变，如清单 [14-6](#PC35) 所示。

```py
def play_multiple_episodes(
    env, n_episodes, n_max_steps, model, loss_fn):
  all_rewards = []
  all_grads = []
  for episode in range(n_episodes):
    current_rewards = []
    current_grads = []
    obs = env.reset()
    for step in range(n_max_steps):
      obs, reward, done, grads = play_one_step(
          env, obs, model, loss_fn)
      current_rewards.append(reward)
      current_grads.append(grads)
      if done:
        break
    all_rewards.append(current_rewards)
    all_grads.append(current_grads)
  return all_rewards, all_grads

Listing 14-6Play Multiple Episodes Function

```

该函数通过调用 *play_one_step* 函数返回所需步数的奖励列表列表。此列表包含每集一个奖励列表。每个奖励列表包含每个步骤的一个奖励。该函数还返回一个渐变列表列表。该列表包含每集一个渐变列表。每个梯度列表包含每个步骤的一个梯度元组。每个元组包含每个可训练变量的一个梯度张量。

简单来说，策略梯度算法就是用 *play_multiple_episodes* 函数来玩几次游戏。然后，它会返回并查看所有奖励，以对其进行折扣和标准化。

#### 折扣和正常化奖励

为了对奖励进行贴现和规范化，我们对奖励进行贴现和规范化。*折扣因子*决定了在训练期间每个奖励在 RL 算法的当前状态下有多重要。这个想法是教代理如何评价学习过程中出现的每个奖励的重要性，以便在培训过程中与未来的奖励进行比较。*标准化折扣奖励*使奖励(正强化)的梯度更陡，惩罚(负强化)的梯度更浅。简单来说，当折扣奖励被标准化时，奖励触发更陡的梯度，惩罚触发更浅的梯度。更陡的梯度加速了损失函数的最小化，这是任何 ML 算法的目标。

*折扣 _ 奖励*函数对奖励进行折扣，如清单 [14-7](#PC36) 所示。

```py
def discount_rewards(rewards, discount_rate):
  discounted = np.array(rewards)
  for step in range(len(rewards) - 2, -1, -1):
    discounted[step] += discounted[step + 1] * discount_rate
  return discounted

Listing 14-7Discount Rewards Function

```

验证该功能是否有效:

```py
discount_rewards([10, 0, -50], discount_rate=0.8)

```

我们给这个函数三个动作。每次行动后，都有回报。第一个奖励 10，第二个奖励 0，第三个奖励–50。我们使用 80%的折扣系数。所以第三个动作得到–50(最后一个奖励的满分)。但是第二个动作只能得到-40 分(最后一个奖励的 80%)。最后，第一次行动获得–40 的 80%(–32)以及第一次奖励的全额积分(+10)，从而获得–22 的折扣奖励。

Note

我们给 *discount_rewards* 函数三个动作，三个值的列表(或向量)，即[10，0，-50]。我们可以通过在列表中添加或删除值来改变动作的数量。

*discount _ and _ normalize _ rewards*函数归一化折扣奖励，如清单 [14-8](#PC38) 所示。

```py
def discount_and_normalize_rewards(
    all_rewards, discount_rate):
  all_discounted_rewards =\
    [discount_rewards(rewards, discount_rate)
    for rewards in all_rewards]
  flat_rewards = np.concatenate(all_discounted_rewards)
  reward_mean = flat_rewards.mean()
  reward_std = flat_rewards.std()
  return [(discounted_rewards - reward_mean) / reward_std
          for discounted_rewards in all_discounted_rewards]

Listing 14-8Normalize Discounted Rewards Function

```

要标准化所有剧集的所有折扣奖励，请计算所有折扣奖励的平均值和标准差。从每个折扣奖励中减去平均值，然后除以标准差。让我们用两集来试试这个功能:

```py
discount_and_normalize_rewards(
    [[10, 0, -50], [10, 20]], discount_rate=0.8)

```

第一集(第一组)的所有行为都被认为是坏的，因为标准化的优势都是负的。这是有道理的，因为奖励的总和是-40 (10 + 0 + -50)。相反，第二集行动(第二组)是好的，因为标准化的优势是积极的。奖励的总和是 30 (10 + 20)。

#### 训练学习者

正如我们所知，RL 使用一个学习模型和一个环境空间来使代理学习如何解决问题。因此，代理在培训过程中学习如何解决问题。所以我们可以把这个过程看作是训练学习者。

首先定义一组超参数:

```py
n_iterations = 150
n_episodes_per_update = 10
n_max_steps = 200
discount_rate = 0.95

```

我们设置了 150 次训练迭代。当然，您可以调整这个数字和其他值。每次迭代玩十集游戏，使每集最多持续 200 步，使用 0.95 的折扣率。

为策略网络定义优化器和损失函数:

```py
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
loss_fn = tf.keras.losses.binary_crossentropy

```

使用二元交叉熵，因为我们正在训练一个二元分类器(两个可能的动作:左和右)。

生成种子，清除以前的模型，并创建简单的策略网络:

```py
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

model = Sequential([
  Dense(5, activation='elu', input_shape=[4]),
  Dense(1, activation='sigmoid'),
])

```

强化学习中的策略网络非常简单。创造一个环境空间是困难的部分。

按照清单 [14-9](#PC43) 所示训练学员。

```py
env = gym.make('CartPole-v1')
env.seed(42);

for iteration in range(n_iterations):
  all_rewards, all_grads = play_multiple_episodes(
      env, n_episodes_per_update, n_max_steps,
      model, loss_fn)
  total_rewards = sum(map(sum, all_rewards))
  print('\rIteration: {}, mean rewards: {:.1f}'.format(
      iteration, total_rewards / n_episodes_per_update),
      end='')
  all_final_rewards = discount_and_normalize_rewards(
      all_rewards, discount_rate)
  all_mean_grads = []
  for var_index in range(len(model.trainable_variables)):
    mean_grads = tf.reduce_mean(
        [final_reward * all_grads[episode_index][step][var_index]
         for episode_index, final_rewards in enumerate(
             all_final_rewards)
           for step, final_reward in enumerate(
               final_rewards)], axis=0)
    all_mean_grads.append(mean_grads)
  optimizer.apply_gradients(
      zip(all_mean_grads, model.trainable_variables))

env.close()

Listing 14-9Train the Learner

```

为了训练学习者，首先调用 *play_multiple_episodes* (在每个训练迭代中)来玩游戏十次，并返回每一集和每一步的所有奖励和梯度。接下来，调用*discount _ and _ normalize _ rewards*来计算每个行为的归一化优势，这给了我们一个衡量每个行为在事后实际上有多好或多坏的方法。对于每个可训练变量，计算所有情节和所有步骤的梯度的加权平均值，并通过 final_reward 进行加权。 *final_reward* 是每个动作的标准化优势。最后，使用优化器应用平均梯度，调整模型的可训练变量，希望使策略变得更好。

Note

训练学习者需要一些时间。所以要有耐心。

#### 渲染来自增强策略梯度算法的帧

渲染帧:

```py
frames_ra = render_policy_net(model)

```

#### 激活政策

动画:

```py
anim = plot_animation(frames_ra, repeat=True, interval=100)
anim

```

波兰人似乎不那么摇摆不定了。代理人自己学会了一个更好的政策，这有点令人惊讶！

## 摘要

我们介绍了 RL 的概念，并用一个简单的实验来演示它。即使对于最简单的环境空间，实验中的代码也相当复杂。RL 应用于现实世界问题的发展在于设计师创造真实代表我们生活的世界的环境空间的能力。