# 5.张量处理单元简介

我们用代码示例向您介绍张量处理单元。**张量处理单元(TPU)** 是一种专用集成电路(ASIC)，旨在加速 ML 工作负载。TensorFlow 中可用的 TPU 是由 Google Brain 团队基于其在 ML 社区中的丰富经验和领导地位从头定制开发的。*谷歌大脑*是谷歌的一个深度学习人工智能(AI)研究团队，他们研究如何让机器变得智能，以改善人们的生活。

章节的笔记本位于以下 URL:

[T2`https://github.com/paperd/deep-learning-models`](https://github.com/paperd/tensorflow)

## TPU 硬件加速器

我们用谷歌 Colab 云服务提供的 TPU 硬件加速器演示了学习实验。我们的示例并没有展示 TPU 的全部功效，但是提供这些示例是为了帮助您入门。我们使用高级 TensorFlow APIs 让模型在云 TPU 硬件上运行。

我们在 Colab 中使用的硬件加速器通常被称为云 TPU。*云 TPU* 旨在通过谷歌云上的人工智能服务运行尖端的人工智能模型。它支持使用 TensorFlow 在谷歌的 TPU 加速器硬件上处理 ML 工作负载。其定制的高速网络在单个 pod 中提供超过 100 petaflops 的性能，这足以将企业转变为人工智能就绪或创造下一个研究突破。云 TPU 旨在实现最大的性能和灵活性，以帮助研究人员、开发人员和企业构建能够利用 CPU、GPU 和 TPU 的 TensorFlow 计算集群。

有关部署云 TPU 的全面指南，请阅读

[T2`https://cloud.google.com/tpu/docs/tpus`](https://cloud.google.com/tpu/docs/tpus)

有关云 TPU 的所有可用教程(在撰写本文时)，请阅读

[T2`https://cloud.google.com/tpu/docs/tutorials`](https://cloud.google.com/tpu/docs/tutorials)

### 云 TPU 的优势

云 TPU 资源加速了线性代数计算的性能，线性代数计算在 ML 应用中被大量使用。当我们训练大型、复杂的神经网络模型时，云 TPU 最大限度地缩短了获得准确性的时间。以前在其他硬件平台上花费数周时间训练的模型可以在云 TPU 上几个小时内收敛。

### 何时使用云 TPU

云 TPU 针对特定工作负载进行了优化。在某些情况下，我们使用 GPU 或 CPU 来运行 ML 工作负载。一般来说，根据以下原则决定使用什么硬件。

#### 中央处理器（central processing units 的缩写）

*   *需要最大灵活性的快速原型制作

*   *简单的模型，不需要很长时间的训练

*   *有效批量较小的小型号

*   *以 C++编写的自定义张量流操作为主的模型

*   *受可用 I/O 或主机系统网络带宽限制的型号

#### 绘图处理器

*   *源代码不存在或修改起来太麻烦的模型

*   *具有大量自定义 TensorFlow 操作的型号，这些操作必须至少部分在 CPU 上运行

*   *云 TPU 上未提供的带 TensorFlow ops 的模型

*   *有效批量较大的中大型型号

#### TPUs 的

*   *以矩阵计算为主的模型

*   *在主训练循环中没有自定义张量流操作的模型

*   *训练数周或数月的模特

*   *有效批量非常大的大型模型

云 TPU 不适合需要频繁分支或受代数元素支配的线性代数程序。云 TPU 经过优化，可以快速执行大型矩阵乘法。因此，与其他平台相比，不受矩阵乘法支配的工作负载不太可能在云 TPU 上表现良好。需要高精度运算的工作负载不适合云 TPU(例如，双精度运算)。以稀疏方式访问内存的工作负载在云 TPU 上可能不可用。

有关云 TPU 的丰富资源，请阅读

[T2`https://cloud.google.com/tpu/docs/tpus`](https://cloud.google.com/tpu/docs/tpus)

## 导入 TensorFlow 库

导入库并将其别名为 **tf** :

```
import tensorflow as tf

```

## 启用 TPU 运行时

在 Colab 笔记本中启用 TPU 非常简单:

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *TPU* 。

4.  点击*保存*。

Note

每个笔记本中的*必须启用 TPU。但它只需启用一次。*

## TPU 检测

设置 TPU 解析程序并验证其正在运行:

```
tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])

```

该消息看起来像这样:

在 TPU 上运行['10.112.96.162:8470']

Tip

如果出现错误**名称‘TF’未定义**，请重新执行代码以导入 TensorFlow 库！出于某种原因，我们有时必须在 Colab 中重新执行 TensorFlow 库导入。我们不知道为什么会这样。

*TF . distribute . cluster _ resolver。TPUClusterResolver()* API 是谷歌云 TPU 的集群解析器。*集群解析器*为 TensorFlow 提供了与各种集群管理系统(如 GCE、AWS 等)通信的方式。)并访问设置分布式培训所需的信息。通过让 TensorFlow 与这些系统通信，它能够自动发现和解析各种 TensorFlow 工作人员的 IP 地址。因此，它最终能够从底层机器故障中自动恢复，并在系统管道中上下扩展 TensorFlow 工作集群。

## 为此笔记本配置 TPU

使群集上的设备可供使用:

```
tf.config.experimental_connect_to_cluster(tpu)

```

初始化 TPU 设备:

```
tf.tpu.experimental.initialize_tpu_system(tpu)

```

Note

这两种配置都是实验性的，这意味着它们可能会在未来发生变化。

## 制定分销策略

**分发策略**是一种抽象，用于在多个 CPU、GPU 或 TPU 之间分发培训。要改变一个模型在给定设备上的运行方式，只需改变发布策略。 *tf.distribute.Strategy* 是 TensorFlow API，用于将分发策略应用于模型。通过应用这个 API，我们可以将现有模型的培训分布到多个设备上，只需对代码进行最少的修改。

API 的设计考虑了三个关键目标:

*   *易于使用并支持多个用户群(例如数据科学家)

*   *提供开箱即用的良好性能

*   *在策略之间轻松切换

为此笔记本创建一个 TPU 策略:

```
tpu_strategy = tf.distribute.TPUStrategy(tpu)

```

将显示可用 TPU 和其他设备的列表。

## 手动放置设备

在 TPU 系统初始化之后，我们可以使用手动设备布局来指导单个 TPU 设备上的计算，如清单 [5-1](#PC6) 所示。

```
a = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]
b = [[7.0, 8.0], [9.0, 10.0], [11.0, 12.0]]
with tf.device('/TPU:7'):
  c = tf.matmul(a, b)
I = [[1.0, 0.0], [0.0, 1.0]]
with tf.device('/TPU:6'):
  d = tf.matmul(c, I)
print('c device:', c.device)
print(c)
print('d device:', d.device)
print(d)

Listing 5-1Run Computations on TPU Devices

```

将矩阵 a 乘以矩阵 b，并将结果放入 TPU 设备 7 范围内的矩阵 c 中。接下来，将矩阵 c 乘以单位矩阵，并将结果放入 TPU 设备 6 范围内的矩阵 d 中。

## 在所有 TPU 内核中运行计算

要复制一个计算，以便它可以在所有 TPU 内核中运行，请将其传递给*策略。运行* API:

```
@tf.function
def matmul_fn(x, y):
  z = tf.matmul(x, y)
  return z

z = tpu_strategy.run(matmul_fn, args=(a, b))
print(z)

```

创建一个将两个矩阵相乘的函数。如果启用了急切行为，将函数设为 *tf.function* 或在 tf.function 内调用 *strategy.run* ，急切行为在 Colab 中自动启用！

使用我们创建的*策略调用函数，运行* API，确保所有 TPU 内核获得相同的输入(a，b ), mat mul 独立应用于每个内核。输出是来自所有核心复制品的值。

## 急切的执行

必须启用快速执行，云 TPU 才能运行。*急切执行*(或急切行为)是一个命令式的编程环境，因为它直接评估操作而不构建图形。所以 TensorFlow 操作返回具体的值，而不是构建一个计算图供以后运行。它还提供了一个直观的界面，允许您自然地构建代码并使用 Python 数据结构。其他好处包括在小模型和小数据上的快速迭代和更容易的调试，因为您可以直接调用 ops 来检查运行的模型和测试更改。

Note

TensorFlow 2.x 中默认启用急切执行。

## 实验

我们包括四个云 TPU 实验。实验从简单的数据集开始，然后发展到更复杂的数据集。使用多个数据集的重要性在于获得经验和信心。每个数据集都不一样，所以实验也有点不同。学习过程大体相同，但每个数据集都有不同的特征。我们呈现多重体验的主要原因是为了给你更多的练习。通过与研讨会参与者的合作，我们发现他们通过使用多个数据集学习得更快，并且他们也更喜欢学习体验！

## 数字实验

*数字*数据集嵌入在 *sklearn.datasets* 包中。它由 1，797 个 8 × 8 的手写数字图像组成，从 0 到 9。 *Scikit-learn* (也称为 sklearn)是 Python 中的一个 ML 库。它包含一些小型标准数据集(如数字)，不需要从外部网站下载任何文件。

导入必需的库:

```
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

```

加载数据集:

```
digits = load_digits()

```

拿钥匙:

```
digits.keys()

```

关键字包括“数据”、“目标”、“目标名称”、“图像”和“描述”关键点*数据*包含作为形状(1797，64)的展平数据矩阵的图像。关键点*目标*包含作为形状向量的标签(1797，)。关键字 *target_names* 是目标类的名称列表。关键点*图像*包含形状矩阵形式的图像(1797，8，8)。关键字 *DESCR* 包含数据集的完整描述。因此 digits 包含 1797 个 8 × 8 像素的图像和 1797 个相应的标量标签。

这个数据集有键的原因是因为它是一个 scikit-learn (sklearn)数据集。它具有各种分类、回归和聚类算法，包括支持向量机、随机森林、梯度推进、k-means 和 DBSCAN。它旨在与 Python 数字和科学库 NumPy 和 SciPy 进行互操作。它还包含像数字这样的练习数据集。

显示图像:

```
images = digits.images
image = images[0]
fig = plt.imshow(image, cmap='binary')
fig = plt.axis('off')

```

### 预处理数据

由于 digits 是一个 scikit-learn 练习集，我们可以非常容易地对它进行预处理。 *train_test_split* 方法自动将数据分成训练集和测试集。您可以通过 *test_size* 参数调整分割比。

创建一个函数来预处理数据，如清单 [5-2](#PC12) 所示。

```
def load_data(digits, splits, random, scale):
  X = digits.images
  y = digits.target
  x_train, x_test, y_train, y_test = train_test_split(
    X, y, test_size=splits, random_state=random)
  x_train, x_test = x_train / scale, x_test / scale
  return (x_train, y_train), (x_test, y_test)

Listing 5-2Function to Preprocess Data

```

预处理数据:

```
splits, seed, scale = 0.33, 0, 255.0

(x_train, y_train), (x_test, y_test) = load_data(
    digits, splits, seed, scale)

```

数据分割是 67%的训练和 33%的测试。当然，您可以通过将 0.33 更改为另一个值来将其调整为不同的分割。

获取目标名称和类别数量:

```
target_names = digits.target_names
num_classes = len(target_names)
num_classes, target_names

```

我们利用内置的 sklearn 方法 *target_names* 来获取目标(类标签)名称。

### 构建输入管道

准备张量流消耗的数据:

```
train_dataset = tf.data.Dataset.from_tensor_slices(
    (x_train, y_train))
test_dataset = tf.data.Dataset.from_tensor_slices(
    (x_test, y_test))

```

检查训练张量:

```
for img, lbl in train_dataset.take(1):
  print (img.shape, lbl)

```

设置参数并构建管道:

```
BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 100

train_ds = train_dataset\
 .shuffle(SHUFFLE_BUFFER_SIZE)\
 .batch(BATCH_SIZE)
test_ds = test_dataset.batch(BATCH_SIZE)

```

### TPU 范围内的模型数据

保留模型的输入形状:

```
for item in train_ds.take(1):
  s = item[0].shape
in_shape = s[1:]
in_shape

```

我们保存输入形状，以便稍后在模型中使用。

导入库:

```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

```

创建一个函数来构建模型，如清单 [5-3](#PC20) 所示。

```
def get_model():
  return tf.keras.Sequential([
    Flatten(input_shape=in_shape),
    Dense(256, input_shape=in_shape, activation='relu'),
    Dense(num_classes, activation='softmax')])

Listing 5-3Function to Create the Model

```

在清单 [5-4](#PC21) 所示的 TPU 分销战略范围内创建并编译模型。

```
with tpu_strategy.scope():
  model = get_model()
  model.compile(
      optimizer='adam',
      loss='sparse_categorical_crossentropy',
      metrics=['accuracy'])

Listing 5-4Create and Compile the Model Within TPU Scope

```

要在云 TPU 中训练一个模型，我们必须在本章前面创建的 TPU 策略范围内创建和编译它。

检查模型:

```
model.summary()

```

训练模型:

```
epochs = 60
history = model.fit(train_ds, epochs=epochs,
                    validation_data=(test_ds))

```

我们可以根据自己的意愿，用尽可能多或尽可能少的纪元来训练一个模型。然而，大型数据集比小型数据集消耗内存快得多。由于数字很小，图像很简单，我们可以在更多的纪元上训练，而不用担心内存消耗。请记住，我们通过大量的实验达到了 60 个纪元。这就是我们鼓励你多练习的原因！

## MNIST 实验

虽然 MNIST 正在被时尚 MNIST 取代，用于深度学习实践，但它仍然比 digits 数据集大得多。

将数据集作为 TFDS 对象加载:

```
import tensorflow_datasets as tfds

train, info = tfds.load(name='mnist', split='train',
                        as_supervised=True, try_gcs=True,
                        with_info=True, shuffle_files=True)
test = tfds.load(name='mnist', split='test',
                        as_supervised=True, try_gcs=True)

```

我们让您来探索元数据，因为我们已经在前面的章节中详细描述了这个数据集对象。

### 构建输入管道

创建一个函数来缩放数据:

```
def scale(image, label):
  image = tf.cast(image, tf.float32) / 255.0
  return image, label

```

构建管道:

```
BATCH_SIZE = 200
SHUFFLE_SIZE = 10000

train_dataset = train.map(scale)\
  .shuffle(SHUFFLE_SIZE).repeat()\
  .batch(BATCH_SIZE).prefetch(1)
test_dataset = test.map(scale)\
  .batch(BATCH_SIZE).prefetch(1)

```

只打乱并重复训练数据集。 *repeat()* 方法重复数据集计数 *n* 次。如果不包含数字，数据集计数是无限的。用于训练的无限数据集的优点是避免每个时期中潜在的最后部分批次，因此用户不需要考虑基于实际批次大小来缩放梯度。由于模型只从训练集学习，所以我们不洗牌，不重复测试集。

### TPU 范围内的模型数据

创建变量以保存图像数量、步长和验证步骤:

```
num_train_img = info.splits['train'].num_examples
num_test_img = info.splits['test'].num_examples
steps_per_epoch = num_train_img // BATCH_SIZE
validation_steps = num_test_img // BATCH_SIZE

```

重复数据时，我们必须包括每个时期的步骤和验证步骤参数的值。

保留模型的输入形状:

```
for item in train_dataset.take(1):
  s = item[0].shape
mnist_shape = s[1:]
mnist_shape

```

创建一个函数来构建模型，如清单 [5-5](#PC29) 所示。

```
def create_model():
  return Sequential([
    Flatten(input_shape=mnist_shape),
    Dense(512, activation='relu'),
    Dense(mnist_classes, activation='softmax')
    ])

Listing 5-5Function to Build the Model

```

清除模型并生成种子:

```
import numpy as np

tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

导入损失函数的库:

```
from tensorflow.keras.losses import SparseCategoricalCrossentropy

```

我们引入损失函数来减少模型中包含的字符。如果我们不这样做，我们就必须告诉模型损失函数在哪里。所以我们必须包含完整的库名。我们将此代码作为我们在之前实验中所做工作的替代。

获取模型的类别数:

```
mnist_classes = info.features['label'].num_classes
mnist_classes

```

在 TPU 策略范围内，创建并编译，如清单 [5-6](#PC33) 所示。

```
with tpu_strategy.scope():
  model = create_model()
  model.compile(
      optimizer='adam',
      steps_per_execution = 50,
      loss=SparseCategoricalCrossentropy(from_logits=True),
      metrics=['sparse_categorical_accuracy'])

Listing 5-6Create and Compile the Model Within TPU Scope

```

在 TPUStrategy 范围内创建模型意味着我们在 TPU 系统上训练模型。这个想法是利用 TPU 来加速学习。在工业中，我们必须与系统工程师合作，利用多个 TPU 和其他内存资源之间的并行处理。尝试使用 *steps_per_execution* 。介于 2 和*之间的任何值都可以提高性能。*

训练模型:

```
history = model.fit(
    train_dataset, epochs=5,
    steps_per_epoch=steps_per_epoch,
    validation_data=test_dataset,
    validation_steps=validation_steps)

```

## 时尚 MNIST 实验

由于时尚-MNIST 是目前 MNIST 的替代者，我们用它来演示 TPU 学习。

*时尚-MNIST* 是 Zalando 文章图像的数据集，由 60，000 个样本的训练集和 10，000 个样本的测试集组成。该数据集旨在作为原始 MNIST 数据集的直接替代，用于机器学习算法的基准测试。

将时尚-MNIST 作为 tf.keras 数据集加载:

```
fashion_train, fashion_test = tf.keras.datasets\
  .fashion_mnist.load_data()

```

### 将数据集转换为影像和标注集

创建图像和标签集:

```
train_img, train_lbl = fashion_train
test_img, test_lbl = fashion_test

```

将灰度颜色维度添加到图像张量:

```
fashion_train_img = np.expand_dims(train_img, -1)
fashion_test_img = np.expand_dims(test_img, -1)

```

检查图像张量:

```
fashion_train_img.shape, fashion_test_img.shape

```

创建一个函数将张量转换为浮点数:

```
def float_it(x):
  return x.astype(np.float32)

```

通过将张量转换为 NumPy 张量，训练性能应该会提高。

调用函数:

```
fash_train_img, fash_train_lbl = float_it(
    fashion_train_img), float_it(train_lbl)
fash_test_img, fash_test_lbl = float_it(
    fashion_test_img), float_it(test_lbl)

```

### TPU 范围内的模型数据

导入库:

```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization,\
  Conv2D, MaxPooling2D, Dropout

```

创建一个函数来构建模型，如清单 [5-7](#PC42) 所示。

```
def create_model():
  return Sequential([
    BatchNormalization(input_shape=(28,28,1)),
    Conv2D(64, (5, 5), padding='same', activation='elu'),
    MaxPooling2D(pool_size=(2, 2), strides=(2,2)),
    Dropout(0.25),
    BatchNormalization(),
    Conv2D(128, (5, 5), padding='same', activation='elu'),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),
    BatchNormalization(),
    Conv2D(256, (5, 5), padding='same', activation='elu'),
    MaxPooling2D(pool_size=(2, 2), strides=(2,2)),
    Dropout(0.25),
    Flatten(),
    Dense(256, activation='elu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
    ])

Listing 5-7Function to Build the Model

```

**批量标准化**是一种用于训练深度神经网络的技术，它将每个小批量的输入标准化到一个层。该技术稳定了学习过程，并极大地减少了训练深度网络所需的训练次数。

清除和播种:

```
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

在 TPU 战略范围内，创建并编译如清单 [5-8](#PC44) 所示的模型。

```
with tpu_strategy.scope():
  model = create_model()
  model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'])

Listing 5-8Create and Compile the Model Within TPU Scope

```

训练模型:

```
history = model.fit(fash_train_img, fash_train_lbl,
    epochs=17, steps_per_epoch=60,
    validation_data=(fash_test_img, fash_test_lbl),
    validation_freq=17)

```

我们添加 *validation_freq* 参数作为另一个选项。只有提供了验证数据，它才是相关的。它指定在执行新的验证运行之前要运行多少个训练时期。所以我们每 17 个时期进行一次验证。我们把它包括进来只是为了显示实验环境中的另一个参数。

评估模型:

```
loss, acc = model.evaluate(fash_test_img, fash_test_lbl)
print ('loss:', loss)
print ('accuracy:', acc)

```

### 保存训练好的模型

我们可以保留来自训练模型的权重:

```
model.save_weights('./fashion_mnist.h5', overwrite=True)

```

### 做出推论

我们在这个数据集上进行推论(预测)，因为它是目前深度学习竞赛的首选数据集。这种比赛非常有助于学生和/或从业者获得经验，并与其他志同道合的人建立联系。前面的数据集很适合实践，但不具有挑战性。就是太容易得到高精度(或者低损耗)。

现在我们已经完成了训练，让我们看看模型对时尚类别的预测有多好！

获取标签名称:

```
class_labels = ['t_shirt', 'trouser', 'pullover', 'dress',
                'coat', 'sandal', 'shirt', 'sneaker',
                'bag', 'ankle_boots']

```

根据已训练模型中保存的权重创建新模型:

```
new_model = create_model()

new_model.load_weights('./fashion_mnist.h5')

```

从测试集中获取 40 个预测数组用于绘图:

```
preds = new_model.predict(fash_test_img)[:40]

```

将预测数组转换为标量预测值:

```
pred_40 = [tf.argmax(i).numpy() for i in preds]

```

因为预测数组不识别实际的预测，所以我们使用 tf.argmax。

获取用于显示的图像和标签，如清单 [5-9](#PC52) 所示。

```
images, labels = [], []
for i in range(40):
  img = tf.squeeze(fash_test_img[i])
  images.append(img)
  labels.append(int(fash_test_lbl[i]))

Listing 5-9Images and Labels for Display

```

创建一个函数来显示预测，如清单 [5-10](#PC53) 所示。

```
def display_test(feature, target, num_images,
                 n_rows, n_cols, cl, p):
  for i in range(num_images):
    plt.subplot(n_rows, 2*n_cols, 2*i+1)
    plt.imshow(feature[i], cmap='nipy_spectral')
    pred = cl[p[i]]
    actual = cl[int(target[i])]
    title_obj = plt.title(actual + ' (' +\
                          pred + ') ')
    if pred == actual:
      title_obj
    else:
      plt.getp(title_obj, 'text')
      plt.setp(title_obj, color='r')
    plt.tight_layout()
    plt.axis('off')

Listing 5-10Function to Display Prediction Performance

```

调用函数:

```
num_rows, num_cols = 10, 4
num_images = num_rows*num_cols
plt.figure(figsize=(20, 20))

display_test(images, labels, num_images, num_rows,
             num_cols, class_labels, pred_40)

```

红色图像表示不正确的预测。黑色的是正确的预测。

## 花卉实验

花卉数据集并不是很大，但图像比 MNIST 时装展上的要复杂得多。花朵图像比本章中的其他数据集包含更多的像素。我们认为处理像 flowers 这样的复杂数据集是一种很好的实践。

TPU 非常快。因此，流入模型的训练数据必须与模型的训练速度保持同步，以充分利用 TPU 的能力。使用 TPU 的首选方法是将数据存储为基于 protobuf 的 TFRecord 格式。 *TFRecord 格式*将二进制字符串序列中的数据存储为 TFRecord。二进制字符串存储和数据传输效率高。

flowers 数据集作为 TFRecords 存储在 Google 云存储(GCS)中。为了充分应用 TPU 提供的并行性并避免数据传输的瓶颈，我们将数据读取为 TFRecord 文件，每个文件大约有 230 个图像。230 这个数字用于将 flowers 数据平均分配到几个桶中。我们通过实验发现了这个数字。我们使用 tf.data.experimental.AUTOTUNE 来优化输入加载的不同部分。我们在这个实验中使用自动调优，因为数据集包含复杂的图像。这种优化技术在早期的实验中是不必要的。

有关 Colab 中 TPUs 的精彩教程，请阅读

[T2`https://colab.research.google.com/notebooks/tpu.ipynb`](https://colab.research.google.com/notebooks/tpu.ipynb)

有关 TPU 数据管道处理的精彩介绍，请阅读

[T2`https://codelabs.developers.google.com/codelabs/keras-flowers-data`](https://codelabs.developers.google.com/codelabs/keras-flowers-data)

### 将 Flowers 数据作为 TFRecord 文件读取

建立 GCS 文件名模式:

```
piece1 = 'gs://flowers-public/'
piece2 = 'tfrecords-jpeg-192x192-2/*.tfrec'
TFR_GCS_PATTERN = piece1 + piece2
tfr_filenames = tf.io.gfile.glob(TFR_GCS_PATTERN)

```

获取桶的数量:

```
num_images = len(tfr_filenames)
print ('Pattern matches {} image buckets.'.format(num_images))

```

图像包含在 16 个桶(或 TFRecord 文件)中。我们从早期的花卉实验中知道，在花卉数据集中有 3670 幅图像。前 15 个 TFRecord 文件每个包含 230 个图像，最后的 TFRecord 文件包含 220 个图像。

显示所有 TFRecord 文件(或桶):

```
filenames_tfrds = tf.data.Dataset.list_files(TFR_GCS_PATTERN)
for filename in filenames_tfrds.take(16):
  print (filename.numpy())

```

数据项(花图像)的数目被写入 TFRecord 文件的名称中。比如 TFRecord 文件 b ' GS://flowers-public/TF records-JPEG-192 x192-2/flowers 02-*230*。“tfrec”有 230 个数据项。

### 设置训练参数

设置图像大小调整、流水线和时期的参数:

```
IMAGE_SIZE = [192, 192]
AUTO = tf.data.experimental.AUTOTUNE
BATCH_SIZE = 64
SHUFFLE_SIZE = 100
EPOCHS = 9

```

设置数据拆分和标签的参数:

```
VALIDATION_SPLIT = 0.19
CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']

```

我们在这个实验中使用不同的分割，因为我们发现这个分割效果最好。

创建数据分割、验证步骤和每个时期的步骤，如清单 [5-11](#PC60) 所示。

```
split = int(len(tfr_filenames) * VALIDATION_SPLIT)
training_filenames = tfr_filenames[split:]
validation_filenames = tfr_filenames[:split]
print ('Splitting dataset into {} training files and {} '\
        'validation files'\
       .format(len(tfr_filenames), len(training_filenames),
               len(validation_filenames)), end = ' ')
print ('with a batch size of {}.'.format(BATCH_SIZE))

validation_steps = int(3670 // len(tfr_filenames) *\
                       len(validation_filenames)) // BATCH_SIZE
steps_per_epoch = int(3670 // len(tfr_filenames) *\
                      len(training_filenames)) // BATCH_SIZE
print ('There are {} batches per training epoch and {} '\
       'batches per validation run.'\
       .format(BATCH_SIZE, steps_per_epoch, validation_steps))

Listing 5-11Create Data Splits, Validation Steps, and Steps per Epoch

```

我们首先将数据分为训练和测试两部分。然后我们显示分割的结果。我们为模型创建验证步骤和每个时期的步骤。我们通过显示这些信息来结束。

### 创建函数来加载和处理 TFRecord 文件

创建一个函数来解析 TFRecord 文件，如清单 [5-12](#PC61) 所示。

```
def read_tfrecord(example):
  features = {
      'image': tf.io.FixedLenFeature([], tf.string),
      'class': tf.io.FixedLenFeature([], tf.int64)
  }
  example = tf.io.parse_single_example(example, features)
  image = tf.image.decode_jpeg(example['image'], channels=3)
  image = tf.cast(image, tf.float32) / 255.0
  image = tf.reshape(image, [*IMAGE_SIZE, 3])
  class_label = example['class']
  return image, class_label

Listing 5-12Function to Parse a TFRecord File

```

该函数接受来自 TFRecord 文件的示例。字典保存 TFRecords 通用的数据类型。tf.string API 将图像转换为字节字符串(字节列表)。tf.int64 API 将类标签转换为 64 位整数标量值。该示例被解析为(图像，标签)元组。图像元素(JPEG 编码的图像)被解码成 uint8 图像张量。图像张量被缩放到[0，1]范围，以便更快地训练。然后，它被重塑为模型消费的标准尺寸。class label 元素被转换为标量。

创建一个函数以 tf.data.Dataset 的形式加载 TFRecord 文件，如清单 [5-13](#PC62) 所示。

```
def load_dataset(filenames):
  option_no_order = tf.data.Options()
  option_no_order.experimental_deterministic = False
  dataset = tf.data.TFRecordDataset(
      filenames, num_parallel_reads=AUTO)
  dataset = dataset.with_options(option_no_order)
  dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)
  return dataset

Listing 5-13Function to Load TFRecord Files as a tf.data.Dataset

```

该函数接受 TFRecord 文件。为了获得最佳性能，包含了一次读取多个 TFRecord 文件的代码。options 设置允许顺序改变优化。这样， *n* 个文件被并行读取，为了提高读取速度，数据顺序被忽略。

创建一个函数来扩充训练数据:

```
def data_augment(image, label):
  modified = tf.image.random_flip_left_right(image)
  modified = tf.image.random_saturation(modified, 0, 2)
  return modified, label

```

创建一个函数，从 TFRecord 文件构建一个输入管道，如清单 [5-14](#PC64) 所示。

```
def get_batched_dataset(filenames, train=False):
  dataset = load_dataset(filenames)
  dataset = dataset.cache()
  if train:
    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)
    dataset = dataset.repeat()
    dataset = dataset.shuffle(SHUFFLE_SIZE)
  dataset = dataset.batch(BATCH_SIZE)
  dataset = dataset.prefetch(AUTO)
  return dataset

Listing 5-14Build an Input Pipeline from TFRecord Files

```

该函数接受 TFRecord 文件并调用 *load_dataset* 函数。该函数继续通过缓存、重复、混排、批处理和预取数据集来构建输入管道。重复和洗牌仅映射到训练数据。我们遵循最佳实践，只重复和打乱训练数据。

### 创建训练集和测试集

实例化数据集:

```
training_dataset = get_batched_dataset(
    training_filenames, train=True)
validation_dataset = get_batched_dataset(
    validation_filenames, train=False)
training_dataset, validation_dataset

```

我们为训练集和测试集构建输入管道。

显示图像:

```
for img, lbl in training_dataset.take(1):
  plt.axis('off')
  plt.title(CLASSES[lbl[0].numpy()])
  fig = plt.imshow(img[0])
  tfr_flower_shape = img.shape[1:]

```

我们显示一个图像并为模型保留图像形状。

### 模型数据

清除和播种:

```
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

创建一个函数来构建模型，如清单 [5-15](#PC68) 所示。

```
def create_model():
  return Sequential([
    Conv2D(32, (3, 3), activation = 'relu'),
    MaxPooling2D(2, 2),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(2),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(num_classes, activation='sigmoid')
])

Listing 5-15Function to Create the Model

```

在 TPU 战略范围内，创建并编译如清单 [5-16](#PC69) 所示的模型。

```
with tpu_strategy.scope():
  flower_model = create_model()
  flower_model.compile(
      optimizer='adam',
      loss=tf.losses.SparseCategoricalCrossentropy(),
      metrics=['accuracy'])

Listing 5-16Create and Compile the Model Within TPU Scope

```

训练模型:

```
history = flower_model.fit(training_dataset, epochs=EPOCHS,
                    verbose=1, steps_per_epoch=steps_per_epoch,
                    validation_steps=validation_steps,
                    validation_data=validation_dataset)

```

### 做出推论

让我们看看模型对花卉种类的预测有多好！

从验证(测试)集中抓取 40 个预测:

```
preds = flower_model.predict(validation_dataset)[:40]

```

将预测数组转换为标量预测值:

```
pred_40 = [tf.argmax(i).numpy() for i in preds]

```

创建用于显示的图像和标签:

```
images, labels = [], []
for img, lbl in validation_dataset.take(1):
  for i in range(40):
    actual_img = tf.squeeze(img[i])
    images.append(actual_img)
    labels.append(lbl[i].numpy())

```

创建一个函数来显示预测，如清单 [5-17](#PC74) 所示。

```
def display_test(feature, target, num_images,
                 n_rows, n_cols, cl, p):
  for i in range(num_images):
    plt.subplot(n_rows, 2*n_cols, 2*i+1)
    plt.imshow(feature[i])
    pred = cl[p[i]]
    actual = cl[int(target[i])]
    title_obj = plt.title(actual + ' (' +\
                          pred + ') ')
    if pred == actual:
      title_obj
    else:
      plt.getp(title_obj, 'text')
      plt.setp(title_obj, color='r')
    plt.tight_layout()
    plt.axis('off')

Listing 5-17Function to Display Predictions

```

调用函数:

```
num_rows, num_cols = 10, 4
num_images = num_rows*num_cols
plt.figure(figsize=(20, 20))
display_test(images, labels, num_images, num_rows,
             num_cols, CLASSES, pred_40)

```