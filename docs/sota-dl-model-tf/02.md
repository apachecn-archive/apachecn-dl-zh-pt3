# 2.通过数据扩充增加数据集的多样性

我们指导您创建扩充数据实验，通过应用随机(但真实)转换来增加训练集的多样性。数据增强对于小数据集非常有用，因为深度学习模型渴望大量数据才能表现良好。

章节的笔记本位于以下 URL:

[T2`https://github.com/paperd/deep-learning-models`](https://github.com/paperd/deep-learning-models)

## 日期增加

更多的数据通常会提高模型性能。那么，如果我们有少量的图像训练数据，但无法收集更多的数据，我们该怎么办呢？一种流行的方法是数据扩充。

**数据扩充**是在不手动收集任何新数据的情况下，增加现有训练集的规模和多样性的过程。该过程通过使用产生看起来可信的图像的随机变换来扩充现有示例，从而从现有示例中生成额外的训练数据。有了增强的训练数据，学习模型将暴露于数据的更多方面，这有助于它更好地概括。通常，当训练数据复杂并且包含很少的示例时，需要数据扩充。

数据扩充是通过对现有数据执行一系列随机预处理转换来实现的，例如水平和垂直翻转、倾斜、裁剪、剪切、放大和缩小以及旋转。总体而言，增强数据能够模拟各种细微不同的数据点，而不仅仅是复制相同的数据。增强图像的细微差异应该(有希望)足以帮助训练更健壮的模型。

数据扩充也可以减轻过度拟合。过度拟合通常发生在训练样本数量较少的时候。通过从现有的例子中生成额外的训练数据，可以减轻过度拟合。

当模型学习训练数据中的细节和噪声达到对新数据的模型性能产生负面影响的程度时，就会发生过度拟合。因此，训练数据中的噪声或随机波动被模型拾取并学习为概念。小数据集不包含足够的多样性(或随机性)来减轻学习噪音或随机波动。

深度学习的目标是调整学习模型的参数，以便它可以有效地将特定输入(例如，图像)映射到某些输出(例如，标签)。本质上，我们试图找到模型损失最小的最佳点，这发生在参数以正确的方式调整时。数据增强是一种有效的调优机制，可以帮助实现深度学习的目标，特别是对于小数据集！

有关数据扩充的精彩介绍，请阅读

[T2`www.tensorflow.org/tutorials/images/data_augmentation`](http://www.tensorflow.org/tutorials/images/data_augmentation)

## 导入 TensorFlow 库

导入库并将其别名为 **tf** :

```
import tensorflow as tf

```

将 TensorFlow 库别名为 tf 是常见的做法。

## GPU 硬件加速器

从第 [1](01.html) 章记住，你可以通过使用 Google Colab 云服务提供的 GPU 来大大加快处理速度。为了省去你翻回到第 [1](01.html) 章的麻烦，我们重复指令来启用 GPU:

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

验证 GPU 是否处于活动状态:

```
tf.__version__, tf.test.gpu_device_name()

```

如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

Note

如果出现错误**名称‘TF’未定义**，请重新执行代码以导入 TensorFlow 库！

## 增加了 Keras API

在前一章中，我们在对 flowers 数据集建模时经历了过度拟合。该章中的问题是，相对于训练精度，验证精度较低。让我们看看是否可以使用 Keras API 进行数据扩充来缓解这个问题。

### 检索数据

首先，您需要再次获取数据。通过执行以下代码来实现，该代码检索与第 [1](01.html) 章中使用的相同的 flowers 数据:

```
import pathlib

url1 = 'https://storage.googleapis.com/download.tensorflow.org/'
url2 = 'example_images/flower_photos.tgz'
dataset_url = url1 + url2

data_dir = tf.keras.utils.get_file(origin=dataset_url,
                                   fname='flower_photos',
                                   untar=True)
data_dir = pathlib.Path(data_dir)

```

### 分割数据

*TF . keras . preprocessing . image _ dataset _ from _ directory*实用程序从目录中读取数据。该实用程序还能够对数据进行分割、播种、调整大小和批处理。

设置参数，作为起点，尝试以下三个值:

```
BATCH_SIZE = 32
img_height = 180
img_width = 180

```

批量大小设置为 32。我们将批量大小设置为 32，但是可以随意试验不同的大小。由于图像有不同的形状，我们必须调整它们的大小以供模型使用。我们选择了 180 × 180，但是可以随意试验不同的图像尺寸。但是一定要保持高度和宽度一致。

Note

设置批处理和图像大小不是一门科学。这项任务需要实验。根据我们的研究，批量大小的良好起点是 32 或 64。任何小于 32 的值都会降低学习速度。任何大于 64 的值在计算上都很昂贵。这个想法是将一批数据完全存储在内存中。由于计算机内存的存储容量是 2 的幂，所以建议将批量大小保持为 2 的幂。图像尺寸越小，学习速度越快，但保留的原始图像越少。因此模型性能受到负面影响。较大的图像尺寸会保留更多的原始图像。因此，以计算资源为代价，模型性能受到积极影响。

#### 将图像从磁盘加载到训练集和测试集中

现在创建训练和测试数据集。通常的做法是将数据分成这两种集合类型。从训练集开始，按如下方式创建它:

```
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.19,
  subset='training',
  seed=0,
  image_size=(img_height, img_width),
  batch_size=BATCH_SIZE)

```

设置 *validation_split=0.19* 和*subset =‘training’*，将 81%的数据放入训练集。将*图像尺寸*设置为我们设定的图像尺寸。还要设置种子和批次值。

创建测试集:

```
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
  data_dir,
  validation_split=0.19,
  subset='validation',
  seed=0,
  image_size=(img_height, img_width),
  batch_size=BATCH_SIZE)

```

设置 *validation_split=0.19* 和*subset =‘validation’*，将 19%的数据放入测试集。将*图像尺寸*设置为我们设定的图像尺寸。还要设置种子和批次值。

### 检查训练集

以一个批次为例，显示图像形状和标签:

```
for images, labels in train_ds.take(1):
  print ('image shape:', images.shape)
  print ('labels:', labels.numpy())
  print ('number of labels in a batch:', len(labels))

```

不出所料，我们首批有 32 张 180 × 180 × 3 的图片。 *3* 值表示图像是 RGB 颜色。我们也有 32 个相应的标签。我们有 32 个图像和 32 个相应的标签，因为我们将批量大小设置为 32。

### 获取类别数

使用实用程序中的 *class_names* 方法来显示类标签的数量:

```
class_names = train_ds.class_names
num = len(class_names)
num

```

### 创建缩放函数

创建一个函数来缩放特征图像:

```
def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255.0
  return image, label

```

### 构建输入管道

缩放训练和测试数据。学习模型在较小的图像上训练得更快。两倍大的输入图像需要一个网络从四倍多的像素中学习——而这些额外的训练时间加在一起。只混洗训练数据。缓存和预取训练和测试数据以提高模型性能:

```
SHUFFLE_SIZE = 100

train_fds = train_ds.map(scale).shuffle(SHUFFLE_SIZE).\
            cache().prefetch(1)
test_fds = test_ds.map(scale).cache().prefetch(1)

```

Note

由于训练和测试数据已经被实用程序批处理，因此在构建输入管道时，*不要*批处理！

### 使用预处理层的数据扩充

通过实验性 Keras 预处理层应用数据扩充。Keras 预处理层 API 允许开发者构建 Keras 原生输入处理管道。因此 TensorFlow 模型可以接受原始图像或原始结构化数据作为输入，自行处理特征归一化和特征值索引。简单地说，Keras API 使得原始数据的输入、预处理和建模变得更加容易。

我们从导入必要的库开始，并创建一个简单的增强模型。我们可以像其他层一样在模型中包含预处理层。

让我们通过随机水平翻转、旋转、缩放和平移来放大图像，如清单 [2-1](#PC11) 所示。

```
from tensorflow.keras.layers.experimental.preprocessing\
  import RandomFlip
from tensorflow.keras.layers.experimental.preprocessing\
  import RandomRotation
from tensorflow.keras.layers.experimental.preprocessing\
  import RandomZoom
from tensorflow.keras.layers.experimental.preprocessing\
  import RandomTranslation

data_augmentation = tf.keras.Sequential(
  [
   RandomFlip('horizontal'),
   RandomRotation(0.1),
   RandomZoom(0.1),
   RandomTranslation(height_factor=0.2, width_factor=0.2)
  ]
)

Listing 2-1Data Augmentation with Several Preprocessing Layers

```

Note

Keras 预处理层是实验性的，这意味着操作在将来可能会改变。

我们将图像从左向右翻转，随机旋转 0.1 倍(或 10%)，随机缩放 0.1 倍(或 10%)，随机平移 0.2 倍(或 20%)的高度和宽度。通过反复试验来选择层和参数值设置。因此，请随意尝试其他增强功能。

有关 Keras 实验预处理层的更多信息，请阅读

[T2`www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing)

### 显示增强图像

下面是对同一个图像多次应用数据扩充时发生的情况，如清单 [2-2](#PC12) 所示。

```
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, _ in train_fds.take(1):
  for i in range(9):
    augmented_images = data_augmentation(images)
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(augmented_images[0])
    plt.axis('off')

Listing 2-2Visualize

an Augmented Image

```

我们在索引为 0 的第一幅图像上显示增强。更改索引以查看被增强的其他图像。但是将索引值保持在 0 到 31 之间，以考虑到批量大小为 32。

### 创建模型

清除以前的模型并为可复制性生成种子:

```
import numpy as np
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

导入数据建模所需的库:

```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.layers import Conv2D, MaxPooling2D

```

构建一个多层 CNN，如清单 [2-3](#PC15) 所示。

```
model = tf.keras.Sequential([
  data_augmentation,
  Conv2D(32, 3, activation='relu'),
  MaxPooling2D(),
  Conv2D(32, 3, activation='relu'),
  MaxPooling2D(),
  Conv2D(32, 3, activation='relu'),
  MaxPooling2D(),
  Flatten(),
  Dense(128, activation='relu'),
  Dropout(0.5),
  Dense(num)
])

Listing 2-3Multilayered CNN

```

请注意，第一层是我们构建的数据增强层！

### 编译和训练模型

使用 SparseCategoricalCrossentropy 编译(from_logits=True):

```
model.compile(
  optimizer='adam',
  loss=SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

```

我们在损失函数中设置 *from_logits=True* ，因为我们不使用 softmax 激活来将神经元输入归一化到模型的输出层。Softmax 通过获取每个输出的指数，并通过这些指数的总和归一化每个数字，将对数转换为概率，因此整个输出向量加起来等于 1。所以所有的概率加起来应该是 1。**logit**是多类分类神经网络的最后一个线性层的数字输出。因为我们不使用 softmax，所以我们通知编译器输出 logits。

训练模型:

```
history = model.fit(
    train_fds,
    validation_data=test_fds,
    epochs=10)

```

与我们在前一章中所经历的没有增强的情况相比，过度拟合被减轻了。也就是说，测试精度与训练精度更加一致。

### 可视化性能

让我们用清单 [2-4](#PC18) 中所示的可视化来看看数据扩充的影响。

```
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(10)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

Listing 2-4Visualize Training Performance with Augmentation

```

请注意，对于我们训练了十个时期的模型，过度拟合在很大程度上得到了缓解。所以数据扩充提高了性能。

## 对图像应用增强

在上一节中，我们将 Keras 预处理层添加到神经网络中进行扩充。另一种方法是直接在图像上应用数据扩充变换，然后将它们馈送到神经网络*，而不需要* Keras 预处理层。

在本节中，我们首先演示可以在图像上执行的各种变换。我们从上一节中创建的预处理张量中获取一批数据，并演示每种转换技术。在演示了如何在图像上应用各种变换之后，我们继续在图像上使用这些技术中的一些，并用这些增强的图像训练模型。

让我们首先从我们刚刚创建的训练集中抓取一个示例图像:

```
for batch_images, _ in train_fds.take(1):
  print ('image shape:', batch_images.shape)

```

由于我们已经有了来自前面部分的预处理张量，我们从流水线训练集中获取第一批图像。现在，我们有一批包含在 *batch_images* 中的 32 张图片可以玩。我们首先向您展示如何处理批处理中的图像。

### 设置索引变量

由于批量大小为 32，请将索引值设置在 0 到 31 之间:

```
indx = 0
indx

```

我们将索引设置为 0，以获取该批图像中的第一张图像。请随意更改索引值。但是请记住，索引值必须在 0 到 31 之间！

### 设置图像

设置索引以从批处理中抓取第一个图像。更改索引值(在 0 和 31 之间)以显示不同的图像。

我们的形象:

```
our_image = batch_images[indx]

```

### 展示一个例子

想象第一幅图像:

```
plt.imshow(our_image)
plt.axis('off')
plt.grid(b=None)

```

### 创建显示图像的函数

创建一个可视化函数来显示原始图像和修改后的图像，如清单 [2-5](#PC23) 所示。

```
def show(original_img, trans_img):
  f = plt.figure(figsize=(6, 6))
  f.add_subplot(1,2,1)
  plt.imshow(original_img)
  plt.axis('off')
  f.add_subplot(1,2,2)
  plt.imshow(trans_img)
  plt.axis('off')
  plt.show(block=True)

Listing 2-5Function to Visualize Original Image and Modified Image

```

创建一个可视化函数来显示图像的几种变换，如清单 [2-6](#PC24) 所示。

```
def show_images(img, indx, trans, p1=None, p2=None, b=False):
  plt.figure(figsize=(10, 10))
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    if not b:
      new_img = trans(img[indx])
    elif p2==None:
      new_img = trans(img[indx], p1)
      new_img = np.clip(new_img, 0, 1)
    else:
      new_img = trans(img[indx], p1, p2)
      new_img = np.clip(new_img, 0, 1)
    plt.imshow(new_img)
    plt.axis('off')

Listing 2-6Visualize Transformations of an Image

```

### 裁剪图像

要裁剪图像，请移除或调整其外部边缘。

裁剪图像:

```
new_image = tf.image.random_crop(our_image, [120, 120, 3])
new_image.shape

```

该操作以统一选择的偏移量从图像中切出*形状尺寸*部分。在这种情况下，新的图像大小为 120 × 120。

显示原始图像和修改后的图像:

```
show(our_image, new_image)

```

因为变换是随机的，所以显示多个图像:

```
show_images(batch_images, indx, tf.image.random_crop,
            [120, 120, 3], b=True)

```

我们用这个可视化展示随机的农作物。请注意，每张图像的裁剪都有所不同，因为 API 会随机生成裁剪。

集中裁剪图像:

```
new_image = tf.image.central_crop(our_image, 0.5)
print (new_image.shape)
show(our_image, new_image)

```

该操作将图像大小缩小一半，并集中裁剪。也就是说，它消除了背景噪声，并确保剩余的图像像素居中。

裁剪到边界框:

```
new_image = tf.image.crop_to_bounding_box(
    our_image, 10, 10, 120, 120)
print (new_image.shape)
show(our_image, new_image)

```

该操作将图像裁剪到指定的边界框。

**边界框**是像素图像中由两个经度和两个纬度定义的区域。每个纬度都是–90.0 到 90.0 之间的十进制值。每个经度是一个介于–180.0 和 180.0 之间的十进制值。

### 从左到右随机翻转图像

由于变换是随机的，图像并不总是从左向右翻转:

```
show_images(batch_images, indx, tf.image.random_flip_left_right)

```

### 随机上下翻转图像

由于变换是随机的，图像并不总是上下翻转:

```
show_images(batch_images, indx, tf.image.random_flip_up_down)

```

### 上下翻转图像

图像总是上下翻转:

```
show(our_image, tf.image.flip_up_down(our_image))

```

### 将图像旋转 90 度

通过设置 *k=1* 将图像旋转 90 度:

```
show(our_image, tf.image.rot90(our_image, k=1))

```

*k* 参数是图像旋转 90 度的次数。

通过设置 *k=2* 将图像旋转 180 度:

```
show(our_image, tf.image.rot90(our_image, k=2))

```

通过设置 k=3 将图像旋转 270 °:

```
show(our_image, tf.image.rot90(our_image, k=3))

```

如果我们设置 k=4，我们又回到了开始的地方，因为它将图像旋转了 360 度！

### 调整伽玛

**伽马编码**用于利用人类感知光线和颜色的非线性方式对图像进行编码时优化比特的使用。在普通照明条件下(既不是漆黑的也不是刺眼的亮)，人类对亮度(或明度)的感知遵循近似的幂函数，对暗色调之间的相对差异比对亮色调之间的相对差异更敏感。简单地说，伽马编码可以被认为是图像的强度。

使用伽玛编码调整图像:

```
new_image = tf.image.adjust_gamma(
    our_image, gamma=0.75, gain=1.5)
new_image = np.clip(new_image, 0, 1)
show(our_image, new_image)

```

调整亮度的伽玛。对于小于 1 的 gamma，图像更亮。对于大于 1 的 gamma，图像更暗。强度由*增益*参数控制。尝试使用*伽玛*和*增益*参数，查看对转换图像的影响。

Note

将 gamma 值和增益值视为旋钮，可以调整它们来修改图像外观。

### 调整对比度

**对比度**在图像处理中是使物体可区分的亮度(或颜色)差异。它是由同一视野内的物体和其他物体的颜色和亮度的差异决定的。

固定对比度:

```
new_image = tf.image.adjust_contrast(
    our_image, contrast_factor=1.8)
new_image = np.clip(new_image, 0, 1)
show(our_image, new_image)

```

调整 *contrast_factor* 参数以增加或减少亮度。尝试使用*对比度因子*参数，查看其对转换图像的影响。

随机对比:

```
show_images(batch_images, indx, tf.image.random_contrast,
            0.75, 2.9, b=True)

```

调整下限和上限以更改随机对比度边界。

### 调整亮度

固定亮度:

```
new_image = tf.image.adjust_brightness(our_image, .25)
new_image = np.clip(new_image, 0, 1)
show(our_image, new_image)

```

调整 delta 值以增加图像的像素值。因此，0.25 的增量为图像增加了 25%的亮度。

随机亮度:

```
show_images(batch_images, indx, tf.image.random_brightness,
            0.25, b=True)

```

### 调整饱和度

**饱和度**在图像处理中，饱和度是图像中颜色的深度或强度。图像越饱和，看起来就越丰富多彩，越有活力。色彩饱和度越低，图像看起来就越柔和。

固定饱和度:

```
show(our_image, tf.image.adjust_saturation(our_image, 3.0))

```

调整饱和度系数以增加饱和度。因此，饱和度系数 3.0 是图像饱和度的三倍。

随机饱和:

```
show_images(batch_images, indx, tf.image.random_saturation,
            0.3, 3.5, b=True)

```

调整下限和上限以更改随机饱和度边界。

### 顺化(越南城市)

**色调**是 RGB 颜色的主要指示。这个值告诉我们一个物体是红色、绿色还是蓝色。相比之下，饱和度是感知强度。所以饱和度是物体看起来的颜色，而色相是实际的颜色。

固定色调:

```
show(our_image, tf.image.adjust_hue(our_image, 0.2))

```

随机色调:

```
show_images(batch_images, indx, tf.image.random_hue, 0.2, b=True)

```

调整参数值 0.2，查看其对变换图像的影响。

要查看所有可能的数据扩充转换，请阅读

[T2`www.tensorflow.org/api_docs/python/tf/image/`](http://www.tensorflow.org/api_docs/python/tf/image/)

有关直接在图像上应用增强的一般网站，请阅读

[T2`https://towardsdatascience.com/tensorflow-image-augmentation-on-gpu-bf0eaac4c967`](https://towardsdatascience.com/tensorflow-image-augmentation-on-gpu-bf0eaac4c967)

### 直接在图像上应用变换

既然我们已经讨论了几个潜在的转变，让我们看看我们是否可以提高培训绩效。我们只应用了几个转换，结果相当不错，但是您可以尝试任意多个。

#### 创建一个增强功能

创建一个函数，随机地从左向右翻转图像，并对图像应用饱和操作，如清单 [2-7](#PC45) 所示。

```
def augment(image, label):
  img = tf.image.random_flip_left_right(image)
  final_image = tf.image.random_saturation(img, 0, 2)
  return (final_image, label)

Listing 2-7Augmentation Function

```

面对如此多的转型选择和我们能找到的指导，找到正确的组合并不容易。我们尝试了许多变化，但前面简单的一个最适合我们。我们鼓励你尝试不同的转变，看看你是否能提高学习。

#### 显示增强图像

下面是清单 [2-8](#PC46) 中所示的将*增强*函数多次应用于同一个图像时的情况。

```
plt.figure(figsize=(10, 10))
for i in range(9):
  image, _ = augment(our_image, labels[0])
  ax = plt.subplot(3, 3, i + 1)
  plt.imshow(image)
  plt.axis('off')

Listing 2-8Plot Transformations for an Image

```

#### 构建输入管道

为训练和测试数据构建管道:

```
SHUFFLE_SIZE = 100

train1 = train_ds.map(scale, num_parallel_calls=4)
train2 = train1.map(augment, num_parallel_calls=4)
train_da = train2.shuffle(SHUFFLE_SIZE).cache().prefetch(1)
test_da = test_ds.map(scale).cache().prefetch(1)

```

请注意，我们仅将 augment 函数映射到训练数据。我们添加了 *num_parallel* 参数，以便在训练期间获得更好的性能。

Note

仅增强训练图像！训练数据被呈现给模型，以帮助它变得更加可概括和健壮。测试数据以新数据的形式呈现，以帮助评估模型。

#### 创建模型

清除所有以前的模型，并生成可再现性的种子:

```
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

创建一个多层 CNN，如清单 [2-9](#PC49) 所示。

```
model = tf.keras.Sequential([
  Conv2D(32, 3, activation='relu',
         input_shape=[180, 180, 3]),
  MaxPooling2D(),
  Conv2D(32, 3, activation='relu'),
  MaxPooling2D(),
  Conv2D(32, 3, activation='relu'),
  MaxPooling2D(),
  Flatten(),
  Dense(128, activation='relu'),
  Dropout(0.5),
  Dense(num)
])

Listing 2-9Multilayer CNN

```

请注意，该模型没有像上一节中那样的 Keras 预处理层。由于我们是直接增加图像，我们不需要添加 Keras 层。

#### 编译和训练模型

使用 SparseCategoricalCrossentropy 编译(from_logits=True):

```
model.compile(
  optimizer='adam',
  loss=SparseCategoricalCrossentropy(from_logits=True),
  metrics=['accuracy'])

```

训练模型:

```
history = model.fit(
    train_da,
    validation_data=test_da,
    epochs=5)

```

瞧。我们减轻了过度拟合！我们使用了五个纪元，但是您可以根据需要替换任意数量的纪元。更多的纪元需要更多的内存。由于我们的数据集不是很大，内存真的不是问题。但是对于大型数据集，如果将历元数设置得很高，内存可能会成为一个问题。

## 用 ImageGenerator 扩充数据

到目前为止，我们已经介绍了两种增强技术。第一种技术将 Keras 预处理层添加到模型中。第二种方法是直接对图像进行变换，然后将它们输入到模型中。这两种技术都构建了一个 tf.data 管道来提供给模型。

另一种方法是使用 ImageGenerator 类。 *ImageGenerator 类*使得从磁盘加载图像并以各种方式扩充图像变得容易。我们可以对图像进行平移、旋转、缩放、水平或垂直翻转、剪切或应用变换函数。虽然 ImageGenerator 对于简单的项目来说非常方便，但是构建 tf.data 管道更有利于复杂的项目，因为它可以从任何来源(不仅仅是本地磁盘)并行读取图像，并以任何方式操作数据集。此外，基于 tf.image 操作的预处理函数可以在 tf.data 管道和部署到生产环境的模型中使用。

我们不提倡一方优于另一方。这取决于手头的任务。ImageGenerator 类非常容易使用。所以它在项目的开始阶段应该是有用的。tf.data 管道针对并行处理进行了优化，因此对于需要大量计算资源的正在进行的项目来说，它是一个不错的选择。

要获得这方面的丰富资源，请阅读

[T2`www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)

### 处理花卉数据

该过程比以前的技术简单。利用字典将数据从 flowers 目录直接加载到训练和测试分割中。第一个字典为 ImageGenerator 类中的 ImageDataGenerator 方法提供缩放和拆分信息。第二个字典为方法提供目标和批处理信息。

导入适当的库:

```
from tensorflow.keras.preprocessing.image\
  import ImageDataGenerator

```

创建一个字典来缩放和拆分数据:

```
datagen_kwargs = dict(rescale=1./255, validation_split=.19)

```

数据分成 81%用于训练，19%用于测试。

创建字典来调整图像大小、设定批量大小和插值:

```
BATCH_SIZE = 32
IMAGE_SIZE = (180, 180)

dataflow_kwargs = dict(target_size=IMAGE_SIZE,
                       batch_size=BATCH_SIZE,
                       interpolation='bilinear')

```

### 创建数据集

既然目录设置正确，我们就使用 Keras API 来创建训练集和测试集。首先基于第一字典的构造创建测试集。

创建测试集:

```
valid_datagen = tf.keras.preprocessing.image.\
  ImageDataGenerator(**datagen_kwargs)
valid_generator = valid_datagen.flow_from_directory(
    data_dir, subset='validation', shuffle=False,
    **dataflow_kwargs)

```

创建训练集:

```
train_datagen = valid_datagen
train_generator = train_datagen.flow_from_directory(
    data_dir, subset='training', shuffle=True,
    **dataflow_kwargs)

```

请注意，我们只打乱了训练数据。

检查张量:

```
valid_datagen, train_datagen

```

两个张量都是 ImageDataGenerator 对象。所以张量已经准备好接受训练了，因为课程会处理所有的预处理！

### 创建模型

清除以前的模型并生成种子:

```
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

创建一个多层 CNN，如清单 [2-10](#PC59) 所示。

```
model = Sequential([
  Conv2D(filters=32, kernel_size=(5, 5), activation = 'relu',
         input_shape=(180, 180, 3)),
  MaxPooling2D(2),
  Conv2D(64, (5, 5), activation='relu'),
  MaxPooling2D(2),
  Flatten(),
  Dense(64, activation='relu'),
  Dense(5, activation='softmax')
])

Listing 2-10Multilayer CNN for ImageDataGenerator Objects

```

我们可以使用与上一节相同的模型，但是我们使用了一个稍微不同的模型来看看会发生什么。我们少用了一个卷积层，并将 softmax 应用于模型输出。

### 编译和训练模型

编译模型:

```
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

```

由于 *softmax* 激活被应用于进入我们模型的输出层的神经元，我们**不**用 *from_logits=True* 编译损失度量。Softmax 为多类问题中的每一类分配小数概率，这些概率的总和必须为 1.0。添加这个额外的约束可以帮助训练比其他情况下更快地收敛。

训练模型:

```
history = model.fit(train_generator, batch_size=BATCH_SIZE,
                    epochs=5, validation_data=valid_generator,
                    verbose=1)

```

如果没有数据扩充，过拟合在仅仅几个时期后就开始出现。

### 扩充训练数据

让我们看看使用 ImageDataGenerator 在训练数据上创建数据扩充是否能提高性能:

```
aug_train_datagen = tf.keras.preprocessing.\
  image.ImageDataGenerator(
      rotation_range=40, horizontal_flip=True,
      width_shift_range=0.2, height_shift_range=0.2,
      shear_range=0.2, zoom_range=0.2,
      **datagen_kwargs)

```

旋转和翻转图像。以及移动、剪切和缩放图像。由于只有训练数据用于学习，我们不增加验证(或测试)数据。在这个例子中，我们包括了许多参数值。我们做了相当多的实验来得出这些值。我们强烈鼓励你做自己的实验。

创建带有图像变换的训练集:

```
aug_train_generator = aug_train_datagen.flow_from_directory(
    data_dir, subset='training', shuffle=True, **dataflow_kwargs)

```

检查:

```
aug_train_generator

```

张量是一个*director iterator*对象。

### 重新编译并定型模型

因为我们刚刚增加了训练数据，所以我们必须重新训练模型。因此，从清除以前的模型会话开始，并为可重复性生成一个种子。继续重新编译模型。

清除模型并生成种子:

```
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

重新编译模型:

```
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

```

我们重新编译模型，因为我们是在一个新的数据集上训练。

训练模型:

```
history = model.fit(aug_train_generator, batch_size=BATCH_SIZE,
                    epochs=5, validation_data=valid_generator,
                    verbose=1)

```

该模型显示出较少的过度拟合！

### 检查数据

ImageDataGenerator 数据不同于 tf.data，所以我们来探讨一下。首先处理来自原始训练集的一批和来自扩充训练集的一批。虽然使用这个类非常简单，但是浏览数据确实需要更多的工作。

如清单 [2-11](#PC68) 所示，从原始训练集中抓取一批。

```
data_list = []
batch_index, end_index = 0, 1
while batch_index <= train_generator.batch_index:
  if batch_index < end_index:
    data = train_generator.next()
    data_list.append(data[0])
    batch_index = batch_index + 1
  else: break
original = np.asarray(data_list)

Listing 2-11Process a Batch from the Original Training Set

```

若要遍历 ImageDataGenerator 对象，请使用带有起始索引值(分配给变量)的数据集名称作为方法。因为我们只需要一个批处理，所以将结束索引值设置为 1。用 *next()* 方法抓取第一个元素。将其添加到列表中，并增加批次索引。在一次迭代后中断循环。最后，将列表转换为 NumPy 数组。

检查形状:

```
original[0].shape

```

不出所料，形状是(32，180，180，3)。所以每批由 32 张 180 × 180 × 3 的彩色图像组成。

确认我们抓了一批:

```
print ('We grabbed', len(original), 'batch.')

```

从清单 [2-12](#PC71) 所示的扩充训练集中抓取一批。

```
data_list = []
batch_index, end_index = 0, 1
while batch_index <= aug_train_generator.batch_index:
  if batch_index < end_index:
    data = aug_train_generator.next()
    data_list.append(data[0])
    batch_index = batch_index + 1
  else: break
augmented = np.asarray(data_list)

Listing 2-12Process a Batch from the Augmented Training Set

```

检查形状:

```
augmented[0].shape

```

不出所料，形状是(32，180，180，3)。所以每批由 32 张 180 × 180 × 3 的彩色图像组成。

确认我们抓了一批:

```
print ('We grabbed', len(augmented), 'batch.')

```

### 设想

从原始训练集中抓取第一幅图像:

```
train_image = original[0][0]

```

形象化:

```
plt.imshow(train_image)
plt.axis('off')
plt.grid(b=None)

```

我们看到一个正常的花图像。

可视化原始训练集中的图像，如清单 [2-13](#PC76) 所示。

```
plt.figure(figsize=(10, 10))
for images in original:
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i])
    plt.axis('off')

Listing 2-13Visualize Several Original Training Images

```

我们看到一些正常的花图像。

从增强的训练集中抓取第一幅图像:

```
aug_train_image = augmented[0][0]

```

可视化增强图像:

```
plt.imshow(aug_train_image)
plt.axis('off')
plt.grid(b=None)

```

我们看到一个增强的图像。

如清单 [2-14](#PC79) 所示，可视化来自增强训练集的图像。

```
plt.figure(figsize=(10, 10))
for images in augmented:

  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i])
    plt.axis('off')

Listing 2-14Visualize Several Augmented Training Images

```

我们看到一些增强的花图像。通过设计，扩充数据看起来与原始数据不同，因为我们希望为模型提供新数据。如果我们向数据提供原始数据的副本，性能不会提高。但是扩充数据为模型提供了原始数据。

## 摘要

我们用三种不同的技术演示了数据扩充。Keras 技术很容易实现，但是不像直接在图像上应用增强那样灵活。ImageGenerator 技术是最容易实现的，但是仅限于较小的项目。