# 13.目标检测

**物体检测**是一种自动计算机视觉技术，用于定位数码照片或视频中的物体。具体地，对象检测在位于静止图像或视频数据中的一个或多个有效目标周围绘制边界框。**有效目标**是正在研究的图像或视频数据中的感兴趣对象。在任务开始时就应该知道有效的目标。

## 自然场景中的目标检测

检测自然场景中的物体对我们来说毫不费力，但对计算机算法来说极具挑战性。作为人类，我们看着一个场景，不假思索地识别感兴趣的物体。我们在腹侧视觉流中处理视觉数据，腹侧视觉流是大脑中帮助物体识别的区域层级。我们识别不同类型和大小的物体，并借助视觉皮层中对简单形状(如直线和曲线)做出反应的细胞对它们进行分类。

通过对象检测，数据科学家试图模仿人类使用计算机算法所做的事情。但是在计算机算法能够开始检测物体之前，自然场景必须被捕获为数字图像(或数字视频)。一幅**数字图像**由图片元素(或像素)组成。一个**像素**是数字图形中的基本逻辑单位。简单地说，像素是一个微小的颜色方块。

像素在二维(2D)网格中统一组合，在计算机显示器上形成完整的数字图像、视频、文本或任何可视元素。每个像素都有一个特定的数字，这个数字告诉算法它的颜色。该数字代表像素强度值。像素强度值代表灰度或彩色图像。

在灰度图像中，每个像素仅代表一种颜色的强度。所以一幅灰度图像可以用一个 2D 矩阵(或网格)来表示。在标准 RGB 系统中，彩色图像有三个通道(红色、绿色和蓝色)。所以一幅彩色图像可以用三个 2D 矩阵来表示。一个矩阵表示像素中红色的强度。一个代表像素中绿色的强度。一个代表像素中蓝色的强度。

每个矩阵被认为是一个颜色通道。一个**颜色通道**是一个值数组(每个像素一个),它们共同指定了图像的一个方面或维度。因此，RGB 颜色包含三个颜色通道——红色、绿色和蓝色。每个颜色通道用 0(最不饱和)到 255(最饱和)来表示。因此，16，777，216 (256 <sup>3</sup> 个不同的颜色可以通过组合红色、绿色和蓝色像素强度在 RGB 颜色空间中表示。

一旦数字图像被捕获为灰度(2D 矩阵)或彩色(三色通道 2D 矩阵)图像，它就被处理用于模型消费。只有这样，计算机算法才能检查每个 2D 像素值网格，开始识别模式。

## 检测与分类

图像分类将输入图像呈现给神经网络，因此它可以学习与该图像相关联的单个类别标签。网络还可以学习与类别标签相关联的概率。所学习的类别标签表征了整个图像的内容，或者至少表征了图像的最主要和可见的内容。因此，图像分类网络能够学习如何正确标记猫的图像。

对象检测将输入图像呈现给神经网络，因此它可以学习图像在图片对象(或场景)中的准确位置。为了定位场景中的输入图像，对象检测算法为图片中的每个对象创建边界框列表。边界框被创建为(x，y)坐标输入图像位置。该算法还识别与每个边界框相关联的类别标签以及与每个边界框和类别标签相关联的概率/置信度得分。

简单地说，图像分类涉及一个图像进入网络和一个类别标签出来。对象检测包括将一幅图像输入网络，并输出多个包围盒及其相关的类别标签。

对象检测通常用于精确计数、定位和标记场景中的对象。神经网络是用于对象检测的最先进的方法。卷积神经网络经常用于自动学习对象的固有特征，以通过将对象从其背景中区分出来来在图像中识别它们。

图像分类涉及给图像分配类别标签，而对象定位涉及围绕图像场景中的一个或多个对象绘制边界框。因此，分类将图像分成不同的类别，而目标定位识别图像场景中的有效目标。目标检测比定位更具挑战性，因为它结合了分类和目标定位，以学习如何在图像中的每个感兴趣的目标(或有效目标)周围绘制边界框，并为其分配类别标签。目标定位和目标检测之间的区别是微妙的。目标定位旨在定位图像场景中的主要(或最可见)目标，而目标检测则试图找出所有目标及其边界。

想象一个图像包含两只猫和一个人。对象检测网络能够定位实体的实例，并对图像中发现的实体类型进行分类。因此，一个对象检测网络可以在这张图像中定位两只猫和一个人，并正确地对它们进行分类。

目标检测通常与图像识别相混淆。那么它们有什么不同呢？图像识别为图像分配一个标签。一张狗的图片收到标签*狗*。两只狗的照片仍然被贴上了*狗*的标签。然而，对象检测会在每只狗周围绘制一个边界框，并将该框标记为*狗*。该模型预测每个对象在图像中的位置以及应该应用于它的标签。因此，目标检测提供了比识别更多的图像信息。

## 边界框

**边界框**是一个假想的矩形，作为对象检测的参考点，并为该对象创建一个碰撞框。边界框的区域(通常简称为 *bbox* )由两个经度和两个纬度定义，其中纬度是–90.0 和 90.0 之间的十进制数，经度是–180.0 和 180.0 之间的十进制数。数据标注器绘制 bbox 矩形，通过定义图像(场景)中感兴趣的对象的 x 和 y 坐标来勾勒出该对象的轮廓。

碰撞盒(或碰撞盒)是一种不可见的形状，通常在视频游戏中用于实时碰撞检测。所以它是一种包围盒。它通常是一个矩形(在 2D 游戏中)或长方体(在 3D 中),附着在可视对象(如模型或精灵)上的一个点上并跟随该点。

## 基本结构

深度学习对象检测模型通常有两个部分。一个*编码器*将一幅图像作为输入，并让它通过一系列的块和层，这些块和层学习提取用于定位和标记对象的统计特征。来自编码器的输出然后被传递到一个*解码器*，它预测每个对象的边界框和标签。

最简单的解码器是一个纯回归器。回归变量是用于预测响应变量的回归模型中任何变量的名称。回归变量也被称为解释变量、独立变量、操纵变量、预测变量或特征。建立回归模型的全部目的是理解回归变量的变化如何导致响应变量(或回归变量)的变化。因此，回归变量是一个特征，而回归变量是一个响应变量(或目标)。

**回归**是一种用于预测连续值的监督 ML 技术。回归算法的目标是绘制数据之间的最佳拟合直线或曲线。回归器连接到编码器的输出，并直接预测每个边界框的位置和大小。

回归模型的输出是图像中对象及其范围(或强度)的 x，y 坐标对。但是回归量是有限的，因为我们需要提前指定盒子的数量。如果一幅图像有两只狗，但回归模型被设计成检测单个对象，则其中一只狗未被标记。但是，如果我们提前知道我们需要预测每幅图像中的对象数量，纯回归模型可能是一个好的选择。

回归方法的延伸是区域建议网络。区域建议网络中的解码器建议图像中它认为物体可能存在的区域。属于这些区域的像素然后被馈送到分类子网络以确定标签(或者拒绝该提议)。然后，网络通过分类网络运行包含这些区域的像素。这种方法的好处是更准确和灵活的模型，可以提出任意数量的可能包含边界框的区域。但是精度的提高是以计算效率为代价的。

单触发检测器(SSD)寻求中间立场。SSD 依赖于一组预先确定的区域，而不是使用子网来提议区域。锚点网格放置在输入图像上。在每个锚点处，多种形状和大小的盒子充当区域。对于每个锚点处的每个框，该模型输出对象是否存在于该区域内的预测以及对框的位置和大小的修改，以使其更精确地适合该对象。因为在每个定位点有多个框，并且定位点可能靠得很近，所以 SSD 产生许多重叠的潜在检测。因此，必须对 SSD 输出进行后处理，通过修剪有缺陷的输出来剔除最佳输出。SDD 最流行的后处理技术是非最大抑制。*非最大抑制*用于为对象选择最合适的边界框。

对象检测器输出每个对象的位置和标签，但是我们如何测量性能呢？对象位置最常见的度量是交集/并集(IOU)。给定两个边界框，IOU 计算交集的面积并除以并集的面积。该值的范围从 0(无交互)到 1(完全重叠)。标签可以使用简单的正确率度量。

章节的笔记本位于以下 URL:

[T2`https://github.com/paperd/deep-learning-models`](https://github.com/paperd/deep-learning-models)

我们用一个端到端的代码实验来演示对象检测。通过导入主 TensorFlow 库并实例化 GPU，开始设置 Colab 生态系统。

## 导入 TensorFlow 库

导入库并将其别名为 **tf** :

```py
import tensorflow as tf

```

将 TensorFlow 库别名为 tf 是常见的做法。

## GPU 硬件加速器

为了方便起见，我们提供了在 Colab 笔记本中启用 GPU 的步骤:

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

验证 GPU 是否处于活动状态:

```py
tf.__version__, tf.test.gpu_device_name()

```

如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

Note

如果出现错误**名称‘TF’未定义**，请重新执行代码以导入 TensorFlow 库！

## 物体探测实验

我们从 Google Drive 和 Wikimedia Commons 上抓取图片进行实验。我们使用预先训练好的*fast _ rcnn/open images _ v4/inception _ resnet _ v2*对象检测模块进行图像对象检测。使用 ImageNet 预训练的 Inception ResNet V2(版本 2)作为图像特征提取器，在开放图像 V4(版本 4)上训练对象检测模型。该模块在内部执行非最大抑制。输出的最大检测数为 100。所以它在给定的场景中最多可以检测 100 个物体。为 600 个可装箱的类别输出检测。一个*可装箱类别*是一个能够(或适合)放入一个边界框的类别。

*Open Images* 是一个数据集，包含大约 900 万张带有丰富注释的图像，包括图像级标签、对象边界框、对象分割、视觉关系和本地叙述。这些图像非常多样，通常包含具有多个对象的复杂场景(平均每个场景 8.4 个)。

*开放图像 V4* 的训练集包含 174 万个图像上的 600 个对象类的 1460 万个边界框，这使其成为具有对象位置注释的最大现有数据集(截至本文撰写之时)。这些框大部分是由专业注释者手工绘制的，以确保准确性和一致性。这些图像非常多样，通常包含带有多个对象的复杂场景(平均每幅图像 8.4 个)。此外，数据集用跨越数千个类的图像级标签进行了注释。

Note

建议在 GPU 上运行该模块，以获得可接受的推理时间。

### 导入必需的库

启用对 TF-hub 模块的访问:

```py
import tensorflow_hub as hub

```

访问绘图模块:

```py
import matplotlib.pyplot as plt

```

用于文件处理和操作内存中数据的访问模块:

```py
import tempfile
from six.moves.urllib.request import urlopen
from six import BytesIO

```

这个 *tempfile* 模块创建临时文件和目录。 *urlopen* 模块是 Python 的统一资源定位符(URL)处理模块。它用于获取一个 URL。URL 是对 web 资源的引用，指定了它在计算机网络上的位置以及检索它的机制。 *BytesIO* 模块处理内存中的字节数据。

PIL 图书馆的访问模块:

```py
from PIL import Image
from PIL import ImageColor
from PIL import ImageDraw
from PIL import ImageFont
from PIL import ImageOps

```

*图像*模块允许将图像上传至内存。其余模块支持图像处理。

导入 NumPy:

```py
import numpy as np

```

### 为实验创建函数

第一个函数显示图像:

```py
def display_image(image):
  fig = plt.figure(figsize=(20, 15))
  plt.grid(False)
  plt.imshow(image)
  plt.axis('off')

```

我们提出了许多不同的显示功能，以提供显示图像的替代方式。为了练习，创建您自己的显示函数，并在本实验中使用它。

第二个函数在场景中的图像周围绘制一个边界框，如清单 [13-1](#PC9) 所示。

```py
def draw_bounding_box_on_image(
    image, ymin, xmin, ymax, xmax,
    color, font, thickness=4, display_str_list=()):
  """Adds a bounding box to an image."""
  draw = ImageDraw.Draw(image)
  im_width, im_height = image.size
  (left, right, top, bottom) = (
      xmin * im_width, xmax * im_width,
      ymin * im_height, ymax * im_height)
  draw.line([(left, top), (left, bottom),
             (right, bottom), (right, top),
             (left, top)],
             width=thickness, fill=color)
  display_str_heights = [font.getsize(ds)[1]
                         for ds in display_str_list]
  total_display_str_height = (
      1 + 2 * 0.05) * sum(display_str_heights)
  if top > total_display_str_height:
    text_bottom = top
  else:

    text_bottom = top + total_display_str_height
  for display_str in display_str_list[::-1]:
    text_width, text_height = font.getsize(display_str)
    margin = np.ceil(0.05 * text_height)
    draw.rectangle(
        [(left, text_bottom - text_height - 2 * margin),
         (left + text_width, text_bottom)], fill=color)
    draw.text(
        (left + margin, text_bottom - text_height - margin),
        display_str, fill='black', font=font)
    text_bottom -= text_height - 2 * margin

Listing 13-1Function to Draw Bounding Boxes

```

该函数接受经过处理的图像、x 和 y 的最小和最大坐标、颜色、字体、粗细以及显示字符串列表。处理后的图像由 *draw_boxes* 函数发送，该函数将在下面介绍。x 和 y 坐标为图像提供边界框的边界。颜色、字体、粗细和字符串列表由*draw _ box*函数提供。该函数的其余部分为图像添加一个边界框。

首先创建一个 ImageDraw 对象，并将其分配给变量 *draw* 。 *ImageDraw* 模块用于创建新图像、注释或修饰现有图像，以及动态生成图形供网络使用。图像的 x 和 y 坐标被分配给变量，边界线被分配给 ImageDraw 对象。

该函数继续将显示字符串列表分配给变量 *display_str_heights* 。显示字符串用于标记场景中每个图像的边界框。如果添加到边界框顶部的显示字符串的总高度超过了图像的顶部，我们需要将字符串堆叠在边界框的下方，而不是上方。原因是为了确保标识边界框中每个图像的字符串是可读的。 *total_display_str_height* 变量确保每个显示字符串都有一个合理的顶部和底部边距用于显示。您可以尝试这个值，但是这个设置已经很不错了。下一个逻辑检查顶部和底部边距，以确保图像在边界框容器内的良好匹配。

该函数的其余部分(使用 for 循环)从下到上反转显示字符串列表。 *draw.rectangle* 方法在每个图像周围绘制一个边界框。 *draw.text* 方法用适当的标签标记每个边界框。当循环完成时，包括所有边界框的图像被创建。

第三个函数接受图像、盒子、类别标签、分数、最大盒子数和最小分数。除图像之外的参数由预先训练的模型生成。使用接受的参数值，它将带有格式化分数和标签名称的标签框覆盖在图像上，如清单 [13-2](#PC10) 所示。

```py
def draw_boxes(
    image, boxes, class_names, scores,
    max_boxes=10, min_score=0.1):
  # Overlay labeled boxes on an image with formatted scores and label names.
  colors = list(ImageColor.colormap.values())
  one = '/usr/share/fonts/truetype/liberation/'
  two =  'LiberationSansNarrow-Regular.ttf'
  font_url = one + two
  try:
    font = ImageFont.truetype(font_url, 25)
  except IOError:
    print('Font not found, using default font.')
    font = ImageFont.load_default()
  for i in range(min(boxes.shape[0], max_boxes)):
    if scores[i] >= min_score:
      ymin, xmin, ymax, xmax = tuple(boxes[i])
      display_str = '{}: {}%'.format(
          class_names[i].decode('ascii'),
          int(100 * scores[i]))
      color = colors[hash(class_names[i]) % len(colors)]
      image_pil = Image.fromarray(
          np.uint8(image)).convert('RGB')
      draw_bounding_box_on_image(
          image_pil, ymin, xmin, ymax, xmax,
          color, font, display_str_list=[display_str])
      np.copyto(image, np.array(image_pil))
  return image

Listing 13-2Container Function for Drawing Bounding Boxes

```

draw_boxes 函数是一个容器，因为它接受来自预训练模型的图像和字典，调用*draw _ bounding _ box _ on _ image*，并返回在检测到的对象周围绘制了边界框的图像。

尽管这个函数看起来很复杂，但它非常简单。它创造了图像中固有的颜色。我们需要颜色，因为我们从场景中重新创建检测到的对象，这样我们就可以在它们周围绘制边界框。然后它创建我们用来标记每个边界框的字体。为了练习，你可以改变字体。然后，它遍历所有对象以检索(从预训练模型的字典输出)它们在场景中的 x 和 y 坐标、标签、分数、图像像素和颜色，以提供给*draw _ bounding _ box _ on _ image*函数。该函数为每个对象绘制边界框，并返回一个带有边界框图像的场景。

Note

为了避免混淆，请将原始图像视为一个场景。预训练模型学习如何识别场景中的图像对象。它输出一组字典，其中包含它检测到的对象的信息以及场景中边界框的坐标。然后我们从字典信息中画出边界框。因此，当我们谈论图像对象时，我们指的是从场景中检测到的对象。

### 加载预训练的对象检测模型

加载对象检测模块，并将其应用于下载的图像:

```py
p1 = 'https://tfhub.dev/google/faster_rcnn/'
p2 = 'openimages_v4/inception_resnet_v2/1'
URL = p1 + p2
module_handle = URL
obj_detect = hub.load(module_handle).signatures['default']

```

预训练模型是在开放图像 V4 上训练的对象检测模型，使用 ImageNet 预训练的 Inception ResNet V2 作为图像特征提取器。

开放图像是巨大的！它包含 600 个类别上的 15，851，536 个框，350 个类别上的 2，785，498 个实例分段，1，466 个关系上的 3，284，280 个关系注释，675，155 个本地化叙述，以及 19，957 个类别上的 59，919，574 个图像级标签。它还包括一个可选的扩展，包含 478，000 个众包图像，超过 6，000 个类别。

该模块在内部执行非最大抑制。输出的最大检测数为 100。为 600 个可装箱的类别输出检测。建议在 GPU 上运行该模块，以获得可接受的推理时间。

该模型接受可变大小的三通道图像。它输出几个字典，包括 detection _ boxes(边界框坐标)、detection_class_entities(检测类名)、detection_class_names(人类可读的类名)、detection_class_labels(作为张量的标签)和 detection_scores(检测分数)。检测分数表示模型在标记对象时的置信度。

Note

我们为好奇者提供公开图像的细节。预先训练的模型处理工作负载。我们只是用它的输出来创建一个可视化。

### 从 Google Drive 加载图片

将 Google Drive 安装到 Colab 笔记本上:

```py
from google.colab import drive
drive.mount('/content/gdrive')

```

确保图像位于 Google Drive 的*适当目录*(即 Colab 笔记本)中！

访问并显示图像:

```py
img_path = 'gdrive/My Drive/Colab Notebooks/images/cats_dogs.jpg'
pil_image = Image.open(img_path)
display_image(pil_image)

```

将 JPEG 图像转换为 Python 图像库(PIL)图像并显示。PIL 是一个支持打开、操作和保存许多不同图像文件格式的库。它也被称为枕头图书馆。

检查图像大小:

```py
pil_image.size

```

### 准备图像

为图像文件生成临时路径:

```py
_, filename = tempfile.mkstemp(suffix='.jpg')
filename

```

准备要处理的图像，并将其保存到临时文件路径:

```py
pil_image_rgb = pil_image.convert('RGB')
pil_image_rgb.save(filename, format='JPEG', quality=90)
print('Image downloaded to %s.' % filename)
display_image(pil_image)

```

### 对图像运行对象检测

创建一个函数来加载图像:

```py
def load_img(path):
  img = tf.io.read_file(path)
  img = tf.image.decode_jpeg(img, channels=3)
  return img

```

该函数加载图像，并为预训练模型准备图像。

创建一个运行对象检测的函数，如清单 [13-3](#PC18) 所示。

```py
def run_detector(detector, path):
  img = load_img(path)
  converted_img  = tf.image.convert_image_dtype(
      img, tf.float32)[tf.newaxis, ...]
  result = detector(converted_img)
  result = {key:value.numpy()
            for key,value in result.items()}
  print("Found %d objects." %\
        len(result["detection_scores"]))
  image_with_boxes = draw_boxes(
      img.numpy(), result["detection_boxes"],
      result["detection_class_entities"],
      result["detection_scores"])
  display_image(image_with_boxes)

Listing 13-3Object Detection Function

```

该函数接受加载的预训练模型签名和图像(场景)的路径。它加载图像并将其转换为 NumPy 数组以供模型使用。在 NumPy 映像上运行模型签名。从预训练模型输出的字典中检索字典键/值对。创建一个检测到的物体周围有边界框的场景，并显示它。

运行检测器:

```py
run_detector(obj_detect, filename)

```

正如我们所知，预先训练的模型仅限于检测 100 个对象。但是这条信息是误导性的，因为它说无论什么场景被输入到模型中，都会发现 100 个对象。

检测很完美，但是不要太兴奋。该模型功能强大，足以检测简单场景中的图像。场景很简单，因为背景没有干扰，每只狗/猫都是单独呈现的。

让我们试试另一个:

```py
img_path = 'gdrive/My Drive/Colab Notebooks/images/butterfly.jpg'
pil_image = Image.open(img_path)
display_image(pil_image)

```

处理图像:

```py
_, filename = tempfile.mkstemp(suffix='.jpg')
pil_image_rgb = pil_image.convert('RGB')
pil_image_rgb.save(filename, format='JPEG', quality=90)
print('Image downloaded to %s.' % filename)

```

运行检测器:

```py
run_detector(obj_detect, filename)

```

检测很完美，但是图像很简单。

### 从复杂场景中检测图像

让我们尝试在更复杂的图像上进行检测。我们已经找到了一些来自维基共享的图片，但是你可以通过几个简单的步骤找到你自己的图片:

1.  转到以下网址: [`https://commons.wikimedia.org/wiki/Main_Page`](https://commons.wikimedia.org/wiki/Main_Page)

2.  单击图像链接。

3.  单击图像。

4.  右键单击图像。

5.  从下拉菜单中选择“复制链接地址”。

6.  将链接地址粘贴到代码单元格中。

7.  用单引号或双引号将链接地址括起来。

8.  赋给一个变量。

#### 创建下载功能

创建一个函数来下载、处理和保存图像到一个临时文件路径，如清单 [13-4](#PC23) 所示。

```py
def download_and_resize_image(
    url, new_width=256, new_height=256,
    display=False):
  _, filename = tempfile.mkstemp(suffix='.jpg')
  response = urlopen(url)
  image_data = response.read()
  image_data = BytesIO(image_data)
  pil_image = Image.open(image_data)
  pil_image = ImageOps.fit(
      pil_image, (new_width, new_height),
      Image.ANTIALIAS)
  pil_image_rgb = pil_image.convert('RGB')
  pil_image_rgb.save(
      filename, format='JPEG', quality=90)
  print('Image downloaded to %s.' % filename)
  if display:
    display_image(pil_image)
  return filename

Listing 13-4Download and Preprocess Function

```

该函数为图像文件生成一个临时路径。然后，它从提供的 URL 中读取图像文件。该功能继续将图像文件转换为 PIL 图像。然后调整 PIL 图像的大小，将其转换为 RGB，并保存到临时文件路径。该函数以返回 PIL 图像的文件名结束。

#### 加载图像场景

从 Wikimedia Commons URL 加载场景:

```py
p1 = 'https://upload.wikimedia.org/wikipedia/commons/7/79/'
p2 = 'At_taverna_under_the_church%2C_Ano_Potamia%2C_Naxos%'
p3 = '2C_190574.jpg'
URL = p1 + p2 + p3

downloaded_image_path = download_and_resize_image(
    URL, 1280, 856, True)

```

图像场景的源位于

[T2`https://commons.wikimedia.org/wiki/File:At_taverna_under_the_church,_Ano_Potamia,_Naxos,_190574.jpg`](https://commons.wikimedia.org/wiki/File:At_taverna_under_the_church,_Ano_Potamia,_Naxos,_190574.jpg)

#### 发现

运行对象检测:

```py
run_detector(obj_detect, downloaded_image_path)

```

对于更复杂的场景，检测并不完美。但是它确实做出了一些正确的检测。

#### 检测更多场景

拼凑一些路径，如清单 [13-5](#PC26) 所示。

```py
p1 = 'https://upload.wikimedia.org/wikipedia/commons/4/45/'
p2 = 'Green_Dragon_Tavern_%2836196%29.jpg'
tavern = p1 + p2

p1 = 'https://upload.wikimedia.org/wikipedia/commons/3/31/'
p2 = 'Circus_Circus_Hotel-Casino_sign.jpg'
casino = p1 + p2

p1 = 'https://upload.wikimedia.org/wikipedia/commons/9/91/'
p2 = 'Leon_hot_air_balloon_festival_2010.jpg'
balloon = p1 + p2

p1 = 'https://upload.wikimedia.org/wikipedia/commons/d/d8/'
p2 = '2012_Festival_of_Sail_-_7943922284.jpg'
sail = p1 + p2

p1 = 'https://upload.wikimedia.org/wikipedia/commons/a/ab/'
p2 = '17_mai_2018.jpg'
flag = p1 + p2

p1 = 'https://upload.wikimedia.org/wikipedia/commons/4/43/'
p2 = 'Fruit_baskets.jpg'
basket= p1 + p2

p1 = 'https://upload.wikimedia.org/wikipedia/commons/c/c7/'
p2 = 'Fruit_stands%2C_Rue_de_Seine%2C_Paris_22_May_2014.jpg'
stand= p1 + p2

p1 = 'https://upload.wikimedia.org/wikipedia/commons/9/95/'
p2 = 'Wine_tasting_%40_brown_brothers.jpg'
wine = p1 + p2

Listing 13-5Image Paths

```

创建一个函数来检测场景中的图像:

```py
def detect_img(image_url):
  image_path = download_and_resize_image(image_url, 640, 480)
  run_detector(obj_detect, image_path)

```

对其中一个场景运行对象检测:

```py
detect_img(wine)

```

所以场景复杂度限制了检测精度。

再试一个:

```py
detect_img(sail)

```

#### 找到源头

我们在研究中有时会碰到维基共享图片。但是这种图像的来源从来没有(至少在我们的经验中)被包括在内。如果我们想以任何方式使用图像，我们必须找到它的来源，看看它是否被允许。

#### 找到维基共享图片的来源

我们可以通过几个步骤找到图像的来源:

1.  用 *commons* 代替*上传。*

2.  将*维基百科*改为*维基。*

3.  用*commons/(number)/(number)*替换*文件:*

4.  将 *%(number)* 翻译成其对应的 *HTML 编码。*

要找到 HTML 编码的等效内容，请阅读

[T2`https://krypted.com/utilities/html-encoding-reference/`](https://krypted.com/utilities/html-encoding-reference/)

Note

我们不能保证这个过程适用于每张图片，但是它适用于我们用过的图片。

让我们在酒馆图像上尝试这个过程:

[T2`https://upload.wikimedia.org/wikipedia/commons/4/45/Green_Dragon_Tavern_%2836196%29.jpg`](https://upload.wikimedia.org/wikipedia/commons/4/45/Green_Dragon_Tavern_%252836196%2529.jpg)

用 *commons* 替换 *upload* :

[T2`https://commons.wikimedia.org/wikipedia/commons/4/45/Green_Dragon_Tavern_%2836196%29.jpg`](https://commons.wikimedia.org/wikipedia/commons/4/45/Green_Dragon_Tavern_%252836196%2529.jpg)

将*维基百科*改为*维基*:

[T2`https://commons.wikimedia.org/wiki/commons/4/45/Green_Dragon_Tavern_%2836196%29.jpg`](https://commons.wikimedia.org/wiki/commons/4/45/Green_Dragon_Tavern_%252836196%2529.jpg)

将*文件*替换为*commons/(number)/(number)*:

[T2`https://commons.wikimedia.org/wiki/File:Green_Dragon_Tavern_%2836196%29.jpg`](https://commons.wikimedia.org/wiki/File:Green_Dragon_Tavern_%252836196%2529.jpg)

翻译:

[T2`https://commons.wikimedia.org/wiki/File:Green_Dragon_Tavern_(36196).jpg`](https://commons.wikimedia.org/wiki/File:Green_Dragon_Tavern_%252836196%2529.jpg)

%28 和%29 代码从 HTML 编码参考翻译成左括号和右括号。

赌场的形象更简单，因为我们不需要做任何翻译:

[T2`https://commons.wikimedia.org/wiki/File:Circus_Circus_Hotel-Casino_sign.jpg`](https://commons.wikimedia.org/wiki/File:Circus_Circus_Hotel-Casino_sign.jpg)

以下是其余图像的来源:

[T2`https://commons.wikimedia.org/wiki/File:Leon_hot_air_balloon_festival_2010.jpg`](https://commons.wikimedia.org/wiki/File:Leon_hot_air_balloon_festival_2010.jpg)

[T2`https://commons.wikimedia.org/wiki/File:2012_Festival_of_Sail_-_7943922284.jpg`](https://commons.wikimedia.org/wiki/File:2012_Festival_of_Sail_-_7943922284.jpg)

[T2`https://commons.wikimedia.org/wiki/File:17_mai_2018.jpg`](https://commons.wikimedia.org/wiki/File:17_mai_2018.jpg)

[T2`https://commons.wikimedia.org/wiki/File:Fruit_baskets.jpg`](https://commons.wikimedia.org/wiki/File:Fruit_baskets.jpg)

[T2`https://commons.wikimedia.org/wiki/File:Fruit_stands,_Rue_de_Seine,_Paris_22_May_2014.jpg`](https://commons.wikimedia.org/wiki/File:Fruit_stands,_Rue_de_Seine,_Paris_22_May_2014.jpg)

[T2`https://commons.wikimedia.org/wiki/File:Wine_tasting_@_brown_brothers.jpg`](https://commons.wikimedia.org/wiki/File:Wine_tasting_%2540_brown_brothers.jpg)

## 摘要

我们在几个图像场景上使用了一个强大的预训练图像检测器模型来演示对象检测。该模型在简单场景下工作得非常好，但在复杂场景下就不那么好了。随着深度学习中的对象检测不断发展，我们相信未来的模型将大大提高检测能力。