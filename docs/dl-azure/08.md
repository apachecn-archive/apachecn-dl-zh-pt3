# 8.生成对抗网络

对于许多人工智能项目，深度学习技术正越来越多地被用作创新解决方案的构建模块，从图像分类到对象检测，图像分割，图像相似性和文本分析(例如，情感分析，关键短语提取)。由 Goodfellow 等人(2014 年)首先介绍的 GANs 是一种强大的新方法，用于教会计算机如何通过生成过程完成复杂任务。正如 Yann LeCun(在 [`http://bit.ly/LeCunGANs`](http://bit.ly/LeCunGANs) )所说，GANs 确实是“过去 20 年里机器学习中最酷的想法”

近年来，GANs 显示出巨大的潜力，并已应用于各种场景，从图像合成到提高图像质量(超分辨率)、图像到图像的翻译、文本到图像的生成等等。此外，gan 是在将 AI 用于艺术、音乐和创意(例如，音乐生成、音乐伴奏、诗歌生成等)方面取得进步的基础。).

这一章描述了 GANs 背后的秘密。我们首先来看看 gan 是如何在各种人工智能应用和场景中使用的。然后，我们一步一步地看一个名为 CycleGAN 的新型 GAN 的代码样本，以理解 gan 是如何工作的。为此，我们使用 Azure DLVM 作为计算环境。有关设置 DLVM 以运行代码示例的详细信息，请参见第 [4](04.html) 章。

## 什么是生成性对抗网络？

甘正在成为无监督和半监督学习的强大技术。基本的 GAN 包括以下内容:

*   一个*生成模型*(即生成器)生成一个对象。生成器对真实对象一无所知，通过与鉴别器交互来学习。例如，生成器可以生成图像。

*   一个*判别模型*(即判别器)确定一个物体是真的(通常用接近 1 的值表示)还是假的(用接近 0 的值表示)。

*   鉴别器向生成器提供一个*对抗损失*(或错误信号),使得生成器能够生成尽可能接近真实对象的对象。

图 [8-1](#Fig1) 显示了发生器和鉴别器之间的相互作用。鉴别器是一个分类器，用于确定提供给它的图像是真图像还是假图像。生成器使用噪声向量和来自鉴别器的反馈来尽力生成尽可能接近真实图像的图像。这一直持续到 GAN 算法收敛。在许多 GANs 中，生成器和鉴别器通常由模仿 DenseNet、U-Net 和 ResNet 的通用网络架构组成。在第 [3](03.html) 章中讨论了一些示例网络架构。

![../images/463582_1_En_8_Chapter/463582_1_En_8_Fig1_HTML.jpg](../images/463582_1_En_8_Chapter/463582_1_En_8_Fig1_HTML.jpg)

图 8-1

显示发生器和鉴别器之间相互作用的基本 GAN

图 [8-2](#Fig2) (受 Goodfellow 等人 2014 年工作的启发)描述了 GAN 如何工作的理论基础。发生器(G)和鉴别器(D)分别由实线和虚线表示。数据生成分布由虚线表示。图 [8-2](#Fig2) 中的两条水平线分别表示 *z* 被均匀采样的区域(下面的线)和 *x* 的区域(上面的线)。从下方到上方的箭头表示映射*x*=*G*(*z*)。从图 [8-2](#Fig2) 中，你会注意到，随着时间的推移，随着 GAN 的收敛，实线和虚线彼此接近(或几乎相似)。在这一点上，鉴别器 D 不再能够区分所产生的真实和伪造对象。

![../images/463582_1_En_8_Chapter/463582_1_En_8_Fig2_HTML.jpg](../images/463582_1_En_8_Chapter/463582_1_En_8_Fig2_HTML.jpg)

图 8-2

gan 的工作原理:生成器生成的对象如此真实，以至于鉴别器再也无法分辨真假。来源:Goodfellow 等人(2014 年)。

在 GANs 的早期版本中，生成器和鉴别器被实现为完全连接的神经网络(Goodfellow 等人，2014)。这些 GANs 用于从深度学习中常用的各种数据集生成图像:例如，CIFAR10、MNIST(手写数字)和多伦多人脸数据集。随着 GANs 架构的发展，CNN 被越来越多地使用。使用深度 CNN 的 GAN 的一个例子是 DC GAN(拉德福德、梅斯和钦塔拉，2016)。Creswell 等人(2017 年)对不同类型的天然气水合物进行了全面概述。

自 2014 年以来，出现了非常创新的使用 gan 的方法。GANs 在利用人工智能进行创意方面展示了前景，例如艺术和音乐创作以及计算机辅助设计(CAD)。其中一种方法是使用文本描述自动生成图像。InfoGAN (Chen 等人，2016 年)是一种无监督的方法，可以从几个众所周知的数据集(例如，数字[MNIST]、CelebA 人脸和门牌号[SVHN])中提取语义和隐藏表示。InfoGAN 背后的秘密是最大化潜在变量和观测值之间的交互信息。堆叠式生成对抗网络(StackGANZhang 等人，2016)提出使用文本描述来生成照片级真实感图像。例如，给定文本“这只鸟有黄色的腹部和跗骨，灰色的背部、翅膀和棕色的喉咙，黑色的颈背”，StackGAN 将使用两个阶段生成一只鸟的图片。在阶段 1 中，计算低分辨率图像，其由基本形状和颜色组成。在第 2 阶段，第 1 阶段的结果和文本描述用于创建照片级的高分辨率图像。

图 [8-3](#Fig3) 显示了堆垛的两个阶段。

![../images/463582_1_En_8_Chapter/463582_1_En_8_Fig3_HTML.jpg](../images/463582_1_En_8_Chapter/463582_1_En_8_Fig3_HTML.jpg)

图 8-3

StackGAN:从文本生成图像。资料来源:张等(2016)。

和 StackGAN 一样，AttnGAN 也采用了多阶段方法。此外，AttnGan 引入了一种新颖的注意力驱动方法，该方法专注于(或关注)文本描述中的不同单词，并使用这种方法来合成图像每个子区域的细粒度细节。图 [8-4](#Fig4) 显示了 AttnGAN 的工作原理，以及它聚焦于不同单词的图像的不同部分。第一行显示了由不同发生器产生的图像，每个发生器产生不同尺寸的图像(从 64 `×` 64，到 128 `×` 128，到 256 `×` 256)。第二和第三行显示了前五个最受关注的单词(即，具有由每个注意力模型定义的最高值的单词)。

![../images/463582_1_En_8_Chapter/463582_1_En_8_Fig4_HTML.jpg](../images/463582_1_En_8_Chapter/463582_1_En_8_Fig4_HTML.jpg)

图 8-4

AttnGAN 如何使用文本描述的不同部分来为图像的每个区域生成细节

在我们深入研究其中一些 GAN 的实现之前，需要注意的是，当今许多 GAN 实现都是为从实值连续数据分布中生成数据(如图像)而设计的。当试图应用 GAN 来生成离散的数据序列(例如，文本、诗歌、音乐)时，许多现有的 GAN 实现将不能很好地处理它。此外，GANs 被设计为仅在已经生成整个数据序列(例如，图像)时确定损失(或对抗性损失)。

另一种有趣的干将是 SeqGAN。SeqGAN 是一种新颖的方法，用于将强化学习概念集成到 GAN 中，以克服现有 gan 在用于生成离散数据序列时面临的各种挑战。在 SeqGAN 中，生成器被设计为强化学习的代理，其当前状态是迄今为止生成的离散令牌，而动作是下一个要生成的令牌。鉴别器评估生成的令牌，并提供反馈以帮助生成器学习。SeqGAN 被证明在诗歌生成、音乐生成以及应用于语言和语音任务方面是有效的。

今天，遗传算法可以很好地解决多种类型的问题，但众所周知，它们很难训练，因为它们不能保证收敛到最优甚至稳定的解。GANs 的另一个常见问题是众所周知的模式崩溃，即生成器生成的样本变化极小。它们需要非常仔细地选择超参数和参数初始化以及其他因素，以便很好地工作。例如，在 2016 年 NIPS 对抗训练研讨会上，如何解释和解决 GANs 训练中的问题是一个主要议题(视频记录可在 [`http://bit.ly/NIPS2016`](http://bit.ly/NIPS2016) `)`观看)。不过幸运的是，许多技巧被用来稳定甘的训练。一个这样的技巧是将附加信息包括在输入空间中(例如，将连续噪声添加到鉴别器的输入)，或者将信息添加到输出空间中(例如，真实例子的不同类别)。其他技巧着眼于在训练期间引入正则化方案。

### 注意

在 [`http://bit.ly/GANsEvolve`](http://bit.ly/GANsEvolve) `.`了解甘的进化

本章介绍了一种 GANs，称为周期一致对抗网络(CycleGANs) ***。*** 我们学习如何使用 CycleGANs 进行图像到图像的翻译。通过浏览代码，我们将开始了解 GANs 以及 GANs 在您的人工智能项目中的创新应用。您可以利用微软人工智能平台来训练和部署这些 gan 到云、移动和边缘设备。

## 循环一致的敌对网络

cycle g an 是一种用于将图像从源域 X 转换到目标域 y 的新颖方法。cycle GAN 的优势之一是 GAN 的训练不需要训练数据具有匹配的图像对。如 Zhu、Park、Isola 和 Efros (2017)所述，CycleGANs 已成功应用于以下用例:

![../images/463582_1_En_8_Chapter/463582_1_En_8_Fig5_HTML.jpg](../images/463582_1_En_8_Chapter/463582_1_En_8_Fig5_HTML.jpg)

图 8-5

物体变形(马变斑马，苹果变橘子)。资料来源:朱、朴、伊索拉和欧洲综合研究中心(2017 年)。

*   把莫奈的画翻译成照片。

*   使用各种著名艺术家(莫奈、梵高、塞尚和浮世绘)风格的照片风格转换。

*   对象变形，用于改变照片中的对象类型。图 [8-5](#Fig5) 展示了 CycleGANs 如何用于物体变形(马变斑马，斑马变马，苹果变橘子，橘子变苹果等)。).

*   将照片从一个季节(例如，夏天)转换到另一个季节(例如，冬天)。

*   通过缩小景深增强照片，等等。

CycleGANs 的目标是学习如何将图像从一个域 X 映射到另一个域 Y。图 [8-6](#Fig6) 显示了两个映射函数 G 和 F 以及两个鉴别器 D <sub>X</sub> 和 D <sub>Y</sub> 的使用。鉴别器 D <sub>X</sub> 用于验证来自 X 的图像和平移的图像 F(y)。类似地，鉴别器 D <sub>Y</sub> 用于验证来自 Y 的图像和平移的图像 G(x)。使用 CycleGANs 进行图像转换的有效性背后的秘密是使用了循环一致性损失。直观地，循环一致性损失用于确定是否可以从翻译的图像中恢复来自域 X 的图像。

![../images/463582_1_En_8_Chapter/463582_1_En_8_Fig6_HTML.jpg](../images/463582_1_En_8_Chapter/463582_1_En_8_Fig6_HTML.jpg)

图 8-6

具有两个映射函数 G 和 F 以及两个对立鉴别器 D <sub>X</sub> 和 D <sub>Y</sub> 的 CycleGANs 模型

### 注意

朱等人(2017)首次提出了 CycleGANs。CycleGANs 的原始实现(PyTorch)可在 [`http://bit.ly/CycleGAN`](http://bit.ly/CycleGAN) 获得。

### CycleGAN 代码

让我们首先浏览将用于训练 CycleGAN 的整个 CycleGAN 代码，然后通过将图像从源域 A 翻译到目标域 b 来测试它。例如，经过训练的 cycle gan 将执行对象变形，并将包含一匹马的照片翻译成斑马(反之亦然)。然后结果被可视化为一个 HTML 文件。

让我们首先导入将在这段代码中使用的 Python 库。从清单 [8-1](#PC1) 中，你会看到我们正在使用 TensorFlow，并从一个`model.py`文件中导入 CycleGAN 的定义。我们将在本节的后面部分深入讨论`model.py`文件的细节。

### 注意

我们建议提供 Azure DLVM 来运行本章中的代码示例。更多信息请参见第 [4](04.html) 章。

```
PYTHON

import os

import tensorflow as tf

from model import cyclegan

Listing 8-1Importing the Required Python Libraries

```

接下来，我们定义将用于训练和测试 CycleGAN 的参数。从清单 [8-2](#PC2) 中，您会看到我们为 200 个时期指定了 0.0002 的学习率(用`lr`和`epoch_step`表示)。此外，我们还指定了用于加载训练数据、输出测试图像和存储检查点文件的目录的位置。为了能够修改 phase(即，train 或 test)的值，我们还将其指定为一个名为`phase`的属性，并为其定义了相关的 getter 或 setter。

```
PYTHON

# Define the argument class

class args:
  dataset_dir='horse2zebra'
  epoch=1
  lr=0.0002
  epoch_step=200
  batch_size=1
  train_size=1e8
  load_size=286
  fine_size=256
  ngf=64
  ndf=64
  input_nc=3
  output_nc=3
  beta1=0.5
  which_direction='AtoB'
  save_freq=1000
  print_freq=100
  continue_train=False,
  checkpoint_dir='./checkpoint'
  sample_dir='./sample'
  test_dir='./test'
  L1_lambda=10.0
  use_resnet=True
  use_lsgan=True
  max_size=50
  _phase='train'

  @property
  def phase(self):
    return type(self)._phase

  @phase.setter
  def phase(self,val):
    type(self)._phase=val

Listing 8-2Specifying the Training and Testing Arguments

```

接下来，我们在本地文件系统上创建相关的目录，这些目录将用于加载训练数据、存储输出图像和检查点文件(如清单 [8-3](#PC3) 所示)。

```
PYTHON

os.makedirs(args.checkpoint_dir, exist_ok=True)
os.makedirs(args.sample_dir, exist_ok=True)
os.makedirs(args.test_dir, exist_ok=True)

Listing 8-3Create the Directories for Output, Sample, and Checkpoint

```

我们现在准备训练 CycleGAN(如清单 [8-4](#PC4) 所示)。由于一台机器可能有多个设备(CPU 或 GPU)可用于训练，我们将`allow_soft_placement`指定为`True`。设置`allow_soft_placement`指定如果一个操作没有 GPU 实现，它将在 CPU 上运行。

接下来，我们指定`gpu_options.allow_growth`为`True`。TensorFlow 默认映射进程可用的所有 GPU 内存。这有助于减少 GPU 内存碎片。通过将`gpu_options.allow_growth`设置为`True`，该过程将仅从所需的内存开始，并在训练期间根据需要增加分配的内存。

我们现在准备开始训练 CycleGAN。创建 TensorFlow 会话后，我们调用 CycleGAN 对象的`train method`，并向其传递我们之前定义的参数，如清单 [8-4](#PC4) 所示。

```
PYTHON

tfconfig = tf.ConfigProto(allow_soft_placement=True)
tfconfig.gpu_options.allow_growth = True

with tf.Session(config=tfconfig) as sess:
  model=cyclegan(sess,args)
  model.train(args)

Listing 8-4Training the CycleGAN

```

### 注意

使用单个 Tesla K80 GPU，训练 200 个纪元的 CycleGAN 需要一段时间。如果要测试代码，应该减少历元的数量。

在本章的后面，我们将描述 CycleGAN 的体系结构。在此之前，我们先来看看训练代码。一旦 CycleGAN 的训练完成，你就可以测试 CycleGAN 了。清单 [8-5](#PC5) 展示了我们如何调用 CycleGAN 对象的`test`方法。根据清单 [8-2](#PC2) 中显示的参数，我们正在执行从域 A 到域 b 的图像转换。结果图像存储在`test`文件夹中。此外，`AtoB_index.html`、**、**的 HTML 文件被写入`test`文件夹，以使您能够查看 CycleGAN 执行翻译前后的图像。

```
PYTHON

tfconfig = tf.ConfigProto(allow_soft_placement=True)
tfconfig.gpu_options.allow_growth = True

tf.reset_default_graph()
args.phase='test'

with tf.Session(config=tfconfig) as sess:
  model=cyclegan(sess,args)
  model.test(args)

Listing 8-5Testing the CycleGAN

```

### 发生器和鉴别器的网络结构

要构建任何类型的 GAN，首先必须定义鉴别器和发生器。让我们探讨一下发生器和鉴别器的网络架构。发生器在任何 GAN 中的作用是产生图像来欺骗鉴别器。CycleGAN 发生器的网络架构改编自快速神经式传递工作(Justin，Alexandre，& Li，2016)。

发电机代码如清单 [8-6](#PC6) 所示。发生器由 9 个剩余块组成，这些剩余块将用于用 256 个`×` 256 个图像(从 g_r1 到 g_r9)进行训练。每个残差块具有两个 3 `×` 3 层，并应用了卷积、实例归一化和 ReLU。

### 注意

Zhu 等人(2017)指出，在残差块中使用实例归一化提高了图像质量。

```
PYTHON

def generator_resnet(image, options, reuse=False,
         name="generator"):

    with tf.variable_scope(name):
        # image is 256 x 256 x input_c_dim
        if reuse:
            tf.get_variable_scope().reuse_variables()
        else:
            assert tf.get_variable_scope().reuse is False

        def residual_block(x,dim,ks=3,s=1,name='res'):
            p=int((ks-1)/2)
            y=tf.pad(x, [[0,0], [p,p], [p,p], [0,0]],
              "REFLECT")

            y=instance_norm(conv2d(y,dim,ks,s,
              padding='VALID',name=name+'_c1'),name+'_bn1')

            y=tf.pad(tf.nn.relu(y), [[0,0], [p,p], [p,p],
               [0,0]],"REFLECT")

            y=instance_norm(conv2d(y,dim,ks,s,
               padding='VALID',name=name+'_c2'),name+'_bn2')

            return y+x

        # Justin Johnson's model from
        # https://github.com/jcjohnson/fast-neural-style/
        c0=tf.pad(image, [[0,0], [3,3], [3,3], [0,0]],
             "REFLECT")

        c1=tf.nn.relu(instance_norm(conv2d(c0,options.gf_dim,
           7,1,padding='VALID',name='g_e1_c'),'g_e1_bn'))

        c2=tf.nn.relu(instance_norm(conv2d(c1,options.gf_dim*2,
           3,2,name='g_e2_c'),'g_e2_bn'))

        c3=tf.nn.relu(instance_norm(conv2d(c2,options.gf_dim*4,
           3,2,name='g_e3_c'),'g_e3_bn'))

        # define G network with 9 resnet blocks
        r1=residule_block(c3,options.gf_dim*4,name='g_r1')
        r2=residule_block(r1,options.gf_dim*4,name='g_r2')
        r3=residule_block(r2,options.gf_dim*4,name='g_r3')
        r4=residule_block(r3,options.gf_dim*4,name='g_r4')
        r5=residule_block(r4,options.gf_dim*4,name='g_r5')
        r6=residule_block(r5,options.gf_dim*4,name='g_r6')
        r7=residule_block(r6,options.gf_dim*4,name='g_r7')
        r8=residule_block(r7,options.gf_dim*4,name='g_r8')
        r9=residule_block(r8,options.gf_dim*4,name='g_r9')

        d1=deconv2d(r9,options.gf_dim*2,3,2,name='g_d1_dc')
        d1=tf.nn.relu(instance_norm(d1,'g_d1_bn'))
        d2=deconv2d(d1,options.gf_dim,3,2,name='g_d2_dc')
        d2=tf.nn.relu(instance_norm(d2,'g_d2_bn'))
        d2=tf.pad(d2, [[0,0], [3,3], [3,3], [0,0]],
               "REFLECT")

        pred=tf.nn.tanh(conv2d(d2,options.output_c_dim,7,1,
                  padding='VALID',name='g_pred_c'))

        return pred

Listing 8-6
CycleGAN

generator

```

CycleGAN 的鉴别器(如清单 [8-7](#PC7) 所示)获取输入图像，并预测它是原始图像还是生成器生成的图像。

```
PYTHON

def discriminator(image, options, reuse=False, name="discriminator"):

    with tf.variable_scope(name):
        # image is 256 x 256 x input_c_dim
        if reuse:
            tf.get_variable_scope().reuse_variables()
        else:
            assert tf.get_variable_scope().reuse is False

        h0=lrelu(conv2d(image,options.df_dim,
           name='d_h0_conv'))

        # h0 is (128 x 128 x self.df_dim)
        h1=lrelu(instance_norm(conv2d(h0,options.df_dim*2,
           name='d_h1_conv'),'d_bn1'))

        # h1 is (64 x 64 x self.df_dim*2)
        h2=lrelu(instance_norm(conv2d(h1,options.df_dim*4,
           name='d_h2_conv'),'d_bn2'))

        # h2 is (32x 32 x self.df_dim*4)
        h3=lrelu(instance_norm(conv2d(h2,options.df_dim*8,s=1,
            name='d_h3_conv'),'d_bn3'))

        # h3 is (32 x 32 x self.df_dim*8)
        h4=conv2d(h3,1,s=1,name='d_h3_pred')
        # h4 is (32 x 32 x 1)
        return h4

Listing 8-7CycleGAN Discriminator

```

鉴别器由第一层组成，该层对图像应用泄漏和卷积。对于随后的三层，应用卷积、实例规范化和 ReLU。最终卷积应用于最终层(由 *h4* 表示)，产生一维输出。

### 更多信息

本章的代码基于胡小卫的工作，可在 Github 的 [`http://bit.ly/GANsCode1`](http://bit.ly/GANsCode1) 获得。Jupyter 笔记本的创建是为了让您能够快速开始运行 CycleGAN 代码。该笔记本在 [`http://bit.ly/GANsCode2`](http://bit.ly/GANsCode2) 的 Github 上有售。我们在 Azure DLVM 上测试了代码，使用了一个 Tesla K80 GPU。

### 定义 CycleGAN 类

接下来，让我们看看 CycleGAN 类。在`model.py`文件中的`Train`方法中，我们使用了批量大小为 1 的 Adam 优化器。清单 [8-8](#PC8) 展示了我们如何指定鉴别器和生成器将使用的优化器。

```
PYTHON

self.d_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \
            .minimize(self.d_loss,var_list=self.d_vars)

self.g_optim = tf.train.AdamOptimizer(self.lr, beta1=args.beta1) \
           .minimize(self.g_loss,var_list=self.g_vars)

Listing 8-8CycleGAN (model.py): Defining the Optimizer Used for the Generator and Discriminator

```

一个 CycleGAN 由两个发生器(XtoY 和 YtoX)和两个鉴别器(D <sub>X</sub> 和 D <sub>Y</sub> )组成，如图 [8-6](#Fig6) 所示。您将在`model.py`中的`_build_model`方法中看到这一点。从清单 [8-9](#PC9) 中的代码，你会看到我们如何在初始定义`generatorA2B`和`generatorB2A`时将`reuse`参数的值设置为`False`，并分别使用变量`real_A`和`fake_B`。这决定了变量是否被重用。在`generatorB2A`和`generatorA2B`的后续定义中，`reuse`的值被设置为`True`，并使用变量`real_B`和`fake_A`。两个鉴别器在`model.py`中定义，如清单 [8-10](#PC10) 所示。感兴趣的读者应该深入研究提供的代码，以了解生成器的细节。

```
PYTHON

self.DB_fake = self.discriminator(self.fake_B, self.options,
                   reuse=False,name="discriminatorB")

self.DA_fake = self.discriminator(self.fake_A, self.options,
                   reuse=False,name="discriminatorA")

Listing 8-10Defining the Two Discriminators: discriminatorB and discriminatorA

```

```
PYTHON

self.real_data = tf.placeholder(tf.float32,
                [None,self.image_size,self.image_size,
                self.input_c_dim+self.output_c_dim],
                name='real_A_and_B_images')

self.real_A = self.real_data[:, :, :, :self.input_c_dim]
self.real_B = self.real_data[:, :, :, self.input_c_dim:self.input_c_dim + self.output_c_dim]

self.fake_B = self.generator(self.real_A, self.options,
                            False,name="generatorA2B")

self.fake_A_ = self.generator(self.fake_B, self.options,
                             False,name="generatorB2A")

self.fake_A = self.generator(self.real_B, self.options,
                            True,name="generatorB2A")

self.fake_B_ = self.generator(self.fake_A, self.options,
                             True,name="generatorA2B")

Listing 8-9Defining the two generators, generatorA2B and generatorB2A

```

### 对抗性循环损失

在 GAN 的训练期间，生成器 G 生成类似于在域 Y 中找到的图像的图像 G(x)。同时，鉴别器 D <sub>Y</sub> 需要区分生成的图像 G(x)和来自 Y 的真实样本。因此，G 总是试图最小化其不利损失，而鉴别器 D 试图最大化其损失。

如 Zhu 等人(2017)所述，如果网络的容量大，映射 G 和 F 可以潜在地将来自源域 X 的输入图像映射到域 y 中的图像的任意随机排列。因此，减少可能的映射函数的空间是重要的。CycleGAN 的秘密之一是使用*周期一致性损失* *。*使用循环一致性损失背后的直觉是，学习到的映射函数应该能够将翻译后的图像恢复到其原始图像。

## 结果

在我们运行了 150 个周期的 CycleGAN 训练之后，我们运行了清单 [8-5](#PC5) 中所示的测试代码。这将 CycleGAN 模型应用于在`dataset`目录中找到的图像，并将翻译后的图像输出到`test`目录。还会生成一个 HTML 文件。这使你可以同时看到原文和译文。在提供的 Jupyter 笔记本中，代码(如清单 [8-11](#PC11) 所示)使 HTML 文件能够在笔记本单元格中被查看。

```
PYTHON

from IPython.display import HTML
HTML(filename='test/AtoB_index.html')

Listing 8-11Python Code to Visualize HTML File in the Cell

```

在图 [8-7](#Fig7) 中，我们展示了生成图像的子集。

![../images/463582_1_En_8_Chapter/463582_1_En_8_Fig7_HTML.jpg](../images/463582_1_En_8_Chapter/463582_1_En_8_Fig7_HTML.jpg)

图 8-7

CycleGAN 测试的输出(150 个周期后)

## 摘要

gan 在人工智能的创造力、音乐和艺术方面有着巨大的潜力。自 2014 年首次提出以来，GANs 的创新正以惊人的速度发展。本章描述了如何将 GANs 应用于各种用例。我们展示了发生器和鉴别器在 GAN 架构中的使用，以及如何使用它们。

接下来，我们讨论了 CycleGAN 是如何工作的，并展示了如何使用它将对象从一个域转换到另一个域。在本章给出的代码示例中，我们重点关注如何训练和测试一种新型 GAN，称为 CycleGAN。

本章中的所有代码都运行在一个 Linux DLVM 上，可以在 Azure 上找到。关于训练人工智能模型(如 GANs)选择的更多细节，如计算环境和如何进行大规模训练，将在下一章讨论。