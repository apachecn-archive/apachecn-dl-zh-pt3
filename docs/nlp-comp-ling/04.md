

# 五、Gensim——向量化文本、转换和 N 元语法

*   Gensim 简介
*   向量以及我们为什么需要它们
*   Gensim 中的矢量变换
*   N 元语法和一些预处理
*   摘要



# Gensim 简介

到目前为止，我们还没有谈到如何发现隐藏的信息——更多的是如何使我们的文本数据成形。我们将暂时离开空间来讨论向量空间和开源 Python 包 Gensim——这是因为其中一些概念将在接下来的章节中有用，我们希望在继续之前打下基础。然而，我们只是触及了 Gensim 能力的表面。本章将向你介绍在文本分析中大量使用的数据结构，包括机器学习技术- **向量** [ [1](https://en.wikipedia.org/wiki/Euclidean_vector) 。

这意味着我们仍处于预处理领域，并为进一步的机器学习分析准备好数据。这看起来有点过头了，只关注设置我们的文本/数据，但是就像我们之前说过的——垃圾进，垃圾出。虽然前一章主要涉及文本清理，但我们将在本章讨论将我们的文本表示转换为数字表示，特别是从字符串转换为向量。

当我们在本章中讨论表示和转换时，我们将探索将字符串表示为向量的不同方式，例如词袋、 **TF-IDF** ( **词频-逆文档频率**)、 **LSI** ( **潜在语义索引**，以及最近更流行的 word2vec。我们将很快在 *Vectors 中解释这些方法，以及为什么我们需要它们* 部分，其余的在[第 8 章](02e3238f-db9c-4d6e-ba96-70ba9db11dc0.xhtml)、*主题模型* ( **主题建模**与 Gensim)和[第 12 章](020dd724-0a11-4ba3-9a7c-050df9dfa1e4.xhtml)、 *Word2Vec、Doc2Vec 和 Gensim* 部分，Gensim 包括完成上述所有工作的方法。转换后的向量可以轻松地插入 scikit-learn 机器学习方法。Gensim 最初是 Radim Rehurek 的一个小项目，主要是他的博士论文[ [17](https://radimrehurek.com/phd_rehurek.pdf) ]、*自然语言处理中语义分析的可扩展性*、 [2](https://radimrehurek.com/phd_rehurek.pdf) 的讨论。其主要算法包括**潜在狄利克雷分配** [ [3](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) ] ( **LDA** )和**潜在语义分析** [ [4](https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing) ]的新颖实现，以及 TF-IDF 和**随机投影** [ [5](https://en.wikipedia.org/wiki/Random_projection) 的实现。从那以后，它已经发展成为最大的 NLP/信息检索 Python 库之一，并且与以前用于语义建模的大量学术代码(例如，*Stanford Topic modeling Toolkit*[[6](https://nlp.stanford.edu/software/tmt/tmt-0.4/))相比，它既节省内存又可伸缩。

Gensim 设法具有可伸缩性，因为它使用 Python 的内置生成器和迭代器来处理流数据，所以数据集实际上从未完全加载到 RAM 中。大多数红外算法涉及矩阵分解-这涉及矩阵乘法。这是由 numpy 执行的，numpy 进一步构建在 FORTRAN/C 之上，并针对数学运算进行了高度优化。由于所有繁重的工作都交给了这些底层的 BLAS 库，Gensim 提供了 Python 的易用性和 c 的强大功能。

Gensim 的主要特性是其独立于内存的特性、潜在语义分析的多核实现、潜在狄利克雷分配、随机投影、**分层狄利克雷处理**(**)和 **word2vec 深度学习**，以及在计算机集群上使用 LSA 和 LDA 的能力。它还无缝地插入到 Python 科学计算生态系统中，并可以用其他向量空间算法进行扩展。Gensim 的 Jupyter 笔记本目录[ [7](https://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks) ]是一个重要的文档来源，其中的教程涵盖了 Gensim 提供的大部分内容。Jupyter 笔记本是在实时服务器上运行代码的一种有用方式——文档页面[ [8](http://jupyter-notebook.readthedocs.io/en/stable/notebook.html) ]值得一看！**

 **教程页面可以帮助您开始使用 Gensim，但接下来的部分也将描述如何开始使用 Gensim，以及 vectors 在我们剩余的时间探索机器学习和文本处理中将发挥多么重要的作用。** **

# 向量以及我们为什么需要它们

我们现在转向文本分析的机器学习部分——这意味着我们现在将开始少玩一点文字，多玩一点数字。甚至当我们使用 spaCy 时，例如，词性标注和 NER 标注是通过统计模型完成的——但内部工作对我们来说很大程度上是隐藏的——我们传递了 Unicode 文本，经过一些魔法，我们有了带注释的文本。

然而，对于 Gensim，我们希望将向量作为输入传递给 IR 算法(如 LDA 或 LSI)，这主要是因为底层发生的是涉及矩阵的数学运算。这意味着我们必须将之前的字符串表示为向量——这种表示或模型被称为**向量空间模型** [ [9](https://en.wikipedia.org/wiki/Vector_space_model) 。

从数学的角度来看，向量是一个有大小和方向的几何对象。我们不需要太关注这一点，而是将向量视为一种将单词投影到数学空间的方式，同时保留这些单词提供的信息。

机器学习算法使用这些向量来进行预测。我们可以将机器学习理解为一套统计算法以及对这些算法的研究。这些算法的目的是通过减少预测误差从提供的数据中学习。因此，这是一个广泛的领域-我们将解释特定的机器学习算法，然后它们会出现。

同时让我们讨论一下这些表现的几种形式。



# 词汇袋

**单词袋**模型可以说是将句子表示为向量的最直接的形式。让我们从一个例子开始:

```py
S1:"The dog sat by the mat."
S2:"The cat loves the dog."
```

如果我们遵循我们在[第 3 章](be6f4ff3-1217-474a-8923-5eeeed17bfa1.xhtml)、 *spaCy 的语言模型*的*语言模型*部分中所做的相同的预处理步骤，我们将以下面的句子结束:

```py
S1:"dog sat mat."
S2:"cat love dog."
```

如 Python 列表所示，这些现在看起来像这样:

```py
S1:['dog', 'sat', 'mat']
S2:['cat', 'love', 'dog']
```

如果我们想把它表示成一个向量，我们需要首先构建我们的词汇，也就是在句子中找到的独特的单词。我们的词汇向量现在如下:

```py
Vocab = ['dog', 'sat', 'mat', 'love', 'cat']

```

这意味着我们对句子的表达也是长度为 5 的向量，我们也可以说我们的向量有 5 个维度。我们还可以考虑将我们词汇中的每个单词映射到一个数字(或索引)，在这种情况下，我们也可以将我们的词汇称为字典。

单词袋模型包括使用词频来构建我们的向量。我们现在的句子会是什么样子？

```py
S1:[1, 1, 1, 0, 0]
S2:[1, 0, 0, 1, 1]
```

这很容易理解——词汇表中的第一个单词`dog`出现了 1 次，第一句话中的`love`出现了 0 次，因此根据词频给适当的索引赋值。如果第一句话中出现了两次单词`dog`，它将被表示为:

```py
S1: [2, 1, 1, 0, 0]
```

这只是一个单词包表示背后的想法的例子 Gensim 处理单词包的方式略有不同，我们将在下一节中看到这一点。我们必须记住的单词袋模型的一个重要特征是，它是一种无序的文档表示——只有单词的数量才是重要的。我们可以在上面的例子中看到，通过查看生成的句子向量，我们不知道哪个单词先出现。这导致空间信息的丢失，进而导致语义信息的丢失。然而，在许多信息检索算法中，单词的顺序并不重要，只要单词出现的次数就足够了。

单词袋模型可以用于垃圾邮件过滤的一个例子是，被标记为垃圾邮件的电子邮件很可能包含与垃圾邮件相关的单词，如*购买*、*金钱*和*股票*。通过将电子邮件中的文本转换成单词袋模型，我们可以使用**贝叶斯概率** [10]来确定一封邮件是否更有可能出现在垃圾邮件文件夹中。这很有效，因为就像我们之前讨论的那样，在这种情况下，单词的顺序并不重要，重要的是它们是否存在于邮件中。



# TF-IDF

TF-IDF 是术语频率-逆文档频率的缩写。在搜索引擎中大量使用它来基于查询找到相关的文档，这是一种将我们的句子转换成向量的相当直观的方法。

顾名思义，TF-IDF 试图对两种不同的信息进行编码——术语频率和逆文档频率。**词频** ( **TF** )是一个词在文档中出现的次数。

IDF 帮助我们理解文档中某个单词的重要性。通过计算包含该词的文档的对数比例倒数(通过将文档总数除以包含该词的文档数获得)，然后取该商的对数，我们可以测量该词在所有文档中的常见或罕见程度。

万一前面的解释不太清楚，把它们表达成公式会有帮助！

*TF(t) =(术语 t 在文档中出现的次数)/(文档中的总术语数)*

*IDF(t) = log_e(文档总数/包含术语 t 的文档数)*

TF-IDF 就是这两个因素的简单产物——TF 和 IDF。它将更多的信息封装到向量表示中，而不是像单词袋向量表示那样只使用单词的计数。TF-IDF 把生僻字做得更突出，忽略了常见的字，比如*是*、的*、那个*的*，这些字可能会出现很多次，但重要性不大。*

有关 TF-IDF 如何工作的更多信息，尤其是 TF-IDF 的数学本质和已解决的示例，TF-IDF 上的维基百科页面[ [11](https://en.wikipedia.org/wiki/Tfidf) ]是一个很好的资源。



# 其他陈述

扩展这些表示是可能的——事实上，我们将在后面探讨的主题模型就是这样一个例子。单词向量也是单词的一种有趣的表示，我们训练了一个浅层神经网络(具有 1 或 2 层的神经网络)来将单词描述为向量，其中每个特征都是单词的语义解码。我们将用整整一章来讨论单词向量，尤其是 Word2Vec。为了体验一下词向量的作用，这篇名为*词向量的惊人力量* [ [12](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/) ]的博客是一个好的开始。



# Gensim 中的矢量变换

现在我们知道了什么是矢量变换，让我们习惯于创建和使用它们。我们将使用 Gensim 执行这些转换，但是甚至可以使用 scikit-learn。稍后我们还将看一看 scikit-learn 的方法。

让我们现在创建我们的语料库。我们之前讨论过，语料库是文档的集合。在我们的例子中，每个文档只有一个句子，但是在我们将要处理的大多数真实世界的例子中，情况显然不是这样。我们还应该注意，一旦我们完成了预处理，我们就去掉了所有的标点符号——至于我们的向量表示，每个文档只是一个*句子*。

当然，在我们开始之前，一定要安装 Gensim。与 spaCy 一样，pip 或 conda 是根据您的工作环境实现这一目标的最佳方式。

```py
from gensim import corpora

documents = [u"Football club Arsenal defeat local rivals this weekend.", u"Weekend football frenzy takes over London.", u"Bank open for takeover bids after losing millions.", u"London football clubs bid to move to Wembley stadium.", u"Arsenal bid 50 million pounds for striker Kane.", u"Financial troubles result in loss of millions for bank.", u"Western bank files for bankruptcy after financial losses.", u"London football club is taken over by oil millionaire from Russia.", u"Banking on finances not working for Russia."]
```

只需注意——我们确保所有的字符串都是 Unicode 字符串，这样我们就可以使用 spaCy 进行预处理。

```py
import spacy
nlp = spacy.load("en")
texts = []
for document in documents:
    text = []
    doc = nlp(document)
    for w in doc:
        if not w.is_stop and not w.is_punct and not w.like_num:
            text.append(w.lemma_)
    texts.append(text)
print(texts)
```

当我们引入 spaCy 时，我们执行了非常相似的预处理。我们的文件现在是什么样的？

```py
[[u'football', u'club', u'arsenal', u'defeat', u'local', u'rival', u'weekend'],
[u'weekend', u'football', u'frenzy', u'take', u'london'],
[u'bank', u'open', u'bid', u'lose', u'million'],
[u'london', u'football', u'club', u'bid', u'wembley', u'stadium'],
[u'arsenal', u'bid', u'pound', u'striker', u'kane'],
[u'financial', u'trouble', u'result', u'loss', u'million', u'bank'],
[u'western', u'bank', u'file', u'bankruptcy', u'financial', u'loss'],
[u'london', u'football', u'club', u'take', u'oil', u'millionaire', u'russia'],
[u'bank', u'finance', u'work', u'russia']]
```

让我们首先为我们的小型语料库创建一个单词袋表示法。Gensim 通过它的`dictionary` 类让我们非常方便地做到这一点。

```py
dictionary = corpora.Dictionary(texts)
print(dictionary.token2id)

{u'pound': 17, u'financial': 22, u'kane': 18, u'arsenal': 3, u'oil': 27, u'london': 7, u'result': 23, u'file': 25, u'open': 12, u'bankruptcy': 26, u'take': 9, u'stadium': 16, u'wembley': 15, u'local': 4, u'defeat': 5, u'football': 2, u'finance': 31, u'club': 0, u'bid': 10, u'million': 11, u'striker': 19, u'frenzy': 8, u'western': 24, u'trouble': 21, u'weekend': 6, u'bank': 13, u'loss': 20, u'rival': 1, u'work': 30, u'millionaire': 29, u'lose': 14, u'russia': 28}
```

我们的语料库中有 32 个独特的单词，所有这些单词都在我们的字典中表示，每个单词都分配有一个索引值。此后，当我们提到一个单词的 word_id 时，这意味着我们正在讨论由字典生成的单词整数 id 映射。
我们将使用`doc2bow` 方法，顾名思义，该方法有助于将我们的文档转换为单词袋。

```py
corpus = [dictionary.doc2bow(text) for text in texts]
```

如果我们打印我们的语料库，我们将有我们所使用的文档的单词包表示。

```py
[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], 
[(2, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(10, 1), (11, 1), (12, 1), (13, 1), (14, 1)], 
[(0, 1), (2, 1), (7, 1), (10, 1), (15, 1), (16, 1)], [(3, 1), (10, 1), (17, 1), (18, 1), (19, 1)], 
[(11, 1), (13, 1), (20, 1), (21, 1), (22, 1), (23, 1)], 
[(13, 1), (20, 1), (22, 1), (24, 1), (25, 1), (26, 1)], 
[(0, 1), (2, 1), (7, 1), (9, 1), (27, 1), (28, 1), (29, 1)], [(13, 1), (28, 1), (30, 1), (31, 1)]]
```

这是一个列表的列表，其中每个单独的列表代表一个文档的单词包表示。提醒:您可能会在列表中看到不同的数字，这是因为每次创建字典时，都会出现不同的映射。与我们演示的例子不同，其中缺少一个单词是一个`0`，，我们使用元组来表示(word_id，word_count)。我们可以通过检查原始句子、将每个单词映射到其整数 ID 并重建我们的列表来轻松验证这一点。我们还可以注意到，在这种情况下，每个文档中的每个单词都不超过一个计数——在较小的语料库中，这种情况往往会发生。

瞧啊。我们的语料库已经组装好了，我们已经准备好随时对它们施展机器学习/信息检索魔法。但是在我们咬紧牙关之前...让我们多花点时间了解一些关于尸体的细节。

我们之前提到过 Gensim 的强大之处，因为它使用了流式语料库。但是在这种情况下，整个列表被加载到 RAM 中。这对我们来说并不麻烦，因为这是一个玩具示例，但是在任何真实世界的情况下，这可能会引起问题。我们要如何度过这一关？

我们可以从存储语料库开始，一旦它被创建，就存储到磁盘。实现此目的的一种方法如下:

```py
corpora.MmCorpus.serialize('/tmp/example.mm', corpus)
```

通过将语料库存储到磁盘，然后再从磁盘加载，我们的内存效率大大提高，因为一次最多只有一个向量驻留在 RAM 中。关于语料库和向量空间的 Gensim 教程[ [13](https://radimrehurek.com/gensim/tut1.html) ]比我们到目前为止讨论的内容多一点，可能对一些读者有用。

例如，用 Gensim 将一包单词表示转换成 TF-IDF 也非常容易。我们首先从 Gensim 模型目录中选择我们想要的模型/表示。

```py
from gensim import models
tfidf = models.TfidfModel(corpus)
```

这意味着`tfidf` 现在在我们的语料库上代表一个 TF-IDF 表*训练过的*。注意，在 TFIDF 的情况下，*训练*只需遍历一次提供的语料库，并计算其所有特征的文档频率。训练其他模型，如潜在语义分析或潜在狄利克雷分配，要复杂得多，因此需要更多的时间。我们将在主题建模的章节中探讨这些转换。同样重要的是要注意，所有这样的向量转换都需要相同的输入特征空间——这意味着相同的字典(当然还有词汇表)。

那么，我们语料库的 TF-IDF 表示是什么样子的呢？我们要做的就是这个:

```py
for document in tfidf[corpus]:
    print(document)  
```

这为我们提供了以下信息:

```py
[(0, 0.24046829370585293), (1, 0.48093658741170586), (2, 0.17749938483254057), (3, 0.3292179861221232), (4, 0.48093658741170586), (5, 0.48093658741170586), (6, 0.3292179861221232)] [(2, 0.24212967666975266), (6, 0.4490913847888623), (7, 0.32802654645398593), (8, 0.6560530929079719), (9, 0.4490913847888623)] [(10, 0.29592528218102643), (11, 0.4051424990000138), (12, 0.5918505643620529), (13, 0.2184344336379748), (14, 0.5918505643620529)] [(0, 0.29431054749542984), (2, 0.21724253258131512), (7, 0.29431054749542984), (10, 0.29431054749542984), (15, 0.5886210949908597), (16, 0.5886210949908597)] [(3, 0.354982288765831), (10, 0.25928712547209604), (17, 0.5185742509441921), (18, 0.5185742509441921), (19, 0.5185742509441921)] [(11, 0.3637247180792822), (13, 0.19610384738673725), (20, 0.3637247180792822), (21, 0.5313455887718271), (22, 0.3637247180792822), (23, 0.5313455887718271)] [(13, 0.18286519950508276), (20, 0.3391702611796705), (22, 0.3391702611796705), (24, 0.4954753228542582), (25, 0.4954753228542582), (26, 0.4954753228542582)] [(0, 0.2645025265769199), (2, 0.1952400253294319), (7, 0.2645025265769199), (9, 0.3621225392416359), (27, 0.5290050531538398), (28, 0.3621225392416359), (29, 0.5290050531538398)] [(13, 0.22867660961662029), (28, 0.4241392327204109), (30, 0.6196018558242014), (31, 0.6196018558242014)]

```

如果你还记得我们所说的 TF-IDF，你将能够识别每个 word_id **-** 旁边的浮动，它是特定单词的 TF 和 IDF 分数的乘积，而不仅仅是之前出现的单词计数。分数越高，文档中的单词越重要。

我们也可以使用这个表示作为 ML 算法的输入，并且我们还可以通过对它们执行另一个转换来进一步链接这些向量表示。

让我们转到一个小的，但有趣的(和有用的！)文本分析的一部分——二字格和 n 字格。



# N 元语法和一些预处理

处理文本数据时，上下文可能非常重要。正如我们之前讨论的，在向量表示中，我们有时会丢失上下文，只知道每个单词的计数。N-grams ，尤其是 **bi-grams** 将帮助我们解决这个问题，至少在某种程度上是这样。

一个 n-gram 是文本中一系列连续的 *n* 项。在我们的例子中，我们将处理作为*项*的单词，但是根据用例的不同，它甚至可以是字母、音节，或者有时是语音中的音素。当 *n = 2* 时是一个双字母。

在文本中计算二元模型的一种方法是通过计算由前面的记号给出的记号的条件概率。它也可以通过选择相邻出现的单词来计算，但是使用更有可能成对出现的双词对我们来说更有用。这样的双字母组合叫做搭配。这意味着我们试图找到更有可能出现在彼此周围的成对单词。例如，*纽约州*或*机器学习*可能是由二元语法创造的两对可能的单词。换句话说，基于训练数据(通常是语料库)，我们识别出单词 *York* 跟随单词 *New* 的概率很高，并且值得将 *New York* 视为一个身份。在对我们的语料库运行二元模型之前，我们必须小心地去除停用词，因为可能会形成无意义的二元模型。Gensim 二元模型基本上是搭配识别的实现。

我们可以清楚地看到这是如何有用的——我们现在可以从我们的语料库中提取短语，并且 *New York* 无疑比单独的单词 *New* 和 *York* 为我们提供了更多的信息。这意味着它可以被添加到我们的预处理管道中。

Gensim 通过简单地用下划线组合两个高概率记号来处理二元模型。代币*新*和*纽约*现在将改为*纽约*。类似于 TF-IDF 模型，可以使用另一个 Gensim 模型- `Phrases`创建二元模型。

```py
import gensim
bigram = gensim.models.Phrases(texts)
```

现在，我们的语料库有了一个经过训练的二元模型。我们可以像使用 TF-IDF 一样对文本执行我们的*转换*。我们像这样重建我们的语料库:

```py
texts = [bigram[line] for line in texts]
```

现在每一行都创建了所有可能的二元模型。应该注意的是，在我们的玩具示例中，我们不会创建二元模型或无意义的二元模型。要查看二元模型提供有用信息的示例，我写的关于主题建模的 Jupyter notebook [ [14](https://github.com/bhargavvader/personal/tree/master/notebooks/text_analysis_tutorial) ]很有用。

因为通过创建新短语，我们可以将单词添加到字典中，所以这一步必须在我们创建字典之前完成。我们必须运行这个:

```py
dictionary = corpora.Dictionary(texts)

corpus = [dictionary.doc2bow(text) for text in texts]
```

创建完二元语法后，我们可以通过在语料库中多次运行短语模型来创建三元语法和其他 N 元语法。二元语法仍然是最常用的 N 元语法模型，尽管浏览一下 N 元语法实现的其他用途和种类是值得的。同样，维基百科页面[ [15](https://en.wikipedia.org/wiki/N-gram) ]是一个很好的介绍资源。

这就把我们带到了本书所涉及的预处理技术的结尾。但是必须注意，没有一个完美的预处理管道或规则集——它很大程度上取决于我们的用例、我们正在处理的数据类型以及我们希望保留(或丢失)什么样的信息！).

例如，一种流行的预处理技术包括去除高频和低频单词。我们可以在 Gensim 中使用`dictionary` 模块来完成这项工作。假设我们想要去掉在少于 20 个文档中出现的单词，或者在多于 50%的文档中出现的单词，我们将添加以下内容:

```py
dictionary.filter_extremes(no_below=20, no_above=0.5)
```

我们还可以删除最常用的令牌或删除某些令牌 id。你可以参考文档[ [16](https://radimrehurek.com/gensim/corpora/dictionary.html) 来查看`dictionary` 类可以提供给我们的预处理工具的完整范围。

通常情况下，在多次重复预处理和运行我们的算法之后，我们才能找到我们希望使用的正确的预处理技术。对我们来说，重要的是知道有什么样的工具可以做到这一点，以及做这一切背后的原因是什么。

我们现在配备了 Gensim 和 scikit-learns 算法开始工作所需的一切。



# 摘要

在这一章中，我们已经看到了为什么将文本的表示方式从单词改为数字是有意义的，以及为什么这是计算机唯一能理解的语言。计算机可以有不同的方式解释单词，TF-IDF 和单词袋就是两种这样的向量表示。Gensim 是一个 Python 包，它为我们提供了生成这种向量表示的方法，这些向量表示后来被用作各种机器学习和信息检索算法的输入。

还有进一步的预处理技术，如创建 N 元语法，搭配和删除低频词，这可以帮助我们达到更好的结果。向量的概念构成了自然语言处理的基础，我们现在可以回到使用 spaCy 的管道；的确，[第五章](e7fc28c4-94a9-41aa-8836-ad4c9a55e880.xhtml)、*词性标注及其应用*、[第六章](6aa68517-4c79-4d41-8cc2-04b73b85e6f0.xhtml)、 *NER 标注及其应用*，以及[第七章](bdd3d124-b0be-4848-8a54-a8913ab8214e.xhtml)、*依存解析*，都展示了 spaCy 的强大，我们将从使用 spaCy 的词性标注算法开始。



# 参考

[1]矢量:
[https://en.wikipedia.org/wiki/Euclidean_vector](https://en.wikipedia.org/wiki/Euclidean_vector)

[2]自然语言处理中语义分析的可扩展性:
[https://radimrehurek.com/phd_rehurek.pdf](https://radimrehurek.com/phd_rehurek.pdf)

[3]潜狄利克雷分配:
[https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)

[4]潜在语义索引:
[https://en . Wikipedia . org/wiki/Latent _ semantic _ analysis # Latent _ semantic _ indexing](https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing)

[5]随机投影:
[https://en.wikipedia.org/wiki/Random_projection](https://en.wikipedia.org/wiki/Random_projection)

[6]斯坦福 TMT:
[https://nlp.stanford.edu/software/tmt/tmt-0.4/](https://nlp.stanford.edu/software/tmt/tmt-0.4/)

[7] Gensim 笔记本:
[https://github . com/RaRe-Technologies/Gensim/tree/develop/docs/notebooks](https://github.com/RaRe-Technologies/gensim/tree/develop/docs/notebooks)

[8] Jupyter Notebooks: 
[http://jupyter-notebook.readthedocs.io/en/stable/notebook.html](http://jupyter-notebook.readthedocs.io/en/stable/notebook.html)

[9]向量空间模型:
[https://en.wikipedia.org/wiki/Vector_space_model](https://en.wikipedia.org/wiki/Vector_space_model)

[10]贝叶斯概率:
[https://en.wikipedia.org/wiki/Bayesian_probability](https://en.wikipedia.org/wiki/Bayesian_probability)

[11]TF-IDF:
[https://en.wikipedia.org/wiki/Tf-idf](https://en.wikipedia.org/wiki/Tf-idf)

【12】词语向量的惊人力量:
[https://blog . acolyer . org/2016/04/21/The-Amazing-power-of-word-vectors/](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)

[13]语料库与向量空间:
[https://radimrehurek.com/gensim/tut1.html](https://radimrehurek.com/gensim/tut1.html)

[14] Bi-Gram 示例笔记本:
[https://github . com/bhargavader/personal/tree/master/notebooks/text _ analysis _ tutorial](https://github.com/bhargavvader/personal/tree/master/notebooks/text_analysis_tutorial)

[15]N-grams:
[https://en.wikipedia.org/wiki/N-gram](https://en.wikipedia.org/wiki/N-gram)

[16] Gensim 词典:
[https://radimrehurek.com/gensim/corpora/dictionary.html](https://radimrehurek.com/gensim/corpora/dictionary.html)

[17]自然语言处理中语义分析的可扩展性:
[https://radimrehurek.com/phd_rehurek.pdf](https://radimrehurek.com/phd_rehurek.pdf)**