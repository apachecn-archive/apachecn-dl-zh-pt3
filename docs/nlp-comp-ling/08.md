

# 八、主题模型

到目前为止，我们一直在处理计算语言学算法和空间，我们知道如何使用这些计算语言学算法来注释我们的数据，以及理解句子结构。虽然这些算法帮助我们理解了文本的细节，但我们仍然没有获得数据的全貌——什么样的词在我们的语料库中出现得比其他词更频繁？我们能把数据分组或找到潜在的主题吗？我们将试图回答这些问题以及本章中更多的问题。以下是我们将在本章中涉及的主题:

*   什么是主题模型？
*   Gensim 中的主题模型
*   scikit-learn 中的主题模型



# 什么是主题模型？

我们现在将首次涉足概率模型和文本的机器学习。当然，我们之前确实遇到过这样的模型(在[第 5 章](e7fc28c4-94a9-41aa-8836-ad4c9a55e880.xhtml)、*词性标注及其应用*、[第 6 章](6aa68517-4c79-4d41-8cc2-04b73b85e6f0.xhtml)、 *NER 标注及其应用*和[第 7 章](bdd3d124-b0be-4848-8a54-a8913ab8214e.xhtml)、*依存解析*)，特别是以*的方式*我们训练了我们的 NER 和词性标注者，但是我们在前面几章的目标不是提出一个涉及我们文本数据的统计模型。

什么是主题模型？顾名思义，它是一个概率模型，包含文本中主题的信息。我们现在必须问一问*主题*到底是什么——我们可以将主题理解为一个主题，或者文本中所表达的潜在思想。例如，如果我们正在处理报纸文章的语料库，可能的主题将是*天气*、*政治*、*体育*等等。

为什么这样的主题模型在文本处理领域如此重要？传统上，信息检索和搜索技术涉及使用单词来识别相似性或相关性-现在，我们可以用主题而不是单词来更广泛地搜索和排列我们的文件。但是到底什么是话题呢？它们是单词的分布，特别是单词的概率分布。我们可以进一步使用这个模型将我们的文档描述为主题的概率分布。因为我们知道文档中的单词和字数，所以我们可以使用这些知识来生成这些主题模型。一旦我们有了主题模型，我们就可以开始将所有文档表示为主题分布了！

因此，这意味着在我们之前讨论的报纸语料库中，我们现在可以根据主题进行聚类，而不是基于 **TF-IDF** 或**词汇袋**进行聚类。我们还可以浏览每个主题中的文档，并进一步放大这些文档以更好地理解主题。当我们想要探索我们的数据集时，为您的文本语料库创建主题模型也是有用的，通过观察主题来查看我们的语料库包含什么类型的文档。

通过按时间顺序排列我们的文档，我们可以进一步看到一个主题中的文档是如何随时间演变的。为什么这很有趣，或者有用？当研究期刊《科学》中按时间排列的文档在记住时间戳的情况下进行主题建模时(一种称为**动态主题建模**的技术)，结果尤其令人着迷。

我们与**原子物理学**联系在一起的话题始于 1881 年，有很高的几率找到单词*物质*、*运动*、*光*。到了 1999 年，同一话题下的这些词很快就变成了*态*、*能*、*电子*！

你可以看到我们是如何使用一个考虑了时间戳的主题模型来观察一个主题词是如何随着时间的推移而演变的——主题模型让我们以一种前所未有的方式来看待和理解我们的数据。

然而，我们必须记住，一个主题仅仅是单词的概率分布，并不创造它自己的标签或标题。例如，我们在报纸语料库中称之为*天气*话题的话题将仅仅是单词的集合(例如*太阳*、*温度*、*风*、*风暴*和*预报*)，以及这些单词在该话题中出现的关联概率。像*天气*主题这样的主题将包含我们之前提到的单词，这些单词出现在该主题上的概率很高。通过根据概率排列单词，我们可以了解主题所代表的内容。当然，在我们的代码中，这些主题将被简单地称为主题 0、主题 1、主题 2...主题 *n-1* ，其中 *n* 是我们希望在语料库中识别的主题总数。在这一点上，人类只需简单地将他们喜欢的任何主题标签分配给概率分布集合。

既然我们用文档而不是单词来表示主题，我们就有效地减少了数据(文档或文章)的维度，从总的词汇大小减少到主题的数量。事实上，最早的信息检索算法之一，潜在语义分析[ [1](https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing) ]无意中做了很多这样的工作，通过降低维度，我们得到了语料库中主题的表示。

关于什么是主题模型，我们已经讨论了很多——但是如何生成它们呢？有不止一种方法可以做到这一点，我们将使用 Gensim [ [2](https://radimrehurek.com/gensim/) ]来创建我们的模型，它实现了**潜在狄利克雷分配** ( **LDA** )、**潜在语义分析** ( **LSA** )、**分层狄利克雷过程** ( **HDP** )和**动态主题建模** ( **DTM** )来帮助我们所有这些算法都有一些共同点——它们假设文档中的单词具有潜在的概率分布，并试图找出这些分布。这些分布最终成为我们的主题。我们试图识别这些分布的方式(使用数学和统计技术)是这些算法的不同之处。

至于这些主题模型的数学基础，这超出了本书的范围，但是 Blei 等人描述 LDA[[3](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)的论文是一个很好的读物。埃德温·陈(Edwin Chen)的博客文章[ [4](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/) ]提供了一种更为随意的理解方式。这篇 Quora 文章[ [5](https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation) ]也有一个很好的 LDA 解释库，这可能需要一点数学背景。Blei 的这篇名为*概率主题模型* [ [6](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf) 的论文也是一个很好的资源，它总结了迄今为止已经开发的各种主题模型。



# Gensim 中的主题模型

Gensim [ [2](https://radimrehurek.com/gensim/index.html) ]可以说是免费提供的最受欢迎的主题建模工具包，它使用 Python 意味着它正好适合我们的生态系统。Gensim 的流行是因为它有各种各样的主题建模算法、简单明了的 API 和活跃的社区。当然，我们之前已经介绍过 Gensim，在[第 4 章](ad7d7774-1bef-4a55-ae7c-0d77097a43b7.xhtml)、 *Gensim -矢量化文本和转换以及向量空间上的 n-grams* 。我们需要知道如何为我们将使用的主题建模算法建立我们的语料库，所以现在是一个很好的时间来温习 Gensim 部分的*矢量转换的内容，在[第 4 章](ad7d7774-1bef-4a55-ae7c-0d77097a43b7.xhtml)、 *Gensim -矢量化文本和转换以及 n-grams* 。*

完成了吗？现在我们可以开始使用 Gensim 提供的强大工具了。Jupyter notebook [ [7](https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb) ]运行我们之前讨论过的相同的语料库生成技术，并加载 Lee Newspaper 语料库，该语料库可在 Gensim 代码库中找到。笔记本将附在本章末尾。笔记本中的代码在 Python 2.7 中是为了适应所有用户，在 Python 3 中也可以使用。该语料库包含 2000-2001 年约 300 篇文献的标题文本。

关于这个语料库的更多信息可以在这篇研究论文中找到，*文本文档相似性模型的实证评估* [ [8](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF) 。

这个语料库将有助于说明主题模型是如何工作的，因为它足够大，我们可以有连贯的主题，但也不会太大，以至于需要很长的训练时间。

我们不会过多地关注“如何”,而是更多地关注“是什么”,尽管我们非常鼓励您看看在引擎盖下发生了什么，我们将在描述如何使用这些算法时链接到相关的阅读材料。我们选择忽略*如何*的原因是因为 Gensim 为我们很好地抽象了它；另外，因为解释结果也很有挑战性——这就是主题建模的*和*。

为了提醒我们正在处理的数据类型，让我们看看`texts`和`corpus`是什么样子的。这将是在 Jupyter 笔记本中的第 8 个^(和第 9 个^(单元之后。))

```py
texts[1][0:10] 
[u'indian', 
 u'security_force', 
 u'shoot_dead', 
 u'suspect', 
 u'militant', 
 u'night', 
 u'long', 
 u'encounter', 
 u'southern', 
 u'kashmir']

corpus[1][0:10]
[(51, 1), 
 (53, 1), 
 (95, 1), 
 (108, 1), 
 (109, 3), 
 (110, 2), 
 (111, 1), 
 (112, 1), 
 (113, 4), 
 (114, 1)]
```

文本包含原始文本数据的标记化和清理版本，语料库是我们的单词表示包，我们将把它输入到机器学习算法中。



# 潜在狄利克雷分配

让我们从最流行的主题建模算法开始——潜在狄利克雷分配，或者我们之前称之为 LDA。LDA 模型由 Blei 等人于 2003 年创建，并在论文*Latent Dirichlet Allocation*【3】中进行了描述。

就像我们之前讨论的，LDA 帮助我们基于主题分布对语料库建模，主题分布又由词分布组成。到底什么是词的分布？Gensim 让我们很容易理解和使用这一点。

Jupyter 笔记本的 15 号和 16 号单元格让你看到这个。

```py
ldamodel = LdaModel(corpus=corpus, num_topics=10, id2word=dictionary)
```

这就是创建模型的简单之处——只需指定语料库、字典映射和我们希望在模型中使用的主题数量。

请记住，我们在第一个单元格中从`gensim.models`导入了`LdaModel`。

现在我们有了一个训练好的模型，让我们来看看我们的数据集中隐藏了哪些主题。

```py
ldamodel.show_topics()
```

这为我们提供了以下信息:

```py
[(0, 
  u'0.006*"force" + 0.006*"year" + 0.005*"australian" + 0.004*"new" + 0.004*"afghanistan" + 0.004*"people" + 0.004*"official" + 0.004*"area" + 0.004*"fire" + 0.004*"day"'), 
 (1, 
  u'0.005*"attack" + 0.005*"people" + 0.004*"man" + 0.004*"group" + 0.004*"report" + 0.004*"company" + 0.003*"australia" + 0.003*"force" + 0.003*"kill" + 0.003*"come"'), 
 (2, 
  u'0.009*"australia" + 0.005*"australian" + 0.005*"government" + 0.004*"day" + 0.003*"new" + 0.003*"united_states" + 0.003*"child" + 0.003*"come" + 0.003*"report" + 0.003*"good"'), 
 (3, 
  u'0.005*"day" + 0.005*"people" + 0.004*"police" + 0.004*"australian" + 0.004*"australia" + 0.003*"today" + 0.003*"test" + 0.003*"palestinian" + 0.003*"attack" + 0.003*"centre"'), 
 (4, 
  u'0.008*"australian" + 0.005*"fire" + 0.005*"year" + 0.005*"government" + 0.005*"people" + 0.004*"union" + 0.004*"south" + 0.004*"centre" + 0.003*"company" + 0.003*"day"'), 
 (5, 
  u'0.008*"israeli" + 0.006*"palestinian" + 0.005*"force" + 0.004*"fire" + 0.004*"people" + 0.004*"kill" + 0.004*"government" + 0.004*"police" + 0.004*"day" + 0.004*"australia"'), 
 (6, 
  u'0.008*"australian" + 0.007*"year" + 0.006*"world" + 0.005*"australia" + 0.005*"force" + 0.004*"government" + 0.004*"people" + 0.003*"economy" + 0.003*"metre" + 0.003*"win"'), 
 (7, 
  u'0.005*"government" + 0.004*"australia" + 0.004*"pakistan" + 0.004*"people" + 0.003*"tell" + 0.003*"force" + 0.003*"israeli" + 0.003*"time" + 0.003*"claim" + 0.003*"company"'), 
 (8, 
 u'0.005*"day" + 0.004*"good" + 0.004*"year" + 0.003*"new" + 0.003*"australian" + 0.003*"australia" + 0.003*"wicket" + 0.003*"take" + 0.003*"hour" + 0.003*"area"'), 
 (9, 
  u'0.005*"people" + 0.005*"australia" + 0.005*"man" + 0.004*"arrest" + 0.004*"union" + 0.004*"tell" + 0.004*"india" + 0.004*"pakistan" + 0.003*"claim" + 0.003*"united_states"')] 
```

主题模型是概率性的，你可能会看到不同的结果，用不同的词，概率和主题编号。

让我们花一些时间来理解这个输出的性质。

元组中的第一个值是主题 id，这是我们识别主题的方式。让我们回到话题 5，看看我们能从中理解什么。

```py
(5, 
  u'0.008*"israeli" + 0.006*"palestinian" + 0.005*"force" + 0.004*"fire" + 0.004*"people" + 0.004*"kill" + 0.004*"government" + 0.004*"police" + 0.004*"day" + 0.004*"australia"') 
```

这是什么意思？这意味着主题 ID 5 由单词`israeli`、`palestinian`、`force`、`fire`等组成，并且这些是在主题中出现概率最高的单词。这个词所乘以的数字(如`0.008`与以色列)，就是这个词出现在主题分布中的概率。我们可以看概率最高的词来理解我们题目的主题。

很明显，这个话题是关于以色列-巴勒斯坦冲突的，这在 21 世纪初肯定会成为报纸头条。简单浏览一下其他主题，你会发现大多数主题中都有单词*澳大利亚*，这也是有意义的，因为这是一个澳大利亚新闻数据集。

我们可以用主题模型做很多事情，比如聚类、给 word 文档着色和主题模型可视化。我们将在下一章讨论主题模型的所有这些进一步的功能([第九章](a3906dc9-57a9-470f-adbe-5695111700e2.xhtml)，*高级主题模型*)；让我们先看看 Gensim 提供的其他主题模型。



# 潜在语义索引

除了 LDA，Gensim 中首先实现的另一个算法是**潜在语义索引** ( **LSI** )。设置我们的 LSI 模型只需要我们从`gensim.models`导入模型，并按照我们设置 LDA 模型的方式进行设置。

```py
lsimodel = LsiModel(corpus=corpus, num_topics=10, id2word=dictionary)
```

要查看我们的主题，请使用:

```py
lsimodel.show_topics(num_topics=5)  # Showing only the top 5 topics 
```

这给了我们以下内容:

```py
[(0, 
  u'-0.216*"israeli" + -0.211*"palestinian" + -0.196*"arafat" + -0.181*"force" + -0.149*"official" + -0.148*"kill" + -0.142*"people" + -0.142*"attack" + -0.129*"government" + -0.127*"australian"'), 
 (1, 
  u'-0.321*"palestinian" + -0.306*"israeli" + -0.299*"arafat" + 0.171*"australia" + 0.166*"australian" + -0.158*"israel" + 0.149*"afghanistan" + -0.137*"sharon" + -0.134*"hamas" + -0.124*"west_bank"'), 
 (2, 
  u'-0.266*"afghanistan" + -0.242*"force" + -0.191*"al_qaeda" + 0.180*"fire" + -0.176*"bin_laden" + -0.153*"pakistan" + 0.138*"good" + 0.138*"sydney" + -0.131*"tora_bora" + -0.129*"afghan"'), 
 (3, 
  u'0.373*"fire" + 0.270*"area" + 0.199*"sydney" + -0.191*"australia" + 0.176*"firefighter" + 0.160*"south" + 0.157*"north" + 0.148*"wind" + -0.146*"good" + 0.132*"wales"'), 
 (4, 
  u'-0.238*"company" + -0.221*"union" + 0.199*"test" + -0.187*"qantas" + -0.152*"australian" + 0.145*"good" + 0.141*"match" + 0.137*"win" + -0.136*"government" + -0.136*"worker"')]

```

将或多或少相似的主题作为我们的 LDA 输出是有意义的。巴以话题再次浮现！对于我们的用例，可以忽略数字前的负号——解释数字的意义并不容易，而且与 LSI 运行期间执行的**奇异值分解**(**SVD**)【9】有关。奇异值分解是一种分解矩阵的矩阵分解方法。关于 LSI 如何实际工作的更多数学信息，Deerwester 等人的原始论文*潜在语义分析索引*【10】和 Hoffman 的出版物*概率潜在语义索引*【11】可以作为有用的资源。



# 分层狄利克雷过程

Gensim 中流行的另一个标准主题建模算法是**分层狄利克雷过程**(**HDP**)——这也是迈克尔的一个创意。乔丹和大卫·布雷。它不同于 LDA 和 LSI，因为它是非参数的——我们不需要提及我们需要的主题数量。

同样，要在 Gensim 中使用它，我们需要从`gensim.models`导入模型。

```py
hdpmodel = HdpModel(corpus=corpus, id2word=dictionary)
```

注意，我们不需要指定主题的数量。

```py
hdpmodel.show_topics()
```

这将允许我们查看以下主题:

```py
[(0, 
  u'0.005*israeli + 0.003*arafat + 0.003*palestinian + 0.003*hit + 0.003*west_bank + 0.003*official + 0.002*sharon + 0.002*force + 0.002*afp + 0.002*arrest + 0.002*militant + 0.002*storm + 0.002*hamas + 0.002*strike + 0.002*come + 0.002*military + 0.002*source + 0.002*group + 0.002*soldier + 0.002*kill'), 
 (1, 
  u'0.004*company + 0.003*administrator + 0.002*yallourn + 0.002*entitlement + 0.002*traveland + 0.002*staff + 0.002*austar + 0.002*union + 0.002*travel + 0.002*employee + 0.002*end + 0.002*cent + 0.002*government + 0.002*remain + 0.002*go + 0.002*seek + 0.002*leave + 0.002*people + 0.002*agreement + 0.002*$'), 
 (2, 
  u'0.003*airport + 0.003*taliban + 0.002*kill + 0.002*opposition + 0.002*kandahar + 0.002*force + 0.002*night + 0.002*leave + 0.002*man + 0.002*lali + 0.002*near + 0.002*city + 0.001*wound + 0.001*end + 0.001*agha + 0.001*civilian + 0.001*gul + 0.001*people + 0.001*military + 0.001*injure'), 
 (3, 
  u'0.002*job + 0.002*australian + 0.002*cent + 0.002*read + 0.002*mysticism + 0.002*drop + 0.002*band + 0.001*survey + 0.001*wales + 0.001*olivier + 0.001*beatle + 0.001*week + 0.001*intensive + 0.001*result + 0.001*add + 0.001*alarming + 0.001*harrison + 0.001*cite + 0.001*big + 0.001*song'), 
 (4, 
  u'0.003*group + 0.003*palestinian + 0.002*government + 0.002*sharon + 0.002*kill + 0.002*choose + 0.002*israeli + 0.002*attack + 0.002*bright + 0.002*call + 0.002*security + 0.002*arafat + 0.002*defend + 0.002*suicide_attack + 0.002*terrorism + 0.002*hamas + 0.001*militant + 0.001*human_right + 0.001*gaza_strip + 0.001*civilian'), 
 (5, 
  u'0.003*match + 0.003*israeli + 0.002*ask + 0.002*team + 0.002*rafter + 0.002*tennis + 0.002*play + 0.002*not + 0.002*australia + 0.002*guarantee + 0.001*france + 0.001*be + 0.001*role + 0.001*hobart_yacht + 0.001*government + 0.001*kill + 0.001*late + 0.001*attack + 0.001*world + 0.001*topple'), 
 (6, 
  u'0.003*australian + 0.002*afghanistan + 0.002*state + 0.002*reach + 0.002*day + 0.002*head + 0.001*give + 0.001*go + 0.001*couple + 0.001*view + 0.001*plan + 0.001*government + 0.001*crash + 0.001*aware + 0.001*report + 0.001*future + 0.001*editor + 0.001*prevent + 0.001*blake + 0.001*party'), 
 (7, 
  u'0.004*storm + 0.003*tree + 0.002*ses + 0.002*work + 0.002*sydney + 0.002*damage + 0.002*hornsby + 0.002*service + 0.002*area + 0.002*home + 0.002*call + 0.002*bad + 0.001*hit + 0.001*bring + 0.001*australia + 0.001*afternoon + 0.001*power + 0.001*large + 0.001*electricity + 0.001*sutherland'), 
 (8, 
  u'0.004*arrest + 0.003*indonesia + 0.002*year + 0.002*smuggle + 0.002*howard + 0.002*agreement + 0.002*summit + 0.002*police + 0.002*president + 0.002*australia + 0.002*people + 0.002*megawati + 0.001*meeting + 0.001*palestinian + 0.001*meet + 0.001*council + 0.001*leader + 0.001*loya + 0.001*structure + 0.001*host'), 
 (9, 
  u'0.004*director + 0.003*friedli + 0.003*india + 0.002*union + 0.002*reply + 0.002*day + 0.002*unwell + 0.002*mistake + 0.002*report + 0.002*ask + 0.002*river + 0.002*sector + 0.001*unforeseeable + 0.001*australia + 0.001*people + 0.001*court + 0.001*trip + 0.001*australians + 0.001*swiss + 0.001*people_die'), 
 (10, 
  u'0.003*guide + 0.003*adventure_world + 0.002*people + 0.002*canyon + 0.002*interlaken + 0.002*charge + 0.002*year + 0.002*tourist + 0.002*republic + 0.001*swiss + 0.001*tragedy + 0.001*atrocity + 0.001*tomorrow + 0.001*include + 0.001*inexperienced + 0.001*kill + 0.001*change + 0.001*sweep + 0.001*allow + 0.001*court'), 
 (11, 
  u'0.002*australian + 0.002*commission + 0.002*company + 0.002*call + 0.002*people + 0.002*collapse + 0.001*  + 0.001*power + 0.001*theatre + 0.001*martin + 0.001*begin + 0.001*dickie + 0.001*wisdom + 0.001*refund + 0.001*national + 0.001*include + 0.001*determine + 0.001*arafat + 0.001*procedural + 0.001*today'), 
 (12, 
  u'0.002*high + 0.002*lee + 0.001*year + 0.001*inject + 0.001*match + 0.001*lockett + 0.001*passage + 0.001*casa + 0.001*day + 0.001*test + 0.001*compare + 0.001*bond + 0.001*presence + 0.001*outlook + 0.001*osaka + 0.001*canada + 0.001*maintenance_worker + 0.001*china + 0.001*game + 0.001*$'), 
 (13, 
  u'0.003*krishna + 0.003*ash + 0.002*hare + 0.002*ganges + 0.002*harrison + 0.002*ceremony + 0.002*hindu + 0.002*devotee + 0.002*sect + 0.002*hundred + 0.002*holy + 0.002*river + 0.002*closely + 0.002*benares + 0.001*task + 0.001*scatter + 0.001*place + 0.001*devout + 0.001*official + 0.001*rescue'), 
 (14, 
  u'0.003*harrison + 0.002*george + 0.002*beatle + 0.002*die + 0.002*tonight + 0.002*liverpool + 0.002*  + 0.002*memory + 0.002*music + 0.002*seventh + 0.001*decisive + 0.001*percent + 0.001*hold + 0.001*silence + 0.001*people + 0.001*tree + 0.001*minute + 0.001*pole + 0.001*stabbing + 0.001*plant'), 
 (15, 
  u'0.003*strong + 0.003*economy + 0.002*forward + 0.002*australia + 0.002*olympic + 0.002*hoon + 0.002*follow + 0.002*proposal + 0.002*extensive + 0.002*australian + 0.002*year + 0.001*goner + 0.001*mystery + 0.001*haggle + 0.001*constitutional + 0.001*fazalur + 0.001*weekend + 0.001*limit + 0.001*term + 0.001*set'), 
 (16, 
  u'0.002*tell + 0.002*launceston + 0.002*virgin + 0.002*airline + 0.002*terminal + 0.002*flight + 0.001*daily + 0.001*melbourne + 0.001*morning + 0.001*new + 0.001*second + 0.001*check + 0.001*sherrard + 0.001*administrator + 0.001*shot + 0.001*sabotage + 0.001*unacceptable + 0.001*coroner + 0.001*ansett + 0.001*hayden'), 
 (17, 
  u'0.002*choose + 0.002*aids + 0.002*hiv + 0.001*official + 0.001*state_emergency + 0.001*reporter + 0.001*europe + 0.001*soviet + 0.001*find + 0.001*late + 0.001*rush + 0.001*double + 0.001*today + 0.001*union + 0.001*number_people + 0.001*service + 0.001*report + 0.001*arabian + 0.001*footing + 0.001*state'), 
 (18, 
  u'0.003*know + 0.002*accident + 0.002*company + 0.002*carry + 0.002*organise + 0.002*region + 0.002*charge + 0.001*appear + 0.001*loot + 0.001*defunct + 0.001*market + 0.001*question + 0.001*live + 0.001*accuse + 0.001*initially + 0.001*rhino + 0.001*stephan + 0.001*canyoning + 0.001*possibility + 0.001*bayu'), 
 (19, 
  u'0.003*afghanistan + 0.003*powell + 0.002*taliban + 0.002*southern + 0.002*want + 0.002*developer + 0.001*face + 0.001*marines + 0.001*officer + 0.001*bin_laden + 0.001*pakistan + 0.001*kilometre + 0.001*united_states + 0.001*kandahar + 0.001*vacate + 0.001*force + 0.001*ground + 0.001*troop + 0.001*time + 0.001*secretary')]
```

同样，我们可以看到类似的主题是如何出现在我们的结果中的。我们对 HDP 特别感兴趣，因为它与前两种方法略有不同，因为它是非参数化的，并为我们提供了根据层次结构对主题进行聚类的能力。描述 HDP 的论文是 NIPS 会议记录的一部分，标题是*在相关群体中共享集群:等级狄利克雷过程* [ [13](http://papers.nips.cc/paper/2698-sharing-clusters-among-related-groups-hierarchical-dirichlet-processes.pdf) 。



# 动态主题模型

虽然我们之前的主题模型侧重于识别整个语料库中的主题，但我们引入的下一个主题模型也考虑了文档存在的时间框架。使用这些额外的信息，我们可以在每个时间框架内对我们的主题进行建模，并试图理解这些主题如何随着时间的推移而演变。

主题的性质在第一个时间框架内是固定的——我们不可能看到久而久之引入一个新的主题，而是可以看到这些主题如何随着时间的推移而变化——特别是，我们可以看到哪些词取代了哪些词。我们在介绍部分给出了一个例子，什么是主题模型？，我们在这里谈到了*原子物理学*的话题。

我为 Gensim 写的 Jupyter 笔记本涵盖了该理论的大部分内容，以及动态主题模型的所有可能用途。笔记本包含在附录中，你也可以在 GitHub 上的 Gensim 库[ [14](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb) ]中找到。



# scikit-learn 中的主题模型

Gensim 并不是唯一一个为我们提供主题模型能力的包:scikit-learn 虽然不是专门针对文本的，但仍然提供了 LDA 和**非负矩阵分解** ( **NMF** )的快速实现，可以帮助我们识别主题。

我们已经讨论了 LDA 的工作原理，Gensim 和 scikit-learn 实现之间的唯一区别如下:

1.  这里的困惑界限并不完全一致，因为在 Gensim 和 sklearn 中界限的计算是不同的。这些界限是我们在主题建模算法中计算主题如何收敛的方法。
2.  Sklearn 使用 cython 来创建数字的第 6 个^(小数点差。)

与 LDA 不同，非负矩阵分解(NMF) [ [15](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) ]不是一种主要限于文本挖掘的方法(尽管有趣的是，LDA 的变体也已经用于遗传学和图像处理)。NMF [ [16](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization) 是一种线性代数方法，涉及将单个矩阵 *V* 重构为两个矩阵 *W* 和 *H* 。当这些矩阵彼此相乘时，近似重构 *V* 。然后使用 *W* 和 *H* 来标识我们的主题，因为它们最能代表原始矩阵 *V* 。这里，矩阵 *V* 是文档术语矩阵，包含关于哪些单词在哪些文档中的信息。

NMF 的另一个关键方面是矩阵不能有负元素。这种非负性使得生成的矩阵更容易检查。此外，在诸如音频频谱图处理或文本处理的应用中，非负性是所考虑的数据所固有的。因为这个问题一般来说是不可精确求解的，所以通常使用不同的距离范数来进行数值近似。我们通常在二维空间中使用的欧几里德距离就是这样一个范数，而**库尔巴克-莱布勒**散度 [17](https://projecteuclid.org/euclid.aoms/1177729694) 是另一个更复杂的度量。这种因式分解可以用于例如降维、源分离或主题提取。在我们的例子中，我们使用了广义的 Kullback-Leibler 散度，它等价于**概率潜在语义索引**(**PLSI**)[[1](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis)][[11](https://arxiv.org/ftp/arxiv/papers/1301/1301.6705.pdf)]。

Scikit-learn 有一个非常简单的 API，这使得它很容易使用，还因为它在所有模型之间实现了高度的一致性——大多数模型都有基于模型目的的 fit、transform 和 predict 方法。在我们的例子中，由于它们是分解模型，我们将只使用 fit 方法，并使用模型的组件来打印我们的主题。让我们看一些训练两个模型并打印主题的代码。

```py
from sklearn.decomposition import NMF, LatentDirichletAllocation

no_topic = 10

nmf = NMF(n_components=no_topic).fit(tfidf_corpus) 

lda = LatentDirichletAllocation(n_topics=no_topics).fit(tf_corpus) 
```

这里，`tfidf_corpus`和`tf_corpus`是`tfidf`和`tf`转换后的语料；您可以使用 Gensim 或 scikit-learn 来完成此操作。这里，`tf_feature_names`和`tfidf_feature_names`是包含按字母顺序排列的全部词汇的列表；你可以在这里使用 Gensim 的字典方法，同样有效。

现在让我们编写一个小函数，它将帮助我们打印主题:

```py
def display_topics(model, feature_names, no_top_words): 
    for topic_idx, topic in enumerate(model.components_): 
        print "Topic %d:" % (topic_idx) 
        print " ".join([feature_names[i] 
                        for i in topic.argsort()[:-no_top_words - 1:-1]]) 
```

`model.components_`对象是主题词分布的变化参数。由于主题单词分布的完整条件是狄利克雷式的，`components_[i, j]`可以被视为一个伪计数，它表示单词`j`被分配给主题`i`的次数。

让我们运行这个:

```py
no_top_words = 10

display_topics(nmf, tfidf_feature_names, no_top_words) 
```

我们得到以下结果:

```py
Topic 0:
afghanistan bin laden qaeda al force taliban tora bora afghan

Topic 1:
palestinian arafat israeli israel hamas gaza attack suicide sharon militant

Topic 2:
qantas union worker industrial maintenance dispute wage freeze action relations

Topic 3:
test africa south match day waugh bowler wicket cricket lee

Topic 4:
river guide adventure canyon court trip interlaken australians swiss accident

Topic 5:
detainee centre woomera detention facility department damage overnight visa night

Topic 6:
hollingworth dr governor abuse general anglican child school allegation statement

Topic 7:
new year australia south government people sydney australian wales state

Topic 8:
harrison beatle cancer george krishna lord lung know ceremony life

Topic 9:
commission hih royal collapse hearing company report union martin evidence
```

现在让我们运行这个:

```py
display_topics(lda, tf_feature_names, no_top_words) 
```

我们得到以下结果:

```py
Topic 0:
space station shuttle endeavour russian crew ice vaughan centre launch

Topic 1:
test south day australia match lee africa wicket waugh cricket

Topic 2:
afghanistan force taliban government laden bin president australian united al

Topic 3:
russian people christmas authority security cause economy drop america kilometre

Topic 4:
union qantas worker industrial action company maintenance dispute pay relations

Topic 5:
palestinian israeli arafat attack hamas suicide Gaza sharon israel kill

Topic 6:
win metre good year race event world new australia australian

Topic 7:
year company commission people australian report world director royal child

Topic 8:
new australia south people government sydney state australian storm year

Topic 9:
flight virgin disease airline melbourne blue tell second ansett japan
```

我们先简单考察一下话题——可能是又要找我们以色列和巴勒斯坦话题了？是的。来自 NMF 的主题 id 1 和来自 LDA 的主题 id 5 描述了我们在之前使用 Gensim 进行的所有主题建模实验中看到的相同主题！

通过运行基于[第 8 章](02e3238f-db9c-4d6e-ba96-70ba9db11dc0.xhtml)、*主题模型*的 Jupyter 笔记本，你应该能够重现相同的结果。

现在我们可以用主题来描述我们的文本数据，在两个不同的 Python 机器学习框架中。到目前为止，我们只真正看到了如何识别和打印文本中的主题；但是对于主题模型，我们还可以做更多的事情，特别是在探索文档的方式上。我们将在下一章探索更多的主题建模技术以及更好地训练主题模型的方法。



# 摘要

在这一章中，我们看到了 Gensim 的机器学习算法的首次使用，特别是主题模型。主题模型是我们处理未标记数据的一种很好的方式，它们帮助我们找到文本中的潜在结构。我们有多种方法来识别文本中的主题，LDA、LSI、HDP 和 NNMF 是最流行的方法，我们已经讨论了在 scikit-learn 和 Gensim 中使用所有这些方法的方法。

在下一章，我们将进入使用主题模型的高级操作。



# 参考

[1]潜在语义分析:
[https://en . Wikipedia . org/wiki/Latent _ Semantic _ Analysis # Latent _ Semantic _ indexing](https://en.wikipedia.org/wiki/Latent_semantic_analysis#Latent_semantic_indexing)

[2]gens im:[https://radimrehurek . com/gens im/](https://radimrehurek.com/gensim/)

[3]潜狄利克雷分配:
[http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)

[4]LDA 简介:
[http://blog . echen . me/2011/08/22/Introduction-to-latent-Dirichlet-allocation/](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)

[5]LDA 的解释:
[https://www . quora . com/What-a-a-good-Explanation-of-Latent-Dirichlet-Allocation](https://www.quora.com/What-is-a-good-explanation-of-Latent-Dirichlet-Allocation)

[6]概率话题模型:
[http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)

[7] Jupyter 笔记本:
[https://github . com/bhargavader/personal/blob/master/notebooks/text _ analysis _ tutorial/topic _ modeling . ipynb](https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis_tutorial/topic_modelling.ipynb)

[8]文本文档相似度模型实证评估:
[http://www . socsci . UCI . edu/~ mdlee/lee _ pincombe _ Welsh _ Document。PDF](http://www.socsci.uci.edu/~mdlee/lee_pincombe_welsh_document.PDF)

[9]奇异值分解:
[https://en.wikipedia.org/wiki/Singular-value_decomposition](https://en.wikipedia.org/wiki/Singular-value_decomposition)

[10]潜在语义分析索引:
[https://search . proquest . com/openview/a 1907164 BD 88 DFC 38 a 4875 b 73 a3 f 7 B3 d/1？pq-origsite = gscholar&CBL = 1818555](https://search.proquest.com/openview/a1907164bd88dfc38a4875b73a3f7b3d/1?pq-origsite=gscholar&cbl=1818555)

[11]概率潜在语义标引:
[https://dl.acm.org/citation.cfm?id=312649](https://dl.acm.org/citation.cfm?id=312649)

[12]NIPS:
[https://nips.cc/](https://nips.cc/)

[13]相关组间共享聚类:分层狄利克雷过程:
[http://papers . nips . cc/paper/2698-Sharing-Clusters-inter-Related-Groups-Hierarchical-Dirichlet-Processes . pdf](http://papers.nips.cc/paper/2698-sharing-clusters-among-related-groups-hierarchical-dirichlet-processes.pdf)

[14]动态话题模型:
[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/ldaseqmodel.ipynb)

[15]NNMF:
[https://en . Wikipedia . org/wiki/Non-negative _ matrix _ factorization](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)

[16]NNMF 算法:
[http://papers . nips . cc/paper/1861-非负矩阵分解算法](http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization)

[17]论信息与充分性:
[https://projecteuclid.org/euclid.aoms/1177729694](https://projecteuclid.org/euclid.aoms/1177729694)