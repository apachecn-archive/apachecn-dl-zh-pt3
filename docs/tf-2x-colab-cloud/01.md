# 1.深度学习简介

我们介绍深度学习的基本概念。我们使用 TensorFlow 2.x、Google 云服务和 Google Drive Interactive，通过 Python 编码示例让概念变得生动起来。

章节的笔记本位于以下网址: [`https://github.com/paperd/tensorflow`](https://github.com/paperd/tensorflow) 。

那么什么是深度学习呢？**深度学习**是一种机器学习技术，它通过自动学习算法提供对数据的洞察，目的是为决策提供信息。深度学习算法使用连续的层从原始输入中逐步提取更高级别的特征。咻，那是一口。让我们把它分解一下。深度学习强调从数据中学习越来越有意义的表示的连续层。深度学习模型的每一层都从数据中学习。所以每一层都把学到的东西传递给下一层。在图像处理中，较低层可以识别边缘，而较高层可以识别与人相关的概念，例如数字、字母或脸。不要担心这是否令人困惑，因为我们还没有定义基础，这是我们现在要做的。

## 神经网络

连续的层几乎总是由称为神经网络的模型来学习。**神经网络**是一套大致模仿人脑的算法，旨在识别模式。这些网络通过一种基于标记或聚类原始输入的机器感知来解释感官数据。

层是深度学习中的核心构建块。一个**层**是一个人工神经元的容器，通常接收加权输入，用一组主要是非线性的函数进行转换，然后将这些值作为输出传递给下一层。**人工神经元**是神经网络中的基本单元，接收一个或多个输入并将它们相加以产生一个输出。神经网络的每一层都是由人工神经元组成的。

层可以被认为是数据处理模块，充当数据的过滤器。数据进入一个层，它以一种更有用的形式出来。也就是说，层从提供给它们的数据中提取表示。当然，我们希望这些陈述对帮助我们解决手头的问题是有意义的。要获得有意义的利益，需要大量的实践和 T2 实验。因此，我们演示并解释了大量的代码示例，以帮助您获得洞察力。但我们建议多次研究这些例子，因为深度学习是一个非常复杂和错综复杂的主题。

一个非常常见的深度学习问题是数字 0–9 的识别。我们可以通过创建一个由连续层组成的神经网络来解决这个问题，以帮助我们从图像数据中自动预测数字。

例如，假设我们的数据集中有一个数字 8 的图像。如果我们的神经网络是健壮的，它应该能够在没有人为干预的情况下，从图像数据中正确预测出数字是 8！也就是说，网络模型能够以高精度预测数字图像。当然，人类可以轻松区分 0 和 9 之间的数字，但计算机模型做到这一点的能力是惊人的，也是深度学习的核心所在。

神经网络是由*个神经元*和*个突触*连接而成的集合。它分为三个主要部分:

*   输入层

*   隐蔽层

*   输出层

训练神经网络时，数据最初被传递到输入层。因此，**输入层**将初始数据带入系统，供后续人工神经元层进一步处理。然后，在将数据传递到第一个隐藏层之前，它通过激活函数传递数据。一个**激活函数**是一种算法，它定义了给定一个输入或一组输入的神经元的输出。

一个**隐藏层**位于输入层和输出层之间，其中人工神经元接受一组加权输入，并通过激活函数产生一个输出。网络可以有多个隐藏层。**输出层**产生给定输入的结果。它是完成所有计算的地方。神经元往往非常简单，只有一个浮点值、一个输入和一个输出。这个浮点值就是我们所说的神经元的*权重*。

因此，神经元从其前一层获取输入，通过激活函数将其转换以将值保持在可管理的范围内，并将转换后的输入及其权重发送给下一层的神经元。由于输入图层的值通常以零为中心，并且已经进行了适当的缩放，因此不需要使用激活函数进行变换。

## 从数据中学习表示

机器学习算法发现执行数据处理任务的规则。因此，要进行机器学习预测，我们需要三样东西:

1.  输入数据点

2.  预期产出的例子

3.  一种测量算法性能的方法

*输入数据点*是某种数据。输入数据的一个例子可以是图片。深度学习中的图像识别需要图片。当然，深度学习模型需要数值数据。那么模型是如何解读图像的呢？这些图片必须以某种方式进行转换！不要担心这是如何做到的，因为我们将在下一章中介绍它。

Note

所有数据，不仅仅是图像数据，在被深度学习模型消费之前，必须符合数字表示。

为了从深度学习的数据中做出预测，我们需要预期输出的*个例子。因此，数据必须包含每个数据示例的表示以及每个数据示例所代表的内容。举个例子我们就能更好的理解了。预测数字时，数据必须包含每个数字的表示形式以及该数字所代表的内容。如果数据中的一个例子是数字 9，我们必须有数字 9 的表示和目标值 9。我们将在下一章介绍如何表示一个数字及其目标值。*

最后，我们需要*评估算法性能*。我们通过确定*算法的当前输出与其预期输出*之间的距离来做到这一点。这个距离通常被称为损耗或误差。*损失*被用作反馈来调整算法的工作方式。这样的调整叫做*学习*。例如，如果我们的神经网络模型预测一个数字是 3，但它实际上是 8，我们的模型至少有一些损失。也就是说，模型预测的结果与其预期输出(实际目标值)之间存在一定的距离。

## TensorFlow 2.x

**TensorFlow** 是一个用于数值计算的 *Python* 开源库，旨在促进机器学习和深度学习问题的解决。TensorFlow 将机器学习模块、深度学习模块和相关算法捆绑在一起，形成一个通用的编程环境。TensorFlow 2.x 是该软件的最新版本。我们使用 2.x 名称是因为软件变化如此之快。

## Google Colab

**Google Colab**(Google co laboratory 的简称)是一项云服务，为 *Python* 提供数据科学工作空间，非常类似于 *Jupyter Notebook* 。实际上，任何 Jupyter 笔记本都可以直接加载到 Colab 云服务中。

Colab 笔记本存储在 Google Drive 中，可以像使用 Google Docs 或 Sheets 一样共享。只需点击任意协作笔记本右上角的共享按钮，或者按照 Google Drive 共享说明进行操作。

熟读 [`https://colab.research.google.com/notebooks/welcome.ipynb`](https://colab.research.google.com/notebooks/welcome.ipynb) 入门。该网站提供了一个不错的教程，但你可以浏览 YouTube 视频或其他教程来加深你的 Colab 技能。当然，我们会教你一些基本知识。

## Google Drive

**Google Drive** 是一个基于云的文件存储和同步服务，你很可能已经很熟悉了，所以我们不会在上面花太多时间。但我们需要展示如何将 Google Colab 与 Google Drive 连接起来。只需要几个简单的步骤:

1.  登录您的 Google 电子邮件帐户。

2.  打开一个新的浏览器标签，浏览到 *Google Colab* 。

3.  点击 *Google Colab* 链接。

4.  在弹出窗口的顶部菜单中点击 *Google Drive* 。

您的 Google Drive 帐户上的所有笔记本都会出现在窗口中。您应该看不到任何笔记本出现，除非您过去使用过 Colab。笔记本保存在 Google Drive My Drive 的 *Colab 笔记本*目录下。这个目录是在 Colab 连接到 Google Drive 时自动创建的。

如果要创建新的笔记本，点击*新建笔记本*。或者点击*取消*，进入*欢迎来到合作实验室*界面。该屏幕提供了 Google Colab 的主菜单以及帮助您入门的目录。

Note

我们刚刚在 Google Colab 和 Google Drive 之间建立的连接是*持久的*。也就是说，我们只需要建立这个连接*一次*，除非浏览器历史被清除。

## 创建新笔记本

在 Colab 环境中，很容易创建新的笔记本。在浏览器中打开 Google Colab(如果尚未打开)。在弹出的窗口中，点击*新建笔记本*。如果已经在 Colab 环境中，点击*下左上角菜单中的*文件*。从下拉菜单中点击*新建笔记本*。代码单元现在已经准备好执行 Python 代码了！点击 *+代码*或 *+文本*按钮，添加代码或文本单元格。更多选项，点击菜单中的*插入*。*

若要创建第一段代码，请在“代码”单元格中添加以下内容:

```py
string = 'Peter picked a pail of pickled peppers'
string

```

要执行代码，单击左边的小箭头。代码单元的输出显示了*字符串*变量的内容。

Tip

我们建议从网站上复制并粘贴代码。

## GPU 硬件加速器

为了大大加快处理速度，我们使用了 Google Colab 云服务提供的 GPU。Colab 免费提供了 12 GB 左右的 Tesla K80 GPU。在 Colab 笔记本中启用 GPU 非常简单:

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

Note

必须在每个笔记本的*中启用 GPU。但它只需启用一次。*

测试 GPU 是否处于活动状态:

```py
import tensorflow as tf

# display tf version and test if GPU is active
tf.__version__, tf.test.gpu_device_name()

```

导入 *tensorflow* 库，显示 tensorflow 的版本以及 GPU 的状态。如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

## 从 URL 下载文件

我们开始工作吧！我们可以用 *tf.keras.utils.get_file* 实用程序直接从 URL 下载文件。我们需要 tensorflow 库，但我们已经导入了它。我们建议通过点击 *+代码*创建一个新的代码单元。

以下代码单元格从 URL 下载 CSV 文件:

```py
# import keras module
from tensorflow import keras

ds = 'auto-mpg.data'
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'
dataset_path = tf.keras.utils.get_file(ds, url)
dataset_path

```

从 tensorflow 库中导入 keras 模块。使用 *tf.keras.utils.get_file* 从 *UCI 机器学习库*下载数据集。这个知识库是数据库、领域理论和数据生成器的集合，用于机器学习算法的经验分析。

Tip

我们强烈建议在代码单元中测试小段代码，以减少调试时间和工作量。

## 准备数据集

照这样，数据集需要一些预处理。例如，它没有特征标题。我们建议为每个示例创建一个新的代码单元。现在点击 *+代码*即可。

以下代码单元通过从我们在前面的代码单元中创建的路径访问 CSV 文件来创建 pandas 数据帧:

```py
# import the pandas library
import pandas as pd

cols = ['MPG','Cylinders','Displacement','Horsepower','Weight',
        'Acceleration', 'Model Year', 'Origin']

raw_dataset = pd.read_csv(dataset_path, names=cols,
                      na_values = "?", comment="\t",
                      sep=" ", skipinitialspace=True)

```

导入熊猫并创建一个包含特征名称的列表。使用 *read_csv()* 函数将 csv 数据放入 pandas 数据帧。一个 **pandas dataframe** 是一个二维(或 2D)数据结构，数据以表格的行列方式排列。让我们看看最近的五条记录:

```py
raw_dataset.tail()

```

Pandas *tail* 方法返回最后 n 行。默认情况下，它返回最后五行。这对于快速验证数据非常有用。

我们可以通过创建副本来保存原始数据:

```py
# create a copy
data = raw_dataset.copy()

# verify contents by displaying some data
data.tail()

```

## 晚上吃可乐

异常终止(也称为异常结束或异常终止)是软件的异常终止或程序崩溃。当我们长时间(几个小时)运行 Colab，将一个大型数据集读入内存，并处理所述数据或创建一个非常大的笔记本时，它可能会崩溃。当这种情况发生时，我们有两个选择:

1.  重新启动运行时。

2.  关闭笔记本，从头开始。

重启运行时，点击顶部菜单中的*运行时*，在下拉菜单中点击*重启运行时*，出现提示时点击*是*。从头开始重新看你的笔记本。Colab 推荐这个选项。要从头开始，请清除浏览器历史记录并打开 Colab。

## 奇怪的结果

使用 Colab 时，有时会出现意外错误或其他奇怪的结果。如果发生这种情况，重新启动运行时或从头开始打开 Colab，如“Colab Abends”一节所述。

## 张量

一个**张量**是一个*数字*数据的容器。张量可以包含任意数量的维度。一个维度通常被称为*轴*。

在深度学习中，张量被认为是由 *n 维*数组表示的矩阵的推广。张量的维数通常用它的轴数来描述。所以张量是由它们总共有多少轴来定义的。**秩**是由张量表示的轴的数量。

理解张量的最好方法是通过例子。所以我们从最简单的类型开始。

## 标量(0D 个张量)

一个**标量**是只有一个数的张量。所以标量被认为是零维(或 0D)张量。例如 numpy float32 或 float64 号码。

让我们看一个例子:

```py
import numpy as np

# create numpy scalar 9
scalar = np.array(9)
scalar

```

导入 numpy 模块。将包含 9 的 numpy 数组赋给一个变量并显示结果。

用信号表示标量张量的秩如下:

```py
# signal its rank
print (str(scalar.ndim) + 'D')

```

*ndim* 属性表示 numpy 张量的轴数(或维度)。因为我们的变量是一个标量，我们看到显示 0D。

## 向量(1D 张量)

一个**向量**是一个数字数组。所以矢量是一维(或 1D)张量。1D 张量只有一个轴。

创建一个包含六个元素的 numpy 向量:

```py
# create numpy vector [0, 1, 0, 0, 0, 0]
vector = np.array([0, 1, 0, 0, 0, 0])
vector

```

我们看到显示[0，1，0，0，0，0]。

表示向量的等级:

```py
# signal its rank
print (str(vector.ndim) + 'D')

```

我们看到 1D 展示。

## 矩阵(2D 张量)

一个**矩阵**是一个向量数组。最简单的矩阵是二维(或 2D)张量。2D 张量有两个轴。其轴线一般称为*行*和*列*。

创建一个 numpy 矩阵:

```py
# create a numpy matrix
matrix = np.array([[0, 1, 0, 0, 0, 0],
                   [0, 0, 1, 0, 0, 0],
                   [0, 0, 0, 1, 0, 0],
                   [0, 0, 0, 0, 1, 0],
                   [0, 0, 0, 0, 0, 1]])
matrix

```

行是来自第一个轴的条目，列是来自第二个轴的条目。因此，我们示例中的第一行是[0，1，0，0，0，0]，第一列是[0，0，0，0，0，0]。由于该矩阵具有五行和六列，所以它被认为是 5 `×` 6 矩阵。

表示矩阵的等级:

```py
# signal its rank
print (str(matrix.ndim) + 'D')

```

我们看到 2D 展示。

## 3D 矩阵(3D 张量)

我们可以通过将 2D 张量打包成一个新的数组来创建一个 3D 张量。3D 张量可以被形象地解释为一个数字的立方体。

让我们看一个例子:

```py
# create a 3D tensor
D3 = np.array([[[0, 1, 2]],
               [[3, 4, 5]],
               [[6, 7, 8]]])

# signal its rank
print (str(D3.ndim) + 'D')

```

通过将 3D 张量打包成一个数组，我们可以创建 4D 张量等等。在深度学习中，我们通常操纵 0D——4D 张量。有了视频处理，我们可以去 5D。

## 张量的关键属性

1.  军阶

2.  形状

3.  数据类型

如前所述， **rank** 是轴的数量，它与 *ndim* 属性一起显示。0D 张量有零个轴，1D 张量有一个轴，2D 张量有两个轴，而 3D 张量有三个轴。

**Shape** 是一组整数，描述每个轴上的维数。所以我们的 3D 矩阵有 shape (3，1，3)，2D 矩阵有 shape (5，6)，vector 有 shape (6，)标量有空 shape()。我们可以用*形状*属性显示形状。

让我们看看 3D 矩阵的形状:

```py
# 3 instances of 1 x 3 matrices
D3.shape

```

展示我们的 2D 矩阵的形状:

```py
# 5 rows and 6 columns (or 5 x 6 matrix)
matrix.shape

```

显示我们矢量的形状:

```py
# 6 element vector
vector.shape

```

显示我们标量的形状:

```py
# just a scalar number
scalar.shape

```

**数据类型**是张量中包含的数据的描述。使用 *dtype* 属性显示数据类型。

显示我们张量的数据类型:

```py
# dtype of tensors

print (scalar.dtype)
print (vector.dtype)
print (matrix.dtype)
print (D3.dtype)

```

我们看到所有的张量都包含 int64 值。

## 输入流水线

**输入流水线**是一系列操纵和应用数据转换的数据处理组件。流水线在机器学习和深度学习系统中非常常见，因为这些系统是*数据丰富的*。也就是说，它们需要大量的数据才能运行良好。输入流水线是转换大型数据集的最佳方式，因为它们将数据分解为可管理的组件。

输入流水线的每个组件都接收大量数据，以某种方式进行处理，然后输出结果。下一个组件获取结果数据，以另一种方式处理它，并输出自己的输出。流水线继续运行，直到它的所有组件都完成了它们的工作。

## tf.data API

TensorFlow 应用围绕着封装在 tf.data API 中的数据集的概念。 *tf.data API* 使您能够从简单、可重用的部分构建快速、灵活且易于使用的输入流水线。它是在 TensorFlow 中构建输入流水线的推荐 API。

让我们创建一个简单的TensorFlow张量:

```py
import tensorflow as tf

X = tf.range(5)
X

```

如有必要，导入 tensorflow 库。创建一个形状为(5，)数据类型为 int32 的TensorFlow张量。该向量包含值[0，1，2，3，4]，对应于形状(5，)。

Tip

将您的输出与网站代码输出进行比较，以验证结果。

使用 *numpy()* 属性显示TensorFlow张量的值:

```py
X.numpy()

```

我们还可以从张量中访问单个元素:

```py
# first element from tensor
X[0].numpy()

```

或者访问多个元素:

```py
# 2nd, 3rd, and 4th elements from tensor
X[1:4].numpy()

```

该代码从张量中截取第二、第三和第四个元素。

## 来自张量切片的函数

函数 *from_tensor_slices* 从张量的所有切片创建 tf.data.Dataset。在我们的例子中，它创建了一个数据集，其元素是 *X* (沿着第一维)的所有切片。一个 **tf.data.Dataset** 是构建输入流水线的标准 TensorFlow API。它代表了一个潜在的大型元素集。

让我们用一个例子来说明:

```py
dataset = tf.data.Dataset.from_tensor_slices(X)
dataset

```

我们刚刚创建了一个 TensorSliceDataset 对象，它从 X 切片而来，具有 shape()和数据类型 tf.int32。

## 迭代 tf.data.Dataset

用一个简单的循环迭代一个 *tf.data.Dataset* :

```py
for item in dataset:
  print (item)

```

我们看到每个值及其形状和数据类型。

## 张量和数字

TensorFlow 张量和 numpy 玩的很好。我们可以从 numpy 数组创建一个TensorFlow张量，反之亦然。我们甚至可以对 numpy 数组应用 TensorFlow 运算，对 TensorFlow 张量应用 numpy 运算。

从我们刚刚创建的 TensorFlow 数据集创建一个 numpy 数组，如清单 [1-1](#PC25) 所示。

```py
# create a variable to hold a line break
br = '\n' # this is just a convenient way to include a line break

# import numpy
import numpy as np

# technique 1

ls = []
for item in dataset:
  e = item.numpy()
  ls.append(e)

np_arr = np.asarray(ls, dtype=np.float32)
print (type(np_arr))
print (np_arr, br)

# technique 2

ls = [item.numpy() for item in dataset]
np_arr = np.asarray(ls, dtype=np.float32)

print (type(np_arr))
print (np_arr)

Listing 1-1Create a numpy array from a TensorFlow dataset

```

我们展示了从 TensorFlow 数据集创建 numpy 数组的两种技术。技术 1 初始化一个列表，迭代张量，将每个值转换为 numpy，并将其追加到一个列表中。然后，该列表被转换为 numpy 数组。技术 2 执行相同的逻辑，但是对更紧凑的代码使用列表理解。

我们可以使用*常量*方法将 numpy 数组转换回 TensorFlow 数据集:

```py
tf_arr = tf.constant(np_arr)
tf_arr

```

然而，常数是不可变的。也就是说，不能修改它们的值。所以如果我们需要修改值，我们可以使用*变量*方法。

使用变量方法:

```py
tf_arr = tf.Variable(np_arr)
tf_arr

```

## 链接转换

我们可以通过调用 tf.data.Dataset 的转换方法来对其应用转换。每个方法都返回一个新的数据集，这允许我们链接转换。让我们从一个简单的转换开始。

创建 tf.data.Dataset 并显示其值:

```py
dataset = tf.data.Dataset.range(5)

for item in dataset:
  print (item)

```

数据集包含值[0，1，2，3，4]。

使用 *repeat()* 变换方法重复张量中的元素:

```py
data_rep = dataset.repeat(3)

for item in data_rep:
  print (item)

```

新数据集包含三组原始数据集。我们重复数据以扩大数据集，从而在不获取新数据的情况下获得更好的模型性能。

现在，让我们连锁变换:

```py
data_batch = dataset.repeat(3).batch(7)

for item in data_batch:
  print (item)

```

发生了什么事？第一个转换，*重复(3)* ，创建原始数据集的三个副本。我们用*(batch(7)*将第一个转换链接到第二个转换中，这创建了每个包含七个元素的批次。

所以新的数据集由三个张量组成。第一个张量包含[0，1，2，3，4，0，1]，第二个张量包含[2，3，4，0，1，2，3]，第三个张量包含[4]。到第三批的时候，我们已经没有数据了。

我们可以放弃最后一批:

```py
data_drop = dataset.repeat(3).batch(7, drop_remainder=True)

for item in data_drop:
  print (item)

```

新数据集由两个张量组成。第一个张量包含[0，1，2，3，4，0，1]，第二个张量包含[2，3，4，0，1，2，3]。变换方法不会修改数据集。他们创建新的数据集，这样我们就可以通过不同的命名来跟踪每个数据集。

创建相等的批次:

```py
data_equal = dataset.repeat(3).batch(5)

for item in data_equal:
  print (item)

```

新数据集由三个张量组成。每个张量包含[0，1，2，3，4]。

## 映射张量

使用 *map* 方法转换张量中的元素。 **map** 函数返回给定函数应用于给定张量的每个元素后的结果的 map 对象。返回的对象是迭代器。一个**迭代器**是一个 Python 对象，能够一次返回一个成员。列表、元组和字符串是 Python 中常见的迭代器。

咻！让我们看看清单 [1-2](#PC33) 中的例子，以帮助我们理解 map 函数是如何工作的。

```py
# create a dataset
dataset = tf.data.Dataset.range(7)

# repeat and batch it
data_batch = dataset.repeat(3).batch(7)

# display the batched dataset
for row in data_batch:
  print (row)

# map() a function on it
data_map = data_batch.map(lambda x: x ** 2)

# display the first batch
print ()
for item in data_map.take(1):
  print (item)

Listing 1-2A simple map function example

```

我们用值[0，1，2，3，4，5，6]创建一个新的数据集。我们将重复转换链接到批量转换，以创建一个具有三个张量的新数据集。每个张量包含[0，1，2，3，4，5，6]。我们通过用 lambda 函数映射来平方每个元素。一个 **lambda 函数**是一个没有名字声明的单行函数，它可以有任意数量的参数，但只能有一个表达式。我们可以用 *take* 方法获取一个或多个样本，而不是迭代整个数据集。在我们的例子中，我们只是从数据集中取第一个样本(或第一个张量)。

所以映射的数据集包含三个张量。每个张量包含[0，1，4，9，16，25，36]，因为 lambda 函数平方一个值，而 map 函数将 lambda 函数表达式映射到张量中的每个元素上。

## 筛选 tf.data.Dataset

如果我们想过滤一个数据集呢？ **filter** 方法在测试序列中每个元素是否为真的函数的帮助下过滤给定的序列。

让我们看看清单 [1-3](#PC34) 中的例子。

```py
# create a dataset
dataset = tf.data.Dataset.range(7)

# display the dataset
for row in dataset:
  print (row)

# apply a filter
data_filter = dataset.filter(lambda x: x < 6 and x > 3)

print ()
for item in data_filter:
  print (item)

Listing 1-3A simple filter() function example

```

我们用值[0，1，2，3，4，5，6]创建一个新的数据集。我们过滤数据集以提取 3 到 6 之间的值。因为我们在 lambda 函数中使用小于和大于，所以我们不包括 3 或 6 值。因此，过滤后的数据集包含[4，5],因为 lambda 函数提取 3 到 6 之间的值(不含 3 和 6)。

## 洗牌数据集

当训练集中的实例独立且同分布时，深度学习算法效果最佳。确保这一点的一个简单方法是用 *shuffle* 方法来混洗实例。

创建要混洗的 tf.data.Dataset:

```py
# create a dataset
dataset = tf.data.Dataset.range(10).repeat(3)
print ('dataset has', len(list(dataset)), 'elements')

```

无序播放数据集:

```py
# shuffle data into batches of 7
ds = dataset.shuffle(buffer_size=5).batch(7)

for item in ds:
  print (item)

```

我们得到七个元素的张量。注意最后一个张量只有两个元素。我们有四个大小为 7 的张量，等于 28。由于数据集有 30 个元素，我们还剩两个元素。

我们将缓冲区大小设置为 5。因此 TensorFlow 保留了接下来五个样本(或张量)的缓冲区，并随机选择这五个样本中的一个。然后，它将下一个元素添加到缓冲区。每个样本包含一批数据。因此，我们示例中的每个样本包含 7 个元素，因为我们将批处理大小设置为 7。可以通过试验不同的批处理和缓冲区大小来提高性能，但要做到这一点需要时间和精力。

一旦数据集被混洗，每个数据集迭代都会创建一个新的混洗:

```py
# rerun to get a different shuffle
for item in ds:
  print (item)

```

## 张量洛数学

TensorFlow 使用 *tf.math* 模块为数学计算提供了几种运算。熟读 [`www.tensorflow.org/api_docs/python/tf/math`](http://www.tensorflow.org/api_docs/python/tf/math) 中所有可能的数学运算。要执行数学运算，张量必须具有相同的形状。

### 向量张量

让我们创建一些数据:

```py
# create data

v1 = np.array([0, 1, 4, 8, 16])
v2 = np.array([0, 3, 9, 27, 81])

```

将 numpy 数组转换为张量常量，并添加:

```py
conv1 = tf.constant(v1)
conv2 = tf.constant(v2)

result = tf.add(conv1, conv2)
result

```

结果是[0，4，13，35，97]。

将 numpy 数组转换为张量变量，并添加:

```py
varv1 = tf.Variable(v1)
varv2 = tf.Variable(v2)

result = tf.add(varv1, varv2)
result

```

我们得到同样的结果。常量和变量的唯一区别是常量值不能改变。也就是说，它们是不可变的。

减去张量变量:

```py
result = tf.subtract(varv2, varv1)
result

```

var2 减去 var1 就是[0，2，5，19，65]。

混合常量和变量:

```py
result = tf.add(conv1, varv2)
result

```

结果是[0，4，13，35，97]。

等效性测试:

```py
result = tf.equal(varv1, varv2)
result

```

结果是[真，假，假，假，假]。

乘法张量常数:

```py
result = tf.multiply(conv1, conv2)
result

```

结果是[0，3，36，216，1296]。

将张量除以一个值:

```py
result = tf.divide(conv2, 3)
result

```

结果是[0，1，3，9，27]。

### 矩阵张量

数学运算适用于 n 维张量。所以让我们对矩阵张量进行数学运算。

创建一些数据:

```py
# create data
m1 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])
m2 = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])

m1.shape, m2.shape

```

我们刚刚创建了一对 3 `×` 3 矩阵。也就是说，两者都有三行三列。

将 numpy 矩阵转换为张量，并添加:

```py
conm1 = tf.constant(m1)
conm2 = tf.constant(m2)

result = tf.add(conm1, conm2)
result

```

结果是[[2，0，0]，[0，2，0]，[0，0，2]]。

等效性测试:

```py
result = tf.equal(conm1, conm2)
result

```

结果是[[真，真，真]，[真，真，真]，[真，真，真]]，因为张量是等价的。

### tf.data.Dataset 张量

我们可以对 tf.data.Dataset 张量执行数学运算。

创建数据集:

```py
# create a dataset
m = np.array([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])
m

```

我们刚刚创建了一个 3 `×` 4 矩阵。所以这个矩阵由三行四列组成。

将数据集转换为 tf.data.Dataset:

```py
dataset = tf.data.Dataset.from_tensor_slices(m)
dataset

```

tf.data.Dataset 是一个 *TensorSliceDataset* ，因为我们用 *from_tensor_slices* 方法转换了 numpy 数据集。数据集包含三个张量。每个张量的形状是(4，)，这意味着每个张量有四个元素。元素的数据类型为 int64。

显示张量:

```py
for t in dataset:
  print (t)

```

tf.data.Dataset 包含值为[1，2，3，4]、[5，6，7，8]和[9，10，11，12]的三个张量。

变换张量值:

```py
squared_data = dataset.map(lambda x: x ** 2)

# display tensors
for item in squared_data:
  print (item)

```

我们将 lambda 函数映射到张量元素。所以每个元素都是平方的。

## 保存笔记本

虽然在 Google Colab 中实现了 *Autosave* ,但是在执行单元格的时刻和保存发生的时刻之间存在延迟。所以我们建议定期储蓄。

手动保存笔记本只需两步:

1.  点击左上角菜单中的*文件*。

2.  从下拉菜单中点击*保存*。

笔记本保存在 Google Drive My Drive 的 *Colab 笔记本*目录下。

## 将笔记本下载到本地驱动器

Google Drive 是存放 Colab 笔记本的绝佳场所。但是我们也喜欢将笔记本保存到本地驱动器。

将笔记本下载到本地驱动器:

1.  一定要保存好笔记本。

2.  点击左上角菜单中的*文件*。

3.  点击*下载。下拉菜单中的 ipynb* 。

笔记本现在位于*下载*目录中。

## 从本地驱动器加载笔记本

我们使用 Google Drive 作为备份，因为它只提供 15 GB 的可用空间。如果你为一家公司工作，他们可能会提供额外的存储空间。鉴于这种情况，您可能希望使用 Google Drive 作为主存储。

从本地驱动器加载笔记本

1.  Open *谷歌可乐*。

2.  从弹出菜单中，点击*上传*。

3.  点击*选择文件*。

4.  在本地驱动器上找到笔记本并打开它。