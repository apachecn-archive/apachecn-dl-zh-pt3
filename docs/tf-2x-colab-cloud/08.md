# 8.自动文本生成

前馈神经网络通常非常适合分类和回归问题。CNN 非常适合复杂的图像分类。但是前馈网络和 CNN 的激活只在一个方向上流动，从输入层到输出层。由于信号只沿一个方向流动，如果数据模式随时间变化，前馈和卷积网络就不理想。因此，我们需要不同的网络架构来处理受时间影响的数据。

一个**递归神经网络** (RNN)看起来很像一个前馈神经网络，但它也有向后指向的连接。也就是说，一层的输出可以作为网络中另一层的输入。网络中的一层能够更早地通知另一层，这意味着 RNN 具有内置的反馈循环机制，可以充当预测引擎。因此，rnn 作为预测器是很棒的，因为当数据随时间变化时，它们自然工作得很好。

RNN 记得过去，它的决定受到它从过去学到的东西的影响。前馈和卷积网络只记住它们在训练期间学习的内容。例如，前馈图像分类器在训练期间学习图像的样子，然后使用该知识对生产中的其他图像进行分类。虽然 rnn 在训练期间学习类似，但是它们也记住了它们所学的内容，因此它们可以在数据变化时做出好的决策。

章节的笔记本位于以下网址: [`https://github.com/paperd/tensorflow`](https://github.com/paperd/tensorflow) 。

## 自然语言处理

机器学习中一个令人着迷的进步是教会机器如何理解人类交流的能力。专注于理解人类如何交流的机器学习领域是自然语言处理。**自然语言处理** (NLP)是机器学习的一个领域，专注于计算机理解、分析、操纵和潜在生成人类语言的能力。RNNs 是神经网络的一个非常重要的变体，大量用于 NLP。

rnn 非常适合 NLP，因为它们的标准输入是一个单词，而不是前馈和卷积网络等顺序网络作为标准输入的整个样本。因此，rnn 可以灵活地处理不同长度的句子，而序列神经网络由于其固定的结构而无法实现这一点。由于其灵活的结构，rnn 还可以共享跨文本的不同位置学习的特征。

RNN 的反馈循环能力允许它解析句子中的每个单词，并对其进行激活。然后，可以将单词的激活值反馈给解析句子的层。所以激活值告诉句子从每个单词中学到了什么！并且对于每个单词循环继续，直到网络理解句子。在机器学习口语中，RNN 将句子中的每个单词视为在特定时间“t”出现的单独输入，并使用该输入“t-1”的激活值作为对原始句子的反馈。

启用 GPU(如果尚未启用):

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

测试 GPU 是否处于活动状态:

```
import tensorflow as tf

# display tf version and test if GPU is active
tf.__version__, tf.test.gpu_device_name()

```

导入 *tensorflow* 库。如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

## 使用 RNN 生成文本

如上所述，rnn 通常用于自然语言任务。通常，我们可以通过字符或单词来模拟自然语言任务。我们首先构建一个生成文本的*字符级*模型。在下一章中，我们建立一个预测情感的单词级模型。

### 文本文件

我们将学习查尔斯·狄更斯的《双城记》。为了方便起见，我们已经下载了纯文本 UTF-8 和处理它，所以你不必。要获得经过*处理的*文本文件，只需遵循以下简单步骤:

1.  前往本书的 GitHub 网址: [`https://github.com/paperd/tensorflow`](https://github.com/paperd/tensorflow) *。*

2.  定位文件:点击*第八章*，点击*数据*，点击 *two_cities.txt.*

3.  点击 *Raw* 按钮。

4.  复制文本( *Ctrl+A+C* )。

5.  粘贴到*记事本*或其他基本文本编辑器( *Ctrl+V* )。

6.  在电脑上另存为 [*two_cities.txt*](https://github.com/paperd/tensorflow) 。

7.  将文件拖放到 Google Drive*Colab Notebooks*文件夹中。

Note

请获取经过处理的版本，因为我们的代码示例是基于这个版本的。我们希望将学习的重点放在文本生成而不是文本处理上。

如果您想获得原始文本文件并自己处理它，请访问 URL [`www.gutenberg.org/ebooks/98`](https://www.gutenberg.org/ebooks/98) 并遵循以下步骤:

1.  点击 [*纯文本 UTF-8*](https://github.com/paperd/tensorflow) 进行格式选择。

2.  复制文本(Ctrl+A+C)。

3.  将其粘贴到 [*记事本*](https://github.com/paperd/tensorflow) 或其他基本文本编辑器中。

4.  点击 [*另存为…*](https://github.com/paperd/tensorflow) 。

5.  将保存类型更改为 [*所有文件*](https://github.com/paperd/tensorflow) 。

6.  在电脑上另存为 [*two_cities.txt*](https://github.com/paperd/tensorflow) 。

7.  处理文本文件。

在 [`https://jss367.github.io/Getting-text-from-Project-Gutenberg.html`](https://jss367.github.io/Getting-text-from-Project-Gutenberg.html) *有一个处理古登堡计划原始文本的优秀指南。*

Note

如果您自己处理原始文本文件，*不要*在本章的代码示例中使用它，因为您的处理可能与我们的代码不一致。如果你想练习文本处理，我们只是想让它可用。

### 安装 Google Drive

我们必须安装 Google Drive 才能访问文本文件:

```
from google.colab import drive
drive.mount('/content/drive')

```

执行完代码单元格后，点击 URL，选择一个 Google 账户，点击*允许*按钮，复制授权码并粘贴到文本框*中输入你的授权码:*，按下键盘上的*回车键*。

Note

请确保您的 Google Drive 上的 *Colab Notebooks* 目录中有该文件！

### 将文集读入内存

在自然语言处理中，文本文档通常被称为语料库。文集是书面文本的集合，尤其是某一特定作者的全部作品或某一特定主题的文章。

Note

所有代码都是基于*处理过的*版本的文本文件。

将语料库读入内存:

```
two_cities = 'drive/My Drive/Colab Notebooks/two_cities.txt'

with open(two_cities) as f:
  corpus = f.read()

```

### 验证语料库

从语料库的开头显示文本:

```
print (corpus[:74])

```

因为我们从头开始，所以很容易验证。但是验证终点需要更多的工作。

获取语料库的长度:

```
len(corpus)

```

现在，我们知道文集的结尾。

通过一点点尝试和错误，我们可以在结尾显示著名的引用:

```
print (corpus[757116:])

```

如果您想探索 NLP 的其他在线书籍，一个很好的起点是位于以下 URL 的*Project Gutenberg*:[`www.gutenberg.org/`](http://www.gutenberg.org/)。

### 创造词汇

因为我们的目标是用字符级模型生成文本，所以我们训练该模型来预测序列中的下一个字符。然后，我们可以反复调用该模型来生成更长的文本序列。

创建语料库中包含的独特字符的词汇表:

```
# unique characters in the corpus
vocab = sorted(set(corpus))
print ('{} unique characters'.format(len(vocab)))

```

我们在 *vocab* 中存储了 74 个独特字符的词汇表。

### 向量化文本

算法处理数字，而不是文本。因此，我们必须设计一个语料库的数字表示。一个简单的解决方案是将文本矢量化。**文本矢量化**是将文本转换成数字表示的过程。

让我们首先创建字典 *int_map* 来保存唯一字符的整数映射。接下来，创建 numpy 数组 *char_map* 来保存每个整数表示的字符映射。numpy 数组允许我们将编码的整数映射转换回它们的字符表示。一旦我们对语料库进行了矢量化，我们就可以为 TensorFlow 消费构建输入管道。

### 创建整数映射

创建字典:

```
# create a dictionary with integer representations of characters
int_map = {key : value for value, key in enumerate(vocab)}
int_map['a']

```

我们使用字典理解来创建 *int_map* ，它保存了语料库的整数映射。**字典理解**是一种使用简单表达创建字典的方法。字典理解采用 *{key: value for (key，value) in iterable}* 的形式。在我们的例子中，键是语料库中的一个惟一字符，值是惟一字符的整数映射。所以整数 *45* 代表字母 *a* 。

### 创建字符映射

创建保存字符映射的 numpy 数组:

```
# create numpy array to hold character mappings of integers

import numpy as np

char_map = np.array(vocab)
char_map[45]

```

看起来我们的映射工作正常。

### 映射序列

映射一个序列，如清单 [8-1](#PC10) 所示。

```
# create variable to hold line break
br = '\n'

# simple sequence
sequence = 'hello world'
print ('original sequence:', sequence, br)

# map to integer representations
maps = np.array([int_map[c] for c in sequence])
print ('integer mappings:', maps, br)

# map integer representations back into characters
s = [char_map[i] for i in maps]

# create string from list of characters
s = ''.join(s)
print ('translation:', s)

Listing 8-1Map a text sequence

```

序列 hello world 用 *int_map* 矢量化，用 *char_map* 转换回原始序列。

### 向量化语料库

既然我们有了映射算法，我们就可以对语料库进行矢量化了:

```
# vectorize the corpus
encoded = np.array([int_map[c] for c in corpus])
encoded[:20], char_map[encoded[:20]]

```

Numpy 数组*编码的*保存矢量化的语料库。我们显示了 20 个整数映射和它们的等价字符，以验证一切正常。

### 预测下一个字符

在 RNN 中，每个神经元都有多重激活，因为它有内置的反馈回路。一个**时间步长**是一个神经元上的单次激活。在训练过程中的每个时间步，我们的目标是在给定一个字符或字符序列的情况下，预测下一个可能的字符。因此模型的输入必须是字符序列。但是必须构建字符序列才能与我们的模型一起工作。

### 创建训练输入序列

为了创建训练实例，我们将编码的语料库分成输入序列。每个输入序列包含来自语料库的 *seq_length* 字符。 *seq_length* 是我们想要的单个字符输入序列的最大长度句子。为了更好的性能，我们将语料库分成等长的序列。

对于每个输入序列，样本包含输入序列，对应的目标包含向右移动一个字符的输入序列。这样创建目标是因为我们试图预测序列中的下一个字符。因此，我们将文本分成几块 *seq_length + 1* 。

首先将编码的语料库转换成张量流张量:

```
# initialize maximum length sequence for a single input
seq_length = 100

# create training dataset
ds = tf.data.Dataset.from_tensor_slices(encoded)
ds

```

### 展示样品

显示训练数据集中的一些样本:

```
for i in ds.take(6):
  print (i.numpy(), ':', char_map[i])

```

我们用之前创建的 *char_map* 数组将每个字符的整数表示转换回它们的字符状态。

### 批处理序列

创建批量序列:

```
sequences = ds.batch(seq_length + 1, drop_remainder=True)

```

批处理方法让我们可以轻松地将单个字符转换成所需大小的序列。使用参数 *drop_remainder=True* 确保所有批次的大小相同。

显示第一批:

```
for i in sequences.take(1):
  print (char_map[i], br)
  print ('batch size:', len(i))

```

批次大小为 *101* 以说明目标设置。

Note

每个目标通过将输入序列向右移动一个字符来表示。

### 创建样本和目标

构建一个创建样品和目标组的函数:

```
def create_sample_target(piece):
  sample = piece[:-1]
  target = piece[1:]
  return sample, target

```

该功能将输入序列移动 *1* 以形成每批的样本和目标文本。所以每个样本包含前 100 个字符，每个目标包含第二个字符到第 101 个字符。

创建数据集:

```
dataset = sequences.map(create_sample_target)

```

将函数映射到*序列*上，以创建由所需样本和目标集合组成的数据集。

显示第一个输入序列中的样本和目标:

```
for sample, target in  dataset.take(1):
  print ('sample:', char_map[sample], br)
  print ('target:', char_map[target])

```

根据需要，目标在样本前面一个字符，因此算法可以从目标学习如何预测下一个字符。

### 时间步长预测

让我们看看清单 [8-2](#PC19) 中所示的样本集和目标集中的前五个时间步长。

```
for i, (input_idx, target_idx) in enumerate(
    zip(sample[:5], target[:5])):
  print('Step:', i)
  print(' input:', input_idx.numpy(),
        char_map[input_idx])
  print(' expected output:', target_idx.numpy(),
        char_map[target_idx])
  if i < 4: print()

Listing 8-2Time step prediction

```

样本向量和目标向量的每个索引作为单个时间步长来处理。也就是说，在样本和目标中处理的每个字符都是一个时间步长。因此，对于时间步长 0 处的输入，模型接收 *A* 的索引(input_idx ),并尝试预测作为下一个字符的 *a 空格*的索引。在下一个时间步，模型重复相同的过程。但是 RNN 模型除了考虑当前输入字符之外，还考虑前一步的上下文。输出结果验证了样本集和目标集的创建是否正确。

### 创建培训批次

数据集已经被分割成可管理的文本序列。但是，我们需要在将数据输入模型之前，对数据进行洗牌并打包成批。

设置批处理和缓冲区大小:

```
BATCH_SIZE = 64
BUFFER_SIZE = 10000

```

无序播放、批处理、缓存和预取:

```
corpus_ds = (dataset
  .shuffle(BUFFER_SIZE)
  .batch(BATCH_SIZE, drop_remainder=True)
  .cache().prefetch(1))

corpus_ds

```

TensorFlow 数据旨在处理无限序列。所以它不会试图打乱内存中的整个序列。相反，它维护一个缓冲区，在那里它打乱元素。我们设置 *BUFFER_SIZE=10000* 来给 TensorFlow 一个相当大的缓冲区大小，但不能大到导致内存问题。我们看到我们的数据集包含批量大小为 64、序列长度为 100 的训练样本和目标。

### 建立模型

首先初始化一些重要的变量。我们将 *vocab_size* 设置为语料库中唯一字符的数量。我们设置*嵌入维度*为 256。**单词嵌入**是 NLP 中的一种学习技术，将词汇表中的单词或短语映射到实数向量。

在实践中，我们使用维数在 50 到 500 之间的单词嵌入向量。我们使用 256，因为我们相信这是处理时间和性能之间的一个很好的折衷。增加单词嵌入的数量可以提供更好的性能。但是更高的嵌入维数在计算上是昂贵的。我们将 *rnn_units* 设为 1024，代表一层输出的神经元数量。

初始化变量:

```
# length of the vocabulary in chars
vocab_size = len(vocab)

# the embedding dimension
embedding_dim = 256

# number of RNN units
rnn_units = 1024

```

创建如清单 [8-3](#PC23) 所示的模型。

```
# generate seed for reproducibility
tf.random.set_seed(0)
np.random.seed(0)

# clear any previous models
tf.keras.backend.clear_session()

# import libraries
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense,\
Embedding
from tensorflow.keras import losses

# create the model
model = Sequential([
  Embedding(vocab_size, embedding_dim,
            batch_input_shape=[BATCH_SIZE, None]),
  GRU(rnn_units, return_sequences=True,
      stateful=True, recurrent_initializer="glorot_uniform"),
  Dense(vocab_size)
])

Listing 8-3Create the model

```

第一层是嵌入层，以词汇大小、嵌入维度和批处理的输入形状作为输入。来自嵌入层的输出馈入第二层，该层是具有 1024 个神经元的 GRU 层(由 *rnn_units* 参数标识)。为了保留在这一层学到的东西，我们设置 *return_sequences=True* 和 *stateful=True* 。我们还通过设置*recurrent _ initializer =‘glorot _ uniform’*告诉 GRU 层从均匀分布中抽取样本。来自 GRU 层的输出馈入最终的密集层，词汇大小作为输入。

**门控循环单元** (GRUs)是 RNNs 中的门控机制，类似于具有遗忘门的长短期记忆(LSTM)，但比 LSTM 具有更少的参数，并且缺少输出门。

关于格鲁什的深入讨论，请参考以下网址: [`https://arxiv.org/ftp/arxiv/papers/1701/1701.05923.pdf`](https://arxiv.org/ftp/arxiv/papers/1701/1701.05923.pdf) 。

### 显示模型摘要

检查模型:

```
model.summary()

```

第一层是嵌入。因此，通过将词汇量 74 乘以嵌入维数 256 来计算可学习参数的数量，总数为 18，944。

第二层是 GRU。可学习参数的数量因此基于公式*3*`×`*(n*<sup>*2*</sup>`×`*Mn+2n)*其中 *m* 是输入维度而 *n* 是输出维度。乘以 *3* 因为 GRU 有三组运算需要这些大小的权重矩阵。由于 RNN 的反馈回路，将 *n* 乘以 *2* 。所以我们有 3938304 个可学习的参数。下面是我们分析结果的方式:

*   * 3`×`(1024<sup>2</sup>+1024`×`256+2`×`1024)

*   * 3 `×` (1048576 + 262144 + 2048)

*   * 3 `×` 1312768

*   * 3,938,304

正如我们所见，计算第二层的可学习参数相当复杂。所以我们来逻辑分解一下。GRU 层是具有反馈回路的前馈层。前馈网络的可学习参数通过将前一层(256 个神经元)的输出乘以当前层的神经元(1024 个神经元)来计算。对于前馈网络，我们还必须考虑这一层的 1024 个神经元。但是由于 RNN 的反馈机制，我们将这一层的 1024 个神经元乘以 2。最后，当前层的 1024 个神经元被反馈，产生 1024 个 <sup>2</sup> 可学习参数。GRU 使用三组需要权重矩阵的操作(隐藏状态、重置门和更新门)，因此我们将可学习参数乘以 3。

第三层是致密的。因此，通过将输出维度 74 乘以输入维度 1，024 并加上 74 来计算可学习参数的数量，以说明该层的神经元总数为 75，850。

### 检查输出形状

显示数据集中第一批的形状:

```
for sample, target in corpus_ds.take(1):
  example_batch_predictions = model(sample)

example_batch_predictions.shape

```

第一批有 *64* 的 *batch_size* ， *100* 的 *sequence_length* ，以及 *74* 的 *vocab_size* 如预期。请注意，密集图层的模型摘要输出形状为(64，无，74)。不包括序列长度，因为模型可以在任何长度的输入上运行。

### 计算损失

我们从输出分布中取样来预测字符索引。输出分布由我们字符词汇表上的逻辑定义。一个 **logit** 是从 logit 函数中导出的介于 0 和 1 以及负无穷大和无穷大之间的概率值。简单地说，逻辑就是预测。logit 函数与 sigmoid 函数相反，因为它将 Y 轴上的值限制在 0 和 1 之间，而不是 X 轴上的值。由于我们的模型返回 logits，我们需要设置 *from_logits* 标志来计算损失。

构建一个函数来计算损失:

```
def loss(labels, logits):
  return losses.sparse_categorical_crossentropy(
      labels, logits, from_logits=True)

```

调用函数:

```
pre_trained_loss = loss(target, example_batch_predictions)

```

该模型需要一个由批量大小、序列长度和词汇大小组成的 3D 张量。

让我们看看形状是否正确:

```
print('pred shape: ', example_batch_predictions.shape)
print('scalar_loss: ', pre_trained_loss.numpy().mean())

```

太好了！预测形状与预期的一样，批大小为 64，序列长度为 100，词汇大小为 74。我们还显示了预训练模型的平均损失。

### 编译模型

编译模型:

```
model.compile(loss=loss,
              optimizer='adam')

```

### 配置检查点

对于 RNN，我们希望保存模型在每个时间步学到的东西。一种方法是用回调方法保存保存这些信息的检查点。**检查点**捕捉所有张量流参数(或 tf)的精确值。Variable.objects)由模型使用。

清单 [8-4](#PC30) 保存了检查点，这样我们可以回忆一下 RNN 学到了什么。

```
import os

# directory where the checkpoints will be saved
checkpoint_dir = './training_checkpoints'

# name of the checkpoint files
checkpoint_files = os.path.join(checkpoint_dir,
 'ckpt_{epoch}')

# callback method
checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_files,
 save_weights_only=True)

Listing 8-4Configure and save checkpoints

```

### 训练模型

为十个时期训练模型:

```
EPOCHS = 10

history = model.fit(corpus_ds, epochs=EPOCHS,
                    callbacks=[checkpoint_callback])

```

我们可以添加更多的纪元来提高性能。我们告诉模型用 *checkpoint_callback* 保存检查点。

### 为文本创建重建模型

现在我们有了一个训练有素的 RNN，我们重新构建模型，一次预测一个角色。我们分三步重建:

1.  从检查点恢复权重。

2.  批量为 1 的重建。

3.  加载权重并重塑模型，以确保张量的批量大小为 1。

#### 从检查点恢复权重

从训练期间建立的检查点恢复重量。我们恢复检查点，以获得 RNN 在每个时间步学到的东西。

恢复:

```
tf.train.latest_checkpoint(checkpoint_dir)

```

#### 批量为 1 的重建

因为我们构造了我们的序列来预测一个字符，所以我们一次预测一个字符。由于 RNN 状态从一个时间步长传递到另一个时间步长的方式，模型一旦建立就只接受固定的批量大小。

重建批量为 1(而不是 64)的模型，如清单 [8-5](#PC33) 所示。

```
# generate seed for reproducibility
tf.random.set_seed(0)
np.random.seed(0)

# clear any previous models
tf.keras.backend.clear_session()

# set batch size to 1
BATCH_SIZE = 1

# Rebuild model
model = Sequential([
  Embedding(vocab_size, embedding_dim,
            batch_input_shape=[BATCH_SIZE, None]),
  GRU(rnn_units, return_sequences=True,
 stateful=True, recurrent_initializer="glorot_uniform"),
  Dense(vocab_size)
])

Listing 8-5Rebuild the model

```

#### 加载重量和整形

加载从已训练的 RNN 保存的权重，并重塑模型，以确保张量的批量大小为 1:

```
model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
model.build(tf.TensorShape([1, None]))

```

### 模型摘要

检查模型:

```
model.summary()

```

请注意，输出形状的第一个参数是 1，表示批处理大小已按预期更改。

### 创建组件以生成文本

要创建新文本，需要创建一个函数，并初始化一组变量以提供给该函数。要为 TensorFlow 消费准备起始字符串，请在将其传递给函数之前对其进行矢量化和整形。

#### 创建函数

创建文本生成函数，如清单 [8-6](#PC36) 所示。

```
def create_text(model, input_eval, temperature, start_string):

  # Empty string to store our results
  new_text = []

  # Here batch size == 1
  model.reset_states()

  for i in range(n):
    # model encoded input
    predictions = model(input_eval)

    # remove batch dimension so we can manipulate predictions
    predictions = tf.squeeze(predictions, 0)

    # divide predictions by temperature
    predictions = predictions / temperature

    # use a categorical distribution to predict character
    # returned by model
    predicted_id = tf.random.categorical(
        predictions, num_samples=1)[-1,0].numpy()

    # pass predicted character as next input to model
    # with previous hidden state
    input_eval = tf.expand_dims([predicted_id], 0)

    # append generated characters to text
    new_text.append(char_map[predicted_id])

  return (start_string + ''.join(new_text))

Listing 8-6Create the text function

```

该函数接受模型、矢量化起始字符串、温度和原始起始字符串。它首先初始化一个列表来保存创建的新文本，并重置模型的状态。该函数继续迭代 *n* 次(我们希望创建的字符数)。

**温度**是神经网络的超参数，用于在应用 softmax 激活之前，通过缩放逻辑来控制预测的随机性。更简单地说，温度表示在计算 softmax 激活之前要除以多少。

在迭代过程中，该函数对编码的起始字符串进行建模，并将结果放入*预测*中。然后它移除额外的 *1* 维度，这样它就可以用*预测*的内容除以*温度*。该函数的下一个任务是使用分类分布来预测模型返回的下一个字符。该函数需要将 *1* 维加回来，这样它就可以将预测的字符作为下一个输入连同之前的隐藏状态一起传递给模型。新生成的字符被附加到 *new_text* 数组中。重复该过程，直到循环消失。

#### 初始化变量

初始化变量:

```
n = 500
temp = 0.3
start_string = 'Tale'

```

将 *n* 设置为我们希望创建的字符数。继续设置温度和起始弦。较低的温度产生更可预测的文本，而较高的温度产生更令人惊讶的文本。您可以尝试找到最佳设置。您也可以尝试使用起始字符串。我们选择了*故事*，因为我们知道语料库包含这个名字。

#### 对起始字符串进行矢量化和整形

对起始字符串进行矢量化，因为模型只识别数字。重塑张量流消耗的矢量化起始字符串。也就是说，将 1 维添加到起始字符串，以便模型可以处理它。显示形状以验证一切正常。

对起始字符串进行矢量化，并针对 TensorFlow 消费对其进行整形，如清单 [8-7](#PC38) 所示。

```
# vectorize starting string
input_vectorized = [int_map[s] for s in start_string]
print ('original shape:', end=' ')
print (str(np.array(input_vectorized).ndim) + 'D', br)

# reshape string for TensorFlow model consumption
input_vectorized = tf.expand_dims(input_vectorized, 0)
print ('new shape:', input_vectorized.shape)

Listing 8-7Vectorize and reshape the starting string

```

新的形状是(1，4)。 *1* 尺寸表示批量为 1。 *4* 尺寸表示起始字符串的长度。尝试使用起始字符串来查看不同文本的生成。

### 创建新文本

种植随机种子并调用函数:

```
tf.random.set_seed(0)
np.random.seed(0)

print (create_text(model, input_vectorized, temp, start_string))

```

哇！尽管这些句子是无意义的，但是这个模型创造了真实的句子。如果你仔细想想，我们刚刚做的事情是惊人的。该模型摄取了一个语料库，并能够从中学习。