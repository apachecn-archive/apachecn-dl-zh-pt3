# 3.使用TensorFlow数据

我们引入TensorFlow数据集(TFDS)。我们用代码示例讨论了 TFDS 的许多方面。我们继续一个完整的 TFDS 建模例子。

章节的笔记本位于以下网址: [`https://github.com/paperd/tensorflow`](https://github.com/paperd/tensorflow) 。

*TFDS* 是一个数据集集合，可用于 TensorFlow。像所有 TensorFlow 可消费数据集一样，TFDS 被公开为 tf.data.Datasets，这允许我们创建易于使用的高性能输入流水线。

启用 GPU(如果尚未启用):

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

测试 GPU 是否处于活动状态:

```py
import tensorflow as tf

# display tf version and test if GPU is active
tf.__version__, tf.test.gpu_device_name()

```

导入 *tensorflow* 库。如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

## TensorFlow数据集(TFDS)

以下网址提供了有关 TFDS 的更多信息:

*   [T2`www.tensorflow.org/datasets`](http://www.tensorflow.org/datasets)

*   [T2`www.tensorflow.org/datasets/overview`](http://www.tensorflow.org/datasets/overview)

*   [T2`www.tensorflow.org/datasets/catalog/overview`](http://www.tensorflow.org/datasets/catalog/overview)

从介绍 TFDS 的第一个网址开始。第二个 URL 演示了如何显示所有 TFDS 和附加技术信息的列表。第三个网址显示了 TFDS 是如何分类的。

## 晚上吃可乐

当我们长时间(几个小时)不停顿地运行 Google Colab 或将大型数据集加载到内存中并处理所述数据时，它可能会崩溃(或中止)。当这种情况发生时，我们知道你有两种选择:

1.  重新启动所有运行时。

2.  关闭程序，从头开始重新启动。

## 可用 TFDS

让我们从显示可用 TFDS 列表开始:

```py
import tensorflow_datasets as tfds

# See available datasets
tfds.list_builders()

```

首先导入 *tfds* 模块。使用 *list_builders()* 方法进行显示。

找出 tensorflow_datasets 容器中有多少个 TFDS:

```py
print (str(len(tfds.list_builders())) + ' datasets')

```

哇！我们可以使用 244 个数据集来练习TensorFlow。

仔细阅读下面的链接，你会发现一个非常好的关于 TFDS 的教程:

[T2`https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb`](https://colab.research.google.com/github/tensorflow/datasets/blob/master/docs/overview.ipynb)

## 给 TFDS 装子弹

我们可以用一行代码加载一个 TFDS！在 TFDS 网站的教程中，说明我们必须安装 *tensorflow-datasets* 。但是，在 Google Colab 中，我们不必这样做。

Tip

如果您在 Google Colab 之外的环境中工作，您可以通过运行下面的代码片段来安装 tfds 模块:`!pip install tensorflow-datasets`

导入 tfds 模块:

```py
import tensorflow_datasets as tfds

```

用 tfds 直接加载定型数据，Load:

```py
# load train set
train, info = tfds.load('mnist', split="train",
 with_info=True)
info

```

**tfds.load** 函数将命名的数据集加载到 tf.data.Dataset 中。我们添加了 *info* 元素来显示关于数据集的有用元数据。**元数据**是描述和给出关于其他数据的信息的一组数据。

直接加载测试数据:

```py
# load test data
test, info = tfds.load('mnist', split="test", with_info=True)
info

```

我们从 TFDS 容器中装载了 MNIST 火车和测试数据集。 **MNIST** 数据库(改进的国家标准和技术研究所数据库)是一个手写数字的大型数据库，通常*用于训练各种图像处理系统。该数据库也被广泛用于机器学习领域的培训和测试。该数据库由一组 60，000 个训练示例和 10，000 个测试示例组成。我们包含了 *info* 元素，它提供了关于数据集的详细信息。*

Note

在机器学习术语中，数据元素通常被描述为示例或样本。example 这个词可以和 sample 互换使用。

虽然前馈神经网络往往在图像上表现不佳，但 MNIST 是一个例外，因为它经过大量预处理，图像很小。也就是说，MNIST 图像大致具有相同的小尺寸，位于图像空间的中间，并且垂直取向。

## 提取有用的信息

info 元素包含允许我们提取数据集特定信息的方法。

清单 [3-1](#PC7) 提取关于类的信息。

```py
# create a variable to hold a return symbol
br = '\n'

# display number of classes
num_classes = info.features['label'].num_classes
class_labels = info.features['label'].names

# display class labels
print ('number of classes:', num_classes)
print ('class labels:', class_labels)

Listing 3-1Extracting meaningful information from a dataset

```

我们只是用 *features* 方法提取了类和类标签的数量。

## 视察 TFDS

我们有两种方法来检查元素:

1.  打印元素。

2.  用 *lemon_spec* 方法打印元素。

打印元素:

```py
# display training and test set
print (train)
print (test)

```

打印带有 element_spec 的元素:

```py
# display with element_spec method
print (train.element_spec)
print (test.element_spec)

```

无论哪种方式，输出都非常相似。张量形状是(28，28，1)。所以训练和测试数据由 28 张`×` 28 张图像组成。1 值意味着图像以灰度显示。图像数据(特征集)由 tf.uint8 数据组成，标签(或目标)数据由 tf.int64 数据组成。

一幅*灰度*图像是这样一幅图像，其中每个像素的值是仅代表光量的单个样本。也就是说，它只携带强度信息。灰度图像完全由灰色阴影组成。图像的对比度从最弱的黑色到最强的白色。

我们还可以用一行代码显示来自 TFDS 的训练示例:

```py
# Show train feature image examples

fig = tfds.show_examples(train, info)

```

*show_examples* 方法显示来自 tf.data.Dataset 的样本图像。

## 功能词典

所有 TFDS 都包含将特征名称映射到张量值的特征字典。默认情况下，tfds.load 返回一个 tf.Tensors. A **tf 的*字典*。张量**表示 TensorFlow 中数据的矩形数组。

一个典型的数据集，比如 MNIST，有两个键:*图像*和*标签*。让我们用 take(1)检查*一个*样品。我们输入到 *take* 函数中的数字表示我们从数据集接收到的样本数。

从训练数据集中抽取一个样本并显示其关键字:

```py
for sample in train.take(1):
  print (list(sample.keys()))

```

我们果然看到了两把钥匙。图像键引用数据集中的图像。标签键引用数据集中的标签。正式的字典结构表示为*{‘image’:TF。张量，“标签”:tf。张量}* 。

现在我们知道了关键点，我们可以轻松地显示第一个训练样本的特征形状和目标值:

```py
for sample in train.take(1):
  print ('feature shape:', sample['image'].shape)
  print ('target value: ', sample['label'].numpy())

```

第一个特征样本的形状为(28，28，1)，第一个标签的值为 4。我们使用 *numpy()* 方法将目标张量转换为标量值。由于机器学习算法使用的任何数据集*必须*具有**相同的形状**，我们从单个样本中获得数据集的形状！

让我们从训练集中获取九个示例:

```py
n, ls = 9, []
for sample in train.take(n):
  ls.append(sample['label'].numpy())
ls

```

我们看到[4，1，0，7，8，1，2，7，1]，它与上一节中来自 *show_examples* 的标签相匹配。

通过将 *as_supervised=True* 参数与 tfds.load 一起使用，我们获得了一个(要素，标签)元组，而不是一个字典:

```py
ds = tfds.load('mnist', split="train", as_supervised=True)
ds = ds.take(1)

for image, label in ds:
  print (image.shape, br, label)

```

样本在表单中(图片，标签)。

我们还可以得到(特征，标签)的一个 *numpy* 元组:

```py
ds = tfds.load('mnist', split="train", as_supervised=True)
ds = ds.take(1)

for image, label in tfds.as_numpy(ds):
  print (type(image), type(label), label)

```

我们使用 *tfds.as_numpy* 来转换 tf。Tensor 到 np.array，tf.data.Dataset 到 Generator[np.array]。

最后可以得到一个*批量* tf。张量:

```py
image, label = tfds.as_numpy(tfds.load(
    'mnist',
 split='train',
 batch_size=-1,
 as_supervised=True,
))

type(image), image.shape

```

通过使用 *batch_size=-1* ，我们可以在单个批处理中加载完整的数据集。我们看到 numpy 张量(60000，28，28，1)，这意味着训练数据包含 60000 个 28 `×` 28 灰度图像。

综上所述， *tfds.load* 默认返回一个字典，一个 tf 的*为 as_supervised=True* 的元组。张量，或者一个带有 *tfds.as_numpy* 的 np.array。请注意，您的数据集可以容纳在内存中，并且所有示例都具有相同的形状。

## 构建输入流水线

缩放、混洗、批处理和预取训练数据:

```py
train_sc = train.map(lambda items:\
 (tf.cast(items['image'],\
 tf.float32) / 255.,\
 items['label']))

train_ds = train_sc.shuffle(10000).batch(32).prefetch(1)
Use the train dataset we loaded earlier in the chapter. Although this looks complicated, we use a lambda function to divide each image by 255\. We then shuffle, batch, and prefetch.

```

**预取**提高了模型性能，因为它提高了批处理过程的效率。当我们的训练算法处理一批时，TensorFlow 正在并行处理数据集，以准备下一批。

规模、批量和预取测试数据:

```py
test_sc = test.map(lambda items:\
 (tf.cast(items['image'],\
 tf.float32) / 255.,\
 items['label']))

test_ds = test_sc.batch(32).prefetch(1)

```

使用我们在本章前面加载的测试集。我们不会打乱测试数据，因为它被认为是新数据。

检查张量:

```py
train_ds, test_ds

```

正如所料，特征形状为(无，28，28，1)。第一个维度是 *None* ，表示批量可以是任意值。

## 建立模型

导入库:

```py
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout

```

创建模型:

```py
# clear previous model
tf.keras.backend.clear_session()

model = Sequential([
  Flatten(input_shape=[28, 28, 1]),
  Dense(512, activation="relu"),
  Dense(10, activation="softmax")
])

```

该模型有一个输入层、一个密集隐藏层和一个密集输出层。输入图层展平图像，以便在下一图层中进行处理。隐藏层接受 512 个神经元的数据。输出层将来自隐藏层的数据接收到代表十个数字类别的十个神经元中。

## 模型摘要

显示模型的摘要:

```py
model.summary()

```

第一层接受 28 张`×` 28 灰度图像。所以我们得到输出形状(无，784)。*无*是第一个参数，因为TensorFlow模型可以接受任何批量。我们通过将 28 乘以 28 再乘以 1 得到第二个参数 *784* 。所以每张图片有 784 个像素。参数的数量为 0，因为第一层不作用于数据。

第二层输出形状是(无，512)，因为我们有 512 个神经元。可训练参数的数量为 *401920* 。我们将第一层的 784 个神经元乘以这一层的 512 个神经元，得到 401408。然后我们从这一层加上 512 个神经元，得到 401920 个。

第三层输出形状是(None，10 ),因为我们有十个类。我们将第二层的 10 乘以 512 个神经元，再加上这一层的 10 个神经元，得到 *5130* 。

## 编译和训练模型

使用优化器、损耗和度量参数编译模型:

```py
model.compile(optimizer='adam',
 loss='sparse_categorical_crossentropy',
 metrics=['accuracy'])

```

训练模型:

```py
epochs = 3
history = model.fit(train_ds, epochs=epochs, verbose=1,
                     validation_data=test_ds)

```

我们为三个时期训练模型。也就是说，我们通过模型传递数据三次。我们用测试数据验证了该模型。我们得到了相当好的结果，仅仅三个纪元就没有太多的过度拟合！

## 对测试数据进行归纳

根据测试数据进行评估总是一个好主意:

```py
model.evaluate(test_ds)

```

## 可视化性能

将培训记录转换为变量:

```py
# get training record into a variable
history_dict = history.history

```

清单 [3-2](#PC27) 绘制了模型的精确度和损失。

```py
import matplotlib.pyplot as plt

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12,9))
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.ylim((0.5,1))
plt.show()

# clear previous figure

plt.clf()

plt.figure(figsize=(12,9))
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

Listing 3-2Visualize training performance

```

可视化显示，我们的模型非常符合数据，因为训练和测试精度非常一致！

## 数据集生成器(tfds.builder)

由于 tfds.load 实际上只是 DatasetBuilder 的一个简单方便的包装器，我们可以直接使用 tfds.builder 为 MNIST 数据集构建一个输入流水线。也就是说，每个 TensorFlow 数据集都作为 DatasetBuilder 公开。

DatasetBuilder 为 TensorFlow 数据集执行多项任务:

*   从哪里下载数据以及如何提取数据并使用*dataset builder . download _ and _ prepare*将其写入标准格式

*   如何用*dataset builder . as _ dataset*从磁盘加载数据

*   关于数据的所有信息，包括名称、类型、特征形状、每个训练和测试分割中的记录数量，以及带有 *DatasetBuilder.info* 的源 URL

*   能够使用 *tfds.builder* 直接实例化任何 DatasetBuilder

然而，与 tfds.load 不同，我们必须通过名称手动获取 DatasetBuilder，调用 *download_and_prepare()* ，*调用 as_dataset()* 。 *tfds.builder* 的优势在于，如果我们需要的话，它允许对加载过程进行更多的控制。

清单 [3-3](#PC28) 展示了如何用 tfds.builder 加载 MNIST

```py
mnist_builder = tfds.builder('mnist')
mnist_info = mnist_builder.info
mnist_builder.download_and_prepare()
datasets = mnist_builder.as_dataset()

Listing 3-3Load MNIST with tfds.builder

```

从 *tfds.builder* 开始创建数据集。用 *info* 方法包含信息。用*下载准备*方法处理数据。将处理过的数据集放入变量中。

构建训练集和测试集:

```py
mnist_train, mnist_test = datasets['train'], datasets['test']

```

使用特征字典获取关键信息:

```py
for sample in mnist_train.take(1):
  print ('feature shape:', sample['image'].shape)
  print ('target value: ', sample['label'].numpy())

```

我们看到第一个特征的形状为(28，28，1)，目标值为 4。

## MNIST 元数据

像 tfds.load 一样，tfds.builder 可以访问关于 MNIST 的元数据:

```py
mnist_info

```

我们看到了很多关于 MNIST 的有用信息。

访问功能信息:

```py
mnist_info.features

```

我们看到关于图像和标签的有用信息。

清单 [3-4](#PC33) 显示了类别和类别标签的数量。

```py
# display number of classes
num_classes = mnist_info.features['label'].num_classes
class_labels = mnist_info.features['label'].names

# display class labels
print ('number of classes:', num_classes)
print ('class labels:', class_labels)

Listing 3-4Number of classes and class labels

```

访问形状和数据类型:

```py
print (mnist_info.features.shape)
print (mnist_info.features.dtype)

```

访问图像信息:

```py
print (mnist_info.features['image'].shape)
print (mnist_info.features['image'].dtype)

```

访问标签信息:

```py
print (mnist_info.features['label'].shape)
print (mnist_info.features['label'].dtype)

```

训练和测试分割:

```py
print (mnist_info.splits)

```

可用的分割键:

```py
print (list(mnist_info.splits.keys()))

```

训练和测试示例的数量:

```py
print (mnist_info.splits['train'].num_examples)
print (mnist_info.splits['test'].num_examples)

```

Note

tfds.load 可以访问与 tfds.builder 相同的元数据。

## 展示示例

正如本章前面所演示的， *tfds.show_examples* 允许我们方便地可视化来自图像分类数据集的图像(和标签)。

让我们展示测试集中的例子:

```py
fig = tfds.show_examples(mnist_test, info)

```

## 准备 DatasetBuilder 数据

为 DatasetBuilder 培训和测试数据准备输入流水线。

缩放、混洗、批处理和预取训练数据:

```py
train_sc = mnist_train.map(lambda items:\
 (tf.cast(items['image'], \
 tf.float32) / 255.,\
 items['label']))

train_build = train_sc.shuffle(1024).batch(128).prefetch(1)

```

规模、批量和预取测试数据:

```py
test_sc = mnist_test.map(lambda items:\
                         (tf.cast(items['image'],\
 tf.float32) / 255.,\
                          items['label']))

test_build = test_sc.batch(128).prefetch(1)

```

检查张量:

```py
train_build, test_build

```

正如所料，特征形状为(无，28，28，1)。

## 建立模型

创建模型:

```py
tf.keras.backend.clear_session()

model = Sequential([
  Flatten(input_shape=[28, 28, 1]),
  Dense(512, activation="relu"),
  Dense(10, activation="softmax")
])

```

## 编译模型

编译:

```py
model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
 metrics=['accuracy'])

```

## 训练模型

火车:

```py
model.fit(train_build, epochs=3, validation_data=test_build)

```

正如所料，结果与使用 tfds.load 进行训练非常相似。

## 对测试数据进行归纳

基于测试数据进行评估:

```py
model.evaluate(test_build)

```

## 加载 CIFAR-10

让我们使用 DatasetBuilder 方法 tfds.builder 来操作另一个数据集，并展示一些例子。 **CIFAR-10** 数据集由 10 类 60，000 32 `×` 32 张彩色图像组成，每类 6000 张图像。有 50，000 个训练图像和 10，000 个测试图像。

这十个类是

```py
[airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]

```

这些类是完全互斥的。汽车和卡车之间没有重叠。汽车类包括轿车、运动型多功能车和其他类似的东西。*级卡车*只包括大卡车。两者都不包括皮卡车。

数据集分为五个训练批次和一个测试批次，每个批次有 10，000 幅图像。测试批次包含从每个类别中随机选择的 1，000 个图像。训练批次以随机顺序包含剩余的图像，但是一些训练批次可能包含来自一个类的比来自另一个类的更多的图像。在它们之间，训练批次正好包含来自每个类的 5，000 个图像。由于 CIFAR-10 是通过批处理进行预处理的，因此我们可以使用此信息来更好地为训练模型批量处理数据。

清单 [3-5](#PC49) 加载并处理 TensorFlow 消费的数据集。

```py
cifar10_builder = tfds.builder('cifar10')
cifar10_info = cifar10_builder.info
cifar10_builder.download_and_prepare()
cifar10_train = cifar10_builder.as_dataset(split='train')
cifar10_test = cifar10_builder.as_dataset(split='test')

Listing 3-5Prepare the CIFAR-10 dataset for TensorFlow consumption

```

检查列车组:

```py
cifar10_train

```

我们看到像张量有形状(32，32，3)。而标签张量有形状()。所以每张图像由 32 个 32 像素表示。 *3* 值意味着图像是彩色的。图像张量的数据类型为 tf.uint8。每个标签都是一个标量值。标签张量的数据类型是 tf.int64。

TensorFlow 利用 RGB 颜色模型来生成彩色图像。 **RGB 颜色模型**是一种加色模型，其中红色、绿色和蓝色光以各种方式相加，以再现各种颜色。模型的名字来自于三种加色原色的首字母，即红、绿、蓝。

## 检查数据集

获取有关数据集的信息:

```py
cifar10_info

```

访问功能信息:

```py
cifar10_info.features

```

获取类名:

```py
cifar10_info.features['label'].names

```

获取可用的分割键:

```py
print (list(cifar10_info.splits.keys()))

```

显示训练示例:

```py
fig = tfds.show_examples(cifar10_train, info)

```

使用特征字典显示列车标签:

```py
[sample['label'].numpy() for sample in cifar10_train.take(9)]

```

为了简化编码，我们使用了列表理解。

为了完整起见，请展示测试示例:

```py
fig = tfds.show_examples(cifar10_test, info)

```

## 准备输入流水线

缩放、混洗、批处理和预取训练数据:

```py
train_sc = cifar10_train.map(lambda items:\
                             (tf.cast(items['image'],\
 tf.float32) / 255.,\
                              items['label']))

train_cd = train_sc.shuffle(1024).batch(128).prefetch(1)

```

规模、批量和预取测试数据:

```py
test_sc = cifar10_test.map(lambda items:\
                           (tf.cast(items['image'],\
 tf.float32) / 255.,\
                            items['label']))

test_cd = test_sc.batch(128).prefetch(1)

```

检查张量:

```py
train_cd, test_cd

```

## 对数据建模

创建模型:

```py
tf.keras.backend.clear_session()

model = Sequential([
  Flatten(input_shape=[32, 32, 3]),
  Dense(512, activation="relu"),
  Dense(10, activation="softmax")
])

```

我们*必须*得到正确的输入形状。对于 CIFAR-10，输入形状是(32，32，3)！

检查模型:

```py
model.summary()

```

第一层接受并展平 32 个彩色图像。所以我们得到输出形状(无，3072)。*无*是第一个参数，因为TensorFlow模型可以接受任何批量。我们通过将 32 乘以 32 再乘以 3 得到第二个参数 *3072* 。每个图像有 1024 个像素，这是 32 乘以 32 的结果。由于每张图像都是彩色的，我们将 1024 乘以 3 得到 3072 个神经元。参数的数量是 0，因为这是第一层。

第二层输出形状是(无，512)，因为我们有 512 个神经元。参数个数为 *1573376* 。我们将第一层的 3072 个神经元乘以这一层的 512 个神经元，得到 1572864。然后我们从这一层加上 512 个神经元，得到 1，573，376 个。

第三层输出形状是(None，10 ),因为我们有十个类。我们将第二层的 10 乘以 512 个神经元，再加上这一层的 10 个神经元，得到 *5130* 。

编译模型:

```py
model.compile(optimizer='adam',
 loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

```

训练模型:

```py
epochs = 3
history = model.fit(train_cd, epochs=epochs, verbose=1,
                    validation_data=test_cd)

```

50%以下的准确率不好。我们的模型表现很差，因为前馈神经网络不是为处理图像数据而设计的。

我们的目标是向您展示如何加载和建模 TFDS。我们将在后面的章节中介绍适用于图像数据的模型。