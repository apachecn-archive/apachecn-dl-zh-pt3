# 2.用 Google Colab 建立你的第一个神经网络

我们在 Google Colab 云服务中使用 Python 的 TensorFlow 2.x 库完成了一个完整的深度学习示例。我们还演示了如何将您的 Google Drive 与 Colab 云服务链接起来。

章节的笔记本位于以下网址: [`https://github.com/paperd/tensorflow`](https://github.com/paperd/tensorflow) 。

用 TensorFlow 2.x 构建一个有能力的神经网络相对容易。数据科学专业人员遵循几个步骤:

1.  获取原始数据。

2.  探索和预处理原始数据。

3.  将原始数据分成训练测试集。

4.  创建 tf.data.Dataset。

5.  准备输入管道。

6.  创建并训练神经网络模型。

数据科学家想要原始数据，因为它是未被触及的。他们想要清理、修改和塑造原始数据，以便为他们手头的特定问题挖掘出意义。如果数据已经被处理过，它的大部分意义可能已经失去了。在尝试预处理之前，探索数据以了解它的样子总是一个好主意。一旦数据被清理和争论，它被分割成训练测试集。

我们为 TensorFlow 消费创建一个 tf.data.Dataset。一旦形式正确，我们就准备输入管道。在准备期间，进一步的数据清理和争论可能是必要的。然后，我们使用输入管道数据创建并训练神经网络模型。

尽管现实需要所有这些步骤，但我们还是专注于使用 TensorFlow 2.x 进行数据建模，因为对于新手来说，它有一个陡峭的学习曲线。因此，我们从一个简单的预处理数据集开始，将学习者从数据预处理和其他相关任务的负担中解放出来。

## GPU 硬件加速器

为了大大加快处理速度，我们可以使用谷歌的 Colab GPU。但是，我们必须在每个新创建的笔记本电脑中启用 GPU:

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

免费 GPU 有使用限制。更多详情请咨询以下网址: [`https://research.google.com/colaboratory/faq.html#resource-limits`](https://research.google.com/colaboratory/faq.html%2523resource-limits) *。*

测试 GPU 是否处于活动状态:

```
import tensorflow as tf

# display tf version and test if GPU is active
tf.__version__, tf.test.gpu_device_name()

```

导入 *tensorflow* 库。如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

## load_digits 数据集

*load_digits* 数据集是 scikit-learn 库的一部分，sci kit-learn 库是一个针对 *Python* 编程语言的免费软件机器学习库。该库具有易于使用和操作的各种分类、回归和聚类算法。

load_digits 数据集经过大量预处理，因此我们不必担心清理或争论。它由 1，797 张 8 像素的图像组成。每个图像是一个 64 像素的矩阵，代表一个从 0 到 9 的手写数字。我们用 8 乘以 8 得到 64 个像素。**像素**是一个 0 到 255 之间的整数，用来表示图像数据。load_digits 数据集通常用于训练机器学习系统，通过算法*分类*来识别手写数字的图像。

load_digits 中的 *images* 容器保存手写数字的图像数据。*数据*容器保存展平的特征向量。**特征向量**是表示某个对象的数字特征的 n 维向量。在我们的例子中，一个 64 像素的特征向量代表一个手写数字的图像。需要展平步骤来准备输入到完全连接的神经网络层中的图像数据。特征向量具有 64 的长度，因为每个 8 `×` 8 图像通过乘以 8 行和 8 列像素而变平。*目标*容器保存目标值，而*目标名称*容器保存目标名称。 *DESCR* 容器保存数据集的描述。

## 浏览数据集

加载数据集:

```
import tensorflow as tf
from sklearn.datasets import load_digits

# get data
digits = load_digits()

# get available containers (or keys) from dataset
print (digits.keys())

```

如果您还没有这样做，请导入 *tensorflow* 库。另外，从 *sklearn* 库中导入数据集。获取数据并显示其键。

创建变量来保存数据、图像、目标和目标名称:

```
# create variables

data = digits.data
images = digits.images
targets = digits.target
target_names = digits.target_names

```

清单 [2-1](#PC4) 显示了数据集的基本信息。

```
br = '\n' # create a newline variable

# display tensor information

print ('data container:')
print (str(data.ndim) + 'D tensor')
print (data.shape)
print (data.dtype, br)

print ('image container:')
print (str(images.ndim) + 'D tensor')
print (images.shape)
print (images.dtype, br)

print ('targets container:')
print (str(targets.ndim) + 'D tensor')
print (targets.shape)
print (targets.dtype, br)

print ('target names container:')
print (target_names)

Listing 2-1Information about the dataset

```

数据容器是一个 2D 张量，具有 1，797 个 64 像素元素的展平向量。图像容器是一个具有 1，797 ^ 8 个矩阵的 3D 张量。图像容器保存原始的未展平图像。目标容器是一个 1D 张量，有 1，797 个 0 到 9 之间的目标值。target_names 容器包含分类标签 0–9。因此，分类是基于由 0 到 9 之间的数字表示的 10 个类别。

让我们想象一下第一个数据元素:

```
# first image begins at index 0

image = images[0]
import matplotlib.pyplot as plt
plt.imshow(image, cmap="binary")
plt.show()

```

*我们*看到的图像是零，但是电脑不能像我们一样*看到*。然而，我们可以训练他们理解这个数字是零。

显示第一张图像的目标值(或标签)和特征图像(8 `×` 8 矩阵):

```
# first digit begins at index 0
target = targets[0]
print ('digit is:', target, br)

# image matrix of first image
print ('image matrix:', br)
print (image)

```

计算机将第一个标签识别为 0。并且它将第一图像矩阵与该标签相关联。有了图像和它们相关的标签，我们可以训练计算机区分数字图像。

## 图像矩阵

更仔细地检查图像矩阵是训练的关键:

1.  矩阵中的数字代表灰度强度。

2.  每个 0 代表*空白*。

3.  更高的数字代表更深的灰色*到黑色*的阴影*。*

一幅*灰度*图像是这样一幅图像，其中每个像素的值是仅代表光量的单个样本。也就是说，它只携带强度信息。灰度图像完全由灰色阴影组成。图像的对比度从最弱的黑色到最强的白色。

数字越小，灰色越浅(零表示白色)。数字越大，灰色越暗(数字越大，越接近黑色)。因此，计算机能够通过灰度强度矩阵映射来理解数字的形状。

## 将数据分成训练测试集

我们分割数据集以找到与训练数据良好拟合的模型，并用测试数据将该模型推广到新数据。有了 load_digits 数据集，我们可以直接使用影像数据，也可以对展平后的数据进行整形。让我们重塑扁平化的数据。这是一个很好的练习，因为您将来可能需要重塑数据集。

创建变量以保存输入维度:

```
# Input image dimensions
img_rows, img_cols = 8, 8

```

重塑要素数据:

```
# Reshape
X = data.reshape(data.shape[0], img_rows, img_cols)
print ('X reshaped:', X.shape)
print ('number of dimensions:', X.ndim)

```

新的特征数据集是(1797，8，8)，这就是我们想要的。

建立目标数据集:

```
y = targets
y.shape

```

目标数据集是(1797，)。

既然我们已经有了要素数据集和相关联的目标，我们就可以开始分割了:

```
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=0)

```

导入 *train_test_split* 方法。将数据分成训练集和测试集。数据集的三分之二用于训练，三分之一用于测试。随机状态是为结果的再现性而设置的。即，结果是一致的。我们在建模示例中使用这种分割。

或者，我们可以直接使用图像，如清单 [2-2](#PC11) 所示。

```
X_alt = images
y_alt = targets

print ('X:', X_alt.shape)
print ('number of dimensions:', X_alt.ndim, br)

X_tra, X_tes, y_tra, y_tes = train_test_split(
    X_alt, y_alt, test_size=0.33, random_state=0)

print ('X_train:', X_tra.shape)
print ('number of dimensions:', X_tra.ndim)
train_percent = X_tra.shape[0] / X_alt.shape[0]
print ('train data percent of X data:', train_percent, br)

print ('X_test:', X_tes.shape)
print ('number of dimensions:', X_tes.ndim)
test_percent = X_tes.shape[0] / X_alt.shape[0]
print ('test data percent of X data:', test_percent, br)

num_images = X_tra.shape[0] + X_tes.shape[0]
print ('total number of images:', num_images)

Listing 2-2Alternative method to split image data

```

## 构建输入管道

TensorFlow 输入管线要求要素数据为 float32 或 float64，标注数据为 int32 或 int64:

```
X_train.dtype, y_train.dtype

```

要素数据为 float64，标签数据为 int64。

具有大范围值的特征数据可能在神经网络中引起误差，这使得学习过程不稳定。缩放缓解了这个问题。缩放还可以加速算法中的计算。*特征缩放*是一种缩放特征数据范围的技术。

缩放训练和测试特征数据:

```
# scale by dividing by the number of pixels in an image
s_train = X_train / 255.0
s_test = X_test / 255.0

```

我们通过将特征图像除以 255 来缩放它们。图像像素存储为 0-255 范围内的整数，这是单个 8 位字节可以容纳的范围。这种划分确保输入像素在 0.0 和 1.0 之间缩放。

为 TensorFlow 消费创建数据对象:

```
train_dataset = tf.data.Dataset.from_tensor_slices((s_train,
 y_train))
test_dataset = tf.data.Dataset.from_tensor_slices((s_test,
 y_test))

```

让我们看看我们的张量是什么样的:

```
print ('train:', train_dataset)
print ('test: ', test_dataset)

```

我们看到，训练和测试张量都由 8 个`×` 8 float64 图像和 int64 标量目标值组成。

## 探索 tensorflow 数据

让我们探索一下我们刚刚创建的 TensorFlow 数据集中的实际内容。

显示来自*第一个*特征图像的切片及其来自训练集的目标:

```
for feature, label in train_dataset.take(1):
  print (feature[0], br)
  print (label)

```

为了简洁起见，我们显示第一幅图像的第一个切片。我们还显示了第一个目标。 *take* 方法从 TensorFlow 数据集中获取样本。在这种情况下，我们只抓取了第一个样本，但是我们可以取 *n* 个元素。

让我们从测试集中获取前两个标签:

```
for _, label in test_dataset.take(2):
  print (label)

```

现在，我们构建输入管道。

## 混洗数据

在讨论数据洗牌之前，我们需要理解几个概念。一个**时期**是整个训练集的一个周期。训练一个神经网络通常需要多个时期。一个**批**的数据就是一组训练样本。深度学习模型不会一次处理整个数据集。他们把数据分成小批。

在每个时期之后混洗数据确保我们不会被太多的坏批次*卡住*。这是如何工作的？混洗减少了模型方差，从而产生更一般化的结果并减少过度拟合。**过度拟合**是指模型对数据训练得太好。当模型很好地学习了训练数据中的细节和噪声，以至于用新数据对模型的性能产生负面影响时，就会发生过度拟合。

混排确保每个数据点在模型上产生一个独立的*变化，而不会受到前面数据点的影响。也就是说，它确保提供给模型的训练数据包含所有类型的数据。*

一个很好的比喻是*洗牌*。我们在玩纸牌游戏之前洗一副牌，因为我们想确保每个玩家和另一个玩家有同样的机会得到一张特定的牌。正如洗牌可以消除偏见一样，在训练神经网络时，在每个时期之前洗牌也可以消除偏见。

## 继续管道建设

我们需要对 TensorFlow 可消耗的训练数据进行洗牌和批处理。我们只批量测试数据。不需要洗牌，因为是新数据。一旦我们设置了批处理和缓冲区的大小，我们就可以开始洗牌和批处理了。

设置批处理和缓冲区大小:

```
BATCH_SIZE = 64
SHUFFLE_BUFFER_SIZE = 100

```

调整批处理和缓冲区大小可以提高性能。我们*任意*设置批量大小为 64，混洗缓冲区大小为 100。您可以尝试不同的值，看看结果是如何受到影响的。我们建议使缓冲区大小相对较大，否则洗牌不会很有效。

为新数据集命名是一个好主意:

```
train_ds = train_dataset\
.shuffle(SHUFFLE_BUFFER_SIZE)\
.batch(BATCH_SIZE)

test_ds = test_dataset.batch(BATCH_SIZE)

```

请注意，我们只打乱了训练数据。原因是因为测试数据应该代表我们的模型没有看到的数据。也就是说，测试数据应该代表新数据。一旦我们在训练数据上运行 shuffle 方法，模型*在每个时期之前自动*打乱数据！

让我们探索一下我们的新数据集:

```
train_ds, test_ds

```

形状是(无，8，8)。我们得到 *None* 作为额外维度。发生了什么事？增加这个额外的维度是因为 TensorFlow 模型可以适应任何批量！

## 前馈神经网络

前馈模型是最简单的网络类型。**前馈**神经网络是一种信息仅在网络中向前传播的网络。数据从输入节点通过隐藏节点(如果有)移动到输出节点。网络中没有循环或环路。

各层*全连接*，意味着每一层都与下一层全连接。全连接层将前一层中的每个节点(或神经元)与后续层中的每个节点连接起来。也就是说，*层*中的每个神经元接收来自存在于*前一层*中的所有神经元的输入。完全连接的层通常称为*密集连接的*。

## 层数

通常，*更多的层会提高模型性能*。但是更多的层需要更多的计算资源。当我们没有太多的训练数据时，具有几层的简单网络往往表现得与具有许多层的复杂网络一样好或者更好。

探索新数据集时，最好从简单的网络开始。我们还建议在数据集上绘制一个小的随机样本，并用简单的网络对其进行训练，以了解其潜在的性能。遵循这个建议可以预先节省时间和计算资源。我们熟悉了数据集。如果简单模型在小样本上不能很好地训练，我们可以获得更多的数据和/或在转移到更复杂的模型之前尝试找出为什么我们性能差。

## 我们的模型

我们构建的第一个模型包含一个输入层、一个隐藏层和一个输出层。神经网络中的第一层*总是*输入层，通知模型输入数据的形状。

我们模型的第一层是展平层。**展平**是将数据转换成 1D 数组的过程。训练完全连接的网络时，输入图层数据应始终为 1D 向量或 2D 矩阵。

*展平*层将每个张量整形为一个形状，该形状等于张量中包含的元素数量，不包括批量维度。该层没有参数。由于这是*输入*层，我们将 *input_shape* 指定为 *(8，8)* 以通知模型每个特征图像由一个 8 `×` 8 矩阵表示。

第二层是密集连接的*隐藏*层。*密集*层将全连接层添加到神经网络中。它包含 256 个神经元(或节点)并使用 *relu* 激活功能。

最后一层是*输出*层。它也是一个密集连接的层，包含十个神经元，并使用 *softmax* 激活函数。输出层必须始终包含与类别数量相同的神经元。因为我们对数字 0-9 进行分类，所以我们有十个类别。

给定一个输入或一组输入，**激活函数**定义了节点的输出。这是一种激活神经网络中每个节点的算法。

**ReLu** (或整流线性单元)是一个分段线性函数，如果为正则直接输出输入值，如果为负则输出零。它是许多网络的默认激活功能，因为它便于更容易的训练，并且通常比其他激活功能实现更好的性能。

**Softmax** 分数大则输出大，分数小则输出小。它常用于类别互斥的分类问题。在这种情况下，每个图像可以是且只能是一个数字。

让我们从定义输入形状开始:

```
for item in train_ds.take(1):
  s = item[0].shape

in_shape = s[1:]
in_shape

```

从训练数据集中取出第一个样本并检索其形状。由于展平层需要每个图像的形状，我们切片这一部分。

导入库以构建层:

```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

```

构建模型:

```
model = Sequential([
    Flatten(input_shape=in_shape),
    Dense(256, activation="relu"),
    Dense(10, activation="softmax")
])

```

## 模型摘要

**摘要**方法显示模型的特征。它显示层、输出形状和参数。

让我们试试这个方法:

```
model.summary()

```

将显示层、输出形状和参数。该模型从一个输出形状(无，64)和 0 参数的展平层开始。批量大小可以是任意数字，因此*无*显示。第一层接受数据的输入形状，但不以任何方式作用于数据。因此，每个输入张量是一个 64 元素向量，有 0 个可训练参数。请记住，每一层都接收前一层的输出。

第一个密集隐藏层接收来自前一层的张量到该层的 256 个神经元中。输出形状是(无，256)，因为这一层有 256 个神经元。可训练参数通过将该层的神经元乘以前一层的神经元并将该层的神经元加到结果中来确定。因此，这一层有 16，640 个参数，因为它输入来自前一层的 64 个神经元，这些神经元被传递给 256 个神经元。64 乘以 256 得到 16384。但是我们仍然要考虑这一层的 256 个神经元。所以 256 加 16384 得到 16640 个参数！

输出层将隐藏层的张量接收到该层的十个神经元中。输出形状是(无，10)，因为这一层有 10 个神经元。前一个隐藏层有 256 个神经元。10 乘以 256 得到 2560 个参数。但是我们在这一层有十个神经元。所以 10 加 2560 得到 2570 个参数。

## 编译模型

**compile** 方法为训练配置模型。我们设置优化器、损失函数和指标。**损失函数**(或目标函数)是训练时最小化的量。它代表了一种成功的衡量标准。**优化器**找到最小化给定损失函数的参数。

我们使用亚当优化器。 *Adam* 是一种自适应学习率方法，它计算不同参数的学习率。亚当是伟大的，因为它*自动*适应最佳训练表现的学习率。

我们使用*稀疏分类交叉熵*损失函数，因为我们的目标是互斥的整数。也就是说，一个数字只能是十个数字中的一个。

编译模型:

```
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

```

我们使用 *Adam* 优化器，因为它性能很好。我们尝试了其他优化器选项，但这个选项在这种情况下工作得很好，它会自动适应学习速率。

仔细阅读以下 URL，查看可用的优化器:

[T2`www.tensorflow.org/api_docs/python/tf/keras/optimizers`](http://www.tensorflow.org/api_docs/python/tf/keras/optimizers)

## 训练模型

由于 train_ds 和 test_ds 由图像和标签组成，我们将它们作为参数包含在用于训练的 *fit* 方法中。我们运行模型 60 个时期，这意味着我们将数据传递给模型 60 次。我们用 train_ds 训练，用 test_ds 验证。

训练模型:

```
history = model.fit(train_ds, epochs=60, validation_data=(test_ds))

```

我们的训练准确率接近 97%。测试准确率接近 95%。我们的模型有点过度拟合，因为测试精度低于训练精度。由于随机效应，您的结果可能略有不同。

出于泛化目的评估模型总是一个好主意:

```
model.evaluate(test_ds)

```

所以我们的模型*用新数据对*进行了大约 95%的概括。

## 模型历史

fit 方法会自动记录训练期间的损耗和度量值。这就是我们将训练分配给变量*历史*的原因。 *history.history* 对象是一个包含培训记录的字典。

将培训记录分配给变量:

```
history_dict = history.history

```

显示字典中的关键字列表:

```
keys = history_dict.keys()
print ('keys:', keys, br)

```

我们使用 loss、accuracy、val_loss 和 val_accuracy 键来引用训练指标。

获取字典的长度，以便我们可以引用最终的度量值:

```
length = len(history_dict['loss']) - 1

```

从长度中减去 1，因为 Python 列表索引从 0 开始。

清单 [2-3](#PC31) 显示了最终的度量值。

```
final_loss = history_dict['loss'][length]
final_loss_val = history_dict['val_loss'][length]
final_acc = history_dict['accuracy'][length]
final_acc_val = history_dict['val_accuracy'][length]
print ('final loss (train/test):')
print (final_loss, final_loss_val, br)
print ('final accuracy (train/test):')
print (final_acc, final_acc_val)

Listing 2-3Display the final training metrics

```

因为我们有了培训指标，我们可以绘制培训和验证损失以及培训和验证准确性，如清单 [2-4](#PC32) 所示。验证损失和准确性是基于测试数据，因为它是新的模型。

```
import matplotlib.pyplot as plt

acc = history_dict['accuracy']
val_acc = history_dict['val_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12,9))
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

# clear previous figure
plt.clf()

plt.figure(figsize=(12,9))
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.ylim((0.5,1))
plt.show()

Listing 2-4Visualize train and test loss and accuracy

```

导入 matplotlib.pyplot 以启用绘图。准确性和损失度量保存在变量中。代码的其余部分使用绘图方法来显示结果。从可视化中，我们看到了训练过程。我们建议在每次神经网络训练练习中绘制训练损失和准确性。

可视化验证了我们的模型是过度拟合的，因为训练精度高于验证(或测试)精度。当然，过度拟合并不剧烈。可视化还显示了培训和验证指标的汇合点或分歧点。为了使模型能够处理新数据，训练和测试的准确性应该尽可能地保持一致。

## 预言

深度学习利用算法自动建模并找到数据中的模式，目标是预测目标输出或响应。如果我们能从新数据中预测，我们就能获得帮助决策的洞察力。

进行预测的一种方法是对测试数据使用*预测*方法:

```
predictions = model.predict(test_ds)

```

*预测*变量保存基于 *test_ds* 的所有预测。每个预测都由一组提供一组概率的值来表示。数组中值的数量基于目标类的数量。所以每个数组保存十个值。数组中概率最高的值是预测的数字。

让我们看看第一个预测(在索引 0 处):

```
predictions[0]

```

很难从浮点数中确定最高概率，所以让我们更容易看出:

```
predictions[0].round(2)

```

概率最高的数组值就是预测值。每个数组位置代表 0 到 9 之间的一个数字。所以位置 1 代表数字 0，位置 2 代表数字 1，依此类推。

使用以下算法得出第一个预测的可信度:

```
import numpy as np

confidence = 100*np.max(predictions[0])
print (str(np.round(confidence, 2)) + '%')

```

所以我们很有信心我们的预测是正确的。

使用该算法，我们基于测试数据集中的第一幅图像来预测数字:

```
first_pred = np.argmax(predictions[0])
print ('predicted:', first_pred)

```

显示的是基于测试数据集中第一个图像的预测数字。这个预测正确吗？

显示测试数据集中的第一个标签:

```
print ('actual:', y_test[0])

```

如果预测的位数与实际标签相同，则预测正确！由于结果可能不同，我们无法确定预测的是什么数字。

让我们根据测试数据集中的前五幅图像进行预测。

清单 [2-5](#PC39) 显示了测试数据的前五个预测，然后显示了我们对每个预测的信心。

```
# first five predictions based on test data:

print ('first five predictions:', end=' ')

p5 = []
for i in range(5):
  p = predictions[i]
  v = np.max(p)
  p5.append(p.tolist().index(v))

print (p5)

# confidence in first five predictions:

print ()
print ('Confidence in our predictions:', br)

c = []
for i in range(5):
  conf = str(round(100*np.max(predictions[i]), 2))
  c.append(conf)
  print (conf + '% for prediction:', p5[i])

Listing 2-5Five predictions and their confidences

```

我们还可以将前五个预测值与实际目标值进行比较，看看我们的模型表现如何。

清单 [2-6](#PC40) 显示了测试数据集中前五个数据元素的预测位数、实际位数、预测置信度和实际图像。

```
# first five predictions from test data

prediction_5 = [np.argmax(predictions[i])\
                for i, row in enumerate(p5)]

# display predicted digits, actual digits, confidences and images

for i, row in enumerate(prediction_5):
  print ('predicted:', target_names[row])
  print ('actual:', target_names[y_test[i]])
  print (str(c[i]) + '%')
  fig, ax = plt.subplots()
  image = ax.imshow(X_test[i], cmap="bone")
  plt.title(target_names[y_test[i]])
  plt.show()
  print (br)

Listing 2-6First five predictions, actual labels, confidences, and images

```

## 获取图像

要获得这本书的图片，只需遵循以下简单步骤:

1.  前往本书的 GitHub 网址: [`https://github.com/paperd/tensorflow`](https://github.com/paperd/tensorflow) *。*

2.  找到您要下载的图像，然后单击它。

3.  点击*下载*按钮。

4.  右键单击图像内的任意位置。

5.  点击*将图像另存为…* 。

6.  将图像保存在您的计算机上。

7.  将图像拖放到 Google Drive*Colab Notebooks*文件夹中。

对于本课，请转到该书的 URL，单击*第二章*，单击*人物*，单击*Figure0201.png*，单击*下载*按钮，在图像内单击右键，单击*将图像另存为…* ，并将图像保存在您的计算机上。将图像拖放到 Google Drive*Colab Notebooks*文件夹中。

![../images/501128_1_En_2_Chapter/501128_1_En_2_Figa_HTML.jpg](../images/501128_1_En_2_Chapter/501128_1_En_2_Figa_HTML.jpg)

## 安装 Google Drive 以显示图像

我们知道 Colab 笔记本是保存到 Google Drive 的。但是我们也可以从 Google Drive 将图像和其他数据加载到 Colab 笔记本中。如果您没有谷歌电子邮件帐户，请创建一个。只需遵循三个简单的步骤:

1.  安装*枕*模块(如有必要)。

2.  安装 Google Drive。

3.  指向图像并展示它。

应该已经安装了 Pillow 库，但是您可以这样安装它:

```
!pip install Pillow

```

*！* symbol 使我们能够调用 shell 命令，如在笔记本中安装 Python 模块。

运行代码单元开始安装过程:

```
from google.colab import drive
drive.mount('/content/gdrive')

```

要继续安装，请单击 URL 链接，选择您希望使用的 Google 帐户，单击*允许*按钮，复制授权码，将其粘贴到*输入您的授权码:*文本框中，然后按下键盘上的 *Enter* 键。听起来工作量很大，但实际上真的很容易。该驱动器安装在 *gdrive/My Drive/Colab 笔记本*上。

现在，确保你在 *Google Drive* 上有图片。该图片包含在我们的 GitHub 网站上。你只需要把它保存到你的电脑上，然后把它拖到一个 Google Drive 目录下。当然，你可以使用任何你想要的图像。

我们将图像保存到 *Colab 笔记本*目录中。你可以把它保存在任何你想保存的地方，但是一定要正确的指向它。

清单 [2-7](#PC43) 创建图像的路径并显示图像。

```
# Be sure to copy the image to this directory on Google Drive
img_path = 'gdrive/My Drive/Colab Notebooks/Figure0201.png'

from PIL import Image
import matplotlib.pyplot as plt
img  = Image.open(img_path)
plt.imshow(img)

Listing 2-7Display an image from Google Drive

```

图像名称是*Figure0201.png*。我们从 *PIL* 库中导入*图像*。Python 在访问 Pillow 模块时需要 PIL。我们还导入了 *matplotlib.pyplot* ，它是 Python 绘图库 *matplotlib* 中的一个模块。我们打开图像并将其显示在 Colab 笔记本中。