# 10.基于递归神经网络的时间序列预测

我们已经为 NLP 利用了 rnn。在这一章中，我们创建了用时间序列数据进行预测的实验。我们使用著名的*天气*数据集来展示单变量和多变量的例子。

RNN 非常适合于时间序列预测，因为它记得过去，并且它的决策受到从过去学到的东西的影响。因此，当数据发生变化时，它可以做出正确的决策。**时间序列预测**是部署一个模型，根据以前观察到的值来预测未来值。

时间序列数据不同于我们迄今为止所处理的数据，因为它是按时间顺序进行的一系列观察。时间序列数据包括时间维度，时间维度是观测值之间的显式顺序依赖关系。

## 天气预报

预报天气是一项困难而复杂的工作。但谷歌、IBM、孟山都和脸书等前沿公司正在利用人工智能技术实现准确及时的天气预报。鉴于我们课程的入门性质，我们不能指望展示如此复杂的人工智能实验。但是我们将向您展示如何使用天气数据构建简单的时间序列预测模型。

章节的笔记本位于以下网址: [`https://github.com/paperd/tensorflow`](https://github.com/paperd/tensorflow) 。

## 天气数据集

我们介绍了一个单变量问题的时间序列预测与 RNNs。然后我们预测一个多元时间序列。我们使用天气时间序列数据来训练我们的模型。我们使用的数据由马克斯·普朗克生物地球化学研究所记录。

通过阅读以下网址了解更多关于该研究所的信息:

[T2`www.bgc-jena.mpg.de/index.php/Main/HomePage`](http://www.bgc-jena.mpg.de/index.php/Main/HomePage)

启用 GPU(如果尚未启用):

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

测试 GPU 是否处于活动状态:

```
import tensorflow as tf

# display tf version and test if GPU is active
tf.__version__, tf.test.gpu_device_name()

```

导入 *tensorflow* 库。如果显示“/设备:GPU:0”，则 GPU 处于活动状态。'如果'..显示时，常规 CPU 处于活动状态。

获取如清单 [10-1](#PC2) 所示的数据集。

```
import os

p1 = 'https://storage.googleapis.com/tensorflow/'
p2 = 'tf-keras-datasets/jena_climate_2009_2016.csv.zip'
url = p1 + p2

zip_path = tf.keras.utils.get_file(
    origin = url,
    fname='jena_climate_2009_2016.csv.zip',
    extract=True)
csv_path, _ = os.path.splitext(zip_path)

Listing 10-1Get weather data

```

使用 *splitext* 方法从 URL 中提取 CSV 文件。创建适当的路径，以便轻松装入熊猫。

将 CSV 文件加载到 pandas:

```
import pandas as pd

df = pd.read_csv(csv_path)

```

## 探索数据

显示数据框的特征:

```
list(df)

```

显示前几条记录:

```
df.head(3)

```

显示最后几条记录:

```
df.tail(3)

```

我们看到数据收集开始于 2009 年 1 月 1 日，结束于 2016 年 12 月 31 日。最后一次记录的数据是在 2017 年 1 月 1 日，但这与我们的目的无关，因为这是该年唯一记录的数据。我们还看到每 10 分钟记录一次数据。所以这个实验的时间步长是 10 分钟。**时间步长**是事件的单次发生。

第一个时间戳开始于 2009 年 1 月 1 日(01.01.2009)，记录的数据从 00:00:00 到 00:10:00。第二个时间戳在 00:10:00 到 00:20:00 之后立即开始。这种模式持续一整天，最后一个时间步长是 23:50:00。第二天(以及所有后续的日子)，2009 年 1 月 2 日(02.01.2009)，遵循相同的模式。通常，时间序列预测预测下一时间步的观测值。

显示数据帧的简明摘要:

```
df.info()

```

数据集包含 15 列:

*   日期时间

*   p(毫巴)–大气压，单位为毫巴

*   t(摄氏度)–以摄氏度为单位的温度

*   tpot(K)–开尔文温度

*   Tdew(摄氏度)–相对于湿度的温度(摄氏度)

*   RH(%)–相对湿度

*   VPmax(毫巴)–以毫巴为单位的饱和蒸汽压

*   VPact(毫巴)–以毫巴为单位的蒸汽压力

*   VPdef(毫巴)–以毫巴为单位的蒸汽压力不足

*   sh(克/千克)-比湿度，单位为克/千克

*   H2OC(毫摩尔/摩尔)-水蒸气浓度，单位为毫摩尔/摩尔

*   ρ(g/m * * 3)-空气密度，单位为克每立方米

*   wv(米/秒)——风速，单位为米/秒

*   最大值 wv(米/秒)——以米/秒为单位的最大风速

*   wd(deg)–风向，单位为度

我们有 14 个特征，因为*日期时间*是一个参考列。没有丢失数据。除了日期时间引用对象，所有数据都是 float64。数据集包含 420，551 行数据，索引范围从 0 到 420550。

显示 14 个功能的统计数据:

```
stats = df.describe()
stats.transpose()

```

*describe* 方法生成描述性统计数据。*转置*方法转置索引和列。

我们还可以显示一个或多个特征的统计数据:

```
stats = df.describe()
stats[['p (mbar)', 'T (degC)']].transpose()

```

显示数据框的形状:

```
df.shape

```

数据帧包含 420，551 行，每行包含 15 列。

## 绘制相对湿度随时间的变化图

由于数据是在熊猫数据框架中，因此很容易根据*日期时间*时间步长绘制 14 个特征中的任何一个。

绘制相对湿度的年周期图:

```
import matplotlib.pyplot as plt

# create new dataframe with just relative humidity column
rh = df['rh (%)']

# plot it!
rh.plot()

```

因为我们每 10 分钟进行一次观察，所以每小时有 6 次观察。并且每天有 144 次(6 次观测`×` 24 小时)观测。

策划前 10 天:

```
rh10 = df['rh (%)'][0:1439]
rh10.plot()

```

因为我们每天有 144 次观察，所以我们总共绘制了 1440 次(10 天`×` 144 次观察/天)观察。注意，步进开始一个 *0* 。

从数据的角度来看(前 10 天)，我们可以看到每天的周期性。我们也看到波动是非常混乱的，这意味着预测更加困难。

在粒度级别按时间步长探索湿度:

```
df[['Date Time','rh (%)']].head()

```

## 预测单变量时间序列

### 缩放数据

将数据帧转换为 numpy 数组:

```
rh_np = rh.to_numpy()

```

如清单 [10-2](#PC15) 所示，缩放 numpy 数据以进行高效培训。

```
br ='\n'

# original data
print ('first five unscaled observations:', rh_np, br)

# scale relative humidity data
rh_sc = tf.keras.utils.normalize(rh_np)
print ('shape after tf function:', rh_sc.shape)

# squeeze out '1' dimension
rh_sq = tf.squeeze(rh_sc)
print ('shape after squeeze:', rh_sq.shape, br)

# convert to numpy
rh_scaled = rh_sq.numpy()
print ('first five scaled observations:', rh_scaled[:5])

Listing 10-2Scale data

```

缩放相对湿度数据。挤出 TensorFlow 函数增加的额外的 *1* 维，这样我们就可以将 TensorFlow 张量转换成一个 numpy 数组，以便于处理。显示前五个缩放的观测值，以验证缩放是否按预期工作。

### 建立培训分割

计算列车分割尺寸:

```
import numpy as np

# train split with 75% of data
train_split = int(np.round(df.shape[0] * .75))
train_split

```

计算测试分割大小:

```
# test split with 25% of data
test_split = df.shape[0] - train_split
test_split

```

计算训练和测试数据的天数:

```
# calculate number of days of data
print (np.round(train_split / 144, 2))
print (np.round(test_split / 144, 2))

```

对于此实验，使用前 315，413 行数据进行训练，剩余的 105，138 (420，551–315，413)行数据用于测试集。训练数据约占 2，190 (315，413/144)天的数据。而测试数据约占 730 (105，138/144)天的数据。

### 创建要素和标签

创建一个将数据集分割成要素和标注的函数，如清单 [10-3](#PC19) 所示。

```
def create_datasets(data, origin, end, window, target_size):

  # list to hold feature set of windows
  features = []

  # list to hold labels
  labels = []

  # establish starting point that reflects window size
  origin = origin + window

  # enable split for test data
  if end is None:
    end = len(data) - target_size

  # create feature set of 'window-sized' elements
  for i in range(origin, end):

    # create index set to identify each window
    indices = range(i-window, i)

    # reshape data from (window,) to (window, 1)
    features.append(np.reshape(data[indices], (window, 1)))

    # create labels
    labels.append(data[i+target_size])

  return np.array(features), np.array(labels)

Listing 10-3Function that creates features and labels

```

该函数接受一个数据集、一个要开始分割的索引、一个结束索引、每个窗口的大小以及目标大小。参数*窗口*是过去信息窗口的大小。 *target_size* 是我们希望我们的模型学习如何预测未来多远。

该函数创建列表来保存要素和标注。然后，它会建立反映窗口大小的起点。为了创建测试集，该函数检查*结束*值。如果 *None* ，它使用整个数据集的长度减去目标大小作为结束值，这样测试集可以从训练集停止的地方开始。

一旦建立了训练和测试起始点，该函数就创建了特征窗口和标签。每个特征窗口的*索引*被建立为迭代期间的下一个窗口。也就是说，每一组后续的*索引*从上一组结束的地方开始。针对 TensorFlow 消费重新调整功能窗口，并将其添加到*功能*列表中。标签作为下一个窗口中的最后一个观察值被创建，然后添加到*标签*列表中。要素和标注都以 numpy 数组的形式返回。

该函数可能看起来令人困惑，但它真正做的是创建一个保存相对湿度观测窗口(对于我们的实验)的特性集和另一个保存目标的特性集。目标基于下一个数据窗口的最后一次相对湿度观测。这是有意义的，因为下一个窗口的最后相对湿度是未来相对湿度的一个很好的指示。因此，特征集变成了包含时间步长观测值的一组窗口。标签集包含每个窗口的预测。

### 创建训练集和测试集

对于训练集，从数据集的索引 0 开始，继续到 315，412。对于测试集，取余数。将窗口大小设置为 20，目标设置为 0。

调用如清单 [10-4](#PC20) 所示的函数。

```
# create train and test sets

import numpy as np

window = 20
target = 0

x_train, y_train = create_datasets(rh_scaled, 0, train_split,
                                   window, target)

x_test, y_test = create_datasets(rh_scaled, train_split, None,
                                 window, target)

Listing 10-4Create train and test sets

```

检查训练和测试数据:

```
print ('train:', end=' ')
print (x_train.shape, y_train.shape)

print ('test:', end=' ')
print (x_test.shape, y_test.shape)

```

正如所料，形状反映了每个数据集的大小、窗口大小和一维。 *1* 维度表示我们正在对未来做一个预测。列车组包含 315，393 条记录，由包含 20 个相对湿度读数的窗口组成。测试集包含 105，118 条记录，由包含 20 个相对湿度读数的窗口组成。

那么为什么我们有 315，393 个训练观测值而不是原来的 315，413 个呢？原因是我们需要第一个充当历史的窗口。所以只要从 315，413 减去第一个窗口 20。对于测试数据，从 105，138 中减去第一个窗口 20，得到 105，118。

我们可以创建更大的窗口，但这大大增加了我们必须处理的数据量。每个窗口只有 20 个观察值，我们已经有 6，307，860 (315，393 `×` 20)个数据点用于训练，2，102，360 (105，118 `×` 20)个数据点用于测试！

### 查看过去历史的窗口

检查列车组的第一扇车窗:

```
print ('length of window:', len(x_train[0]), br)
print ('first window of past history:')
print (x_train[0], br)
print ('target relative humidity to predict:')
print (y_train[0])

```

正如所料，该窗口包含 20 个相对湿度读数。那么我们是如何得到目标的呢？

从下一个窗口获取最后一个条目:

```
print ('target from the 1st window:', end='   ')
print (np.round(y_train[0], 8))
print ('last obs from the 2nd window:', end=' ')
print (np.round(x_train[1][19][0], 8))

```

通过检查第二个窗口进行验证:

```
print ('second window of past history:')
print (x_train[1], br)
print ('target relative humidity to predict:')
print (y_train[1])

```

让我们看看清单 [10-5](#PC25) 中显示的模式是否适用于接下来的几个窗口。

```
print ('target from the 2nd window:', end='   ')
print (np.round(y_train[1], 8))
print ('last obs from the 3rd window:', end=' ')
print (np.round(x_train[2][19][0], 8), br)

print ('target from the 3rd window:', end='   ')
print (np.round(y_train[2], 8))
print ('last obs from the 4th window:', end=' ')
print (np.round(x_train[3][19][0], 8), br)

print ('target from the 4th window:', end='   ')
print (np.round(y_train[3], 8))
print ('last obs from the 5th window:', end=' ')
print (np.round(x_train[4][19][0], 8), br)

print ('target from the 5th window:', end='   ')
print (np.round(y_train[4], 8))
print ('last obs from the 6th window:', end=' ')
print (np.round(x_train[5][19][0], 8))

Listing 10-5Inspect the patterns for the next few windows

```

### 举一个例子

创建一个函数，返回从*-长度*到 *0* 的时间步长列表:

```
def create_time_steps(length):
  return list(range(-length, 0))

```

Start at -length 设置为 0 以使用前一个窗口作为历史记录。

创建另一个接受单个数据窗口及其目标 delta 和标题的函数，如清单 [10-6](#PC27) 所示。

```
def plot(plot_data, delta=0, title='Data Window'):
  labels = ['history', 'actual future', 'model prediction']
  marker = ['r.-', 'b*', 'g>']
  time_steps = create_time_steps(plot_data[0].shape[0])
  if delta: future = delta
  else: future = 0
  plt.title(title)
  for i, obs in enumerate(plot_data):
    if i:
      plt.plot(future, obs, marker[i], markersize=10,
               label=labels[i])
    else:
      plt.plot(time_steps, obs.flatten(), marker[i],
               label=labels[i])
  plt.legend()
  plt.xlim([time_steps[0], (future+5)*2])
  plt.xlabel('time step')
  return plt

Listing 10-6Function that plots an example

```

参数 *delta* 表示变量的变化。默认情况下没有变化。该函数绘制数据窗口中的每个元素及其相关的时间步长。

基于第一个数据窗口和训练集中的目标调用函数:

```
plot([x_train[0], y_train[0]])

```

每个窗口的*实际未来*是它的标签，是下一个窗口的最后一个条目。

### 创建可视化性能基线

在训练之前，最好创建一个简单的视觉性能基线来与模型性能进行比较。当然，有许多方法可以做到这一点，但一个非常简单的方法是使用最近 20 次观察的平均值。

创建一个函数，返回一个观察窗口的平均值:

```
def baseline(history):
  return np.mean(history)

```

用基线预测绘制第一个数据窗口:

```
plot([x_train[0], y_train[0], baseline(x_train[0])], 0,
     'baseline prediction')

```

### 创建基线指标

创建一个基线度量来与我们的模型进行比较也是一个好主意。最简单的方法是预测每个窗口的最后一个值。然后，我们可以找到预测的平均均方误差，并使用该值作为我们的度量。

创建一个基线指标，如清单 [10-7](#PC31) 所示。

```
# display shape of test set
print ('TensorFlow shape:', x_test[0].shape, br)

# remove '1' dimension for easier processing
x_test_np = tf.squeeze(x_test)
print ('numpy shape:', x_test_np[0].shape, br)

# predict last value for each window
y_pred = x_test_np[:, -1]

# compute average MSE
MSE = np.mean(tf.keras.losses.mean_squared_error(
    y_test, y_pred))
print ('MSE:', MSE)

Listing 10-7Create a baseline metric

```

MSE 非常小，这意味着我们的基线指标可能很难超越。为什么？一般来说，机器学习有相当大的局限性。除非学习算法被硬编码以寻找特定类型的简单模型，否则参数学习有时可能无法找到简单问题的简单解决方案。我们的时间序列问题是一个非常简单的问题。

### 完成输入管道

如清单 [10-8](#PC32) 所示混洗训练数据、批次和缓存。

```
BATCH_SIZE = 256
BUFFER_SIZE = 10000

# prepare the train set
train = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_one = (train.cache()
.shuffle(BUFFER_SIZE)
.batch(BATCH_SIZE)
.repeat())

# prepare the test set
test = tf.data.Dataset.from_tensor_slices((x_test, y_test))
test_one = test.batch(BATCH_SIZE).repeat()

Listing 10-8Finish the input pipeline

```

检查张量:

```
train_one, test_one

```

正如所料，窗口有 20 个观察值和 1 个预测值。

### 浏览数据窗口

验证要素和标签是否在 256 元素窗口中批量处理:

```
for feature, label in train_one.take(1):
  print (len(feature), len(label))

```

显示批处理中的第一个窗口:

```
for feature, label in train_one.take(1):
  print ('feature:')
  print (feature[0].numpy(), br)
  print ('label:', label[0].numpy())

```

正如所料，该窗口包含一个具有 20 个观测值的特征集和一个具有一个预测值的标签。

### 创建模型

建立输入形状:

```
input_shape = x_train.shape[-2:]
input_shape

```

输入形状指示窗口大小为 20，有 1 个预测。

导入必要的库，清除以前的模型会话，生成可再现性的种子，并创建如清单 [10-9](#PC37) 所示的模型。

```
# clear any previous models
tf.keras.backend.clear_session()

#import libraries
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import GRU, Dense

# generate seed to ensure reproducibility
tf.random.set_seed(0)

neurons = 32  # number of neurons in GRU layer

model = Sequential([
  GRU(neurons, input_shape=input_shape),
  Dense(1)
])

Listing 10-9Create the model

```

RNN 非常适合于时间序列数据，因为其图层可以向早期图层提供反馈。具体来说，它一步一步地处理时间序列数据，同时记住它在训练中看到的信息。我们的模型使用了 GRU 层，这是一个专门的 RNN 层，能够长时间记忆信息。所以它特别适合时间序列建模。

### 模型摘要

检查模型:

```
model.summary()

```

我们使用公式*3*`×`*(n*<sup>*2*</sup>`×`*Mn+2n)*来计算 GRU 的可学习参数的数量，其中 *m* 是输入维度， *n* 是输出维度。对于任意一个神经网络，将前一层的输出乘以当前层的神经元( *m* `×` *n* )。此外，考虑当前层的神经元。但是因为反馈( *2n* )而数了两遍。一个 GRU 层有反馈，所以平方输出尺寸( *n* <sup>*2*</sup> )。乘以 *3* ，因为 GRU 有三组操作(隐藏状态、复位门和更新门)需要权重矩阵。

以下是计算 GRU 层参数的细目表:

*   3 `×` (32 <sup>2</sup> + 32 + 2 `×` 32)

*   3 `×` (1024 + 32 + 64)

*   3 `×` 1120

*   *3360*

输出维度为 32。输入维度不存在，因为没有上一层。

密集层具有 33 个可学习的参数，通过将来自前一层(32)的神经元乘以当前层(1)的神经元并加上当前层(1)的神经元数量来计算。

### 验证模型输出

根据模型进行*未经训练的*预测:

```
for x, y in test_one.take(1):
  print(model.predict(x).shape)

```

预测显示批次大小为 256，有 1 个预测。因此，该模型正在按预期运行。

### 编译模型

编译:

```
model.compile(optimizer='adam', loss="mse")

```

### 训练模型

火车:

```
num_train_steps = 400
epochs = 10

history = model.fit(train_one, epochs=epochs,
                    steps_per_epoch=num_train_steps,
                    validation_data=test_one,
                    validation_steps=50)

```

### 对测试数据进行归纳

概括:

```
test_loss = model.evaluate(test_one, steps=num_train_steps)

```

### 做预测

做三个预测，如清单 [10-10](#PC43) 所示。

```
n = 3
title = 'GRU prediction'

for i, (x, y) in enumerate(test_one.take(n)):
  p = model.predict(x)[0]
  plot([x[0].numpy(), y[0].numpy(), p], 0,
       title + ' window ' + str(i))
  plt.show()

Listing 10-10Make some predictions

```

虽然目测很漂亮，但它不能有效地衡量整体性能。

### 绘图模型性能

清单 [10-11](#PC44) 绘制了模型性能:

```
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(loss) + 1)

plt.figure()
plt.plot(epochs, loss, 'bo', label='training loss')
plt.plot(epochs, val_loss, 'b', label='validation loss')
plt.title('training and validation loss')
plt.legend()
plt.show()

Listing 10-11Plot model performance

```

瞧啊。我们的模型表现相当好！

## 预测多元时间序列

我们刚刚演示了如何基于单个特征进行单个预测。现在，让我们对多个变量进行多重预测。我们可以选择 14 个特征中的任何一个(我们不想从日期时间参考中预测)。

以下是可用的 14 个功能:

*   p(毫巴)–大气压，单位为毫巴

*   t(摄氏度)–以摄氏度为单位的温度

*   tpot(K)–开尔文温度

*   Tdew(摄氏度)–相对于湿度的温度(摄氏度)

*   RH(%)–相对湿度

*   VPmax(毫巴)–以毫巴为单位的饱和蒸汽压

*   VPact(毫巴)–以毫巴为单位的蒸汽压力

*   VPdef(毫巴)–以毫巴为单位的蒸汽压力不足

*   sh(克/千克)-比湿度，单位为克/千克

*   H2OC(毫摩尔/摩尔)-水蒸气浓度，单位为毫摩尔/摩尔

*   ρ(g/m * * 3)-空气密度，单位为克每立方米

*   wv(米/秒)——风速，单位为米/秒

*   最大值 wv(米/秒)——以米/秒为单位的最大风速

*   wd(deg)–风向，单位为度

创建一个变量来保存我们希望在实验中考虑的特性:

```
mv_features = ['Tdew (degC)', 'sh (g/kg)', 'H2OC (mmol/mol)', 'T (degC)']

```

我们选择了四个特征–Tdew(degC)、sh (g/kg)、H2OC (mmol/mol)和 T (degC)。 *Tdew (degC)* 是相对于湿度的摄氏温度。 *sh (g/kg)* 是以克/千克为单位的比湿度。 *H2OC (mmol/mol)* 是以毫摩尔/摩尔为单位的水蒸气浓度。并且 *T(摄氏度)*是以摄氏度为单位的温度。我们选择这些特性只是为了演示。特性的选择应该基于问题域。

创建一个数据框来保存要素:

```
mv_features = df[mv_features]
mv_features.index = df['Date Time']
mv_features.head()

```

可视化特征:

```
mv_features.plot(subplots=True)

```

可视化单个特征:

```
mv_features['T (degC)'].plot(subplots=True)

```

### 缩放数据

将数据帧转换为 numpy 数组:

```
f_np = mv_features.to_numpy()
f_np[:5]

```

检查观察值的数量:

```
len(f_np)

```

正如所料，我们有 420，551 个观察值。

秤数据如清单 [10-12](#PC51) 所示。

```
# scale features
f_sc = tf.keras.utils.normalize(f_np)
print ('shape after tf function:', f_sc.shape, br)

# squeeze
f_sq = tf.squeeze(f_sc)

# convert to numpy
f_scaled = f_sq.numpy()
print ('first five scaled observation:')
print (f_scaled[:5])

Listing 10-12Scale data

```

### 多步模型

相对湿度，我们只预测了一个未来点。但我们可以创建一个模型来学习预测一系列未来值，这就是我们将要对刚刚建立的多元数据做的事情。

假设我们想要训练我们的多步模型来学习预测接下来的 6 个小时。因为我们的数据时间步长是 10 分钟(每 10 分钟一次观测)，所以每小时有 6 次观测。假设我们想要预测接下来的 6 个小时，我们的模型做出了 *36* (6 次观察`×` 6 次)预测。

我们还想显示每个样本最近 3 天的模型数据。由于一天有 24 小时，我们每天有 144 次观测。所以我们总共有 *432* (3 `×` 144)个观测值。但是我们希望每小时采样一次，因为我们不希望我们的任何特性在 60 分钟内发生剧烈变化。因此， *72 个* (432 个观测值/每小时 6 个观测值)观测值代表每个数据窗口。

### 发电机

由于我们正在训练多个特征来预测未来值的范围，因此我们创建一个生成器函数来创建训练和测试拆分。一个**生成器**是一个返回对象迭代器的函数，我们一次迭代一个值。生成器的定义类似于普通函数，但是它使用 *yield* 关键字而不是 *return* 生成一个值。所以添加 yield 关键字会自动使一个普通函数成为一个生成器函数。

生成器很容易实现，但是有点难以理解。我们调用生成器函数的方式与调用普通函数相同。但是，当我们调用它时，会创建一个生成器对象。我们必须遍历生成器对象才能看到它的内容。当我们遍历生成器对象时，生成器函数中的所有进程都会被处理，直到到达 yield 语句。一旦发生这种情况，生成器从其内容中产生一个新值，并将执行返回到 for 循环。因此，对于遇到的每个 yield 语句，生成器都会从其内容中生成一个元素。

### 使用发电机的优点

生成器函数允许我们声明一个行为类似迭代器的函数。所以我们可以用一种快速，简单，干净的方式来制作迭代器。**迭代器**是一个可以被迭代的对象。它用于抽象数据容器，使其行为像一个可迭代的对象。作为程序员，我们经常使用字符串、列表和字典等可迭代对象。

生成器节省内存空间，因为它们在实例化时不计算每一项的值。生成器只在被明确要求时才计算值。这种行为被称为*懒惰评估*。当我们处理大型数据集时，惰性求值是有用的，因为它允许我们立即开始使用数据，而不必等到整个数据集都处理完。它还可以节省内存，因为数据仅在需要时生成。

### 发电机警告

*   生成器创建单个对象。所以一个生成器对象只能分配给一个*单个*变量，不管它产生多少个值。

*   一旦迭代结束，发电机就会耗尽。因此必须重新运行才能重新填充。

### 创建一个生成器函数

创建一个发电机，如清单 [10-13](#PC52) 所示:

```
def generator(d, t, o, e, w, ts, s):

  # hold features and labels
  features, labels = [], []

  # initialize variables
  data, target = d, t
  origin, end = o, e
  window, target_size = w, ts
  step = s

  # establish starting point that reflects window size
  origin = origin + window

  # enable split for test data
  if end < 0:
    end = len(data) - target_size

  # create feature set of 'window-sized' elements
  for i in range(origin, end):

    # create index set to identify each window
    indices = range(i-window, i, step)

    # create features
    features.append(data[indices])

    # create labels
    labels.append(target[i:i+target_size])

  yield np.array(features), np.array(labels)

Listing 10-13Generator function

```

### 生成训练和测试数据

调用生成器来创建训练和测试集，如清单 [10-14](#PC53) 所示。

```
window = 432  # observations for 3 days
future_target = 36  # predictions for the next 6 hours
step = 6  # number of timesteps per hour

train_gen = generator(f_scaled, f_scaled[:, 1], 0,
                      train_split, window,
                      future_target, step)

test_gen = generator(f_scaled, f_scaled[:, 1],
                     train_split, -1, window,
                     future_target, step)

Listing 10-14Invoke the generator

```

请注意，我们将生成器对象赋给了一个变量！

### 重构生成的张量

将生成的数据重新制作成 numpy 数组。由于训练和测试数据是生成器，我们必须迭代创建特征和标签。

通过迭代生成器创建要素和标注列表，从训练数据开始:

```
train_f, train_l = [], []
for i, row in enumerate(train_gen):
  train_f.append(row[0])
  train_l.append(row[1])

```

将列表数据转换为 numpy 数组:

```
train_features = np.asarray(train_f, dtype=np.float64)
train_labels = np.asarray(train_l, dtype=np.float64)

```

检查形状:

```
train_features.shape, train_labels.shape

```

正如预期的那样，特征每个窗口有 72 个观测值和 4 个特征。标签有 36 个预测。

用 *tf.squeeze* 功能移除 *1* 尺寸:

```
train_features, train_labels = tf.squeeze(train_features),\
tf.squeeze(train_labels)

train_features.shape, train_labels.shape

```

重组测试数据，如清单 [10-15](#PC58) 所示。

```
# create test data
test_f, test_l = [], []
for i, row in enumerate(test_gen):
  test_f.append(row[0])
  test_l.append(row[1])

# convert lists to numpy arrays
test_features = np.asarray(test_f, dtype=np.float64)
test_labels = np.asarray(test_l, dtype=np.float64)

# squeeze out the '1' dimension created by the generator
test_features, test_labels = tf.squeeze(test_features),\
tf.squeeze(test_labels)
test_features.shape, test_labels.shape

Listing 10-15Reconstitute test data

```

### 完成输入管道

完成输入管道，如清单 [10-16](#PC59) 所示。

```
train_mv = tf.data.Dataset.from_tensor_slices(
    (train_features, train_labels))
train_mv = train_mv.cache().shuffle(
    BUFFER_SIZE).batch(BATCH_SIZE).repeat()

test_mv = tf.data.Dataset.from_tensor_slices(
    (test_features, test_labels))
test_mv = test_mv.batch(BATCH_SIZE).repeat()

Listing 10-16Finish the input pipeline

```

检查张量:

```
train_mv, test_mv

```

检查一批:

```
for feature, label in train_mv.take(1):
  print (feature.shape)
  print (label.shape)

```

每批包含 256 个特征数据窗口和 256 个标签。每个窗口有 72 个观察值，每个观察值包含 4 个特征。每个标签有 36 个预测。

查看第一个培训示例:

```
for feature, label in train_mv.take(1):
  print ('observations:', len(feature[0]))
  print (feature[0], br)
  print ('predictions:', len(label[0]))
  print (label[0])

```

正如所料，第一个窗口有 72 个观察值和 4 个特征，第一个标签有 36 个预测值。

建立输入形状:

```
input_shape_multi = feature.shape[-2:]
input_shape_multi

```

正如所料，窗口大小为 72，每个窗口有 4 个特征。

### 创建模型

创建如清单 [10-17](#PC64) 所示的模型。

```
# clear any previous models
tf.keras.backend.clear_session()

# generate seed to ensure reproducibility
tf.random.set_seed(0)

neurons = 32  # neurons in GRU layer
outputs = 36  # predictions

gen_model = Sequential([
  GRU(neurons, input_shape=input_shape_multi),
  Dense(outputs)
])

Listing 10-17Create the model

```

### 模型摘要

检查型号:

```
gen_model.summary()

```

以下是计算 GRU 层可学习参数的细目表:

*   3`×`(32<sup>2</sup>+32`×`4+2`×`32)

*   3 `×` (1024 + 128 + 64)

*   3 `×` 1216

*   *3648*

唯一的区别是我们有四个特点。所以把输出( *n* )维度乘以 4。

密集层具有 1，188 个可学习的参数，这些参数是通过将来自前一层(32)的神经元乘以当前层(36)的神经元并加上当前层(36)的神经元数量来计算的。

### 编译模型

编译:

```
gen_model.compile(optimizer='adam', loss="mse")

```

### 训练模型

火车:

```
num_train_steps = 400
epochs = 10

gen_history = gen_model.fit(train_mv, epochs=epochs,
                            steps_per_epoch=num_train_steps,
                            validation_data=test_mv,
                            validation_steps=50)

```

### 对测试数据进行归纳

概括:

```
test_loss = gen_model.evaluate(test_mv, steps=num_train_steps)

```

### 剧情表现

图表性能如清单 [10-18](#PC69) 所示。

```
loss = gen_history.history['loss']
val_loss = gen_history.history['val_loss']

epochs = range(1, len(loss) + 1)

plt.figure()
plt.plot(epochs, loss, 'bo', label='training loss')
plt.plot(epochs, val_loss, 'b', label='validation loss')
plt.title('training and validation loss')
plt.legend()
plt.show()

Listing 10-18Plot performance

```

该模型有点过拟合，但性能仍然相当不错。

### 绘制数据窗口

创建一个绘图函数，如清单 [10-19](#PC70) 所示。

```
def multi_step_plot(window, true_future, pred):
  plt.figure(figsize=(12, 6))
  num_in = create_time_steps(len(window))
  num_out = len(true_future)

  plt.plot(num_in, np.array(window[:, 1]), 'm',
           label='history')
  plt.plot(np.arange(num_out)/step, np.array(true_future),
           'bo', label='true future')
  if pred.any():
    plt.plot(np.arange(num_out)/step, np.array(pred), 'go',
             label='predicted future')
  plt.legend(loc='upper left')
  plt.show()

Listing 10-19Plotting function

```

绘制第一批中的第一个训练窗口:

```
for x, y in train_mv.take(1):
  multi_step_plot(x[0], y[0], np.array([0]))

```

### 做一个预测

基于第一个训练窗口进行预测:

```
for x, y in test_mv.take(1):
  y_pred = gen_model.predict(x)[0]
  multi_step_plot(x[0], y[0], y_pred)

```

还不错。