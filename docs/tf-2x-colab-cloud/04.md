# 4.使用其他数据

在上一章中，我们向您展示了如何使用 TFDS。但是如果我们想处理另一种类型的数据集呢？在本章中，我们将向您展示如何使用 TensorFlow 处理其他类型的数据。

章节的笔记本位于以下网址: [`https://github.com/paperd/tensorflow`](https://github.com/paperd/tensorflow) 。

启用 GPU(如果尚未启用):

1.  点击左上角菜单中的*运行时*。

2.  从下拉菜单中点击*改变运行时间类型*。

3.  从*硬件加速器*下拉菜单中选择 *GPU* 。

4.  点击*保存*。

测试 GPU 是否处于活动状态:

```
import tensorflow as tf

# display tf version and test if GPU is active
tf.__version__, tf.test.gpu_device_name()

```

导入 *tensorflow* 库。如果显示“/设备:GPU:0”，则 GPU 处于活动状态。如果'..'显示时，常规 CPU 处于活动状态。

## 基础力学

为了创建输入管道，我们从数据源开始。要从 TensorFlow 可以使用的内存中的数据构造数据集，我们可以使用 TF . data . dataset . from _ tensor _ slices()或 tf.data.Dataset.from_tensors()。 *from_tensor_slices* 方法为输入张量的每一部分创建一个具有单独元素的数据集。 *from_tensors* 方法组合输入并返回包含单个元素的数据集。我们专门用 from_tensor_slices 方法处理*,因为它使我们能够处理每个数据元素。*

创建一个简单的 1D 张量，并使其可用于张量流:

```
# create a 1D tensor
ds = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])
ds.element_spec

```

我们从六元素列表中创建一个 TensorSpec，并使用 from_tensor_slices 方法使它成为 TensorFlow 可消费的。形状是()，因为 from_tensor_slices 从数组的切片创建了一个对象。

清单 [4-1](#PC3) 演示了如何迭代数据集。

```
# iterate and display tensor values
for elem in ds:
  print(elem.numpy())

print ()

# iterate without numpy method
for elem in ds:
  print(elem)

Listing 4-1Iterate and display the from_tensor_slices dataset

```

数据集有六个张量。第一个循环将张量中的每个元素显示为 numpy 值。第二个循环显示原始张量。

我们还可以使用 *iter* 创建一个 Python 迭代器，并使用 *next* 方法消费它的元素:

```
it = iter(ds)

# display the first element
next(it).numpy()

```

我们看到了张量中的第一个元素。

要查看剩余的元素，只需继续运行以下命令:

```
next(it).numpy()

```

现在，用 tf.data.Dataset_from_tensors()创建一个张量:

```
# create a 1D tensor
ds = tf.data.Dataset.from_tensors([8, 3, 0, 8, 2, 1])
ds.element_spec

```

注意，形状是(6，)这意味着单个张量由六个元素组成。

让我们迭代单个张量，如清单 [4-2](#PC7) 所示。

```
# iterate and display tensor values
for elem in ds:
  print(elem.numpy())

print ()

# iterate without numpy method
for elem in ds:
  print(elem)

Listing 4-2Iterate and display the from_tensors dataset

```

我们有一个数据集，它有一个包含六个元素的张量。

## 张量流数据集结构

数据集包含每个都具有相同嵌套结构的元素，并且该结构的各个组件可以具有可由 tf 表示的任何类型。TypeSpec，包括 tf。张量，tf.sparse.SparseTensor，tf。拉格德张量。TensorArray，或 TF . data . dataset .*dataset . element _ spec*属性允许我们检查每个元素组件的类型。该属性返回一个 *tf 的嵌套结构。匹配元素结构的 TypeSpec* 对象。该元素可以是单个组件、组件元组或组件嵌套元组。

通过清单 [4-3](#PC8) 中所示的例子，我们可以更好地理解这个结构。

```
br = '\n'  # enter a line break in Colab

# create random uniform numbers
scope = tf.random.uniform([4, 10])

print ('shape:', scope.shape, br)

ds = tf.data.Dataset.from_tensor_slices(scope)

print (ds.element_spec, br)
# Let's look at the first element:

it = iter(ds)

# print first element
print ('first element with an iterator:', br)
print (next(it).numpy(), br)

print ('all four elements:', br)
for i, row in enumerate(ds):
  print ('element ' + str(i+1))  # add 1 as index starts at 0
  print (row.numpy(), br)

Listing 4-3Data structure example

```

*作用域*的形状是(4，10)，也就是说我们有一个张量，它有四个元素，每个元素包含十个随机生成的 0 到 1 之间的均匀分布的数。我们使用 from_tensor_slices 方法从 scope 生成 TensorFlow 可消费数据集，并使用 element_spec 方法显示 TensorSpec。我们继续显示 TensorSpec 中的每个元素。

Note

除了示例和样本，术语*元素*也用于描述数据集中的张量。

简单地说，我们创建一个包含四个元素的数据集。每个元素包含十个介于 0 和 1 之间的随机均匀数字。我们将数据集转换为 TensorFlow 可以消费(或使用)的数据集。我们通过迭代 TensorFlow 数据集来显示每个元素。

## 读取输入数据

如果所有输入数据都适合内存，那么创建 TensorFlow 消费数据集的最简单方法是将其转换为 tf。带有 Dataset.from_tensor_slices()的张量对象。

## 晚上吃可乐

如前所述，当长时间(例如几个小时)运行 Google Colab 而不暂停或将大型数据集加载到内存中并处理所述数据时，它可能会崩溃(或异常终止)。当这种情况发生时，我们知道你有两种选择:

1.  重新启动所有运行时。

2.  关闭程序，从头开始重新启动。

要重启所有运行时，点击顶部菜单中的*运行时*，在下拉菜单中点击*重启运行时*，出现提示时点击*是*。Google Colab 推荐这个选项。如果从头重新启动，先清除浏览器历史，然后从头启动 Google Colab。

## 批量

**批量**是神经网络模型一遍处理的训练样本数。不要将批量大小与纪元混淆！一个**时期**是对训练数据集的完整遍历。因此，历元数是训练数据集中的完整遍数。历元与训练样本的处理无关！它们只是表示通过网络的次数。简单地说，在通过网络的每次传递(或历元)期间，成批的训练数据集被处理。

批处理的大小必须大于或等于 1，并且小于或等于训练数据集中的样本数。这是有意义的，因为你不能有一个大于训练样本总数的批处理。

TensorFlow 经过*优化*，可以运行比一个大得多的批量训练数据。所以一个批量是非常低效的！因为一个批量代表整个训练数据集，所以我们并没有真正地批量处理数据。所以除非你不想对数据进行批量处理，否则不要使用批量大小为 1 的数据！

## 硬数据

*tf.keras.datasets* 模块提供了七个用于练习张量流的预处理数据集。仔细阅读下面的 URL 以获得关于数据集的更多信息: [`https://keras.io/api/datasets/`](https://keras.io/api/datasets/) 。

让我们来看看喀拉斯 MNIST 数据集:

```
train, test = tf.keras.datasets.mnist.load_data(path='mnist.npz')

```

训练和测试数据都在一个元组中包含 MNIST 图像和标签:

```
type(train), type(test)

```

探索训练数据的形状:

```
print ('train data:', br)
print (train[0].shape)
print (train[1].shape)

```

探索测试数据的形状:

```
print ('test data:', br)
print (test[0].shape)
print (test[1].shape)

```

训练数据由 60，000 个 28 `×` 28 特征图像和 60，000 个标签组成。测试数据由 10，000 个 28 `×` 28 特征图像和 10，000 个标签组成。

### 构建输入管道

将训练和测试数据分割成各自的图像和标签。缩放图像数据。使用 from_tensor_slices 方法创建 TensorFlow 耗材数据。

从训练数据开始:

```
train_images, train_labels = train
train_sc = train_images / 255  # divide by 255 to scale

train_k = tf.data.Dataset.from_tensor_slices(
    (train_sc, train_labels))
train_k.element_spec

```

继续测试数据:

```
test_images, test_labels = test
test_sc = test_images / 255  # divide by 255 to scale

test_k = tf.data.Dataset.from_tensor_slices(
    (test_sc, test_labels))
test_k

```

混洗训练数据、批处理和预取训练和测试数据:

```
BATCH_SIZE = 128
SHUFFLE_BUFFER_SIZE = 1000

train_kd = train_k.shuffle(
    SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(1)
test_kd = test_k.batch(BATCH_SIZE).prefetch(1)

```

检查张量:

```
train_kd, test_kd

```

### 创建模型

导入库:

```
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout

```

从内存中清除以前的模型:

```
# clear any previous models
tf.keras.backend.clear_session()

```

构建模型:

```
model = Sequential([
  Flatten(input_shape=[28, 28]),
  Dense(512, activation="relu"),
  Dropout(0.5),
  Dense(10, activation="softmax")
])

```

该模型是具有密集连接层的前馈神经网络。也就是说，所有的神经元都看到了数据。第一层接受 28 个`×` 28 个图像，并将每个图像展平成由 784 个像素组成的 1D 阵列。第二层接受 512 个神经元的数据，并使用 *relu* 激活来最小化损失。第三层使用下降来减轻过拟合。第四层是输出层。它接受十个神经元的数据，因为我们的数据有十个类别标签。它使用 *softmax* 激活来减少损失。

**Dropout** 是一种正则化技术(谷歌专利)，用于减少神经网络中的过拟合。该技术通过删除神经网络中的单元来工作。

### 模型摘要

显示模型的摘要:

```
model.summary()

```

第一层的输出形状是(无，784)。 *None* 是第一个参数，因为 TensorFlow 模型接受任何批量。我们通过将 28 乘以 28 个图像像素得到第二个参数 784，因为我们想要平面化的图像。我们有 0 个参数，因为该层接受输入形状，但不作用于数据。

第二层的输出形状是(无，512)。第二个参数是 512，因为这是为该层设置的神经元数量。参数的数量是 401，920，通过将 512(该层的神经元)乘以 784(上一层的神经元)并加上 512 来说明该层的神经元而得到。

第三层的输出形状是(无，512)。使用 dropout 不会影响神经元数量。所以这一层从上一层继承(None，512)并且没有参数。

第四层的输出形状是(无，10)。第二个参数是 10，以反映类的数量。参数的数量是 5130，通过将 10(该层的神经元)乘以 512(前一层的神经元)并加上 10 来说明该层的神经元而得到。

### 编译模型

编译:

```
model.compile(optimizer='adam',
 loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

```

### 训练模型

火车:

```
epochs = 10
history = model.fit(train_kd, epochs=epochs, verbose=1,
                    validation_data=test_kd)

```

由于 epochs 设置为 10，我们的模型处理数据集*十次*。由于训练和测试精度紧密一致，我们没有太多的过度拟合。

## sci kit-学习数据

我们还可以从 scikit-learn 库中读取数据。Scikit-learn 是一个免费的 Python 编程语言的机器学习库。

从此库中获取数据集:

```
from sklearn.datasets import fetch_lfw_people

faces = fetch_lfw_people(min_faces_per_person=70, resize=0.4)

```

*fetch_lfw_people* 数据集是从互联网上收集的名人的 JPEG 图片集合。所有详情可在官方网站: [`http://vis-www.cs.umass.edu/lfw/`](http://vis-www.cs.umass.edu/lfw/) 。每张图片都集中在一个面上。典型的机器学习任务是人脸验证。因此，给定一对图片，我们预测这两个图像是否是同一个人。

另一种机器学习任务是人脸识别(或人脸识别)。因此，给定一张不认识的人的脸部照片，我们通过参考先前看过的已识别的人的照片来识别这个人的名字。

### 探索数据

显示密钥:

```
# get available keys from dataset
faces.keys()

```

数据集包含特征图像(在图像容器中)和目标(在目标容器中)。它还包含目标名称和数据集的描述。数据容器由每个图像的展平矢量组成。

清单 [4-4](#PC25) 显示了形状、目标名称和类标签。

```
image, target = faces.images, faces.target
data = faces.data
names = faces.target_names

print ('feature image tensor:', br)
print (image.shape, br)
print ('target tensor:', br)
print (target.shape, br)
print ('flattened image tensor:', br)
print (data.shape, br)
print ('target names:', br)
print (names, br)
print ('class labels:', len(names))

Listing 4-4Information about the dataset

```

特征图像张量由 1，288 50 `×` 37 张人脸图像组成。目标张量由 1288 个目标组成。数据形状由 1，288 个扁平矢量组成，每个矢量包含 1，850 个元素。50 乘以 37 得到 1850。

清单 [4-5](#PC26) 探究了第一个例子。

```
# display the first data example

i = 0

print ('first image example:', br)
print (image[i], br)

print ('first target example:', br)
print (target[i], br)

print ('name of first target:', br)
print (names[target[i]], br)

print ('first data example (flattened image):', br)
print (data[i], br)

print ('first image:', br)

import matplotlib.pyplot as plt

# display the first image in the dataset
plt.imshow(image[i], cmap="bone")
plt.title(names[target[i]])

Listing 4-5Explore the first example

```

每个图像由一个 2D 矩阵表示。第一个图像的目标值是 5，这恰好是 Hugo Chavez 的图像。

### 构建输入管道

创建训练集和测试集。缩放特征图像，如清单 [4-6](#PC27) 所示。

```
from sklearn.model_selection import train_test_split

# create train and test data
X_train, X_test, y_train, y_test = train_test_split(
    image, target, test_size=0.33, random_state=0)

# scale feature image data and create TensorFlow tensors
x_train = X_train / 255.0
x_test = X_test / 255.0

# get shapes
print ('x_train shape:', end=' ')
print (x_train.shape)
print ('x_test shape:', end=' ')
print (x_test.shape)

# get sample entries
print (x_train[0])
print (X_train[0][0][0])
print (x_train[0][0][0])

Listing 4-6Create train and test sets and scale feature images

```

*train_test_split* 模块提供了一种简单的方法将数据集分成训练集和测试集。我们还可以根据需要灵活设置训练和测试规模。*随机状态*参数提供了一种重现我们结果的方法。

我们的训练集包含 67%的数据。剩下的 33%放在测试集中。训练特征数据形状为(862，50，37)，测试特征数据形状为(426，50，37)。因此，训练特征数据由 862 张 50 `×` 37 像素图像组成，测试特征数据由 426 张 50 `×` 37 像素图像组成。

将图像除以 255 进行缩放。缩放使向量运算更简单。

显示训练集中的第一个像素图像。显示来自第一像素图像的第一像素并显示其缩放值。

继续将数据分割成 TensorFlow 消耗品:

```
faces_train = tf.data.Dataset.from_tensor_slices(
    (X_train, y_train))
faces_test = tf.data.Dataset.from_tensor_slices(
    (X_test, y_test))

```

设置批处理和缓冲区大小:

```
BATCH_SIZE = 16
SHUFFLE_BUFFER_SIZE = 100

```

混洗训练数据、批处理和预取训练和测试数据:

```
faces_train_ds = (faces_train
                  .shuffle(SHUFFLE_BUFFER_SIZE)
                  .batch(BATCH_SIZE).prefetch(1))
faces_test_ds = (faces_test
                 .batch(BATCH_SIZE).prefetch(1))

```

检查张量:

```
faces_train_ds, faces_test_ds

```

### 建立模型

构建一个简单的模型并训练数据，如清单 [4-7](#PC32) 所示。

```
import numpy as np

class_labels = len(names)

# clear previous model and generate a seed
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

model = Sequential([
  Flatten(input_shape=[50, 37]),
  Dense(16, activation="relu"),
  Dense(class_labels, activation="softmax")
])

Listing 4-7Model for faces data

```

导入 numpy 库，这样我们可以生成一个随机种子。一个**随机种子**指定了计算机生成随机数序列时的*起点*。我们设置了一个随机种子以确保相同的起点，这样我们就可以获得可重复的结果。也就是说，每次我们训练模型，得到的结果都是一样的。我们将种子设置为 0。当然，您可以将种子设置为任何数字，但是每次都要使用相同的数字。

输入形状反映了要素数据的图像大小。也就是说，每个图像由 50 个`×` 37 像素表示。

### 模型摘要

运行模型摘要:

```
model.summary()

```

第一层的输出形状为(无，1850)。50 乘以 37 得到 1850。参数的数量是 0，因为这是第一层。

第二层的输出形状是(无，16)。我们有 16 个神经元处理这一层的数据。参数的数量是 29，616。16 乘以 1850 得到 29600。我们通过将 16 加到 29，600 得到 29，616，以说明这一层的神经元数量。

第三层的输出形状是(无，7)。我们在这一层有七个神经元来解释七个类别标签。参数的数量是 119。我们用 16 乘以 7 得到 112。我们将 112 加上 7 得到 119，以说明这一层的神经元数量。因为 TensorFlow 接受任何批量大小，所以存在 None 参数。

### 编译模型

编译:

```
model.compile(optimizer='adam',
 loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

```

### 训练模型

火车:

```
history = model.fit(faces_train_ds, epochs=10,
                    validation_data=faces_test_ds)

```

性能不好是因为前馈网络不能很好地处理图像数据。然而，我们创建这个简单的模型是为了向您展示如何训练数据集。

## Numpy 数据

我们可以直接加载 numpy 数据，并对其进行缩放，如清单 [4-8](#PC36) 所示。

```
DATA_URL = 'https://storage.googleapis.com/\
tensorflow/tf-keras-datasets/mnist.npz'

path = tf.keras.utils.get_file('mnist.npz', DATA_URL)
with np.load(path) as data:
  train_examples = data['x_train']
  train_labels = data['y_train']
  test_examples = data['x_test']
  test_labels = data['y_test']

train_scaled = train_examples / 255.
test_scaled = test_examples / 255.

Listing 4-8Load numpy data from a URL

```

标识 numpy 文件的 URL 路径。使用 tf.keras.utils.get_file 访问路径。用 *np.load* 函数加载 numpy 数据。将数据分成训练集和测试集。缩放特征图像数据。

### 用 tf.data.Dataset 加载 Numpy 数组

将训练和测试图像和标签转换为 TensorFlow 耗材形式:

```
train_dataset = tf.data.Dataset.from_tensor_slices(
    (train_scaled, train_labels))
test_dataset = tf.data.Dataset.from_tensor_slices(
    (test_scaled, test_labels))

```

### 为培训准备数据

混洗训练数据、批处理和预取训练和测试数据:

```
BATCH_SIZE = 128
SHUFFLE_BUFFER_SIZE = 1000

train_np = train_dataset.shuffle(
    SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(1)
test_np = test_dataset.batch(BATCH_SIZE).prefetch(1)

```

检查张量:

```
train_np, test_np

```

### 创建模型

清除以前的会话并设置随机种子:

```
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

使用我们在 Keras MNIST 使用的相同模型:

```
model = Sequential([
  Flatten(input_shape=[28, 28]),
  Dense(512, activation="relu"),
  Dropout(0.5),
  Dense(10, activation="softmax")
])

```

### 模型摘要

由于我们使用了相同的模型，总结与之前相同:

```
model.summary()

```

### 编译模型

编译:

```
model.compile(optimizer='adam',
 loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

```

### 训练模型

火车:

```
epochs = 10
history = model.fit(train_np, epochs=epochs, verbose=1,
                    validation_data=test_np)

```

## 从 GitHub 获取葡萄酒数据

您可以通过几个简单的步骤直接从 GitHub 访问这本书的任何独立数据集:

1.  访问本书网址: [`https://github.com/paperd/tensorflow`](https://github.com/paperd/tensorflow) 。

2.  找到数据集并单击它。

3.  点击 *Raw* 按钮。

4.  将 URL 复制到 Colab，并将其赋给一个变量。

5.  使用 Pandas *read_csv* 方法读取数据集。

出于我们的目的，*独立数据集*不是存储在软件环境中的数据集，比如 TFDS。让我们从 GitHub 中读取一个数据集。

在我们的例子中，访问图书 URL，点击*第 4 章*，点击*数据*，点击 *winequality-red.csv* ，点击 *Raw* 按钮，复制 URL，将其粘贴到 Colab 中的代码单元格，并将其赋给一个变量。

我们已经找到了 URL 并将其赋给了一个变量:

```
url = 'https://raw.githubusercontent.com/paperd/tensorflow/\
master/chapter4/data/winequality-red.csv'

```

将数据集读入熊猫数据帧:

```
import pandas as pd

wine = pd.read_csv(url, sep = ';')

```

验证数据是否被正确读取:

```
wine.head()

```

## CSV 数据

一个 *CSV* 数据集是一个逗号分隔值文件，允许数据以表格格式保存。当在像微软 Excel 这样的程序中打开时，它看起来就像一个传统的电子表格，但是有一个*。csv* 扩展。CSV 文件可以用于我们所知的任何电子表格程序，包括 Microsoft Excel 或 Google Sheets。

为机器学习获取 CSV 数据集的一个好地方是 UCI 机器学习资源库。存储库的主站点位于以下 URL: [`https://archive.ics.uci.edu/ml/datasets.php`](https://archive.ics.uci.edu/ml/datasets.php) 。

机器学习文献中经常引用的一个 CSV 数据集是 *winequality-red.csv* 。该数据集包含葡萄牙*葡萄酒*的红色变种。有关详细信息，请阅读以下文章:

P. Cortez、A. Cerdeira、F. Almeida、T. Matos 和 J. Reis 通过物理化学特性的数据挖掘建立葡萄酒偏好模型。在决策支持系统中，爱思唯尔，2009，47(4):547–553。刊号:0167-9236。

有关数据集的一般信息，请阅读以下 URL:

[T2`https://archive.ics.uci.edu/ml/datasets/wine+quality`](https://archive.ics.uci.edu/ml/datasets/wine+quality)

### 数据集特征

数据集由 11 个独立特征变量和 1 个目标变量组成。特征变量包括

*   固定酸度

*   挥发性酸度

*   柠檬酸

*   残糖

*   氯化物

*   游离二氧化硫

*   二氧化硫总量

*   密度

*   pH 值

*   硫酸盐化

*   酒精

目标变量是

*   质量

目标变量(质量)可以取 0 到 10 之间的分数。0 分表示质量很低。10 分意味着质量非常高。数据集包含 1，599 个示例。

### 检索数据

了解如何直接从 UCI 机器学习库中提取数据集是一个好主意。我们现在就开始吧。

标识数据集 URL:

```
url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'

```

建立数据集的路径:

```
path = tf.keras.utils.get_file('winequality-red.csv', url)
path

```

从 CSV 文件创建熊猫数据帧，并将其放入 Python 变量中:

```
import pandas as pd

data = pd.read_csv(path, sep = ';')

```

从数据帧的开头查看记录:

```
data.head()

```

从数据帧的末尾查看记录:

```
data.tail()

```

识别数据集中出现的分类标签:

```
data.quality.unique()

```

由于质量是目标，我们使用 *unique()* 函数从数据集中提取不同的标签。数据集包含分数 3、4、5、6、7 和 8。然而，质量可以取 0 到 10 之间的分数，所以有*十一个*可能的分类标签。

显示数据类型:

```
data.dtypes

```

特征的数据类型为 float64，目标的数据类型为 int64。

显示数据集中的示例数:

```
len(data)

```

我们有 1599 个例子。

### 将数据分成训练集和测试集

创建目标集:

```
# create a copy of the dataframe
df = data.copy()

# create the target
target = df.pop('quality')

```

创建数据帧的副本以保留原始数据。将目标列放入变量中以创建目标数据集。 *pop* 方法从副本中永久删除该列。

显示数据帧拷贝发生了什么:

```
df.head()

```

请注意，质量列不再位于数据框架中。

将数据帧转换为 numpy 值:

```
features = df.values
labels = target.values

```

分成训练集和测试集，并缩放特性数据，如清单 [4-9](#PC59) 所示。

```
X_train, X_test, y_train, y_test = train_test_split(
    features, labels, test_size=0.33, random_state=0)

# scale feature image data and create TensorFlow tensors
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.fit_transform(X_test)

Listing 4-9Split data into train and test sets and scale feature data

```

特征数据不是图像。它们是标量值。因此，在应用机器学习技术之前，我们使用*标准缩放器*模块将连续特征数据转换为 0 的*平均值和 1* 的*标准差。*

### 准备张量流消耗的数据

将训练和测试集切片到 tf 中。数据。数据集数据:

```
train_wine = tf.data.Dataset.from_tensor_slices(
    (X_train_std, y_train))
test_wine = tf.data.Dataset.from_tensor_slices(
    (X_test_std, y_test))

```

定义保存换行符的变量:

```
br = '\n'

```

创建一个函数来查看张量:

```
def see_samples(data, num):
  for feat, targ in data.take(num):
    print ('Features: {}'.format(feat))
    print ('Target: {}'.format(targ), br)

```

查看前三个张量:

```
n = 3
see_samples(train_wine, n)

```

定义批次和缓冲区大小:

```
BATCH_SIZE = 16
SHUFFLE_BUFFER_SIZE = 100

```

混洗训练数据、批处理和预取训练和测试数据:

```
train_wine_ds = (train_wine.shuffle(
    SHUFFLE_BUFFER_SIZE).
    batch(BATCH_SIZE).
    prefetch(1))
test_wine_ds = (test_wine.batch(
    BATCH_SIZE).
    prefetch(1))

```

检查张量:

```
train_wine_ds, test_wine_ds

```

### 建立模型

清除会话并生成种子:

```
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

创建模型:

```
model = Sequential([
  Dense(30, activation="relu", input_shape=[11,]),
  Dense(11, activation="softmax")
])

```

输入形状为[11，]，因为数据集有 11 个要素。输出形状为 11，因为数据集有 11 个类标签。

### 模型摘要

检查模型:

```
model.summary()

```

第一层的输出形状是(无，30)，因为我们在这一层有 30 个神经元。参数的数量是 360。我们用 30 个神经元乘以 11 个特征得到 330。我们把这一层的 30 个神经元加到 330 就得到 360。第二层的输出形状是(None，11 ),因为我们在这一层有 11 个神经元来考虑可能标签的数量。参数的数量是 341。我们将前一层的 30 个神经元乘以这一层的 11 个神经元，得到 330。我们把这一层的 11 个神经元加到 330 就得到 341。

### 编译模型

编译:

```
model.compile(optimizer='adam',
 loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

```

### 训练模型

```
history = model.fit(train_wine_ds, epochs=10,
                    validation_data=test_wine_ds)

```

我们没有得到很好的性能，但我们只是想告诉你如何训练一个 CSV 数据集。

## 从 GitHub 获取鲍鱼数据

在下一节中，我们将使用鲍鱼数据集。我们向您展示如何从 GitHub 加载它作为一种替代方法。

我们已经找到了合适的 URL 并将其赋给了一个变量:

```
url = 'https://raw.githubusercontent.com/paperd/tensorflow/\
master/chapter4/data/abalone.data'

```

将数据集读入熊猫数据帧:

```
# add column headings
cols = ['Sex', 'Length', 'Diameter', 'Height', 'Whole',
        'Shucked', 'Viscera', 'Shell', 'Rings']

abd = pd.read_csv(url, names=cols)

```

验证数据:

```
abd.head()

```

## 数据数据集

数据与*的唯一区别。数据扩展名为*，CSV 数据是文件扩展名。但是我们可以用同样的方式来处理它。我们不用从 GitHub 加载数据集，而是可以从 UCI 资源库下载数据集，复制到 Google Drive，并从 Colab 访问它。

## 鲍鱼数据集

鲍鱼数据集是类型数据。数据的一般位置是

[T2`https://archive.ics.uci.edu/ml/datasets/Abalone`](https://archive.ics.uci.edu/ml/datasets/Abalone)

要下载鲍鱼数据，进入网址，点击*数据文件夹*，点击*鲍鱼. data* 。数据集会自动下载到您的*下载*目录。由于我们使用的是 Colab 云服务，请将文件复制到您的 *Google Drive* 上的 *Colab Notebooks* 目录中。最简单的复制方法是将文件从下载目录拖放到 Google Drive 中。

另一个感兴趣的文件是鲍鱼. names，它提供了对数据集的深入描述。与数据文件一样，可以通过单击*鲍鱼. names* 从数据文件夹中访问它。

### 数据集特征

该数据集用于根据物理测量预测鲍鱼壳的年龄。鲍鱼壳的年龄部分是通过切开球果，染色，并通过显微镜计数年轮数来确定的。其他测量补充了年龄预测。

特征变量包括

*   性

*   长度

*   直径

*   高度

*   全部

*   去壳的

*   内容

*   壳

目标变量是

*   戒指

目标可变环可以取 1 到 29 之间的分数。这样的分数代表鲍鱼壳的环数。所以*环数*就是要预测的值。关于这个数据集的一个有趣的概念是，我们可以将它用作连续值实验或分类问题。

### 将 Google Drive 安装到 Colab

如果我们想直接从 Google Drive 加载数据文件，我们必须将 Colab 挂载到 Google Drive:

```
from google.colab import drive
drive.mount('/content/gdrive')

```

在你执行代码片段之后，点击 URL，选择一个谷歌账户，点击*允许*按钮，复制授权码并粘贴到文本框*中输入你的授权码:*，并按下键盘上的*回车键*。

Note

请确保您的 Google Drive 上的 Colab 笔记本目录中有该文件！

在 Colab 中建立路径:

```
# establish path (be sure to copy file to Google Drive)

path = 'gdrive/My Drive/Colab Notebooks/'
abalone = path + 'abalone.data'
abalone

```

### 读出数据

由于数据集不包含列标题，我们需要在读取数据集之前定义它们:

```
cols = ['Sex', 'Length', 'Diameter', 'Height', 'Whole',
        'Shucked', 'Viscera', 'Shell', 'Rings']

ab_data = pd.read_csv(abalone, names=cols)

```

现在我们有了直接从 GitHub 加载的相同数据集。

### 浏览数据

从数据集的开头显示记录:

```
ab_data.head(3)

```

显示数据集末尾的记录:

```
ab_data.tail(3)

```

返回记录的数量:

```
len(ab_data)

```

我们有 4177 项记录。

显示数据集中使用的输出类:

```
# classes used
print ('classes:', br)
print (np.sort(ab_data['Rings'].unique()))

```

显示输出类的数量:

```
# number of classes
print ('number of classes:', len(ab_data['Rings'].unique()))

```

我们在数据集中使用了 28 个类。

显示类别分布:

```
instance = ab_data['Rings'].value_counts()
instance.to_dict()

```

班级范围从 1 到 27，还有 29。每一级代表鲍鱼壳的年龄。分布很不均匀。例如，我们有 689 个九岁的 shell 实例，但是只有一个一岁的 shell 实例。通常，机器学习算法不能很好地处理不平衡的数据，因为预测偏向于有最多实例的类。**不平衡数据**是训练集中存在类别分布不均匀时的分类问题。然而，像欺诈检测这样的问题通常由机器学习来处理，其中类别预计是不平衡的。

显示数据类型:

```
ab_data.dtypes

```

除了性，其他特征都是浮动的。目标是 int64。

显示所有列的信息:

```
ab_data.info(verbose=True)

```

没有丢失数据。

显示形状:

```
ab_data.shape

```

我们有 4177 个例子，每个例子有九个属性。

### 创建训练集和测试集

分割数据:

```
train, test = train_test_split(ab_data)

print(len(train), 'train examples')
print(len(test), 'test examples')

```

我们有 3132 个训练样本和 1045 个测试样本。如果未指定，默认测试大小为 25%。

### 创建功能和目标集

创建训练集和测试集的副本来保存原始数据是一个好主意。否则，pop 方法会造成严重破坏，因为它会永久删除数据。

创建目标:

```
train_copy, test_copy = train.copy(), test.copy()

# create targets
train_target, test_target = train_copy.pop('Rings'),\
test_copy.pop('Rings')

```

验证目标:

```
len(train_target), len(test_target)

```

验证列车特征数据:

```
train_copy.head(3)

```

当争论数据时，验证内容是一个好主意。

将要素数据转换为数字:

```
train_features, test_features = train_copy.values,\
test_copy.values

```

### 比例特征

我们只能缩放连续值。由于性别特征不是连续的，因此无法缩放。所以我们把连续值切掉。然后，我们用性别特征和缩放的连续值重新创建训练集和测试集。

显示样本以验证切片:

```
train_features[0], test_features[0]

```

切片是复杂的。因此，展示一个示例来确保切片按预期工作是一个好主意。

创建两个训练集(一个具有性别，另一个具有连续值):

```
train_sex = [row[0] for row in train_features]
train_f = [row[1:] for row in train_features]
train_sex[0], train_f[0]

```

到目前为止，一切顺利！

创建两个测试集(一个包含性别，另一个包含连续值):

```
test_sex = [row[0] for row in test_features]
test_f = [row[1:] for row in test_features]
test_sex[0], test_f[0]

```

同样，我们的切片与原始数据相匹配。

缩放连续值:

```
train_sc = scaler.fit_transform(train_f)
test_sc = scaler.fit_transform(test_f)

```

### 使用性别和比例值创建训练集和测试集

既然我们已经缩放了连续值，我们需要用性别特征重新组合它们，如清单 [4-10](#PC96) 所示。

```
train_ds = [np.append(train_sex[i], row)
for i, row in enumerate(train_sc)]

test_ds = [np.append(test_sex[i], row)
for i, row in enumerate(test_sc)]

train_ds[0], test_ds[0]

Listing 4-10Recombine continuous features with the sex feature

```

重新组合训练和测试特征和显示。请注意，连续要素已缩放。

### 将数字要素集转换为熊猫数据框架

要使用不连续的数据正确构建 TensorFlow 可消耗数据集，我们需要 pandas 数据帧形式的要素数据:

```
col = ['Sex', 'Length', 'Diameter', 'Height', 'Whole',
       'Shucked', 'Viscera', 'Shell']

train_ab = pd.DataFrame(train_ds, columns=col)
test_ab = pd.DataFrame(test_ds, columns=col)

```

我们需要将原始的列名添加到数据帧中。

验证列车特征:

```
train_ab.tail(3)

```

验证测试功能:

```
test_ab.tail(3)

```

### 构建输入管道

准备张量流消耗的训练和测试数据:

```
train_ds = tf.data.Dataset.from_tensor_slices(
    (dict(train_ab), train_target))
test_ds = tf.data.Dataset.from_tensor_slices(
    (dict(test_ab), test_target))

```

请注意，我们将训练和测试特征数据转换为 Python 字典。我们这样做是为了能够构造分类特征列。

混洗训练数据、批处理和预取训练和测试数据:

```
BATCH_SIZE = 32
SHUFFLE_BUFFER_SIZE = 100

train_ads = train_ds.shuffle(
    SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(1)
test_ads = test_ds.batch(BATCH_SIZE).prefetch(1)
train_ads, test_ads

```

请注意，这些形状包括每个功能列的名称。

### 探索一批

由于我们将特征数据转换为字典，我们可以显示关于数据的有趣信息，如清单 [4-11](#PC102) 所示。

```
def see_format(data, num, feature, indx):
  for feature_batch, label_batch in data.take(num):
    print('Every feature:', list(feature_batch.keys()))
    print('One example from a batch of ' + feature + ':',
          feature_batch[feature][indx])
    print('One example from a batch of targets:',
          label_batch[indx])

print ('train sample:')
see_format(train_ads, 1, 'Height', 0)
print ()
print ('test sample:')
see_format(test_ads, 1, 'Sex', 0)

Listing 4-11Display information about a sample batch

```

### 分类列

张量流消耗*限于数值数据*。所以我们必须转换任何分类数据。这种情况下唯一的罪魁祸首是“性别”特征，因为它由字符串值“M”、“F”或“I”表示。所以鲍鱼壳的性别要么是雄性，要么是雌性，要么是幼体。

因为我们不能将字符串直接提供给模型，所以我们必须首先将它们映射为数值。分类词汇列特性提供了一种将字符串表示为一个*独热向量*的方法。这个过程被称为**一键编码**，这是一种将分类变量转换成可解释格式的数字的技术。

字符串按以下方式转换:

1.  1 0 0 美元

2.  0 1 0

3.  0 0 1

清单 [4-12](#PC103) one-hot 编码性别特征。

```
from tensorflow import feature_column

sex_one_hot =\
feature_column.categorical_column_with_vocabulary_list(
    'Sex', ['M', 'F', 'I'])

print (type(sex_one_hot))

feature_columns =\
[tf.feature_column.indicator_column(sex_one_hot)]

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

Listing 4-12One-hot encode the sex feature

```

导入*特征列*模块。一键编码“性”。创建要素列列表。我们创建了一个列表，这样我们就可以在一个训练数据集中拥有多个分类特征。最后，为模型创建*特征层*。

有关全面的示例，请阅读以下 URL:

[T2`www.tensorflow.org/tutorials/structured_data/feature_columns`](http://www.tensorflow.org/tutorials/structured_data/feature_columns)

### 建立模型

清除会话并生成种子:

```
tf.keras.backend.clear_session()
np.random.seed(0)
tf.random.set_seed(0)

```

创建模型:

```
model = tf.keras.Sequential([
  feature_layer,
  Dense(128, activation="relu"),
  Dense(128, activation="relu"),
  Dense(29, activation="sigmoid")
])

```

请注意，第一层是 *feature_layer* ，它通知模型关于独热编码特征的信息。

### 编译模型

编译:

```
model.compile(optimizer='adam',
 loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

```

### 训练模型

火车:

```
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)
model.fit(train_ads,
 validation_data=test_ads,
 epochs=1)

```

我们只训练了一个时期，因为我们知道表现会很糟糕。我们是怎么知道的？请查看下一部分，找出答案。

### 不平衡和不规则的数据

鲍鱼数据集*不是*进行预测的好数据集，原因有二:

1.  数据集不平衡。

2.  数据集不规则。

一个**不平衡的数据集**是一个类没有被平等表示的数据集。也就是说，类没有相同数量的例子。这个数据集尤其不平衡，因为有些类只有一个示例，而其他类有数百个示例。用不平衡的数据集进行训练不会产生好的结果。所以我们不会学到很多东西。原因是预测偏向实例多的类！

一个**不规则数据集**有太多的目标(或标签)类，但没有足够的数据。我们应该始终检查数据集中每个标签的样本(或示例)数量。没有足够样本的类别标签更难学习。

### 处理不平衡数据

我们可以用多种方法处理不平衡的数据。我们可以改变算法。有些算法可能比其他算法效果更好。所以尝试各种各样的方法。通过向少数类添加更多实例进行过采样。我们也可以通过从多数类中移除观测值来进行欠采样。最后，我们可以扩充数据。

有关更详细的解释，请参考以下 URL:

[T2`https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18`](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)