# 10.从零开始的逻辑回归

在第 [2](02.html) 章中，我们开发了一个用于单神经元二元分类的逻辑回归模型，并将其应用于 MNIST 数据集的两位数。计算图构造的实际 Python 代码只有十行代码(不包括执行模型训练的部分；回顾章节 [2](02.html) ，如果你不记得我们在那里做了什么)。

```
tf.reset_default_graph()

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [1, None])
learning_rate = tf.placeholder(tf.float32, shape=())

W = tf.Variable(tf.zeros([1, n_dim]))
b = tf.Variable(tf.zeros(1))

init = tf.global_variables_initializer()
y_ = tf.matmul(tf.transpose(W),X)+b
cost = tf.reduce_mean(tf.square(y_-Y))
training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

```

这段代码非常紧凑，编写起来非常快。在幕后真的有很多事情是你用 Python 代码看不到的。TensorFlow 在后台做了很多你可能没有意识到的事情。尝试在不使用 TensorFlow 的情况下，完全从零开始，用数学和 Python 开发这个精确的模型，以观察实际发生的事情，这将是非常有益的。接下来的部分列出了所需的整个数学公式，以及它在 Python 中的实现(只有`numpy`)。目标是建立一个模型，我们可以为二元分类进行训练。

我不会在符号或数学背后的思想上花太多时间，因为你在前面的章节里已经看过几次了。数据集准备的讨论也将非常简短，因为它已经在第 [2](02.html) 章中进行了描述。

我不会一行一行地浏览所有的 Python 代码，因为，如果您阅读了前面的章节，您应该对这里使用的实践和思想有相当好的理解。没有真正的新概念；你已经看到了几乎所有的东西。把这一章当作一个参考，学习如何完全从零开始实现一个逻辑模型。我强烈建议你尝试理解所有的数学，并用 Python 实现一次。它很有启发性，会教会你很多关于调试的知识，关于写好代码有多重要，以及 TensorFlow 这样的库有多舒服。

## 逻辑回归背后的数学

让我们从一些符号和我们将要做的事情的提示开始。我们的预测将是一个变量![$$ \widehat{y} $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_IEq1.png)，它只能是 0 或 1。(我们将用 0 和 1 表示我们试图预测的两个类别。)

![$$ Prediction\to \widehat{y}\in \left\{0,1\right\} $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equa.png)

给定输入情况 *x* ，我们的方法给出的输出或预测将是![$$ \widehat{y} $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_IEq2.png)为 1 的概率。或者，用一种更数学的形式，

![$$ \widehat{y}=P\left(y=1|x\right) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equb.png)

然后，我们将定义一个输入观察值，如果为![$$ \widehat{y}\ge 0.5, $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_IEq3.png)则为类 1，如果为![$$ \widehat{y}&lt;0.5 $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_IEq4.png)则为类 0。正如我们在章节 [2](02.html) (见图 [2-2](02.html#Fig2) )中所述，我们将考虑 *n* <sub>* x *</sub> 输入和一个具有 sigmoid(用 *σ* 表示)激活功能的神经元。我们的神经元输出![$$ \widehat{y} $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_IEq5.png)可以很容易地写成观察 *i* 为

![$$ {\widehat{y}}^{\left[i\right]}=\sigma \left({z}^{\left[i\right]}\right)=\sigma \left({\boldsymbol{w}}^T{\boldsymbol{x}}^{\left[i\right]}+b\right)=\sigma \left({w}_1{x}_1^{\left[i\right]}+{w}_2{x}_2^{\left[i\right]}+\dots +{w}_{n_x}{x}_{n_x}^{\left[i\right]}+b\right) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equc.png)

为了找到最佳的权重和偏差，我们将最小化这里为一次观察写的成本函数

![$$ \mathrm{\mathcal{L}}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)=-\left({y}^{\left[i\right]}\log {\widehat{y}}^{\left[i\right]}+\left(1-{y}^{\left[i\right]}\right)\log \left(1-{\widehat{y}}^{\left[i\right]}\right)\right) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equd.png)

其中，用 *y* 表示我们的标签。我们将使用梯度下降算法，如第 [2](02.html) 章所述，因此我们需要成本函数关于权重和偏差的偏导数。你会记得，在迭代 *n* + 1 时(我们将在这里用方括号中的下标表示该迭代)，我们将从迭代 *n* 用等式更新我们的权重

![
$$ {w}_{j,\left[n+1\right]}={w}_{j,\left[n\right]}-\gamma \frac{\partial \mathcal{L}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)}{\partial {w}_j} $$
](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Eque.png)

对于偏见，

![
$$ {b}_{\left[n+1\right]}={b}_{j,\left[n\right]}-\gamma \frac{\partial \mathcal{L}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)}{\partial b} $$
](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equf.png)

其中 *γ* 是学习率。导数并不复杂，可以用链式法则很容易地计算出来

![$$ \frac{\mathrm{\partial \mathcal{L}}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)}{\partial {w}_j}=\frac{\mathrm{\partial \mathcal{L}}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)}{\partial {\widehat{y}}^{\left[i\right]}}\frac{d{\widehat{y}}^{\left[i\right]}}{d{z}^{\left[i\right]}}\frac{\partial {z}^{\left[i\right]}}{\partial {w}_j} $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equg.png)

*b* 也可以

![$$ \frac{\mathrm{\partial \mathcal{L}}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)}{\partial b}=\frac{\mathrm{\partial \mathcal{L}}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)}{\partial {\widehat{y}}^{\left[i\right]}}\frac{d{\widehat{y}}^{\left[i\right]}}{d{z}^{\left[i\right]}}\frac{\partial {z}^{\left[i\right]}}{\partial b} $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equh.png)

现在，计算导数，你可以验证

![$$ \frac{\mathrm{\partial \mathcal{L}}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)}{\partial {\widehat{y}}^{\left[i\right]}}=-\frac{y^{\left[i\right]}}{{\widehat{y}}^{\left[i\right]}}+\frac{1-{y}^{\left[i\right]}}{1-{\widehat{y}}^{\left[i\right]}} $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equi.png)

![$$ \frac{d{\widehat{y}}^{\left[i\right]}}{d{z}^{\left[i\right]}}={\widehat{y}}^{\left[i\right]}\left(1-{\widehat{y}}^{\left[i\right]}\right) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equj.png)

![$$ \frac{\partial {z}^{\left[i\right]}}{\partial {w}_j}={x}_j^{\left[i\right]} $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equk.png)

![$$ \frac{\partial {z}^{\left[i\right]}}{\partial b}=1 $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equl.png)

当我们把这些放在一起，我们得到

![
$$ {w}_{j,\left[n+1\right]}={w}_{j,\left[n\right]}-\gamma \frac{\partial \mathcal{L}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)}{\partial {w}_j}={w}_{j,\left[n\right]}-\gamma \left(1-{\widehat{y}}^{\left[i\right]}\right){x}_j^{\left[i\right]} $$
](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equm.png)

![$$ {b}_{\left[n+1\right]}={b}_{\left[n\right]}-\gamma \frac{\mathrm{\partial \mathcal{L}}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)}{\partial b}={b}_{j,\left[n\right]}-\gamma \left(1-{\widehat{y}}^{\left[i\right]}\right) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equn.png)

这些方程只对一种训练情况有效；因此，正如我们已经做的，让我们将它们推广到许多训练案例，记住我们为许多观察定义我们的成本函数 *J* 为

![$$ J\left(\boldsymbol{w},b\right)\kern0.6em =\frac{1}{m}\sum \limits_{i=1}^m\mathrm{\mathcal{L}}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equo.png)

这里，像往常一样，我们用 *m* 表示观察值的数量。粗体的 ***w*** 就是所有权重![$$ \boldsymbol{w}=\left({w}_1,{w}_2,\dots, {w}_{n_x}\right) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_IEq6.png)的向量。这里我们也需要我们心爱的矩阵形式(你在前面的章节中已经看过几次了)

![$$ Z={W}^TX+B $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equp.png)

其中我们用 *B* 表示了一个维度矩阵(1，*n*<sub>T5】x</sub>)(为了与我们现在在这里使用的符号一致)，并且所有元素都等于 *b* (在 Python 中，我们不必定义它，因为广播会为我们处理它)。 *X* 将包含我们的观察和特征，并且具有维度( *n* <sub>*x*</sub> ， *m* )(列上的观察，行上的特征)，并且 *W* <sup>*T*</sup> 将是包含所有权重的矩阵的转置，在我们的情况下，其具有维度(1， *n* <sub>*x 矩阵形式的神经元输出将是*</sub>

![$$ \widehat{Y}=\sigma (Z) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equq.png)

其中 sigmoid 函数逐个元素地起作用。偏导数的方程现在变成了

![$$ \frac{\partial J\left(w,b\right)}{\partial {w}_j}=\frac{1}{m}\sum \limits_{i=1}^m\frac{\partial \mathcal{L}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)}{\partial {w}_j}=\frac{1}{m}\sum \limits_{i=1}^m\frac{\partial \mathcal{L}\left({\widehat{y}}^{\left[i\right]},\kern0.375em {y}^{\left[i\right]}\right)}{\partial {\widehat{y}}^{\left[i\right]}}\frac{d{\widehat{y}}^{\left[i\right]}}{d{z}^{\left[i\right]}}\frac{\partial {z}^{\left[i\right]}}{\partial {w}_j}=\frac{1}{m}\sum \limits_{i=1}^m\left(1-{\widehat{y}}^{\left[i\right]}\right){x}_j^{\left[i\right]} $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equr.png)

并且对于 *b*

![$$ \frac{\partial J\left(w,b\right)}{\partial b}=\frac{1}{m}\sum \limits_{i=1}^m\frac{\partial \mathcal{L}\left({\widehat{y}}^{\left[i\right]},{y}^{\left[i\right]}\right)}{\partial b}=\frac{1}{m}\sum \limits_{i=1}^m\frac{\partial \mathcal{L}\left({\widehat{y}}^{\left[i\right]},\kern0.375em {y}^{\left[i\right]}\right)}{\partial {\widehat{y}}^{\left[i\right]}}\frac{d{\widehat{y}}^{\left[i\right]}}{d{z}^{\left[i\right]}}\frac{\partial {z}^{\left[i\right]}}{\partial b}=\frac{1}{m}\sum \limits_{i=1}^m\left(1-{\widehat{y}}^{\left[i\right]}\right) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equs.png)

这些方程可以矩阵形式写成(其中∇ <sub>***w***</sub> 表示相对于 ***w*** 的梯度)如下

![$$ {\nabla}_{\boldsymbol{w}}J\left(\boldsymbol{w},b\right)=\frac{1}{m}X{\left(\widehat{Y}-Y\right)}^T $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equt.png)

而对于 *b* ，

![$$ \frac{\partial J\left(\boldsymbol{w},b\right)}{\partial b}=\frac{1}{m}\sum \limits_{i=1}^m\left({\widehat{Y}}_i-{Y}_i\right) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equu.png)

最后，我们需要为梯度下降算法实现的方程是

![$$ {\boldsymbol{w}}_{\left[n+1\right]}={\boldsymbol{w}}_{\left[n\right]}-\gamma \frac{1}{m}X{\left(\widehat{Y}-Y\right)}^T $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equv.png)

而对于 *b* ，

![$$ {b}_{\left[n+1\right]}={b}_{\left[n\right]}-\gamma \frac{1}{m}\sum \limits_{i=1}^m\left({\widehat{Y}}_i-{Y}_i\right) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equw.png)

至此，您应该已经对 TensorFlow 有了全新的认识。该库在后台为您完成所有这些工作，更重要的是，这些都是自动完成的。记住:我们在这里处理的只是一个神经元。你可以很容易地看到，当你想要为具有许多层和神经元的网络，或者为卷积或递归神经网络计算相同的方程时，它会变得多么复杂。

我们现在拥有了从头开始实现逻辑回归所需的所有数学知识。让我们转到一些 Python。

## Python 实现

让我们开始导入必要的库。

```
import numpy as np
%matplotlib inline
import matplotlib.pyplot as plt

```

注意，我们不导入 TensorFlow。我们这里不需要它。让我们为 sigmoid 激活函数`sigmoid(z)`写一个函数。

```
def sigmoid(z):
    s = 1.0 / (1.0 + np.exp(-z))
    return s

```

我们还需要一个函数来初始化权重。在这种基本情况下，我们可以简单地用零初始化所有东西。逻辑回归无论如何都会起作用。

```
def initialize(dim):
    w = np.zeros((dim,1))
    b = 0
    return w,b

```

然后，我们必须实现以下等式，这些等式已在上一节中计算过:

![$$ {\nabla}_{\boldsymbol{w}}J\left(\boldsymbol{w},b\right)=\frac{1}{m}X{\left(\widehat{Y}-Y\right)}^T $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equx.png)

和

![$$ \frac{\partial J\left(\boldsymbol{w},b\right)}{\partial b}=\frac{1}{m}\sum \limits_{i=1}^m\left({\widehat{Y}}_i-{Y}_i\right) $$](../images/463356_1_En_10_Chapter/463356_1_En_10_Chapter_TeX_Equy.png)

```
def derivatives_calculation(w, b, X, Y):
    m = X.shape[1]
    z = np.dot(w.T,X)+b
    y_ = sigmoid(z)

    cost = -1.0/m*np.sum(Y*np.log(y_)+(1.0-Y)*np.log(1.0-y_))

    dw = 1.0/m*np.dot(X, (y_-Y).T)
    db = 1.0/m*np.sum(y_-Y)

    derivatives = {"dw": dw, "db":db}
    return derivatives, cost

```

现在我们需要更新权重的函数。

```
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):

    costs = [] for i in range(num_iterations):
        derivatives, cost = derivatives_calculation(w, b, X, Y)
        dw = derivatives ["dw"]
        db = derivatives ["db"]

        w = w - learning_rate*dw
        b = b - learning_rate*db

        if i % 100 == 0:
            costs.append(cost)

        if print_cost and i % 100 == 0:
            print ("Cost (iteration %i) = %f" %(i, cost))

    derivatives = {"dw": dw, "db": db}
    params = {"w": w, "b": b}

    return params, derivatives, costs

```

下一个函数`predict()`创建一个维度矩阵(1， *m* ，它包含给定输入 ***w*** 和 *b* 的模型预测。

```
def predict (w, b, X):
    m = X.shape[1]

    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0],1)
    A = sigmoid (np.dot(w.T, X)+b)

    for i in range(A.shape[1]):

        if (A[:,i] > 0.5):
            Y_prediction[:, i] = 1
        elif (A[:,i] <= 0.5):
            Y_prediction[:, i] = 0
    return Y_prediction

```

最后，让我们把所有东西放在`model()`函数中。

```
def model (X_train, Y_train, X_test, Y_test, num_iterations = 1000, learning_rate = 0.5, print_cost = False):

    w, b = initialize(X_train.shape[0])
    parameters, derivatives, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)

    w = parameters["w"]
    b = parameters["b"]

    Y_prediction_test = predict (w, b, X_test)
    Y_prediction_train = predict (w, b, X_train)

    train_accuracy = 100.0 - np.mean(np.abs(Y_prediction_train-Y_train)*100.0)
    test_accuracy = 100.0 - np.mean(np.abs(Y_prediction_test-Y_test)*100.0)

    d = {"costs": costs, "Y_prediction_test": Y_prediction_test, "Y_prediction_train": Y_prediction_train, "w": w, "b": b, "learning_rate": learning_rate, "num_iterations": num_iterations}

    print ("Accuracy Test: ", test_accuracy)
    print ("Accuracy Train: ", train_accuracy)

    return d

```

## 模型测试

建立模型后，一定要看它用一些数据能达到什么结果。在下一节中，我将首先准备我们已经在第 [2](02.html) 章中使用的数据集，来自 MNIST 数据集的两位数字 1 和 2，然后在数据集上训练我们的神经元，并检查我们得到的结果。

### 数据集准备

作为一个优化指标，我们选择了准确性，所以让我们看看我们的模型可以达到什么样的价值。我们将使用与第 [2](02.html) 章相同的数据集:由数字 1 和 2 组成的 MNIST 数据集的子集。在这里，你可以不用解释就找到获取数据的代码，因为我们已经在第 [2](02.html) 章中对它进行了广泛的剖析。

我们需要的代码如下:

```
from sklearn.datasets import fetch_mldata
mnist = fetch_mldata('MNIST original')
X,y = mnist["data"], mnist["target"]

X_12 = X[np.any([y == 1,y == 2], axis = 0)]
y_12 = y[np.any([y == 1,y == 2], axis = 0)]

```

因为我们在一个块中加载了所有图像，所以我们必须创建一个 dev 和一个 train 数据集(分割为 80% train 和 20% dev)，如下所示:

```
shuffle_index = np.random.permutation(X_12.shape[0])
X_12_shuffled, y_12_shuffled = X_12[shuffle_index], y_12[shuffle_index]

train_proportion = 0.8
train_dev_cut = int(len(X_12)*train_proportion)
X_train, X_dev, y_train, y_dev = \
    X_12_shuffled[:train_dev_cut], \
    X_12_shuffled[train_dev_cut:], \
    y_12_shuffled[:train_dev_cut], \
    y_12_shuffled[train_dev_cut:]

```

像往常一样，我们将输入标准化，

```
X_train_normalised = X_train/255.0
X_dev_normalised = X_test/255.0

```

将矩阵转换成正确的格式，

```
X_train_tr = X_train_normalised.transpose()
y_train_tr = y_train.reshape(1,y_train.shape[0])
X_dev_tr = X_dev_normalised.transpose()
y_dev_tr = y_dev.reshape(1,y_dev.shape[0])

```

并定义一些常数。

```
dim_train = X_train_tr.shape[1]
dim_dev = X_dev_tr.shape[1]

```

现在让我们转换一下标签(还记得第 [2](02.html) 章的内容吗？).我们这里有 1 和 2，我们需要 0 和 1。

```
y_train_shifted = y_train_tr - 1
y_test_shifted = y_test_tr - 1

```

### 运行测试

最后，我们可以通过调用来测试模型

```
d = model (Xtrain, ytrain, Xtest, ytest, num_iterations = 4000, learning_rate = 0.05, print_cost = True)

```

虽然您的数字可能会有所不同，但您应该会得到类似于下面的输出，由于篇幅原因，我省略了一些迭代:

```
Cost (iteration 0) = 0.693147
Cost (iteration 100) = 0.109078
Cost (iteration 200) = 0.079466
Cost (iteration 300) = 0.067267
Cost (iteration 400) = 0.060286

.........

Cost (iteration 3600) = 0.031350
Cost (iteration 3700) = 0.031148
Cost (iteration 3800) = 0.030955
Cost (iteration 3900) = 0.030769
Accuracy Test: 99.092131809
Accuracy Train: 99.1003111074

```

结果还不错。 <sup>[1](#Fn1)</sup>

## 结论

你真的应该试着去理解我在这一章中概述的所有步骤，去理解一个图书馆为你做了多少。记住:我们这里有一个非常简单的只有一个神经元的模型。理论上，你可以为更复杂的网络架构写下所有的方程式，但是这将会非常困难并且非常容易出错。TensorFlow 为你计算所有的导数，不考虑网络的复杂程度。如果你有兴趣了解 TensorFlow 可以做什么，我建议你阅读官方文档，可在 [`https://goo.gl/E5DpHK`](https://goo.gl/E5DpHK) 获得。

### 注意

您现在应该会欣赏 TensorFlow 这样的库，并意识到在使用它时后台发生了多少事情。为了优化和调试你的模型，你还应该意识到计算的复杂性和理解算法细节的重要性，以及这些算法是如何实现的。

<aside class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

如果你想知道为什么我们会得到与第 [2](02.html) 章不同的结果，请记住，我们使用了一个略有不同的学习训练集，从而产生了这种差异。

 </aside>