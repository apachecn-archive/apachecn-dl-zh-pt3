# 2.单个神经元

在这一章中，我将讨论什么是神经元及其组成部分。我将阐明我们将需要的数学符号，并涵盖今天在神经网络中使用的许多激活函数。梯度下降优化将被详细讨论，学习率的概念和它的怪癖将被介绍。为了让事情变得更有趣，我们将使用单个神经元在真实数据集上执行线性和逻辑回归。然后我会讨论和解释如何用`tensorflow`实现这两个算法。

为了保持本章的重点和学习效率，我故意省略了一些东西。例如，我们不会将数据集分成训练和测试部分。我们只是使用所有的数据。使用这两个会迫使我们做一些适当的分析，这将分散本章的主要目标，使它太长。在本书的后面，我将对使用几个数据集的后果进行适当的分析，并看看如何正确地做到这一点，特别是在深度学习的背景下。这是一个需要有自己的一章的主题。

通过深度学习，你可以做一些精彩、惊人和有趣的事情。让我们开始开心吧！

## 神经元的结构

深度学习基于由大量简单计算单元组成的大型复杂网络。处于研究前沿的公司正在处理拥有 1600 亿个参数的网络[1]。客观地看，这个数字是我们银河系恒星数量的一半，或者说是曾经居住过的人口数量的 1.5 倍。在基本层面上，神经网络是一大组不同的相互连接的单元，每个单元执行特定的(通常相对容易的)计算。他们回忆起乐高玩具，用非常简单和基本的单元，你可以建造非常复杂的东西。神经网络也类似。使用相对简单的计算单元，您可以构建非常复杂的系统。我们可以改变基本单位，改变它们计算结果的方式、它们如何相互连接、它们如何使用输入值等等。粗略地说，所有这些方面定义了所谓的网络架构。改变它将改变网络学习的方式，预测的准确性等等。

由于与大脑的生物相似性，这些基本单位被称为神经元。基本上，每个神经元都做一件非常简单的事情:接受一定数量的输入(实数)并计算一个输出(也是实数)。在本书中，我们的输入将用*x*<sub>*I*</sub>∈*ℝ*(实数)表示，用 *i* = 1，2，…、 *n* <sub>*x*</sub> 表示，其中 *i* ∈ *ℕ* 为整数， *n* <sub>*x 作为输入特征的例子，你可以想象一个人的年龄和体重(所以，我们会有*n*<sub>*x*</sub>= 2)。 *x* <sub>1</sub> 可能是年龄， *x* <sub>2</sub> 可能是体重。在现实生活中，特征的数量很容易变得很大。在我们将在本章后面用于逻辑回归示例的数据集中，我们将有*n*<sub>*x*</sub>= 784。*</sub>

有几种已经被广泛研究的神经元。在本书中，我们将集中讨论最常用的一种。我们感兴趣的神经元只是将一个函数应用于所有输入的线性组合。用更数学的形式，给定 *n* <sub>*x*</sub> ，实参数*w*<sub>*I*</sub>∈*ℝ*(用 *i* = 1，2，…， *n* <sub>*x*</sub> ，以及一个常数*b*∈*ℝ*

*![$$ z={w}_1{x}_1+{w}_2{x}_2+\cdots +{w}_{n_x}{x}_{n_x}+b $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equa.png)*

 *然后，它将对 *z* 应用函数 *f* ，给出输出![$$ \widehat{y}. $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq1.png)

![$$ \widehat{y}=f(z)=f\left({w}_1{x}_1+{w}_2{x}_2+\cdots +{w}_{n_x}{x}_{n_x}+b\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equb.png)

### 注意

从业者大多使用以下命名法:*w*<sub>T3】IT5】指权重， *b* 偏置， *x* <sub>*i*</sub> 输入特征， *f* 激活函数。</sub>

由于生物学上的相似性，函数 *f* 被称为神经元激活函数(有时也称为传递函数)，这将在接下来的章节中详细讨论。

我们再总结一下神经元计算步骤。

1.  线性组合所有输入*x*<sub>T3】IT5】，计算![$ z={w}_1{x}_1+{w}_2{x}_2+\cdots +{w}_{n_x}{x}_{n_x}+b $](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq2.png)；</sub>

2.  将 *f* 施加到 *z* ，给出输出![$$ \widehat{y}=f(z)=f\left({w}_1{x}_1+{w}_2{x}_2+\cdots +{w}_{n_x}{x}_{n_x}+b\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq3.png)。

你可能还记得在第 [1](01.html) 章中， [I](01.html) 讨论了计算图。在图 [2-1](#Fig1) 中，你会发现之前描述的神经元的图形。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig1_HTML.png](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig1_HTML.png)

图 2-1

文中描述的神经元的计算图

这不是你通常在博客、书籍和教程中找到的。它相当复杂，使用起来不太实用，尤其是当你想画有许多神经元的网络时。在文献中，你可以找到许多神经元的描述。在本书中，我们将使用图 [2-2](#Fig2) 中所示的一种，因为这种方法应用广泛且易于理解。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig2_HTML.png](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig2_HTML.png)

图 2-2

从业者最常用的神经元表示法

图 [2-2](#Fig2) 必须以如下方式解释:

*   投入不是放在一个泡泡里。这只是为了将它们与执行实际计算的节点区分开来。

*   砝码的名称写在箭头上。这意味着，在将输入传递到中心气泡(或节点)之前，输入将首先乘以相对权重，如箭头所示。第一个输入， *x* <sub>1</sub> ，将乘以 *w* <sub>1</sub> ， *x* <sub>2</sub> ，乘以 *w* <sub>2</sub> ，以此类推。

*   中间的气泡(或节点)将同时执行几个计算。首先，它将对输入进行求和(对于 *i* = 1，2，...， *n* <sub>*x*</sub> )然后对结果偏差 *b* 求和，最后，对结果值应用激活函数。

我们将在本书中讨论的所有神经元都具有这种结构。通常，使用更简单的表示法，如图 [2-3](#Fig3) 所示。在这种情况下，除非另有说明，否则输出应理解为

![$$ \widehat{y}=f(z)=f\left({w}_1{x}_1+{w}_2{x}_2+\cdots +{w}_{n_x}{x}_{n_x}+b\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equc.png)

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig3_HTML.png](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig3_HTML.png)

图 2-3

*下面的* *表示法* *是图* [2-2](#Fig2) *的简化版。除非特别说明，通常理解输出为* ![$$ \widehat{y}=f(z)=f\left({w}_1{x}_1+{w}_2{x}_2+\dots +{w}_{n_x}{x}_{n_x}+b\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq4.png) *。神经元表示中通常不明确报告权重。*

### 矩阵符号

在处理大型数据集时，特征的数量很大( *n* <sub>*x*</sub> 会很大)，因此最好对特征和权重使用矢量符号，如下所示:

![$$ x=\left(\begin{array}{c}{x}_1\\ {}\vdots \\ {}{x}_{n_x}\end{array}\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equd.png)

这里我们用粗体字 ***x*** 表示向量。对于权重，我们使用相同的符号:

![$$ w=\left(\begin{array}{c}{w}_1\\ {}\vdots \\ {}{w}_{n_x}\end{array}\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Eque.png)

为了与我们稍后将使用的公式保持一致，为了将 ***x*** 和 ***w*** 相乘，我们将使用矩阵乘法符号，因此，我们将编写

![$$ {w}^Tx=\left({w}_1\dots {w}_{n_x}\right)\left(\begin{array}{c}{x}_1\\ {}\vdots \\ {}{x}_{n_x}\end{array}\right)={w}_1{x}_1+{w}_2{x}_2+\cdots +{w}_{n_x}{x}_{n_x} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equf.png)

其中***w***<sup>*T*</sup>表示 ***w*** 的转置。 *z* 可以用这个向量符号写成

![$$ z={w}^Tx+b $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equg.png)

并且神经元输出![$$ \widehat{y} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq5.png)为

![$$ \widehat{y}=f(z)=f\left({w}^Tx+b\right)\#(3) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equh.png)

现在让我们总结一下定义我们的神经元的不同组件，以及我们将在本书中使用的符号。

*   ![$$ \widehat{y} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq6.png) →神经元输出

*   *f* ( *z* ) →激活函数(或传递函数)应用于 *z*

*   ***w*** →权重(向量带有 *n* <sub>*x*</sub> 分量)

*   *b* →偏置

### Python 实现技巧:循环和 NumPy

我们在等式(3)中概述的计算可以在 Python 中通过标准列表和循环来完成，但随着变量和观察值的增加，这些往往会非常慢。一个好的经验法则是尽可能避免循环，并尽可能多地使用`NumPy`(或`TensorFlow`，我们将在后面看到)方法。

很容易知道`NumPy`能有多快(以及循环有多慢)。让我们首先用 Python 创建两个标准随机数列表，每个列表有 10 个 <sup>7 个</sup>元素。

```py
import random
lst1 = random.sample(range(1, 10**8), 10**7)
lst2 = random.sample(range(1, 10**8), 10**7)

```

实际值与我们的目的无关。我们只是对 Python 能以多快的速度一个元素一个元素地相乘两个列表感兴趣。报告的时间是在 2017 年微软 surface 笔记本电脑上测量的，会有很大差异，取决于代码运行的硬件。我们对绝对值不感兴趣，只关心与标准 Python 循环相比,`NumPy`快了多少。要在 Jupyter 笔记本中记录 Python 代码的时间，我们可以使用“魔法命令”通常，在 Jupyter 笔记本中，这些命令以%%或%开头。一个好主意是查看官方文档，可以从 [`http://ipython.readthedocs.io/en/stable/interactive/magics.html`](http://ipython.readthedocs.io/en/stable/interactive/magics.html) 访问，以便更好地了解它们是如何工作的。

回到我们的测试，让我们测量一台标准笔记本电脑用标准循环一个元素接一个元素地乘以两个列表需要多少时间。使用代码

```py
%%timeit
ab = [lst1[i]*lst2[i] for i in range(len(lst1))]

```

给出以下结果(注意，在您的计算机上，您可能会得到不同的结果):

```py
2.06 s ± 326 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

```

在七次运行中，代码平均需要大约两秒钟。现在让我们尝试做同样的乘法，但是，这一次，使用`NumPy`，我们首先将两个列表转换为`NumPy`数组，代码如下:

```py
import numpy as np
list1_np = np.array(lst1)
list2_np = np.array(lst2)

%%timeit
Out2 = np.multiply(list1_np, list2_np)

```

这一次，我们得到以下结果:

```py
20.8 ms ± 2.5 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

```

`numpy`代码只需要 21 毫秒，换句话说，比使用标准循环的代码大约快 100 倍。`NumPy`更快有两个原因:底层例程是用 C 写的，它尽可能多地使用了矢量化代码来加速对大量数据的计算。

### 注意

矢量化代码是指同时(在一条语句中)对向量(或矩阵)的多个分量执行的操作。将矩阵传递给`NumPy`函数是矢量化代码的一个很好的例子。`NumPy`将同时对大块数据执行操作，从而获得比标准 Python 循环更好的性能，标准 Python 循环必须一次对一个元素进行操作。请注意，`NumPy`表现出的部分良好性能也归功于用 c 编写的底层例程

在训练深度学习模型时，你会发现自己一遍又一遍地做这种操作，因此，这样的速度增益将决定拥有一个可以训练的模型和一个永远不会给你结果的模型。

### 激活功能

我们有许多激活功能来改变神经元的输出。记住:激活函数是一个简单的数学函数，它在输出![$$ \widehat{y} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq7.png)中转换 *z* 。我们来看看用的最多的。

#### 身份功能

这是您可以使用的最基本的功能。通常用 *I* ( *z* )表示。它简单地返回不变的输入值。数学上我们有

![$$ f(z)=I(z)=z $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equi.png)

当我在本章后面讨论一个神经元的线性回归时，这个简单的函数会派上用场。图 [2-4](#Fig4) 显示了它的样子。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig4_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig4_HTML.jpg)

图 2-4

身份功能

用 Python 中的`numpy`实现一个身份函数特别简单。

```py
def identity(z):
    return z

```

#### Sigmoid 函数

这是一个非常常用的函数，只给出 0 到 1 之间的值。通常用 *σ* ( *z* )表示。

![$$ f(z)=\sigma (z)=\frac{1}{1+{e}^{-z}} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equj.png)

它尤其适用于我们必须预测概率作为输出的模型(记住，概率只能取 0 到 1 之间的值)。你可以在图 [2-5](#Fig5) 中看到它的形状。注意，在 Python 中，如果 *z* 足够大，那么函数可能会因为舍入误差而恰好返回 0 或 1(取决于 *z* 的符号)。在分类问题中，我们会经常计算 log *σ* ( *z* )或 log(1*σ*(*z*))，因此，这可能是 Python 中的一个错误源，因为它会尝试计算 log 0，而 log 0 是未定义的。例如，在计算成本函数时，您可以开始看到`nan`出现(稍后将详细介绍)。我们将在本章后面看到这种现象的一个实际例子。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig5_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig5_HTML.jpg)

图 2-5

sigmoid 激活函数是从 0 到 1 的 s 形函数

### 注意

虽然 *σ* ( *z* )永远不应该正好是 0 或 1，但在用 Python 编程时，现实可能会大不相同。由于一个非常大的 *z* (正或负)，Python 可能会将结果精确地四舍五入为 0 或 1。这可能会在计算分类的成本函数时给你带来错误(我将在本章后面给你详细的解释和实际的例子)，因为我们将需要计算 log *σ* ( *z* )和 log(1-*σ*(*z*))，因此，Python 将试图计算 log0，这是没有定义的。例如，如果我们没有正确地标准化输入数据，或者如果我们没有正确地初始化我们的权重，就会发生这种情况。目前，重要的是要记住，虽然数学上一切似乎都在控制之下，但编程时的现实可能更加困难。在调试模型时，记住这一点是很好的，例如，为成本函数提供`nan`作为结果。

带 *z* 的行为见图 [2-5](#Fig5) 。可以使用 numpy 函数将计算写成以下形式:

```py
s = np.divide(1.0, np.add(1.0, np.exp(-z)))

```

### 注意

要知道如果我们有两个`numpy`数组，`A`和`B`，下面是等价的:`A/B`等价于`np.divide(A,B)`，`A+B`等价于`np.add(A,B)`，`A-B`等价于`np.subtract(A,B)`，`A*B`等价于`np.multiply(A,B)`是很有用的。以防你熟悉面向对象编程，我们说在`numpy`中，基本操作，比如/、*、+和-，都是重载的。还要注意的是，`numpy`中的所有这四个基本操作都是逐个元素进行的。

我们可以将 sigmoid 函数写成可读性更好的形式(至少对人类来说是这样),如下所示:

```py
def sigmoid(z):
    s = 1.0 / (1.0 + np.exp(-z))
    return s

```

如前所述，`1.0 + np.exp(-z)`相当于`np.add(1.0, np.exp(-z))`，`1.0 / (np.add(1.0, np.exp(-z)))`相当于`np.divide(1.0, np.add(1.0, np.exp(-z)))`。我想让你注意公式中的另一点。`np.exp(-z)`的维数为`z`(通常是一个向量，其长度等于观测值的数量)，而 1.0 是一个标量(一维实体)。Python 如何将两者相加？发生的就是所谓的*广播。*在这种情况下，1.0 就变成了一个和`z`一样维数的数组，全部用 1.0 填充。这是一个需要理解的重要概念，因为它非常有用。例如，您不必转换数组中的数字。Python 会帮你搞定的。关于广播在其他情况下如何工作的规则相当复杂，超出了本书的范围。但是，要知道 Python 是在后台做一些事情，这一点很重要。

#### 双曲正切激活函数

双曲正切也是从-1 到 1 的 s 形曲线。

![$$ f(z)=\tanh (z) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equk.png)

在图 [2-6](#Fig6) 中，可以看到它的形状。在 Python 中，这很容易实现，如下所示:

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig6_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig6_HTML.jpg)

图 2-6

双曲函数是一条从-1 到 1 的 s 形曲线

```py
def tanh(z):
    return np.tanh(z)

```

#### ReLU(整流线性单位)激活功能

ReLU 功能(图 [2-7](#Fig7) )具有以下公式:

![$$ f(z)=\max \left(0,z\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equl.png)

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig7_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig7_HTML.jpg)

图 2-7

ReLU 函数

花一些时间探索如何在 Python 中以智能的方式实现 ReLU 函数是很有用的。请注意，当我们开始使用`TensorFlow`时，我们已经实现了它，但是观察不同的 Python 实现如何在实现复杂的深度学习模型时产生影响是非常有启发性的。

在 Python 中，可以用几种方法实现 ReLU 函数。下面列出了四种不同的方法。(在继续之前，试着理解它们的工作原理。)

1.  `np.maximum(x, 0, x)`

2.  `np.maximum(x, 0)`

3.  `x * (x > 0)`

4.  `(abs(x) + x) / 2`

这四种方法具有非常不同的执行速度。让我们用 10 个 <sup>8 个</sup>元素生成一个`numpy`数组，如下所示:

```py
x = np.random.random(10**8)

```

现在让我们来测量 ReLU 函数的四个不同版本在应用时所需的时间。让下面的代码运行:

```py
x = np.random.random(10**8)
print("Method 1:")
%timeit -n10 np.maximum(x, 0, x)

print("Method 2:")
%timeit -n10 np.maximum(x, 0)

print("Method 3:")
%timeit -n10 x * (x > 0)

print("Method 4:")
%timeit -n10 (abs(x) + x) / 2

```

结果如下:

```py
Method 1:
2.66 ms ± 500 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)
Method 2:
6.35 ms ± 836 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)
Method 3:
4.37 ms ± 780 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)
Method 4:
8.33 ms ± 784 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)

```

差别是惊人的。方法 1 比方法 4 快四倍。`numpy`库是高度优化的，许多例程都是用 c 语言编写的。但是知道如何高效地编码仍然很重要，并且会产生很大的影响。为什么`np.maximum(x, 0, x)`比`np.maximum(x, 0)`快？第一个版本就地更新 x，而不创建新的数组。这可以节省大量时间，尤其是当数组很大时。如果不想(或者不能)就地更新输入向量，仍然可以使用`np.maximum(x, 0)`版本。

实现可能如下所示:

```py
def relu(z):
    return np.maximum(z, 0)

```

### 注意

记住:在优化你的代码时，即使是很小的变化也会带来巨大的不同。在深度学习程序中，相同的代码块会重复数百万次和数十亿次，因此即使是很小的改进，从长远来看也会产生巨大的影响。花时间优化你的代码是一个必要的步骤，会有回报的。

#### 李奇注意到了

泄漏 ReLU(也称为参数整流线性单元)由下式给出

![$$ f(z)=\Big\{{\displaystyle \begin{array}{ccc}\alpha z&amp; for&amp; z&lt;0\\ {}z&amp; for&amp; z\ge 0\end{array}} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equm.png)

其中 *α* 是典型地大约为 0.01 的参数。在图 [2-8](#Fig8) 中，可以看到 *α* = 0.05 的例子。选择该值是为了使*x*T12】0 和*x*T13】0 之间的差异更加明显。通常，使用较小的 *α* 值，但是需要用您的模型进行测试以找到最佳值。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig8_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig8_HTML.jpg)

图 2-8

α = 0.05 的泄漏 ReLU 激活函数

例如，在 Python 中，如果已经将`relu(z)`函数定义为

```py
def lrelu(z, alpha):
  return relu(z) - alpha * relu(-z)

```

#### Swish 激活功能

最近，谷歌大脑[4]的 Ramachandran，Zopf 和 Le 研究了一种新的激活功能，称为 Swish，在深度学习领域显示出巨大的前景。它被定义为

![$$ f(z)= z\sigma \left(\beta z\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equn.png)

其中 *β* 是一个可学习的参数。在图 [2-9](#Fig9) 中，你可以看到这个激活函数如何寻找参数 *β* 的三个值:0.1、0.5 和 10.0。该团队的研究表明，只需用 Swish 替换 ReLU 激活函数，就可以将 ImageNet 上的分类准确率提高 0.9%。在今天的深度学习世界中，这已经很多了。你可以在 [`www.image-net.org`](http://www.image-net.org) `/`找到更多关于 ImageNet 的信息。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig9_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig9_HTML.jpg)

图 2-9

参数β的三个不同值的 Swish 激活函数

ImageNet 是一个大型图像数据库，通常用于测试新的网络架构或算法，例如，在这种情况下，具有不同激活功能的网络。

#### 其他激活功能

还有许多其他激活功能，但这些很少使用。作为参考，以下是一些额外的。这个列表并不全面，但应该可以让你了解在开发神经网络时可以使用的各种激活函数。

*   阿尔坦

![$$ f(z)={\tan}^{-1}z $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equo.png)

*   指数线性单位(ELU)

![$$ f(z)=\Big\{{\displaystyle \begin{array}{ccc}\alpha \left({e}^z-1\right)&amp; for&amp; z&lt;0\\ {}z&amp; for&amp; z\ge 0\end{array}} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equp.png)

*   软加

![$$ f(z)=\ln \left(1+{e}^z\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equq.png)

### 注意

从业者几乎总是只使用两个激活函数:sigmoid 和 ReLU(ReLU 可能是最常用的)。通过这两种方法，您可以获得良好的结果，而且，如果网络架构足够复杂，这两种方法都可以逼近任何非线性函数[5，6]。请记住，使用`tensorflow`时，您不必自己实现这些功能。`tensorflow`将提供一个高效的实现供您使用。但重要的是要知道每个激活功能的行为，了解何时使用哪一个。

### 成本函数和梯度下降:学习率的怪癖

既然你清楚地理解了什么是神经元，我将讨论它(以及，一般来说，对于神经网络)学习的意义。这将允许我们引入超参数和学习率等概念。在几乎所有的神经网络问题中，学习简单地意味着找到权重(记住，神经网络由许多神经元组成，每个神经元都有自己的一组权重)和网络的偏差，以最小化选定的函数，这通常被称为成本函数，通常由 *J* 表示。

在微积分中，有几种方法可以解析地找到给定函数的最小值。不幸的是，在所有的神经网络应用中，权重的数量如此之大，以至于不可能使用这些方法。必须依靠数值方法，最著名的是梯度下降法。这是最容易理解的方法，它将为你理解本书后面的更复杂的算法提供完美的基础。让我简要概述一下它是如何工作的，因为它是机器学习中最好的算法之一，可以向读者介绍学习率的概念及其古怪之处。

给定一个通用函数 *J* ( ***w*** )，其中 ***w*** 是权重的向量，权重空间中的最小位置(意味着 ***w*** 的值，其中*J*(***w***)具有最小值)可以用基于以下步骤的算法找到:

1.  迭代 0:选择一个随机的初始猜测***w***<sub>**0**</sub>

2.  迭代 *n* + 1(其中 *n* 从 0 开始):迭代 *n* + 1、***w***<sub>*n*+1</sub>的权重将从迭代 *n* 、***w***<sub>*n*</sub>的先前值更新，使用以下公式

![$$ {w}_{n+1}={w}_n-\gamma\ \nabla J\left({w}_n\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equr.png)

利用∇ *J* ( ***w*** ，我们已经指出了代价函数的梯度，它是一个向量，其分量是代价函数相对于权重向量 ***w*** 的所有分量的偏导数，如下:

![$$ \nabla J(w)=\left(\begin{array}{c}\raisebox{1ex}{$\partial J(w)$}\!\left/ \!\raisebox{-1ex}{$\partial {w}_1$}\right.\\ {}\vdots \\ {}\raisebox{1ex}{$\partial J(w)$}\!\left/ \!\raisebox{-1ex}{$\partial {w}_{n_x}$}\right.\end{array}\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equs.png)

为了决定何时停止，我们可以检查成本函数 *J* ( ***w*** )何时停止变化太多，或者，换句话说， 您可以定义一个阈值ϵ，并在满足|*j*(*w*<sub>*q*+1</sub>)*j*(*w*<sub>*q*</sub>)【T37】的任何迭代处停止 这种方法的问题在于它很复杂，并且当用 Python 实现时，这种检查在性能方面非常昂贵(记住:您将不得不多次执行这一步骤)，因此，通常，人们只是让算法运行固定的大量迭代，并检查最终结果。如果结果不是预期的，他们增加固定的大数字。多大？那要看你的问题了。你要做的就是选择一定的迭代次数(比如 10，000 或者 1，000，000)，让算法运行。同时，绘制成本函数与迭代次数的关系图，并检查您选择的迭代次数是否合理。在这一章的后面，你会看到一个实际的例子，在这个例子中，我会告诉你如何检查你选择的数字是否足够大。目前，您应该知道，在固定的迭代次数后，您可以简单地停止算法。

### 注意

为什么这个算法收敛到最小值(以及如何显示它)超出了本书的范围，会使这一章太长，并分散读者对主要学习目标的注意力，主要学习目标是让你理解选择特定学习速率的效果是什么，以及选择太大或太小的速率的后果是什么。

这里我们假设成本函数是可微的。这种情况并不常见，但是对这个问题的讨论远远超出了本书的范围。在这种情况下，人们倾向于使用实用的方法。这些实现工作得非常好，因此这类理论问题通常被大量实践者所忽略。请记住，在深度学习模型中，成本函数变成了一个难以置信的复杂函数，研究它几乎是不可能的。

序列***w***<sub>*n*</sub>在合理数量的迭代之后，将有希望向最小位置收敛。参数 *γ* 称为学习率，是神经网络学习过程中需要的最重要的参数之一。

### 注意

为了区别于权重，学习率被称为超参数。我们会遇到更多这样的情况。超参数是一个参数，其值不是由训练确定的，通常在学习过程开始之前设置。相反，参数 ***、w*** 和 *b* 的值是通过训练得到的。

选择单词*希望是*，是有充分理由的。该算法可能不会收敛到最小值。甚至有可能数列***w***<sub>*n*</sub>会在数值之间振荡，根本不会收敛——或者完全发散。选择 *γ* 太大或者太小，你的模型都不会收敛(或者收敛太慢)。为了理解为什么会出现这种情况，让我们考虑一个实际的案例，看看在选择不同的学习速率时该方法是如何工作的。

### 实例中的学习率

让我们考虑由代码生成的 *m* = 30 个观测值`y`形成的数据集。

```py
m = 30
w0 = 2
w1 = 0.5
x = np.linspace(-1,1,m)
y = w0 + w1 * x

```

作为代价函数，我们选择经典的均方误差(MSE)

![$$ J\left({w}_0,{w}_1\right)=\frac{1}{m}\sum \limits_{i=1}^m{\left({y}_i-f\left({w}_0,{w}_1,{x}^{(i)}\right)\right)}^2 $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equt.png)

其中我们已经用上标( *i* )表示了第 I 次观察。记住，通过下标*I*(*x*<sub>*I*</sub>)，我们已经指出了*I*<sup>*th*</sup>特征。为了重述我们的符号，我们用![$$ {x}_j^{(i)} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq8.png)表示了*j*<sup>*th*</sup>特征和*I*<sup>*th*</sup>观察。在这里的例子中，我们只有一个特性，所以我们不需要下标 *j* 。成本函数可以很容易地在 Python 中实现，如

```py
np.average((y-hypothesis(x, w0, w1))**2, axis=2)/2

```

我们已经定义了

```py
def hypothesis(x, w0, w1):
    return w0 + w1*x

```

我们的目标是找到使 *J* ( *w* <sub>0</sub> ， *w* <sub>1</sub> )最小的 *w* <sub>0</sub> 和 *w* <sub>1</sub> 的值。

要应用梯度下降法，我们必须计算出级数为 *w* <sub>0、*n*T5、 *w* <sub>1、 *n*</sub> 。我们有以下等式:</sub>

![$$ \Big\{{\displaystyle \begin{array}{c}{w}_{0,n+1}={w}_{0,n}-\gamma \frac{\partial J\left({w}_{0,n},{w}_{1,n}\right)}{\partial {w}_0}={w}_{0,n}+\gamma \frac{1}{m}\sum \limits_{i=1}^m2\left({y}_i-f\left({w}_{0,n},{w}_{1,n},{x}_i\right)\right)\frac{\partial f\left({w}_0,{w}_1,{x}_i\right)}{\partial {w}_0}\\ {}{w}_{1,n+1}={w}_{1,n}-\gamma \frac{\partial J\left({w}_{0,n},{w}_{1,n}\right)}{\partial {w}_1}={w}_{1,n}+\gamma \frac{1}{m}\sum \limits_{i=1}^m2\left({y}_i-f\left({w}_{0,n},{w}_{1,n},{x}_i\right)\right)\frac{\partial f\left({w}_0,{w}_1,{x}_i\right)}{\partial {w}_1}\end{array}} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equu.png)

通过计算偏导数来简化方程给出

![$$ \Big\{{\displaystyle \begin{array}{c}{w}_{0,n+1}={w}_{0,n}+\frac{\gamma }{m}\sum \limits_{i=1}^m\left({y}_i-f\left({w}_{0,n},\kern0.375em {w}_{1,n},\kern0.375em {x}_i\right)\right)={w}_{0,n}\left(1-\gamma \right)+\frac{\gamma }{m}\sum \limits_{i=1}^m\left({y}_i-{w}_{1,n}{x}_i\right)\\ {}{w}_{1,n+1}={w}_{1,n}+\frac{\gamma }{m}\sum \limits_{i=1}^m\left({y}_i-f\left({w}_{0,n},\kern0.375em {w}_{1,n},\kern0.375em {x}_i\right)\right){x}_i={w}_{1,n}-\gamma {w}_{0,n}+\frac{\gamma }{m}\sum \limits_{i=1}^m\left({y}_i-{w}_{1,n}{x}_i\right){x}_i\end{array}} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equv.png)

因为 *∂f* ( *w* <sub>0</sub> ， *w* <sub>1</sub> ，*x*<sub>*I*</sub>/*∂w*<sub>0</sub>= 1*∂f*(*w*<sub>0</sub>，*w*<sub>1 <sub>前面的方程是必须用 Python 实现的方程，如果我们想自己编写梯度下降算法的代码。</sub></sub>

### 注意

(2.11)式中方程的推导的目的是说明梯度下降的方程是如何很快变得非常复杂的，即使对于一个非常简单的情况也是如此。在下一节中，我们将使用`tensorflow`构建我们的第一个模型。该库最好的一个方面是，所有这些公式都是自动计算的，您不必费心计算任何东西。实现此处所示的等式并调试它们可能需要相当长的时间，而且在处理由相互连接的神经元组成的大型神经网络时，这被证明是不可能的。

我在本书中省略了这个例子的完整 Python 实现，因为它需要太多的空间。

通过改变学习率来检查模型如何工作是有益的。在图 [2-10](#Fig10) 、 [2-11](#Fig11) 和 [2-12](#Fig12) 中，绘制了成本函数的等高线 <sup>[2](#Fn2)</sup> ，并在此之上绘制了数列( *w* <sub>0、 *n*</sub> 、 *w* <sub>1、 *n*</sub> )在这些图中，最小值由大约位于中心的圆表示。我们会考虑数值 *γ* = 0.8(图 [2-10](#Fig10) )、 *γ* = 2(图 [2-11](#Fig11) )、 *γ* = 0.05(图 [2-12](#Fig12) )。不同的估算，***w***<sub>***n***</sub>，都用点表示。最小值由图像中间的圆圈表示。

在第一种情况下(在图 [2-10](#Fig10) 中)，收敛表现良好，在仅仅八个步骤中，该方法向最小值收敛。当 *γ* = 2(图 [2-11](#Fig11) )时，该方法会使步长过大(记住:步长由 *γ∇J* ( ***w*** )而无法接近最小值。它一直在它周围振荡，却没有到达它。在这种情况下，模型永远不会收敛。最后一种情况，当 *γ* = 0.05(图 [2-12](#Fig12) )时，学习非常慢，要多走很多步才能接近最小值。在某些情况下，成本函数可能在最小值附近非常平坦，以至于该方法需要大量迭代才能收敛，实际上，您将无法在合理的时间内足够接近真正的最小值。在图 [2-12](#Fig12) 中，绘制了 300 次迭代，但是该方法甚至没有非常接近最小值。

### 注意

在对神经网络的学习部分进行编码时，选择正确的学习速率至关重要。选择太大的比率，该方法可能只是在最小值附近反弹，而不会达到最小值。选择太小的速率，算法可能会变得很慢，以至于您无法在合理的时间(或迭代次数)内找到最小值。学习率过大的一个典型标志是成本函数可能变成`nan`(Python 俚语中的“不是数字”)。在训练过程中定期打印成本函数是检查这类问题的好方法。这会给你一个机会停止这个过程，避免浪费时间(万一你看到`nan`出现)。本章后面会有一个具体的例子。

在深度学习问题中，每次迭代都会耗费时间，你要多次执行这个过程。选择正确的学习率是设计一个好模型的关键部分，因为它会让训练快得多(或者让训练变得不可能)。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig12_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig12_HTML.jpg)

图 2-12

学习率太小时梯度下降算法的图示。这种方法非常慢，需要大量的迭代才能收敛到最小值。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig11_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig11_HTML.jpg)

图 2-11

学习率过大时梯度下降算法的图示。该方法不能收敛到最小值。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig10_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig10_HTML.jpg)

图 2-10

*图中的* *梯度下降算法* *收敛良好*

有时在这个过程中改变学习速度是有效的。你从一个较大的值开始，以更快地接近最小值，然后你逐渐减小它，以确保你尽可能地接近真正的最小值。我将在本书的后面讨论这种方法。

### 注意

如何选择合适的学习速度没有固定的规则。这取决于模型，取决于成本函数，取决于起点，等等。一个好的经验法则是从 *γ* = 0.05 开始，然后看看成本函数如何表现。绘制 *J* ( ***w*** )与迭代次数的关系图是很常见的，以检查它的减少和减少的速度。

检查收敛性的一个好方法是绘制成本函数与迭代次数的关系图。这样，你可以检查它的行为。图 [2-13](#Fig13) 显示了上例中我们的三个学习率的成本函数。你可以清楚地看到， *γ* = 0.8 的情况是如何很快变为零的，这表明我们已经达到了一个最小值。 *γ* = 2 的情况甚至没有开始下降。它继续保持几乎相同的初始值。并且，最后， *γ* = 0.05 的情况开始下降，但是比第一种情况慢了很多。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig13_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig13_HTML.jpg)

图 2-13

*成本函数与迭代次数* *(仅考虑前八个)*

因此，以下是我们从图 [2-13](#Fig13) 中得出的三种情况的结论:

*   *γ* = 0.05 → *J* 在减少，这很好，但是经过八次迭代，我们还没有达到平台期，所以我们必须使用更多的迭代，直到我们看到 *J* 不再有太大的变化。

*   *γ* = 2 → *J* 不减。我们应该检查一下我们的学习速度，看看是否有帮助。尝试较小的值将是一个很好的起点。

*   *γ* = 0.8 →成本函数下降相当快，然后保持不变。这是一个好迹象，表明我们已经达到了最低水平。

还要记住，学习率的绝对值是不相关的。重要的是行为。我们可以将成本函数乘以一个常数，这根本不会影响我们的学习。不要看绝对值；检查成本函数的运行速度和方式。此外，成本函数几乎永远不会达到零，所以不要期望它。 *J* 的最小值几乎不会为零(这取决于函数本身)。在关于线性回归的部分，您将看到一个成本函数不会达到零的示例。

### 注意

当训练你的模型时，记住总是检查成本函数与迭代次数(或在整个训练集上的滑动次数，称为时期)。这将给你一个有效的方法来评估训练是否有效，是否有效，并给你如何优化的提示。

现在我们已经定义了基础，我们将使用一个神经元来解决机器学习的两个简单问题:线性和逻辑回归。

### tensorflow 中的线性回归示例

第一种类型的回归将提供一个机会来理解如何在`tensorflow`中建立一个模型。为了解释如何用一个神经元有效地执行线性回归，我必须首先解释一些额外的符号。在前面的章节中，我讨论了输入![$$ x=\left({x}_1,{x}_2,\dots, {x}_{n_x}\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq9.png)。这些就是描述一个观察的所谓特征。通常情况下，我们有许多观察。如前所述，我们将使用一个较高的索引来表示括号之间的不同观察值。我们的 *i* <sup>*第*</sup> 次观测用***x***<sup>(*I*)</sup>表示， *i* <sup>*第*</sup> 次观测的 *j* <sup>*第*</sup> 特征用![$$ {x}_j^{(i)} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq10.png)表示。我们将用 *m* 表示观察次数。

### 注意

在本书中， *m* 是观测值的个数，*n*<sub>T5】x</sub>是特征的个数。我们的*j*<sup>*I**观察的第*</sup> *特征将用![$$ {x}_j^{(i)} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq11.png)表示。在深度学习项目中， *m* 越大越好。所以要准备好处理大量的观察。*

 *你会记得我说过很多次，`numpy`是高度优化的，可以同时执行几个并行操作。为了获得可能的最佳性能，以矩阵形式编写我们的方程并将矩阵馈送给`numpy`是很重要的。这样，我们的代码将尽可能高效。记住:尽可能避免循环。现在让我们花些时间把所有的方程写成矩阵形式。这样以后我们的 Python 实现就容易多了。

整个输入集(特征和观察值)可以写成矩阵形式。我们将使用以下符号:

![$$ X=\left(\begin{array}{ccc}{x}_1^{(1)}&amp; \dots &amp; {x}_1^{(m)}\\ {}\vdots &amp; \ddots &amp; \vdots \\ {}{x}_{n_x}^{(1)}&amp; \dots &amp; {x}_{n_x}^{(m)}\end{array}\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equw.png)

其中每一列是一个观察值，每一行代表矩阵 *X* 中的一个特征，矩阵的维数为*n*<sub>T5】X</sub>×*m*。我们也可以把输出值![$$ {\widehat{y}}^{(i)} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq12.png)写成矩阵形式。如果你还记得我们的神经元讨论，我们已经定义了一个*z*T13】(*I*)=***w***<sup>*T*</sup>***x***<sup>(*I【T31)*</sup>+*b*用于一次观察 *i* 。将每个观察值放在一列中，我们可以使用以下符号:

![$$ z=\left({z}^{(1)}\ {z}^{(2)}\dots {z}^{(m)}\right)={w}^TX+b $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equx.png)

这里我们有***b***=(*b b*…*b*)。我们将![$$ \widehat{y} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq13.png)定义为

![$$ \widehat{y}=\left({\widehat{y}}^{(1)}\ {\widehat{y}}^{(2)}\dots {\widehat{y}}^{(m)}\right)=\left(f\left({z}^{(1)}\right)\kern0.5em f\left({z}^{(2)}\right)\kern0.5em \dots \kern0.5em f\left({z}^{(m)}\right)\right)=f(z) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equy.png)

在使用 *f* ( ***z*** )的情况下，我们打算将函数 *f* 逐个元素地应用于矩阵 ***z*** 。

### 注意

尽管***【z】***的维数为 1 × *m* ，我们将使用术语*矩阵*而不是*向量*，以便在书中使用一致的名称。这也将帮助你记住我们应该总是使用矩阵运算。出于我们的目的， ***z*** 只是一个只有一行的矩阵。

你从第 [1 章](01.html)中知道，在`tensorflow`中，你必须明确声明我们的矩阵(或张量)的维数，所以很好地控制它们是个好主意。以下是我们将使用的所有向量和矩阵的维度概述:

*   *X* 有尺寸*n*<sub>T5】X</sub>×*m*

*   ***z*** 尺寸为 1 × *m*

*   ![$$ \widehat{y} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq14.png)的尺寸为 1 × *米*

*   ***w**T3】有尺寸 *n* <sub>*x*</sub> × 1*

*   ***b*** 尺寸为 1 × *m*

现在形式已经很清楚了，我们将准备数据集。

#### 线性回归模型的数据集

为了让事情更有趣一点，让我们使用一个真实的数据集。我们将使用所谓的波士顿数据集。 <sup>[3](#Fn3)</sup> 这包含了美国人口普查局收集的有关波斯顿周边住房的信息。数据库中的每条记录都描述了波士顿的一个郊区或城镇。数据取自 1970 年波士顿标准大都市统计区(SMSA)。这些属性定义如下[3]:

*   *CRIM* :城镇人均犯罪率

*   *ZN* :面积超过 25，000 平方英尺的住宅用地比例

*   *INDUS* :每镇非零售营业亩数比例

*   *CHAS* :查尔斯河虚拟变量(= 1，如果区域边界为河流；否则为 0)

*   *NOX* :氮氧化物浓度(百万分之一)

*   *RM* :每个住所的平均房间数

*   *楼龄*:1940 年前建成的自住单位比例

*   *DIS* :到五个波士顿就业中心的加权距离

*   *RAD* :放射状公路可达性指标

*   *税*:每万美元的全价值财产税

*   *PTRATIO* :按城镇划分的学生-教师比率

*   B - 1000(Bk - 0.63)^2 - Bk:按城镇分列的黑人比例

*   LSTAT : %较低的人口地位

*   *MEDV* :以千美元计的自有住房中值

我们的目标变量 MEDV，也就是我们想要预测的变量，是每个郊区以 1000 美元为单位的房屋中值价格。对于我们的例子，我们不必理解或研究特性。我在这里的目标是向你展示如何用你所学的知识建立一个线性回归模型。通常，在机器学习项目中，您会首先研究您的输入数据，检查它们的分布、质量、缺失值等等；不过，我会跳过这一部分，专注于如何实现你用`tensorflow`学到的东西。

### 注意

在机器学习中，我们要预测的变量通常被称为*目标变量*。

先导入常用的库，包括`sklearn.datasets`。在`sklearn.datasets`包的帮助下，导入数据和获取特性和目标非常容易。不必下载 CSV 文件并导入。只需运行以下代码:

```py
import matplotlib.pyplot as plt
%matplotlib inline
import tensorflow as tf
import numpy as np
from sklearn.datasets import load_boston

boston = load_boston()
features = np.array(boston.data)
labels = np.array(boston.target)

```

`sklearn.datasets`包中的每个数据集都有一个描述。您可以使用以下命令检查它:

```py
print(boston["DESCR"])

```

现在让我们检查一下我们有多少观察值和特征。

```py
n_training_samples = features.shape[0]
n_dim = features.shape[1]
print('The dataset has',n_training_samples,'training samples.')
print('The dataset has',n_dim,'features.')

```

将数学符号与 Python 代码`n_training_samples`联系起来的是 *m* 和`n_dim`是 *n* <sub>*x*</sub> 。该代码将给出以下结果:

```py
The dataset has 506 training samples.
The dataset has 13 features.

```

根据以下公式标准化定义标准化特征![$$ {x}_{\mathit{\operatorname{norm}},j}^{(i)} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq15.png)的每个数字特征是一个好主意

![$$ {x}_{\mathit{\operatorname{norm}},j}^{(i)}=\frac{x_j^{(i)}-\kern0.5em \left\langle {x}_j^{(i)}\right\rangle }{\sigma_j^{(i)}} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equz.png)

其中![$$ \left\langle {x}_j^{(i)}\right\rangle $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq16.png)是*j*<sup>T5】th</sup>特征的平均值，![$$ {\sigma}_j^{(i)} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq17.png)是其标准差。这可以在`numpy`中用以下函数很容易地计算出来:

```py
def normalize(dataset):
    mu = np.mean(dataset, axis = 0)
    sigma = np.std(dataset, axis = 0)
    return (dataset-mu)/sigma

```

为了规范化我们的特性 numpy `array`，我们必须简单地调用函数`features_norm = normalize(features)`。现在，包含在`numpy array features_norm`中的每个特征的平均值为 0，标准差为 1。

### 注意

通常，对要素进行归一化是一个好主意，这样它们的平均值为零，标准差为一。有时，某些特征比其他特征大得多，可能对模型产生更大的影响，从而导致错误的预测。当数据集被分成训练数据集和测试数据集时，需要特别小心，以实现一致的标准化。

对于这一章，我们将简单地使用所有的数据进行训练，把重点放在实现细节上。

```py
train_x = np.transpose(features_norm)
train_y = np.transpose(labels)

print(train_x.shape)
print(train_y.shape)

```

最后两张照片会告诉我们新矩阵的尺寸。

```py
(13, 506)
(506,)

```

数组的维数是(13，506 ),这正是我们所期望的。记住我们讨论的 *X* 有尺寸*n*T5】T6】XT8】×*m*。

训练目标`train_y`的维数为`(506,)`，这是`numpy`描述一维数组的方式。`tensorflow`想要拥有`(1, 506)`的维度(还记得我们之前的讨论吗？)，所以我们必须以这种方式重塑数组:

```py
train_y = train_y.reshape(1,len(train_y))

print(train_y.shape)

```

我们的打印报表给了我们所需要的:

```py
(1, 506)

```

#### 线性回归的神经元和代价函数

可以执行线性回归的神经元使用身份激活函数。需要最小化的成本函数是 MSE(均方误差),可以写成

![$$ J\left(w,b\right)=\frac{1}{m}\sum \limits_{i=1}^m{\left({y}^{(i)}-{w}^T{x}^{(i)}-b\right)}^2 $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equaa.png)

其中总和超过所有的 *m 个*观察值。

构建这个神经元和定义代价函数的`tensorflow`代码其实很简单。

```py
tf.reset_default_graph()

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [1, None])
learning_rate = tf.placeholder(tf.float32, shape=())
W = tf.Variable(tf.ones([n_dim,1]))
b = tf.Variable(tf.zeros(1))

init = tf.global_variables_initializer()
y_ = tf.matmul(tf.transpose(W),X)+b
cost = tf.reduce_mean(tf.square(y_-Y))
training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

```

注意在`tensorflow`中，不需要显式声明观察次数。可以在代码中使用`None`。通过这种方式，您将能够在任何数据集上运行模型，而不依赖于观察值的数量，而无需修改您的代码。

在代码中，我们已经将神经元输出![$$ \widehat{y} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq18.png)表示为`y_`，因为我们在 Python 中没有帽子。让我澄清一下哪一行代码做了什么。

*   `X = tf.placeholder(tf.float32, [n_dim, None])` →包含矩阵 *X* ，矩阵必须有维度*n*<sub>*X*</sub>×*m*。请记住，在我们的代码中，`n_dim`是 *n* <sub>*x*</sub> 并且 *m* 没有在`tensorflow`中显式声明。在它的位置，我们使用`None`。

*   `Y = tf.placeholder(tf.float32, [1, None])` →包含输出值![$$ \widehat{y} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq19.png)，其尺寸必须为 1 × *米*。在这里，这意味着我们使用`None`而不是 *m* ，因为我们希望对不同的数据集使用相同的模型(这将具有不同数量的观察值)。

*   `learning_rate = tf.placeholder(tf.float32, shape=())` →包含学习率作为参数，而不是常数，这样我们可以运行相同的模型改变它，而不用每次创建一个新的神经元。

*   `W = tf.Variable(tf.zeros([n_dim, 1]))` →定义并初始化权重， ***w*** ，用 0。记住权重， ***w*** ，必须有维度*n*<sub>*x*</sub>×1。

*   `b = tf.Variable(tf.zeros(1))` →用零定义并初始化偏置 *b* 。

记住，在`tensorflow`中，占位符是在学习阶段不会改变的张量，而变量是会改变的。权重、 ***w*** 和偏差、 *b* 将在学习期间更新。现在我们必须定义如何处理所有这些量。记住:一定要计算 ***z*** 。选择的激活函数是恒等函数，所以 ***z*** 也会是我们神经元的输出。

*   `init = tf.global_variables_initializer()` →创建一个初始化变量的图形，并将其添加到图形中。

*   `y_ = tf.matmul(tf.transpose(W),X)+b` →计算神经元的输出。一个神经元的输出是![$$ \widehat{y}=f(z)=f\left({w}^TX+b\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq20.png)。因为线性回归的激活函数是恒等式，所以输出是![$$ \widehat{y}={w}^TX+b $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq21.png)。记住 *b* 作为标量不是问题。Python 广播会处理它，将其扩展到正确的维度，使向量***w***<sup>*T*</sup>*X*和标量 *b* 之间的和成为可能。

*   `cost = tf.reduce_mean(tf.square(y_-Y))` →定义成本函数。`tensorflow`提供了一种简单有效的计算平均值的方法——`tf.reduce_mean()`——简单地计算张量所有元素的和，然后除以元素的个数。

*   `training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)` →告诉`tensorflow`使用哪种算法来最小化成本函数。在`tensorflow`语言中，用于最小化成本函数的算法被称为优化器。我们现在使用给定学习率的梯度下降。在本书的后面，其他优化器将被广泛研究。

你会记得在第 [1](01.html) 章的介绍中，前面的代码不会运行任何模型。它简单地定义了计算图。让我们定义一个函数来执行实际的学习并运行我们的模型。在函数中定义它更容易，这样我们可以重新运行它，例如，改变学习率或我们想要使用的迭代次数。

```py
def run_linear_model(learning_r, training_epochs, train_obs, train_labels, debug = False):
    sess = tf.Session()
    sess.run(init)

    cost_history = np.empty(shape=[0], dtype = float)

    for epoch in range(training_epochs+1):
        sess.run(training_step, feed_dict = {X: train_obs, Y: train_labels, learning_rate: learning_r})
        cost_ = sess.run(cost, feed_dict={ X:train_obs, Y: train_labels, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if (epoch % 1000 == 0) & debug:
            print("Reached epoch",epoch,"cost J =", str.format('{0:.6f}', cost_))

    return sess, cost_history

```

让我们一行一行地再检查一遍代码。

*   `sess = tf.Session()` →创建一个`tensorflow`会话。

*   `sess.run(init)` →运行图形不同元素的初始化。

*   `cost_history = np.empty(shape=[0], dtype = float)` →创建一个空向量(对于具有零个元素的时刻),其中存储了每次迭代的成本函数值。

*   `for loop...` →在此循环中，`tensorflow`执行我们之前讨论过的梯度下降步骤，并更新权重和偏差。另外，它会在数组`cost_history`中保存每次代价函数值:`cost_history = np.append(cost_history, cost_)`。

*   `if (epoch % 1000 == 0)...` →每隔 1000 个时期，我们将打印成本函数的值。这是检查成本函数是否真的在下降或者是否出现了`nan`的简单方法。如果您在一个交互式环境(比如 Jupyter 笔记本)中执行一些初始测试，如果您发现成本函数的行为不符合您的预期，您可以停止这个过程。

*   `return sess, cost_history` →返回会话(万一你想计算别的东西)和包含成本函数值的数组(我们将用这个数组来绘图)。

运行模型就像使用调用一样简单。

```py
sess, cost_history = run_linear_model(learning_r = 0.01,
                                training_epochs = 10000,
                                train_obs = train_x,
                                train_labels = train_y,
                                debug = True)

```

该命令的输出将是每 1000 个时期的成本函数(查看函数定义`if`，从`if (epoch % 1000 == 0))`开始)。

```py
Reached epoch 0 cost J = 613.947144
Reached epoch 1000 cost J = 22.131165
Reached epoch 2000 cost J = 22.081099
Reached epoch 3000 cost J = 22.076544
Reached epoch 4000 cost J = 22.076109
Reached epoch 5000 cost J = 22.07606
Reached epoch 6000 cost J = 22.076057
Reached epoch 7000 cost J = 22.076059
Reached epoch 8000 cost J = 22.076059
Reached epoch 9000 cost J = 22.076054
Reached epoch 10000 cost J = 22.076054

```

成本函数明显减小，然后达到一个值，并几乎保持不变。你可以在图 [2-14](#Fig14) 中看到它的曲线图。这是一个好迹象，表明成本函数已经达到最小值。这并不意味着我们的模型是好的，或者它会给出好的预测。这只是告诉我们，学习是有效的。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig14_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig14_HTML.jpg)

图 2-14

*产生我们的模型的成本函数应用于* *波士顿数据集* *，学习率为γ* = *0.01。我们只绘制前 500 个时期，因为成本函数几乎已经达到其最终值。*

如果能够形象地显示我们的契合度有多好，那就太好了。因为我们有 13 个功能，所以不可能绘制价格与其他功能的对比图。但是，了解模型对观测值的预测效果是很有帮助的。正如我在图 [2-15](#Fig15) 中所做的那样，这可以通过绘制我们预测的目标变量与观察到的目标变量来实现。如果我们可以完美地预测我们的目标变量，那么所有的点都应该在图中的一条对角线上。线周围的点分布越广，我们的模型预测就越差。让我们检查一下我们的模型做得怎么样。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig15_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig15_HTML.jpg)

图 2-15

*我们模型的预测目标值与测量目标值* *，应用于我们的三角测量数据*

这些点相当好地分布在这条线上，所以看起来我们可以在一定程度上预测我们的价格。估计我们回归的准确性的一个更定性的方法是 MSE 本身(在我们的例子中，它只是我们的成本函数)。我们获得的价值(1000 美元中的 22.08)是否足够好取决于您试图解决的问题，或者您已经被给予的约束和要求。

#### 满足和优化指标

我们已经看到，决定一个模型好不好并不容易。图 [2-15](#Fig15) 不允许我们定量描述我们的模型有多好(或不好)。为此，我们必须定义一个度量标准。

最简单的方法是建立所谓的*单一数字评估指标*。这意味着您计算一个单一的数字，并基于该数字进行模型评估。这很简单，也很实用。例如，您可以在分类的情况下使用精确度或 F1 分数，或者在回归的情况下使用 MSE。通常，在现实生活中，您会收到模型的目标和约束。例如，你的公司可能想用均方差< 20(以 1000 美元计)来预测房价，而你的模型应该能够在 iPad 上运行，或者在不到 1 秒的时间内运行。因此，区分两种类型的指标是有用的:

*   **满足性度量** →搜索可用的备选方案，直到满足可接受性阈值，例如，代码运行(RT)时间，其最小化服从 RT < 1 秒的成本函数，或者在模式中选择具有 RT < 1 秒的模式

*   **优化指标** →搜索可用备选方案以最大化特定指标，例如，选择最大化准确性的模型(或超参数)

### 注意

如果你有几个度量标准，你应该总是选择一个最优化的，其余的令人满意。

我们已经编写了代码，能够用不同的参数运行我们的模型。现在这样做很有教育意义。下面是三种不同学习速率下成本函数的表现:0.1、0.01 和 0.001。您可以查看图 [2-16](#Fig16) 中的不同行为。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig16_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig16_HTML.jpg)

图 2-16

线性回归的成本函数应用于波士顿数据集的三个学习率:0.1(实线)、0.01(虚线)和 0.001(虚线)。学习率越小，学习过程越慢。

正如对于非常小的学习率(0.001)所预期的，梯度下降算法在寻找最小值时非常慢，而对于更大的值(0.1)，该方法工作很快。这种图表对于让你了解学习过程有多快和多好非常有用。在本书的后面，您将会看到成本函数表现不佳的情况。例如，当应用退出正则化时，成本函数将不再平滑。

## 逻辑回归的例子

逻辑回归是一种经典的分类算法。为了简单起见，我们在这里将考虑一个二元分类。这意味着我们将处理识别两个类的问题，我们将只把它们标记为 0 或 1。我们将需要一个不同于线性回归所用的激活函数，一个不同的最小化成本函数，以及一个对神经元输出的轻微修改。我们的目标是能够建立一个模型，预测某个新的观察结果是否属于两类中的一类。神经元应该给出输入 *x* 属于第 1 类的概率 *P* ( *y* = 1| *x* )作为输出。然后，如果*P*(*y*= 1 |*x*)>0.5，我们将把我们的观察分类为第 1 类，或者如果*P*(*y*= 1 |*x*)<0.5，我们将把我们的观察分类为第 0 类。

### 价值函数

作为成本函数，我们将使用交叉熵。 <sup>[4](#Fn4)</sup> 一次观测的功能是

![$$ L\left({\widehat{y}}^{(i)},{y}^{(i)}\right)=-\left({y}^{(i)}\ \log {\widehat{y}}^{(i)}+\left(1-{y}^{(i)}\right)\log \left(1-{\widehat{y}}^{(i)}\right)\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equab.png)

在存在多于一个观察值的情况下，成本函数是所有观察值的总和

![$$ J\left(w,b\right)=\frac{1}{m}\sum \limits_{i=1}^m\kern0.375em L\left({\widehat{y}}^{(i)},{y}^{(i)}\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equac.png)

在第 [10 章](10.html)中，我将从头开始提供逻辑回归的完整推导，但目前，`tensorflow`将处理所有细节——导数、梯度下降实现等等。我们只需要建立正确的神经元，就可以上路了。

### 激活功能

记住:我们希望我们的神经元输出我们观察到的概率是 0 或 1。因此，我们需要一个只能取 0 到 1 之间值的激活函数。否则，我们不能将其视为一种概率。对于我们的逻辑回归，我们将使用 sigmoid 函数作为激活函数。

![$$ \sigma (z)=\frac{1}{1+{e}^{-z}} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equad.png)

### 数据集

为了建立一个有趣的模型，我们将使用 MNIST 数据集的修改版本。你会从以下链接找到所有相关信息: [`http://yann.lecun.com/exdb/mnist/`](http://yann.lecun.com/exdb/mnist/) 。

MNIST 数据库是一个手写数字的大型数据库，我们可以用它来训练我们的模型。MNIST 数据库包含 70，000 幅图像。“来自 NIST 的原始黑白(二级)图像被尺寸归一化以适合 20×20 像素的盒子，同时保持它们的纵横比。作为归一化算法所使用的抗锯齿技术的结果，所得到的图像包含灰度级。通过计算像素的质心，并平移图像以将该点定位在 28×28 视场的中心，图像在 28×28 图像中居中”(来源: [`http://yann.lecun.com/exdb/mnist/`](http://yann.lecun.com/exdb/mnist/) )。

我们的特征将是每个像素的灰度值，因此我们将有 28 × 28 = 784 个特征，其值将从 0 到 255(灰度值)。数据集包含从 0 到 9 的所有十位数字。使用下面的代码，您可以准备在下面的部分中使用的数据。像往常一样，我们先导入必要的库。

```py
from sklearn.datasets import fetch_mldata

```

那我们来加载数据吧。

```py
mnist = fetch_mldata('MNIST original')
X,y = mnist["data"], mnist["target"]

```

现在`X`包含输入图像，`y`包含目标标签(记住，我们想要预测的值在机器学习术语中称为目标)。只要输入`X.shape`就会得到`X`的形状:(70000，784)。注意`X`有 70，000 行(每行是一幅图像)和 784 列(在我们的例子中，每列是一个特征或一个像素灰度值)。让我们检查一下数据集中有多少位数字。

```py
for i in range(10):
    print ("digit", i, "appears", np.count_nonzero(y == i), "times")

```

这给了我们以下信息:

```py
digit 0 appears 6903 times
digit 1 appears 7877 times
digit 2 appears 6990 times
digit 3 appears 7141 times
digit 4 appears 6824 times
digit 5 appears 6313 times
digit 6 appears 6876 times
digit 7 appears 7293 times
digit 8 appears 6825 times
digit 9 appears 6958 times

```

定义一个函数来可视化这些数字是很有用的，可以了解它们的样子。

```py
def plot_digit(some_digit):

    some_digit_image = some_digit.reshape(28,28)

    plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = "nearest")
    plt.axis("off")
    plt.show()

```

例如，我们可以随机绘制一个(见图 [2-17](#Fig17) )。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig17_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig17_HTML.jpg)

图 2-17

数据集中的第 36，003 位数字。很容易辨认出它是 5 号

```py
plot_digit(X[36003])

```

这里我们要实现的模型是一个简单的二元分类逻辑回归，因此数据集必须减少到两个类，或者在本例中，减少到两位数。我们选择一和二。让我们从数据集中只提取代表 1 或 2 的图像。我们的神经元将试图识别给定图像是属于 0 类(数字 1)还是 1 类(数字 2)。

```py
X_train = X[np.any([y == 1,y == 2], axis = 0)]
y_train = y[np.any([y == 1,y == 2], axis = 0)]

```

接下来，必须对输入观测值进行归一化。(记住:在使用 sigmoid 激活函数时，你不希望你的输入数据太大，因为你有 784 个。)

```py
X_train_normalised = X_train/255.0

```

我们选择 255，因为每个特征是图像中一个像素的灰度值，源图像中的灰度从 0 到 255。在本书的后面，我将详细讨论为什么我们需要规范化输入特征。现在，相信我，这是必要的一步。在每一列中，我们希望有一个输入观测值，每一行应该代表一个特征(一个像素灰度值)，因此我们必须对张量进行整形

```py
X_train_tr = X_train_normalised.transpose()
y_train_tr = y_train.reshape(1,y_train.shape[0])

```

我们可以定义一个变量`n_dim`来包含特性的数量

```py
n_dim = X_train_tr.shape[0]

```

现在到了非常重要的一点。导入的数据集中的标签将是 1 或 2(它们只是告诉您图像代表哪个数字)。然而，我们将构建我们的成本函数，假设我们的类的标签是 0 和 1，所以我们必须重新调整我们的`y_train_tr`数组。

### 注意

进行二元分类时，记得检查用于训练的标签的值。有时，使用错误的标签(不是 0 和 1)可能会花费你相当长的时间来理解为什么模型不起作用。

```py
y_train_shifted = y_train_tr - 1

```

现在，所有代表 1 的图像都将具有标签 0，所有代表 2 的图像都将具有标签 1。最后，让我们为 Python 变量使用一些合适的名称。

```py
Xtrain = X_train_tr
ytrain = y_train_shifted

```

图 [2-18](#Fig18) 显示了我们正在处理的一些数字。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig18_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig18_HTML.jpg)

图 2-18

从数据集中随机选择六个数字。括号中给出了相对重新标度的标签(记住:我们数据集中的标签现在是 0 或 1)。

### tensorflow 实现

`tensorflow`的实现并不困难，几乎与线性回归相同。首先，让我们定义占位符和变量。

```py
tf.reset_default_graph()

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [1, None])
learning_rate = tf.placeholder(tf.float32, shape=())

W = tf.Variable(tf.zeros([1, n_dim]))
b = tf.Variable(tf.zeros(1))

init = tf.global_variables_initializer()

```

请注意，代码与我们用于线性回归模型的代码相同。然而，我们必须定义不同的成本函数(如前所述)和不同的神经元输出(sigmoid 函数)。

```py
y_ = tf.sigmoid(tf.matmul(W,X)+b)
cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))
training_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

```

我们使用了 sigmoid 函数作为神经元的输出，用`tf.sigmoid()`。运行该模型的代码与我们用于线性回归的代码相同。我们只改变了函数的名称。

```py
def run_logistic_model(learning_r, training_epochs, train_obs, train_labels, debug = False):
    sess = tf.Session()
    sess.run(init)

    cost_history = np.empty(shape=[0], dtype = float)

    for epoch in range(training_epochs+1):
        sess.run(training_step, feed_dict = {X: train_obs, Y: train_labels, learning_rate: learning_r})
        cost_ = sess.run(cost, feed_dict={ X:train_obs, Y: train_labels, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if (epoch % 500 == 0) & debug:
            print("Reached epoch",epoch,"cost J =", str.format('{0:.6f}', cost_))

    return sess, cost_history

```

让我们运行模型，看看结果。我们会选择从 0.01 的学习率开始。

```py
sess, cost_history = run_logistic_model(learning_r = 0.01,
                                training_epochs = 5000,
                                train_obs = Xtrain,
                                train_labels = ytrain,
                                debug = True)

```

我们代码的输出(3000 个周期后停止)如下:

```py
Reached epoch 0 cost J = 0.678598
Reached epoch 500 cost J = 0.108655
Reached epoch 1000 cost J = 0.078912
Reached epoch 1500 cost J = 0.066786
Reached epoch 2000 cost J = 0.059914
Reached epoch 2500 cost J = 0.055372
Reached epoch 3000 cost J = nan

```

发生了什么事？突然，在某个时刻，我们的成本函数采用了值`nan`(不是一个数字)。似乎模型在某个点之后做的并不好。如果学习率太大，或者您错误地初始化了您的权重，您的![$$ {\widehat{y}}^{(i)}=P\left({y}^{(i)}=1|{x}^{(i)}\right) $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_IEq22.png)值可能非常接近 0 或 sigmoid 函数假定 *z* 的非常大的负值或正值非常接近 0 或 1)。请记住，在成本函数中，您有两个术语`tf.log(y_)`和`tf.log(1-y_)`，并且因为对数函数没有定义为零的值，如果`y_`为 0 或 1，您将获得一个`nan`，因为代码将尝试评估`tf.log(0)`。例如，我们可以用 2.0 的学习率运行模型。仅在一个时期之后，您已经将获得成本函数的`nan`值。如果在第一个训练步骤之前和之后打印出 *b* 的值，就很容易理解为什么了。只需修改您的模型代码并使用以下版本:

```py
def run_logistic_model(learning_r, training_epochs, train_obs, train_labels, debug = False):
    sess = tf.Session()
    sess.run(init)

    cost_history = np.empty(shape=[0], dtype = float)

    for epoch in range(training_epochs+1):

        print ('epoch: ', epoch)
        print(sess.run(b, feed_dict={X:train_obs, Y: train_labels, learning_rate: learning_r}))

        sess.run(training_step, feed_dict = {X: train_obs, Y: train_labels, learning_rate: learning_r})

        print(sess.run(b, feed_dict={X:train_obs, Y: train_labels, learning_rate: learning_r}))

        cost_ = sess.run(cost, feed_dict={ X:train_obs, Y: train_labels, learning_rate: learning_r})
        cost_history = np.append(cost_history, cost_)

        if (epoch % 500 == 0) & debug:
            print("Reached epoch",epoch,"cost J =", str.format('{0:.6f}', cost_))
    return sess, cost_history

```

您将获得以下结果(仅在一个时期后停止训练):

```py
epoch:  0
[ 0.]
[-0.05966223]
Reached epoch 0 cost J = nan
epoch:  1
[-0.05966223]
[ nan]

```

你看 *b* 怎么从 0 到-0.05966223 再到`nan`？因此，***z***=***w***<sup>*T*</sup>***X***+***b***变成了`nan`，那么***y***=*(***z***)也变成了这仅仅是因为学习速度太快了。*

 *解决方法是什么？你应该尝试不同的(阅读:小得多)学习速度。

我们试试看能不能得到一个 2500 个纪元后更稳定的结果。我们通过调用运行模型，如下所示:

```py
sess, cost_history = run_logistic_model(learning_r = 0.005,
                                training_epochs = 5000,
                                train_obs = Xtrain,
                                train_labels = ytrain,
                                debug = True)

```

该命令的输出是

```py
Reached epoch 0 cost J = 0.685799
Reached epoch 500 cost J = 0.154386
Reached epoch 1000 cost J = 0.108590
Reached epoch 1500 cost J = 0.089566
Reached epoch 2000 cost J = 0.078767
Reached epoch 2500 cost J = 0.071669
Reached epoch 3000 cost J = 0.066580
Reached epoch 3500 cost J = 0.062715
Reached epoch 4000 cost J = 0.059656
Reached epoch 4500 cost J = 0.057158
Reached epoch 5000 cost J = 0.055069

```

我们的产量没有了。您可以在图 [2-19](#Fig19) 中看到成本函数的曲线图。为了评估我们的模型，我们必须选择一个优化指标(如前所述)。对于二进制分类问题，一个经典的度量是精确度(我们可以用*和*来表示)，它可以被理解为一个结果和它的“真实”值之间的差异的度量。数学上，它可以计算为

![$$ a=\frac{number\ of\ cases\ correctly\ identified}{total\ number\ of\ cases} $$](../images/463356_1_En_2_Chapter/463356_1_En_2_Chapter_TeX_Equae.png)

为了获得准确性，我们可以运行下面的代码。(记住:我们将一个观察值 *i* 分类为 0 类 if*P*(*y*T6】(*I*)= 1 |***x***<sup>(*I*)</sup>)<0.5，或者在 1 类 if*P*(*y*<sup>(*)*</sup>

```py
correct_prediction1 = tf.equal(tf.greater(y_, 0.5), tf.equal(Y,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction1, tf.float32))
print(sess.run(accuracy, feed_dict={X:Xtrain, Y: ytrain, learning_rate: 0.05}))

```

通过这个模型，我们达到了 98.6%的准确率。对于只有一个神经元的网络来说还不错。

![../images/463356_1_En_2_Chapter/463356_1_En_2_Fig19_HTML.jpg](../images/463356_1_En_2_Chapter/463356_1_En_2_Fig19_HTML.jpg)

图 2-19

学习率为 0.005 时成本函数与时期的关系

您也可以尝试运行之前的模型(学习率为 0.005)运行更多的时期。你会发现在大约 7000 个纪元时，`nan`会再次出现。这里的解决方案是随着历元数量的增加而降低学习率。一个简单的方法，比如每 500 个历元将学习率减半，将消除`nan`。我将在本书后面更详细地讨论类似的方法。

## 参考

1.  Jeremy Hsu，“有史以来最大的神经网络推动了 AI 深度学习”， [`https://spectrum.ieee.org/tech-talk/computing/software/biggest-neural-network-ever-pushes-ai-deep-learning`](https://spectrum.ieee.org/tech-talk/computing/software/biggest-neural-network-ever-pushes-ai-deep-learning) ，2015。

2.  劳尔·罗哈斯，*神经网络:系统介绍*，柏林:斯普林格出版社，1996 年。

3.  Delve(有效实验中评估学习的数据)，《波士顿住房数据集》， [`www.cs.toronto.edu/~delve/data/boston/bostonDetail.html`](http://www.cs.toronto.edu/%257Edelve/data/boston/bostonDetail.html) ，1996 年。

4.  Prajit Ramachandran，Barret Zoph，Quoc V. Le，“寻找激活函数”，arXiv:1710.05941 [cs。NE]，2017。

5.  Guido F. Montufar，Razvan Pascanu，Kyunghyun Cho，Yoshua Bengio，“关于深度神经网络的线性区域的数量”， [`https://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf`](https://papers.nips.cc/paper/5422-on-the-number-of-linear-regions-of-deep-neural-networks.pdf) ，2014 年。

6.  Brendan Fortuner，“神经网络能解决任何问题吗？”， [`https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6`](https://towardsdatascience.com/can-neural-networks-really-learn-any-function-65e106617fc6) ，2017。

<aside class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

你可以在官方文档中找到更多关于`numpy`如何使用广播的解释，可以在 [`https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html`](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html) 找到。

  [2](#Fn2_source)

函数的等高线是一条曲线，函数沿着这条曲线有一个常数值。

  [3](#Fn3_source)

Delve(有效实验中评估学习的数据)，《波士顿住房数据集》， [`www.cs.toronto.edu/~delve/data/boston/bostonDetail.html`](http://www.cs.toronto.edu/%257Edelve/data/boston/bostonDetail.html) ，1996 年。

  [4](#Fn4_source)

对交叉熵的意义的讨论超出了本书的范围。在 [`https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/`](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/) 以及很多关于机器学习的入门书籍中都可以找到很好的介绍。

 </aside>***