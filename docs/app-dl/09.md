# 9.一个研究项目

通常，当谈到深度学习时，人们会想到图像识别、语音识别、图像检测等等。这些是最知名的应用，但深度神经网络的可能性是无穷的。在本章中，我将向您展示深度神经网络如何成功应用于一个不太传统的问题:在传感应用中提取参数。对于这个特定的问题，我们将开发传感器的算法，我将在后面描述，以确定介质中的氧浓度，例如气体。

这一章是这样组织的:首先，我将讨论要解决的研究问题，然后我将解释解决这个问题所需的一些介绍性材料，最后，我将向你们展示这个正在进行的研究项目的第一批结果。

## 问题描述

许多传感器设备的工作原理是基于通常容易测量的物理量的测量，例如电压、体积或光强度。这个量必须与另一个物理量密切相关，该物理量是待确定的，并且通常难以直接测量，例如温度，或者在这个例子中是气体浓度。如果我们知道这两个量是如何相互关联的(通常是通过数学模型)，从第一个量，我们可以推导出第二个量，我们真正感兴趣的量。因此，以一种简化的方式，我们可以将传感器想象为一个黑匣子，当给定一个输入(温度、气体浓度等)时，它会产生一个输出(电压、音量或光强)。输出对输入的依赖性是传感类型的特征，并且可能极其复杂。这使得在实际硬件中实现必要的算法非常困难，甚至是不可能的。这里，我们将使用神经网络而不是一组公式来使用该方法从输入确定输出。

该研究项目利用“发光淬灭”原理测量氧气浓度:一种敏感元件，一种染料物质，与一种气体接触，我们要测量其中的氧气含量。用所谓的激发光(通常在光谱的蓝色部分)照射染料，在吸收一部分之后，它重新发射光谱的不同部分(通常在红色部分)的光。发射光的强度和持续时间强烈依赖于与染料接触的气体中的氧气浓度。如果气体中含有一些氧气，染料发出的部分光被抑制或“猝灭”(从这里，测量原理的名称)，气体中的氧气含量越高，这种效应越强。该项目的目标是开发新的算法，根据测量信号确定氧浓度(输入)，即激发光和发射光之间的所谓相移(输出)。如果你不明白这意味着什么，不要担心。它不是理解本章内容所必需的。直观地理解这种相移测量了激发染料的光和“猝灭”效应后发射的光之间的变化，并且这种“变化”受到气体中包含的氧的强烈影响，这就足够了。

传感器实现的困难在于系统的响应(非线性地)取决于几个参数。对于大多数染料分子来说，这种依赖性非常复杂，以至于几乎不可能写出氧浓度作为所有这些影响参数的函数的方程。因此，典型的方法是开发一个非常复杂的经验模型，并手动调整许多参数。

发光测量的典型设置如图 [9-1](#Fig1) 所示。这个设置用于获取验证数据集的数据。含有发光染料物质的样品被激发光(图中的蓝光)照射，激发光由发光二极管或激光器发射，用透镜聚焦。在另一个透镜的帮助下，发射的发光(图右侧的红光)被检测器收集。样品架包含染料和气体，在图中用样品表示，我们要测量其氧浓度。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig1_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig1_HTML.jpg)

图 9-1

发光测量系统的示意图

由检测器收集的发光强度在时间上不是恒定的，而是降低的。它下降的速度取决于存在的氧气量，通常由衰减时间量化，用 *τ* 表示。对这种衰减的最简单描述是通过单指数衰减函数，*e*<sup>*t*/*τ*</sup>，其特征在于衰减时间τ。实践中用于确定这种衰减时间的常用技术是调制激发光的强度，或者换句话说，以周期方式改变强度，频率为*f*= 2*ωω*，其中ω是所谓的角频率。重新发射的冷光具有也被调制的强度，或者换句话说，它周期性地变化，但是以相移θ为特征。该相移与衰减时间τ有关，如 tan*θ*=*ψτ*。为了让你对这种相移有一个直观的理解，请考虑以最简单的形式将光表示为一种振幅随三角函数变化的波(如果你是物理学家，请原谅)。

![$$ \sin \left(\omega t+\theta \right) $$](../images/463356_1_En_9_Chapter/463356_1_En_9_Chapter_TeX_Equa.png)

量 *θ* 称为波的相位常数。现在发生的情况是，激发染料的光有相位常数*θ*<sub>T5】exc</sub>，发出的光有不同的相位常数 *θ* <sub>*发出*</sub> 。测量原理测量的正是这种相位变化，*θ*≦*θ*<sub>*exc*</sub>—*θ*<sub>*发射*</sub> ，因为这种变化受气体中氧含量的强烈影响。请记住，这个解释非常直观，从物理学的角度来看，并不完全正确，但它应该让您对我们正在测量的东西有一个大致的了解。

综上所述，测得的信号就是这个相移θ，下文中简称为相位，而搜索到的量(我们要预测的量)就是与染料接触的气体中的氧气浓度。

不幸的是，在现实生活中，情况更加复杂。光相移不仅取决于调制频率 *ω* 和气体中的氧浓度 *O* 2，还非线性地取决于染料分子周围的温度和化学成分。此外，光强度的衰减很少能只用一个衰减时间来描述。最常见的是，至少需要两个衰减时间，这进一步增加了描述系统所需的参数数量。给定激光调制频率 *ω* ，以摄氏度为单位的温度 *T* ，以及氧气浓度 *O* 2(以空气中所含氧气的百分比表示)，系统返回相位 *θ* 。在图 [9-2](#Fig2) 中，可以看到*t*= 45<sup>∘</sup>*c*和 *O* 2 = 4%的典型测量 tan *θ* 的曲线图。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig2_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig2_HTML.jpg)

图 9-2

*T* = 45 <sup>∘</sup> *C* 和 *O* 2 = 4%的实测 *tanθ* 图

这个研究项目的想法是能够从数据中获得氧气浓度，而不需要为传感器的行为开发任何理论模型。为了做到这一点，我们将尝试使用深度神经网络，让它们从人工创建的数据中学习任何给定阶段气体中的氧气浓度，然后我们将把我们的模型应用到真实的实验数据中。

## 数学模型

让我们来看一个可以用来确定氧气浓度的数学模型。一方面，它给出了它有多复杂的想法，另一方面，它将在本章中用于生成训练数据集。不涉及测量技术中涉及的物理学(这超出了本书的范围)，描述相位 *θ* 如何与氧浓度 *O* 2 相关联的简单模型可以由以下公式描述:

![$$ \frac{\tan \theta \left(\omega, T,O2\right)}{\tan \theta \left(\omega, T,O2=0\right)}=\frac{f\left(\omega, T\right)}{1+ KS{V}_1\left(\omega, T\right)\kern0.5em \cdot \kern0.5em O2}+\frac{1-f\left(\omega, T\right)}{1+ KS{V}_2\left(\omega, T\right)\kern0.5em \cdot \kern0.5em O2} $$](../images/463356_1_En_9_Chapter/463356_1_En_9_Chapter_TeX_Equb.png)

量 *f* ( *ω* 、 *T* )、 *KSV* <sub>1</sub> ( *ω* 、 *T* )、*KSV*<sub>2</sub>(*ω*、 *T* )是解析形式未知的参数，并且特定于所使用的染料分子、其年龄我们的目标是在实验室中训练一个神经网络，然后将其部署在可以在现场使用的传感器上。这里的主要问题是确定 *f* 、 *KSV* <sub>1</sub> 、 *KSV* <sub>2</sub> 的频率和温度相关函数形式。由于这个原因，商用传感器通常依赖于多项式或指数近似，使用足够的参数和拟合程序来确定数量的足够好的近似。

在本章中，我们将使用刚刚描述的数学模型创建训练数据集，然后将它应用于实验数据，以查看我们可以预测氧浓度的程度。目标是进行可行性研究，以检验这种方法的效果如何。

在这种情况下准备训练数据集有点棘手和复杂，所以在开始之前，让我们看一个类似但容易得多的问题，以便您了解在更复杂的情况下我们想要做什么。

## 回归问题

让我们首先考虑下面的问题。给定一个带参数 *A* 的函数 *L* ( *x* )，我们想训练一个神经网络从该函数的一组值中提取参数值 *A* 。换句话说，给定输入变量 *x* <sub>*i*</sub> 的一组值，对于 *i* = 1，…, *N* ，我们将计算一个由 *N* 值*L*<sub>*I*</sub>=(*x*<sub>*I 我们将训练网络给我们一个输出*。作为一个具体的例子，让我们考虑下面的函数:</sub>

![$$ L(x)=\frac{A^2}{A^2+{x}^2} $$](../images/463356_1_En_9_Chapter/463356_1_En_9_Chapter_TeX_Equc.png)

这就是所谓的洛伦兹函数，最大值在 *x* = 0，最大值在 *L* (0) = 1。这里我们要解决的问题是确定 *A* ，给定这个函数的一定数量的数据点。在这种情况下，这是相当简单的，因为我们可以做到这一点，例如，用经典的非线性拟合，甚至通过求解一个简单的二次方程，但假设我们想教一个神经网络这样做。我们想要一个神经网络来学习如何对这个函数进行非线性拟合。根据你在本书中学到的知识，这不会太难。让我们从创建一个训练数据集开始。首先，让我们为 *L* ( *x* )定义一个函数

```
def L(x,A):
    y = A**2/(A**2+x**2)
    return y

```

现在让我们考虑 100 个点，并生成一个包含我们想要使用的所有 *x* 个点的数组。

```
number_of_x_points = 100
min_x = 0.0
max_x = 5.0
x = np.arange(min_x, max_x, (max_x-min_x)/number_of_x_points )

```

最后，让我们生成 1000 个观察值，我们将使用它们作为我们网络的输入。

```
number_of_samples = 1000
np.random.seed(20)
A_v = np.random.normal(1.0, 0.4, number_of_samples)

for i in range(len(A_v)):
    if A_v[i] <= 0:
        A_v[i] = np.random.random_sample([1])
data = np.zeros((number_of_samples, number_of_x_points))
targets = np.reshape(A_v, [1000,1])

for i in range(number_of_samples):
    data[i,:] = L(x, A_v[i])

```

数组`data`现在将包含所有的观察值，每一个都在一行上。注意，为了避免 *A* 为负值，我们在代码中构建了一个检查。

```
if A_v[i] <= 0:
        A_v[i] = np.random.random_sample([1])

```

这样，如果为 *A* 选择的随机值是负的，则分配一个新的随机值。你可能已经注意到，在关于 *L* ( *x* )的等式中，数量 *A* 总是平方，所以乍一看，负值不是问题。但是记住这个负值会是我们要预测的目标变量。当第一次开发这个模型时，我有几个负值。网络不能区分正值和负值，因此得到错误的结果。

如果你检查数组`A_v`的形状，你会得到

```
(1000, 100)

```

这转化为 1000 个观察值，每个观察值有 100 个值，它们是不同的 *L* 的值，是根据我们生成的 *x* 的值计算的。当然，我们还需要一个开发数据集。

```
number_of_dev_samples = 1000

np.random.seed(42)
A_v_dev = np.random.normal(1.0, 0.4, number_of_samples)

for i in range(len(A_v_dev)):
    if A_v_dev[i] <= 0:
        A_v_dev[i] = np.random.random_sample([1])

data_dev = np.zeros((number_of_samples, number_of_x_points))
targets_dev = np.reshape(A_v_dev, [1000,1])

for i in range(number_of_samples):
    data_dev[i,:] = L(x, A_v_dev[i])

```

在图 [9-3](#Fig3) 中，你可以看到我们将用作输入的函数的四个随机例子。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig3_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig3_HTML.jpg)

图 9-3

函数 *L* ( *x* )的四个随机例子。在图例中，您可以看到用于绘图的*和*的值。

现在让我们建立一个简单的一层十个神经元的网络，尝试提取这个值。

```
tf.reset_default_graph()

n1 = 10
nx = number_of_x_points
n2 = 1

W1 = tf.Variable(tf.random_normal([n1,nx]))/500.0
b1 = tf.Variable(tf.ones((n1,1)))/500.0
W2 = tf.Variable(tf.random_normal([n2,n1]))/500.0
b2 = tf.Variable(tf.ones((n2,1)))/500.0

X = tf.placeholder(tf.float32, [nx, None]) # Inputs
Y = tf.placeholder(tf.float32, [1, None]) # Labels

Z1 = tf.matmul(W1,X)+b1
A1 = tf.nn.sigmoid(Z1)
Z2 = tf.matmul(W2,A1)+b2
y_ = Z2
cost = tf.reduce_mean(tf.square(y_-Y))
learning_rate = 0.1
training_step = tf.train.AdamOptimizer(learning_rate).minimize(cost)

init = tf.global_variables_initializer()

```

注意，我们已经随机初始化了权重，我们将不使用小批量梯度下降。让我们训练网络 20000 个纪元。

```
sess = tf.Session()
sess.run(init)

training_epochs = 20000
cost_history = np.empty(shape=[1], dtype = float)

train_x = np.transpose(data)
train_y = np.transpose(targets)

cost_history = []
for epoch in range(training_epochs+1):
    sess.run(training_step, feed_dict = {X: train_x, Y: train_y})
    cost_ = sess.run(cost, feed_dict={ X:train_x, Y: train_y})
    cost_history = np.append(cost_history, cost_)

    if (epoch % 1000 == 0):
        print("Reached epoch",epoch,"cost J =", cost_)

```

模型收敛非常快。MSE(成本函数)从开始的 1.1 到 ca。10，000 个周期后的 2.5 10<sup>—4</sup>。经过 20，000 个周期后，MSE 达到 10<sup>6</sup>。我们可以绘制预测值与实际值的对比图，直观地检查系统的运行情况。在图 [9-4](#Fig4) 中，你可以看到这个系统运行得有多好。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig4_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig4_HTML.jpg)

图 9-4

*A* 的预测值与实际值

顺便说一下，dev 集上的 MSE 是 3 ^ 10<sup>-5</sup>，所以我们可能稍微过度拟合了训练数据集。原因之一是我们正在考虑一个相对狭窄的 *x* 范围:只有从 0 到 5。因此，当您处理非常大的 *A* 值(例如，2.5 的量级)时，系统往往不会做得很好。如果您检查与图 [9-4](#Fig4) 相同的图，但针对开发数据集(图 [9-5](#Fig5) )，您可以看到模型如何在 *A* 的大值时出现问题。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig5_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig5_HTML.jpg)

图 9-5

开发数据集 *A* 的预测值与实际值

在较高的 *A* 值下，这些糟糕的预测还有另一个原因。当我们生成训练数据时，我们对 *A* 的值使用了下面一行代码:

```
A_v = np.random.normal(1.0, 0.4, number_of_samples)

```

这意味着为 *A* 选择的值按照正态分布分布，平均值为 1.0，标准偏差为 0.4。很少有观察到 *A* 大于 2.0。您可以重做我们已经完成的整个练习，但是这一次，从带有代码的均匀分布中选择 *A* 的值

```
A_v = np.random.random_sample([number_of_dev_samples])*3.0

```

这一行代码会给你 0 到 3.0 之间的随机数。经过 20000 个纪元，现在我们将得到*MSE*<sub>*train*</sub>= 3.8 10<sup>—6</sup>和*MSE*<sub>*dev*</sub>= 1.7 10<sup>—6</sup>。这一次，我们在 dev 数据集上的预测要好得多，似乎没有过度拟合。

### 注意

当您人工生成训练数据时，您应该始终检查它的极值。你的训练数据应该涵盖你期望在现实生活中看到的所有可能的案例；否则，你的预测将会失败。

## 数据集准备

现在让我们开始生成项目所需的数据集。这将会稍微困难一些，正如你将会看到的，我们将不得不花更多的时间来做这件事，而不是开发和调整我们的网络。本章的目标是向您展示如何将神经网络用于“经典”用例基础之外的研究项目，例如图像识别。实验数据由 *θ* 的 50 次测量组成；5 温度:5<sup>∘</sup>T4】c，15<sup>∘</sup>t8】c，25<sup>∘</sup>T12】c，35<sup>∘</sup>t16】c，45<sup>∘</sup>*c*；以及 10 种不同的氧气浓度值:0%、4%、8%、15%、20%、30%、40%、60%、80 %和 100%。每次测量由以下 *ω* 值的 22 次频率测量组成:

```
0 62.831853
1 282.743339
2 628.318531
3 1256.637061
4 3141.592654
5 4398.229715
6 6283.185307
7 9424.777961
8 12566.370614
9 18849.555922
10 25132.741229
11 31415.926536
12 37699.111843
13 43982.297150
14 50265.482457
15 56548.667765
16 62831.853072
17 75398.223686
18 87964.594301
19 100530.964915
20 113097.335529
21 125663.706144

```

对于训练数据集，我们将只使用 3000 *赫兹*和 100，000 *赫兹*之间的频率。由于实验设置的限制，在 3000 *Hz* 以下和 10000*Hz*以上开始出现伪像和误差。试图使用所有数据会使网络性能更差。这不会是一个限制。

即使您没有数据文件，我也会解释我是如何准备它们的，这样您就可以为您的案例重用代码。首先，文件保存在一个名为`data`的文件夹中。我创建了一个列表，上面有我们想要加载的所有文件的名称:

```
files = os.listdir('./data')

```

温度和氧气浓度等信息被编码在文件名中，所以我们必须从中提取它们。文件的名字看起来像这样:`20180515_ PST3-1_45C_Mix8_00.txt`。`45C`是温度，`8_00`是氧气浓度。为了提取信息，我编写了一个函数。

```
def get_T_O2(filename):
    T_ = float(filename[17:19])
    O2_ = float(filename[24:-4].replace('_','.'))
    return T_, O2_

```

这将返回两个值，一个包含温度值(`T_`)，另一个包含氧气浓度值(`O2_`)。然后我将文件的内容转换到熊猫数据框中，以便能够使用它。

```
def get_df(filename):
    frame = pd.read_csv('./data/'+filename, header = 10, sep = '\t')
    frame = frame.drop(frame.columns[6], axis=1)

    frame.columns=['f', 'ref_r', 'ref_phi', 'raw_r', 'raw_phi', 'sample_phi']

    return frame

```

这个函数是这样写的，当然，是根据文件的结构写的。文件的第一行是这样的:

```
StereO2

Probe: PST3-1
Medium: N2+Mix, Mix 0 %
Temperatur: 5 °C
Detektionsfilter: LP594 + SP682
HW Config Ref:
D:\Projekt\20180515_Quarzglas_Reference_00.ini
HW Config Sample: D:\Projekt\20180515_PST3_Sample_00.ini
Date, Time: 15.05.2018, 10:37
Filename: D:\Projekt\20180515_ PST3-1_05C_Mix0_00.txt
$Data$
Frequency (Hz)  Reference R (V)  Reference Phi (deg)  Sample Raw R (V)  Sample Raw Phi (deg)  Sample Phi (deg)
10.00E+0        247.3E-3         18.00E-3        s     371.0E-3          258.0E-3              240.0E-3
45.00E+0        247.4E-3         72.00E-3             371.0E-3          1.164E+0              1.092E+0
100.0E+0        248.4E-3         108.0E-3             370.9E-3          2.592E+0              2.484E+0
200.0E+0        247.5E-3         396.0E-3             369.8E-3          5.232E+0              4.836E+0

```

如果您想做类似的事情，自然您可能需要修改函数。现在让我们遍历所有文件，用值 *T* 、 *O* 2 和数据帧创建列表。在文件夹`data`中有 50 个文件。

```
frame = pd.DataFrame()
df_list = []
T_list = []
O2_list = []

for file_ in files:
    df = get_df(file_)
    T_, O2_ = get_T_O2(file_)

    df_list.append(df)
    T_list.append(T_)
    O2_list.append(O2_)

```

我们可以检查其中一个文件的内容。让我们打个比方，

```
get_df(files[2]).head()

```

你可以在图 [9-6](#Fig6) 中看到索引为 2 的文件的前五条记录。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig6_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig6_HTML.jpg)

图 9-6

索引为 2 的文件的前五条记录

这个文件包含的信息比我们需要的要多。

我们有频率 *f* ，必须换算成角频率 *ω* (两者之间有因子 2 *π* ，必须计算 *θ* 的正切。我们使用以下代码来实现这一点:

```
for df_ in df_list:
    df_['w'] = df_['f']*2*np.pi
    df_['tantheta'] = np.tan(df_['sample_phi']*np.pi/180.0)

```

以这种方式向每个数据帧添加两个新列。此时，我们必须为 *f* 、 *KSV* 、 <sub>1</sub> 、 *KSV* 、 <sub>2、</sub>找到一个好的近似值，以便能够创建我们的数据集。为了给大家举个例子，也为了让本章更简洁，我们只考虑一个温度:*t*= 45<sup>∘</sup>*c*。让我们从所有的数据中筛选出在这个温度下测得的数据。为此，我们可以使用以下代码:

```
T = 45

Tdf = pd.DataFrame(T_list, columns = ['T'])
Odf = pd.DataFrame(O2_list, columns = ['O2'])
filesdf = pd.DataFrame(files, columns = ['filename'])

files45 = filesdf[Tdf['T'] == T]
filesref = filesdf[(Tdf['T']==T) & (Odf['O2']==0)]
fileref_idx = filesref.index[0]
O5 = Odf[Tdf['T'] == T]
dfref = df_list[fileref_idx]

```

首先，我们将列表`T_list`和`O2_list`转换为 pandas 数据帧，因为在这种格式下，选择正确的数据更容易。那么你可能会注意到，我们选择了数据帧`files45`中具有 *T* = 45 <sup>∘</sup> *C* 的所有文件。另外，我们选择数据帧为*t*= 45<sup>∘</sup>*c*和 *O* 2 = 0%，我们称之为`dfref`。原因是在一开始，我给了你一个 *θ* 的公式，涉及到谭 *θ* ( *ω* ， *T* ， *O* 2 = 0)。`dfref`将准确包含 tan *θ* ( *ω* ， *T* ， *O* 2 = 0)的测量数据。记住，我们必须对数量进行建模

![$$ \frac{\tan \theta \left(\omega, T={45}^{\circ }C\right)}{\tan \theta \left(\omega, T={45}^{\circ }C,\kern0.6em O2=0\right)} $$](../images/463356_1_En_9_Chapter/463356_1_En_9_Chapter_TeX_Equd.png)

我知道这变得越来越复杂，但是坚持住，我们就快完成了。从数据框列表中选择正确的数据框稍微复杂一些，但可以通过以下方式完成:

```
from itertools import compress
A = Tdf['T'] == T
data = list(compress(df_list, A))

B = (Tdf['T']==T) & (Odf['O2']==0)
dataref_ = list(compress(df_list, B))

```

`compress`很容易理解。您可以在 [`https://goo.gl/xNZEHH`](https://goo.gl/xNZEHH) 的官方文档页面上找到更多信息。基本上，这个想法是，给定两个列表，`d`和`s`,`compress(d,s)`的输出由一个等于[ `(d[0] if s[0]), (d[1] if s[1]), ...]`的新列表给出。在我们的例子中，`A`和`B`是由布尔值组成的列表，所以对于列表`A`中有`True`的位置，代码只返回`df_list`的值。

使用非线性拟合，我们将为我们拥有的每个 *ω* 值找到 *f* 、 *KSV* <sub>1、</sub>和 *KSV* <sub>2</sub> 的值。我们必须循环 *ω* 的所有值，拟合函数

![$$ \frac{\tan \theta \left(\omega, T={45}^{\circ }C,O2\right)}{\tan \theta \left(\omega, T={45}^{\circ }C,O2=0\right)} $$](../images/463356_1_En_9_Chapter/463356_1_En_9_Chapter_TeX_Eque.png)

关于 *O* 2，使用函数

```
def fitfunc(x,  f, KSV, KSV2):
    return (f/(1.0+KSV*x)+ (1.0-f)/(1+KSV2*x))

```

为每个 *O* 2 值提取 *f* 、 *KSV* 、 <sub>1</sub> 、 *KSV* 、 <sub>2</sub> 。我用这段代码做到了:

```
f = []
KSV = []
KSV2 = []

for w_ in wred:

    # Let's prepare the file

    O2x = []
    tantheta = []
    #tantheta0 = float(dfref[dfref['w']==w_]['tantheta'])
    tantheta0 = float(dataref_[0][dataref_[0]['w']==w_]['tantheta'])

    # Loop over the files
    for idx, df_ in enumerate(data_train):
        O2xvalue = float(Odf.loc[idx])
        O2x.append(O2xvalue)

        tanthetavalue = float(df_[df_['w'] == w_]['tantheta'])
        tantheta.append(tanthetavalue)

    popt, pcov = curve_fit(fitfunc_2, O2x, np.array(tantheta)/tantheta0, p0 = [0.4,0.06, 0.003])

    f.append(popt[0])
    KSV.append(popt[1])
    KSV2.append(popt[2])

```

花些时间研究代码。代码非常复杂，因为每个文件都包含固定值为 2 的数据。我们希望为每个频率值构建一个数组，其中包含我们希望拟合为 *O* 2 的函数的值。这就是为什么我们必须做一些数据争论。在列表`f`、`KSV`和`KSV2`中，我们现在已经找到了与频率相关的值。让我们首先只选择 3000 到 100，000 之间的角频率值。

```
w_ = w[4:20]
f_ = f[4:20]
KSV_ = KSV[4:20]

```

在图 [9-7](#Fig7) 中可以看到 *f* 、 *KSV* <sub>1</sub> 、 *KSV* <sub>2</sub> 对角频率的依赖关系。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig7_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig7_HTML.jpg)

图 9-7

*f* 、 *KSV* 、 <sub>1</sub> 、 *KSV* 、 <sub>2</sub> 对角频率的依赖性

还有一个小问题我们必须克服。我们必须能够计算出 *ω* 的任意值的 *f* 、 *KSV* 、 *KSV* 、T7】 2 ，而不仅仅是我们已经得到的值。为此，我们必须使用插值。为了节省时间，我们不会从头开始开发插值函数。相反，我们将使用 SciPy 包中的`interp1d`函数。

```
from scipy.interpolate import interp1d

```

我们将这样做:

```
finter = interp1d(wred, f, kind="cubic")
KSVinter = interp1d(wred, KSV, kind = 'cubic')
KSV2inter = interp1d(wred, KSV2, kind = 'cubic')

```

注意，`finter`、`KSVinter`和`KSV2inter`是接受 *ω* 的值作为输入的函数，作为一个 NumPy 数组，并分别返回 *f* 、 *KSV* <sub>1</sub> 和 *KSV* <sub>2</sub> 的值。图 [9-8](#Fig8) 中的实线表示由图 [9-7](#Fig7) 中的点得到的插值函数。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig8_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig8_HTML.jpg)

图 9-8

*f* 、 *KSV* 、1、 *KSV* 、 <sub>2</sub> 对角频率的依赖性。实线是插值函数。

在这一点上，我们有我们需要的所有成分。我们最终可以用公式创建我们的训练数据集

![$$ \frac{\tan \theta \left(\omega, T,O2\right)}{\tan \theta \left(\omega, T,O2=0\right)}=\frac{f\left(\omega, T\right)}{1+ KS{V}_1\left(\omega, T\right)\kern0.5em \cdot \kern0.5em O2}+\frac{1-f\left(\omega, T\right)}{1+ KS{V}_2\left(\omega, T\right)\kern0.5em \cdot \kern0.5em O2} $$](../images/463356_1_En_9_Chapter/463356_1_En_9_Chapter_TeX_Equf.png)

现在让我们为 *O* 2 的随机值创建 5000 个观察值。

```
number_of_samples = 5000
number_of_x_points = len(w_)
np.random.seed(20)
O2_v = np.random.random_sample([number_of_samples])*100.0

```

我们需要前面的数学函数

```
def fitfunc2(x, O2, ffunc, KSVfunc, KSV2func):
    output = []
    for x_ in x:
        KSV_ = KSVfunc(x_)
        KSV2_ = KSV2func(x_)
        f_ = ffunc(x_)
        output_ = f_/(1.0+KSV_*O2)+(1.0-f_)/(1.0+KSV2_*O2)
        output.append(output_)
    return output

```

计算

![$$ \frac{\tan \theta \left(\omega, T,O2\right)}{\tan \theta \left(\omega, T,O2=0\right)} $$](../images/463356_1_En_9_Chapter/463356_1_En_9_Chapter_TeX_Equg.png)

对于角频率和 *O* 2 的值。数据可以用代码生成

```
data = np.zeros((number_of_samples, number_of_x_points))
targets = np.reshape(O2_v, [number_of_samples,1])

for i in range(number_of_samples):
    data[i,:] = fitfunc2(w_, float(targets[i]), finter, KSVinter, KSV2inter)

```

在图 [9-9](#Fig9) 中，你可以看到我们生成的数据的一些随机例子。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig9_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig9_HTML.jpg)

图 9-9

我们生成的数据的随机例子

## 模特培训

让我们开始建立网络。由于篇幅的原因，我们在这里将自己局限于一个简单的三层网络，每层有五个神经元。

```
tf.reset_default_graph()

n1 = 5 # Number of neurons in layer 1
n2 = 5 # Number of neurons in layer 2
n3 = 5 # Number of neurons in layer 3
nx = number_of_x_points
n_dim = nx
n4 = 1

stddev_f = 2.0

tf.set_random_seed(5)

X = tf.placeholder(tf.float32, [n_dim, None])
Y = tf.placeholder(tf.float32, [10, None])
W1 = tf.Variable(tf.random_normal([n1, n_dim], stddev=stddev_f))
b1 = tf.Variable(tf.constant(0.0, shape = [n1,1]) )
W2 = tf.Variable(tf.random_normal([n2, n1], stddev=stddev_f))
b2 = tf.Variable(tf.constant(0.0, shape = [n2,1]))
W3 = tf.Variable(tf.random_normal([n3,n2], stddev = stddev_f))
b3 = tf.Variable(tf.constant(0.0, shape = [n3,1]))
W4 = tf.Variable(tf.random_normal([n4,n3], stddev = stddev_f))
b4 = tf.Variable(tf.constant(0.0, shape = [n4,1]))

X = tf.placeholder(tf.float32, [nx, None]) # Inputs
Y = tf.placeholder(tf.float32, [1, None]) # Labels

# Let's build our network

Z1 = tf.nn.sigmoid(tf.matmul(W1, X) + b1) # n1 x n_dim * n_dim x n_obs = n1 x n_obs
Z2 = tf.nn.sigmoid(tf.matmul(W2, Z1) + b2) # n2 x n1 * n1 * n_obs = n2 x n_obs
Z3 = tf.nn.sigmoid(tf.matmul(W3, Z2) + b3)
Z4 = tf.matmul(W4, Z3) + b4
y_ = Z2

```

至少，我是这样开始的。我选择了一个具有身份激活功能的神经元作为网络的输出。不幸的是，这种训练不起作用，而且非常不稳定。因为我必须预测一个百分比，所以我需要一个介于 0 和 100 之间的输出。所以，我决定尝试一个乘以 100 的 sigmoid 激活函数。

```
y_ = tf.sigmoid(Z2)*100.0

```

突然间，训练变得非常有效。我用了亚当优化器。

```
cost = tf.reduce_mean(tf.square(y_-Y))
learning_rate = 1e-3
training_step = tf.train.AdamOptimizer(learning_rate).minimize(cost)
init = tf.global_variables_initializer()

```

这次，我用的是 100 号的小批量。

```
batch_size = 100
sess = tf.Session()
sess.run(init)
training_epochs = 25000
cost_history = np.empty(shape=[1], dtype = float)

train_x = np.transpose(data)
train_y = np.transpose(targets)
cost_history = []
for epoch in range(training_epochs+1):

    for i in range(0, train_x.shape[0], batch_size):
        x_batch = train_x[i:i + batch_size,:]
        y_batch = train_y[i:i + batch_size,:]

        sess.run(training_step, feed_dict = {X: x_batch, Y: y_batch})

    cost_ = sess.run(cost, feed_dict={ X:train_x, Y: train_y})
    cost_history = np.append(cost_history, cost_)

    if (epoch % 1000 == 0):
        print("Reached epoch",epoch,"cost J =", cost_)

```

不用过多解释，你现在应该明白这段代码了。这基本上是我们以前用过几次的。你可能已经注意到我已经随机初始化了权重。我尝试了几种策略，但这似乎是最有效的一种。检查训练进行得如何很有指导意义。在图 [9-10](#Fig10) 中，您可以看到在训练数据集和实验开发数据集上评估的成本函数。你可以看到它是如何振荡的。这主要有两个原因。

*   首先是我们使用小批量，因此，成本函数振荡

*   第二个原因是实验数据有噪声，因为测量仪器不完善。例如，气体混合器具有大约 1–2%的绝对误差，这意味着如果我们有一个关于 *O* 2 = 60%的实验观察，它可能低至 58%或 59%,高至 61%或 62%。

考虑到这一误差，平均而言，绝对误差应为±ca。我们预测的 1%。

### 注意

粗略地说，一个训练有素的网络的输出永远不可能超过所用目标变量的精度。记住，要经常检查你的目标变量可能存在的误差，以估计它们有多精确。在前面的例子中，因为我们的目标值为 *O* 2 具有 1%的最大绝对误差(即实验误差)，所以网络结果的预期误差将是这个数量级。**注意**给定特定的输入，网络将学习产生特定输出的函数。如果输出是错的，学习到的函数也会是错的。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig10_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig10_HTML.jpg)

图 9-10

成本函数与在训练和开发数据集上评估的时期

最后，让我们检查一下网络的性能。

您应该记住，我们的 dev 数据集很小，这使得振荡更加明显。在图 [9-11](#Fig11) 中，可以看到 *O* 2 的预测值与测量值的对比。如你所见，它们漂亮地位于对角线上。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig11_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig11_HTML.jpg)

图 9-11

*O* 2 的预测值与测量值

在图 [9-12](#Fig12) 中，您可以看到针对*O*2∈【0，100】在 dev 数据集上计算的绝对误差。

![../images/463356_1_En_9_Chapter/463356_1_En_9_Fig12_HTML.jpg](../images/463356_1_En_9_Chapter/463356_1_En_9_Fig12_HTML.jpg)

图 9-12

针对*O*2∈[0，100]在开发数据集上计算的绝对误差

结果非常好。对于 *O* 2 的所有值，除 100%外，误差都在 1%以下。请记住，我们的网络是从人工创建的数据集学习的。该网络现在可以用于这种类型的传感器，而不需要实现复杂的数学方程来估计 *O* 2。该项目的下一阶段将是自动获得 ca。在温度 *T* 和氧气浓度 *O* 2 的不同值下进行 10，000 次测量，并使用这些测量作为训练集来同时预测 *T* 和 *O* 2。