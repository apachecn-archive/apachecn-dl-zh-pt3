# 3.深度学习简介

在本章中，您将了解深度学习网络。您还将学习深度神经网络如何工作，以及如何使用 Keras 和 PyTorch 实现深度学习神经网络。

简而言之，本章将涵盖以下主题:

*   什么是深度学习？

*   Keras 简介:一个简单的分类器模型

*   PyTorch 简介:一个简单的分类器模型

## 什么是深度学习？

深度学习是机器学习的一个特殊子领域，它处理不同类型的人工神经网络。从大脑的结构和功能中汲取灵感，人工神经网络的核心是多层互连的独立单元，称为神经元，每个神经元在给定输入数据的情况下执行特定的功能。

特别是在“深度”学习中，一些最好的模型由几十层和数百万个神经元组成，并在数千兆字节的数据上进行训练。一般来说，深度学习模型并不总是需要这么大才能在某些任务上表现良好，大型模型预期要执行的任务很复杂，从概述图像中的各种对象到生成文章的摘要。

得益于最近 GPU(图形处理单元)计算能力和可用性的提高，任何拥有足够体面的 GPU 的人都可以训练自己的深度学习模型，记住更大的模型可能需要更多的 GPU 资源，如内存。

今天，由于深度学习提供的极端多功能性和性能，它正在席卷全球。机器学习中更传统的模型存在一个问题，即添加更多的训练样本会导致性能停滞，但深度学习不存在这个问题。相反，随着样本越来越多，深度学习模型变得越来越好，这意味着它们在数据集大小方面的扩展性更好，因此获得了更好的性能。深度学习模型可以应用于几乎任何任务，并取得巨大成功，因此被应用于网络安全、气象学、金融和股票市场、语音识别、医学、搜索引擎等领域。深度学习到底是什么让它如此伟大？首先，我们来看看什么是人工神经网络。

### 人工神经网络

**人工神经网络**是多层相互连接的节点，或人工神经元，其功能受到生物神经网络的启发。图 [3-1](#Fig1) 显示了一个神经元的例子。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig1_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig1_HTML.jpg)

图 3-1

一个神经元看起来像什么的例子

输入通过树突接收，然后神经元决定是否触发。一旦触发，神经元沿着轴突向其末端轴突发送信号，在那里信号被输出到任何其他神经元。这种信号传输被称为突触，如图 [3-2](#Fig2) 所示。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig2_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig2_HTML.jpg)

图 3-2

两个神经元如何连接形成一个链并通过该连接传递信号。第一个神经元的末端轴突连接到第二个神经元的树突

我们在人工神经网络中使用类似的概念(图 [3-3](#Fig3) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig3_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig3_HTML.jpg)

图 3-3

*人工神经网络中的人工神经元如何发挥作用。这种对生物神经元的模仿是人工神经网络的基础*

在这种人工神经元的情况下，我们找到输入向量 **X** 和权重向量 **W** 之间的点积。x 表示输入数据，W 表示该节点携带的要与输入向量相乘的权重列表。回想一下，点积是向量中的每个元素与第二个向量中相应的元素相乘，如图 [3-4](#Fig4) 。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig4_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig4_HTML.jpg)

图 3-4

这就是点积的工作原理。这里显示的是一个有两种不同类型的向量符号的例子

这两种方法都是表示向量的不同方式，尽管考虑到您的数据和权重很可能采用矩阵的形式，第二种方法是理想的。

之后，有一个可选的偏置函数，其中值 **b** (称为**偏置**)被加到点积结果上。从那里，它通过一个**激活函数**，决定整个节点是否发送数据。在这种情况下，激活函数仅在 0 和 1 之间变化，这取决于点积加上偏差是否达到某个值(阈值)。有可能具有其他激活函数，例如 sigmoid 函数，其输出 0 和 1 之间的某个值。

调用输出 y 和输入 x，每个节点的基本函数可以由图 [3-5](#Fig5) 中的等式表示。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig5_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig5_HTML.jpg)

图 3-5

一个描述人工神经元基本功能的方程。在这种情况下，f(x)是一个激活函数

人工神经网络由这些节点的互连层组成，看起来像图 [3-6](#Fig6) 。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig6_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig6_HTML.jpg)

图 3-6

一个人工神经网络的例子

**隐藏层**位于输入层和输出层之间。网络中可以有多个隐藏层。现在，您已经看到了人工神经网络的样子，让我们看看数据是如何在这个网络中流动的。首先，我们只从网络中的输入数据开始，并假设神经元仅完全激活(神经元可以根据激活函数进行部分激活，但在本例中，每个神经元要么输出 1，要么输出 0)(图 [3-7](#Fig7) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig7_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig7_HTML.jpg)

图 3-7

输入数据流经输入层，选择性节点根据接收到的输入触发

输入层接受所有相应的输入，并产生一个链接到第一个隐藏层的输出。在输入层中激活的节点的输出现在是隐藏层的输入，新的数据相应地流动(图 [3-8](#Fig8) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig8_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig8_HTML.jpg)

图 3-8

输入层中激活的神经元的输出传递到第一个隐藏层。这些输出现在是下一层的输入，选择性神经元基于该输入而激活

隐藏层 1 以与输入层相似的方式处理数据，只是激活函数、权重、偏差等的参数不同。数据通过这一层，这一层的输出成为下一个隐藏层的输入。在这种情况下，只有两个节点根据前一层的输入激活(图 [3-9](#Fig9) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig9_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig9_HTML.jpg)

图 3-9

对隐藏层 2 重复该过程

隐藏层 2 处理数据并将数据发送到称为输出层的新层，在该层中只有一个节点被激活。在这种情况下，输出层的第一个节点被激活(图 [3-10](#Fig10) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig10_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig10_HTML.jpg)

图 3-10

最后，来自第二个隐藏层的数据进入输出层，在这种情况下，一个神经元在这里触发

输出图层中的节点可以表示您要赋予输入数据的不同标签。例如，在鸢尾数据集中，您可以对鸢尾花进行各种测量，并根据这些数据训练人工神经网络来对花的种类进行分类。

在初始化时，模型的权重将远非理想。在整个训练过程中，来自模型的数据流向前(从左到右从输入到输出)，然后在所谓的**反向传播**中向后，以重新计算每个激活节点的权重和偏差。

在反向传播中，**成本函数**考虑了模型对训练数据通过网络的一次传递的预测以及实际预测应该是什么。成本函数为您提供了模型权重在预测正确结果方面有多好的指标。对于这个例子，假设图 [3-11](#Fig11) 显示了成本函数的公式。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig11_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig11_HTML.jpg)

图 3-11

均方误差代价函数的公式

这个成本函数被称为**均方误差**，这样命名是因为给定输入 *θ* 的函数，即权重，找到预测值和实际值之间的平均差平方。参数 *h* <sub>*θ*</sub> 表示传入了权重参数 *θ* 的模型，所以*h*<sub>*θ*</sub>(*x*<sup>*I*</sup>)给出了模型权重 *θ为 *x* <sup>*i*</sup> 的预测值参数 *y* <sup>*i*</sup> 表示索引 I 处数据点的实际预测。如果您传入的参数包括权重和偏差，则它看起来更像图 [3-12](#Fig12) 。*

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig12_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig12_HTML.jpg)

图 3-12

均方差成本函数的公式，使用更具体的符号来分隔权重和偏差

注意 *h* <sub>*w* ，*b*</sub>(*x*<sup>*I*</sup>)会有图 [3-13](#Fig13) 中的公式。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig13_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig13_HTML.jpg)

图 3-13

图 [3-12](#Fig12) 中函数 h(w，b)含义的详细说明

成本函数反映了当前权重参数下模型的整体性能，因此成本函数输出的最理想值将尽可能小。由于成本函数是模型预测值与实际值之间距离的度量，因此您希望成本函数的输出尽可能小，因为这意味着您的预测值与实际预测值相差无几。

为了最小化成本函数，你需要告诉模型如何调整权重，但是你怎么做呢？如果你回想一下微积分，最优化问题包括求导和求解临界点(原始方程的导数为 0 的点)。在您的情况下，您想要找到**梯度**，它可以被认为是类似于导数，但在多维设置中，并在一个方向上调整权重，以改变梯度，使其接近 0。

有几种优化算法可以帮助模型获得最佳权重，包括**梯度下降**。梯度下降是一种优化算法，它找到成本函数的梯度，并在局部最小值的方向上迈出一步，以生成用于调整权重和偏差的值。

你走多少步是由**学习速率**控制的。学习率越大，每次迭代的步长就越大，接近局部最小值的速度就越快。学习率越小，训练时间越长，因为步数越小。然而，学习率过大的问题是它可能完全超过局部最小值，导致完全不能达到局部最小值。太小的学习率和局部最小值可能需要太长的时间才能达到。当模型开始达到理想的性能水平时，梯度应该接近 0，因为权重会使成本函数达到局部最小值，这表明模型预测和实际预测之间的差异非常小。

在称为**反向传播**的过程中，计算梯度并为层中的每个节点调整权重，然后对之前的层执行相同的过程，直到所有层的权重都已调整。通过模型传递数据并反向传播以重新调整权重的整个过程构成了深度学习中模型的训练过程。

虽然整个训练过程听起来可能很复杂，计算量很大，但 GPU 有助于更快地训练模型，因为它们经过优化，可以执行图形处理所需的矩阵计算。

既然你对什么是深度学习和人工神经网络如何运作有了更多的了解，那么一个问题可能会出现在**为什么**我们应该使用深度学习来进行异常检测。

首先，由于 GPU 技术的进步，我们可以在巨大的数据集上训练更深入的深度学习模型(许多层，有许多参数)。这本身导致了网络令人难以置信的性能，并允许该模型具有更强大的应用。

这不仅导致了一组不同的模型，每个模型都适合于不同的应用(图像分类、视频字幕、对象检测、语言翻译、可以概括文章的生成模型等)。)，但是模型在各自的任务上不断变得越来越好。

这些模型的可扩展性也远高于传统模型，因为深度学习模型不会随着数据条目数量的增加而在训练准确性上达到高原，这意味着我们可以将深度学习模型应用于海量数据。深度学习模型的这一属性与当今社会的大数据趋势非常吻合。

在本章中，你将看到应用深度学习模型来分类手写数字，作为使用 Python 中两个伟大的、流行的深度学习框架的介绍: **Keras** ，带有 TensorFlow 后端，和 **PyTorch** 。这些框架可以帮助你只用几十行代码创建定制的深度学习模型，而不是完全从头开始创建。

Keras 是一个高级框架，可以让你快速创建、训练和测试强大的深度学习模型，同时为你抽象出所有的小细节。 **PyTorch** 更像是一个低级框架，但它不像 **TensorFlow** (一个更流行的深度学习框架)那样携带大量语法。然而，与 Keras 相比，仍然有更多你必须定义的东西，因为它不再是抽象的。

使用`PyTorch`超过 TensorFlow 或反之亦然更多是个人偏好，但`PyTorch`更容易掌握。两者都提供非常相似的功能，如果 TensorFlow 有 PyTorch 没有的功能，您仍然可以使用 PyTorch API 实现它们。

另一个需要注意的是，TensorFlow 已经将`Keras`集成到了它的 API 中，因此如果您将来想要使用 TensorFlow，您仍然可以使用`tf.keras`来构建您的模型。

## Keras 简介:一个简单的分类器模型

在开始之前，建议您安装 TensorFlow 的 GPU 版本及其所有依赖项，包括 CUDA 和 cuDNN。虽然它们不一定需要训练深度学习模型，但拥有 GPU 有助于大幅减少训练时间。TensorFlow 和 PyTorch 在训练时都利用 CUDA 和 cuDNN 访问 GPU，Keras 运行在 TensorFlow 之上。

如果您对 Keras 有任何疑问，请随意参考附录 A，以便更好地理解 Keras 的工作方式及其提供的功能。

以下是所使用的必要 Python 3 包的确切版本:

*   tensorlow-GPU 1 . 10 . 0 版

*   硬版本 2.0.8

*   torch 版本 0.4.1(这是 PyTorch)

*   CUDA 版本 9.0.176

*   cuDNN 版本 7.3.0.29

您将使用 MNIST 数据集在 Keras 中创建、训练和评估一个称为卷积神经网络(CNN)的深度学习架构。您不需要下载该数据集，因为它包含在 TensorFlow 中。

MNIST 数据集，或修改的国家标准和技术研究所数据集，是一个手写图像的大集合，用于训练计算机视觉和图像处理模型，如 CNN。这是一个常见的数据集，基本上类似于计算机视觉的“hello world”数据集。

数据集包含 60，000 个训练图像和 10，000 个手写数字 0-9 的测试图像，每个图像的尺寸为 28×28 像素。

首先，导入所有的依赖关系(图 [3-14](#Fig14) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig14_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig14_HTML.png)

图 3-14

导入创建模型所需的模块

现在定义一些你稍后会用到的变量(图 [3-15](#Fig15) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig15_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig15_HTML.png)

图 3-15

以后要使用的变量

整个数据集通过模型的一次传递被称为一个**时期**。**批量大小**是在一次迭代中有多少数据条目通过模型。在这种情况下，训练数据一次通过模型 128 个条目，直到所有条目都通过，这标志着一个时期的结束。类的数量是 10，代表从 0-9 的 10 个数字中的每一个。这些变量也被称为**超参数，即在训练过程之前设置的**参数。

让我们创建您的培训和测试数据集。需要注意的一点是，可以使用数据帧、数组、矩阵等。作为你的数据集。运行图 [3-16](#Fig16) 中的代码。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig16_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig16_HTML.png)

图 3-16

定义培训和测试数据集

您可以使用 matplotlib 来查看这些图像的外观。运行图 [3-17](#Fig17) 中的代码，并查看图 [3-18](#Fig18) 中的结果。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig18_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig18_HTML.jpg)

图 3-18

运行图 [3-17](#Fig17) 中代码的输出

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig17_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig17_HTML.png)

图 3-17

导入`matplotlib.pyplot`以查看这些训练图像的样子

您可以输入 0 到 59，999 之间的任意值，以在 x_train 中显示样本。

只要看看数字 1 的 10 个例子，你就可以看到数据集中有很多变化(见图 [3-19](#Fig19) 和图 [3-20](#Fig20) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig20_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig20_HTML.jpg)

图 3-20

运行图 [3-19](#Fig19) 中代码的输出。请注意变化的数量，以及您几乎不会认为是数字的异常数据

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig19_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig19_HTML.png)

图 3-19

用于生成显示特定类的一些示例图像的绘图的代码

现在，将形状延伸一个维度。目前，训练和测试装置的尺寸如图 [3-21](#Fig21) 和图 [3-22](#Fig22) 所示。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig22_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig22_HTML.jpg)

图 3-22

运行图 [3-21](#Fig21) 中代码的输出

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig21_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig21_HTML.png)

图 3-21

输出训练和测试数据集形状的代码

出于训练模型的目的，您希望将该形状扩展到(60000，28，28，1)和(10000，28，28，1)。

图像的一个特性是彩色图像有三维，灰度图像有二维。灰度图像只是简单的 x 行 x 列，因为它们没有颜色通道。另一方面，彩色图像可以格式化为行 x 列 x 通道或通道 x 行 x 列。对于彩色图像，变量通道是 3，因为您想知道红色、绿色和蓝色(RGB)的像素值。

在这种情况下，它是灰度级，所以您不必担心 channel 变量，但是如果您最终使用带有颜色的数据集，如 CIFAR-10 数据集，下面的代码将考虑这两种情况。CIFAR-10 与 MNIST 非常相似，但这次您是根据汽车、鸟类、船只等标签对 32x32 图像进行分类。它们是彩色的。运行图 [3-23](#Fig23) 中的代码。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig23_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig23_HTML.png)

图 3-23

根据通道是否是第一个来重塑定型和测试数据集的代码，然后定义模型的输入形状

现在将这些值转换成 float32 并除以 255。现在，这些值都是范围从 0 到 255 的整数值，但是您希望将这些值转换为 float，使它们从 0 到 1。这是一个被称为**标准化**或**特征缩放**的过程，在这个过程中，你试图将数据重新缩放到更小、更易于管理的值。在这个例子中，你使用一种叫做**最小-最大归一化**的方法，由图 [3-24](#Fig24) 中的公式定义。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig24_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig24_HTML.jpg)

图 3-24

最小-最大归一化公式

你的数值范围从 0 到 255。对于每个值，你从 x 中“减去”0，然后除以 255–0，也就是 255。在图像任务中，从[0，255]到[0，1]的范围内重新调整像素值是很常见的，也可以用彩色图像来完成。

还有其他方法，包括**均值归一化**、**标准化(z 分数归一化)**、**单位长度缩放**。

每种方法的公式如下:

**均值归一化**(图 [3-25](#Fig25) )

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig25_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig25_HTML.jpg)

图 3-25

均值归一化公式

这个公式类似于最小-最大归一化，除了您在分子中使用 *x* <sub>*平均*</sub> 超过 *x* <sub>*最小*</sub> 。

**标准化**(图 [3-26](#Fig26) )

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig26_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig26_HTML.jpg)

图 3-26

标准化公式

您基本上找到了每个 x 的 z 得分值，并使用这些值代替原始的 x 值。

**单位长度缩放**(图 [3-27](#Fig27) )

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig27_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig27_HTML.jpg)

图 3-27

单位长度换算公式

你找到 x 的单位向量，然后用它来代替。单位向量的大小为 1。

下一个代码块如图 [3-28](#Fig28) 所示。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig28_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig28_HTML.png)

图 3-28

将`x_train`和`x_test`转换为 float32，并通过除以 255 应用最小-最大归一化。对于`y_train`和`y_test`，你把它们转换成一个独热编码格式

`keras.utils.to_categorical()`所做的是获取类别的向量，并创建类别数量的二进制类别矩阵。假设你有一个最多用 6 个类表示`y_train`的向量，从 0-5(图 [3-29](#Fig29) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig29_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig29_HTML.jpg)

图 3-29

一个表示`y_train`的向量，它有 6 个值在 0-5 范围内的类

在运行了`keras.utils.to_categorical(y_train, n_classes)` where `n_classes = 5`之后，图 [3-30](#Fig30) 显示了你现在会从`y_train`那里得到什么。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig30_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig30_HTML.jpg)

图 3-30

图 [3-39](#Fig39) 中`y_train`向量的一个热码表示

类还是一样的，但是这一次你必须通过它们的索引而不是直接的值来获取类。在原始向量的索引 1(第 1 行，如果你认为这是一个有 1 列的矩阵)，你会看到类标签是 5。在您转换后的`y_train`数据(现在是一个矩阵)中，在第 1 行(转换前的索引为 1)，您会看到除了第 5 列的值之外，该索引处的向量中的所有值都是 0。因此，`y_train`在索引 1 处仍然是 5，但是它的格式不同了。

现在让我们检查图 [3-31](#Fig31) 和图 [3-32](#Fig32) 中你的转换数据的形状。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig32_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig32_HTML.jpg)

图 3-32

最终输出

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig31_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig31_HTML.png)

图 3-31

打印转换数据的形状

### 注意

字符告诉 Python 你想继续下一行。没有它，代码不会运行，因为 Python 看不到第二个“，”表示的字符串的结尾，但是\告诉 Python 在下一行继续。

现在您可以继续定义和编译您的模型了。

运行图 [3-33](#Fig33) 中的代码。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig33_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig33_HTML.png)

图 3-33

定义深度学习模型并向其添加层的代码

在 Keras 中，**顺序**模型是一个层的堆栈。Conv2D 是一个二维卷积层。

在卷积神经网络中，**卷积层**对数据进行过滤，并将每个值按元素乘以过滤器中的权重，并将它们相加以生成一个值。在这种情况下，它是一个 3x3 的过滤器，滑过每个像素，生成一个更小的层，称为**激活图**或**特征图**。然后，在第二卷积层中对该特征图应用另一个滤波器，以生成另一个更小的特征图。在反向传播期间优化的权重可以在滤波器中找到。为了更好地理解这一点，我们来看一些例子。

假设一张 5x5 像素的图片，如图 [3-34](#Fig34) 。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig34_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig34_HTML.jpg)

图 3-34

5x5 像素的图片，0 代表黑色像素，1 代表白色像素

还假设您的内核大小(过滤器尺寸)是 2x2。图 [3-35](#Fig35) 显示了盘旋将如何进行。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig35_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig35_HTML.jpg)

图 3-35

输入图像的 2x2 部分上的 2x2 滤镜的一次乘法示例。滤波器权重按元素方式应用，并产生一个输出值，该值是该卷积层输出的特征图的一部分

首先，你有一组随机的 2x2 **过滤器**或者**内核**的权重。过滤器检查图像中的第一个 2x2 区域，并对过滤器中的值和图像的 2x2 区域中的值的逐元素乘积求和。该值是特征地图的第一个元素，特征地图是 4x4 图层图像。给定一个 **nxn** 滤镜和 **mxm** 图像，你的特征图尺寸将是一个 **m-n+1 x m-n+1** 维图像。在这种情况下，您的图像是 5x5，内核是 2x2，因此特征映射是 5–2+1 = 4x 4 像素。

滤波器逐个像素地穿过图像中的每个区域，如图 [3-36](#Fig36) 所示。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig36_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig36_HTML.jpg)

图 3-36

在图 [3-35](#Fig35) 中的操作之后，过滤器移动到下一组数据进行相乘，产生特征图中的第二个值

过滤器继续这样做，直到它到达图像的右侧。之后，滤镜向下一级，从图像左侧开始，如图 [3-37](#Fig37) 所示。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig37_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig37_HTML.jpg)

图 3-37

显示了滤镜到达图像最右侧后的情况。它向下移动一位(在这种情况下，至少；当调用该层时，您可以指定希望过滤器移动多少作为参数),然后照常继续其操作

从这里开始，过滤器继续以逐个像素的方式向右移动(参见图 [3-38](#Fig38) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig38_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig38_HTML.jpg)

图 3-38

过滤器继续正常移动，向特征图添加更多的值

一旦到达末尾，它将返回第一列并向下移动一行，继续其操作，直到到达右下角区域(见图 [3-39](#Fig39) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig39_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig39_HTML.jpg)

图 3-39

一旦过滤器达到该值，卷积操作停止，将特征地图输出到下一层

由于权重的随机性，特征图没有多大意义。

在两个卷积层之后，您会遇到 MaxPooling2D 层。 **Max pooling** 是输入数据被过滤器扫描的地方，在这种情况下是 2x2 过滤器，图像的 2x2 区域中的最大值被选择为新的 n 维图像中的值。如果没有给出**步长**，默认情况下，Keras 选择池的大小。步长是过滤器应该移动的距离，它在确定特征图大小时起作用。在这种情况下，由于步幅长度是 2，池过滤器大小也是 2×2，所以输入数据的维数减少了一半。

假设图 [3-40](#Fig40) 中的 4x4 图像是池大小为 2x2 的最大池层的输入。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig40_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig40_HTML.jpg)

图 3-40

最大池化操作在 4x4 图像上是什么样子

由于池大小是 2x2，并且在这种情况下步长也是 2(没有为步长提供参数)，池层碰巧将整个图像分割成 2x2 池过滤器的区域。

如果步幅长度为 1，那么您将遇到类似于前面看到的卷积示例的情况，并且特征图的维数将为 4-2+1 = 3×3。这个汇集的过程也可以称为**下采样**。

**池层**有助于减少数据的大小，以便于计算。此外，它还有助于模式识别，因为选择了每个区域中的最大值，从而使模式更加突出。

接下来是**辍学**层。Dropout 是一种正则化技术，其中随机选择的节点的一部分(这是传入的参数)在训练过程中被“丢弃”或忽略。

**Flatten** 是将整个输入压缩成一维的层。假设你正试图展平一个 3x3 的图像，如图 [3-41](#Fig41) 。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig41_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig41_HTML.jpg)

图 3-41

显示拼合图层对输入 3x3 图像的影响

**密集**层只是一层规则节点，类似于人工神经网络示例中的节点。它们以相同的方式运行，但在这种情况下，节点的数量从第一个密集层的 128 个变化到第二个密集层的 10 个。激活函数也发生变化，从第一密集层中的“relu”或**整流线性单元(ReLU)** 变为第二密集层中的 **softmax** 。

从数学上讲， **ReLU** 函数被定义为 y = max(0，x)，因此当节点计算输入和权重之间的点积并添加偏差时，它只是输出 0 或计算值之间的较大值。

ReLU 的曲线图如图 [3-42](#Fig42) 所示。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig42_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig42_HTML.jpg)

图 3-42

显示 ReLU 函数的图形

softmax 的一般公式如图 [3-43](#Fig43) 所示。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig43_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig43_HTML.jpg)

图 3-43

softmax 激活函数的公式

至于**优化器**，设置为**亚当优化器**，一种基于梯度的优化器。默认情况下，被称为**学习率**的参数被设置为 0.001。回想一下，学习率有助于确定优化算法所采用的步长，以查看调整权重的幅度。

执行图 [3-43](#Fig43) 中的代码后，得到图 [3-44](#Fig44) 中的输出。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig44_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig44_HTML.jpg)

图 3-44

图 [3-33](#Fig33) 中代码的输出。注意它是如何告诉你每一层的输出形状和参数个数的；当创建自定义模型并发现图层预期的维度与其实际接收的维度不匹配时，这将非常有用

现在让我们继续训练数据。根据您的设置，这可能需要几秒到几分钟的时间。如果没有 cuda，预计这将需要更长的时间。

运行图 [3-45](#Fig45) 中的代码。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig45_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig45_HTML.png)

图 3-45

用于训练模型并打印测试集的准确度和损失值的代码

变量 checkpoint 将以 keras_MNIST_CNN.h5 的名称将模型存储在与此代码相同的文件夹中。如果您不想保存模型，请改为运行图 [3-46](#Fig46) 中的代码。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig46_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig46_HTML.png)

图 3-46

如果您不想保存模型，请运行以下代码

如果成功，您应该会看到类似图 [3-47](#Fig47) 的内容。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig47_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig47_HTML.jpg)

图 3-47

运行训练函数的输出，伴随着测试集的损失和准确度值

让我们检查一下这个的 AUC 分数。运行图 [3-48](#Fig48) 中的代码。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig48_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig48_HTML.png)

图 3-48

基于测试集生成此模型的 AUC 分数的代码

基本上，变量预测是一个包含 10 个元素的数组列表，每个元素包含每个`x_test`数据样本的类预测的概率值。

要在进行`np.round()`之前检查预测值，运行图 [3-49](#Fig49) 中的代码，并查看图 [3-50](#Fig50) 中的结果。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig50_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig50_HTML.jpg)

图 3-50

运行图 [3-49](#Fig49) 中代码的输出

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig49_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig49_HTML.jpg)

图 3-49

在对预测进行舍入之前，编写代码来查看预测的实际情况

除了正确预测的类别之外，其他每个类别的预测数据值都非常小，因此对它们进行四舍五入是没有意义的。AUC 评分如图 [3-51](#Fig51) 所示。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig51_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig51_HTML.jpg)

图 3-51

为模型生成的 AUC 分数。这是运行图 [3-48](#Fig48) 中代码的输出

这是一个非常好的 AUC 分数！这个分数表明这个模型真的很擅长识别手写数字，只要它们的格式与你在训练中使用的 MNIST 数据集相似。

回到卷积层，让我们运行一些代码，看看与原始图像相比，前两个卷积层后的特征图是什么样子。

运行图 [3-52](#Fig52) 中的代码，查看图 [3-53](#Fig53) 中的输出。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig53_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig53_HTML.jpg)

图 3-53

运行图 [3-52](#Fig52) 中代码的输出

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig52_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig52_HTML.png)

图 3-52

生成图像在模型不同阶段的图形的代码

当图像通过卷积层时，它的尺寸变小，图案变得更加明显。虽然对我们来说，这可能看起来不太像 3，但该模型从原始图像中识别出这些模式，并以此为基础进行预测。

所以现在你对 CNN 有了更好的理解，以及如何使用 Keras 来轻松创建和训练你自己的深度神经网络。如果您想进一步了解该框架，请随时查看附录 A。如果您有任何进一步的问题，或者想了解附录 A 之外的 Keras，请查看 Keras 官方文档。

## PyTorch 简介:一个简单的分类器模型

现在您对 CNN 是什么以及 Keras 中的分类器模型是什么样子有了更好的了解，让我们直接进入用 PyTorch 实现 CNN。

PyTorch 不像 Keras 那样抽象所有东西，所以涉及到更多的语法。如果您想进一步探索这个框架，请查看附录 B，其中我们介绍了 PyTorch 的基础知识及其功能，并将其应用于您将在第 [7](7.html) 章中探索的模型。

然而，就像在 Keras 中一样，首先要导入必要的模块并定义超参数(图 [3-54](#Fig54) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig54_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig54_HTML.png)

图 3-54

导入所需模块并定义运行 PyTorch 的设备(CPU 或 GPU)的代码

在 PyTorch 中，如果 GPU 存在，您必须指定要使用它。在 Keras 中，由于您使用 tensorflow-gpu 作为后端(Keras 在其上运行)，因此预计您已经安装了 gpu、CUDA 和 cuDNN。

现在配置您的超参数(图 [3-55](#Fig55) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig55_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig55_HTML.png)

图 3-55

定义要使用的超参数的代码

在本例中，您将尽 PyTorch 所允许的，尽可能地匹配 Keras 示例中使用的模型架构。TensorFlow 和 PyTorch 之间并不是每个函数都是等价的，但绝大多数是等价的。

现在创建您的测试和训练数据集(图 [3-56](#Fig56) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig56_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig56_HTML.png)

图 3-56

使用 PyTorch 的特性 DataLoaders 来获取训练和测试数据

在 PyTorch 中加载 MNIST 数据的过程可能有所不同，使用数据加载器而不是数据框，但是在将数据框、数组等转换为张量后，您仍然可以在 PyTorch 中使用它们。该过程通常是将数据帧转换为 numpy 数组，然后转换为 PyTorch 张量。

让我们继续创建您的模型(图 [3-57](#Fig57) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig57_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig57_HTML.png)

图 3-57

PyTorch 中卷积神经网络的创建

该程序与 Keras 中的略有不同。在本例中，主要层定义在`__init__`下，即两个卷积层和两个密集层。其余层在`forward()`下定义。在`forward()`中，你设置 x 等于第一个卷积层的激活函数的输出。这个新的 x 现在是下一个卷积层的输入，你设置 x 等于第二个卷积层的激活函数的输出。同样的过程对其他层重复，但是确切的数据流可能有点混乱，所以图 [3-58](#Fig58) 显示了这个代码实际做什么的例子。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig58_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig58_HTML.jpg)

图 3-58

F.relu 是 f(x)，x 是训练数据，self.conv1 是第一个卷积层

x、self.conv1 和 F.relu 的原始输入可以如此显示。x 传入卷积层，该层的输出通过 ReLU 函数。然后你得到你的最终输出 X '(图 [3-59](#Fig59) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig59_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig59_HTML.jpg)

图 3-59

f(x)的输出现在是新的 x，基本上，x = f(x)。在这种情况下，输出 x '是新的 x

现在，X 是 X '，这个新的 X 被传递到下一层(图 [3-60](#Fig60) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig60_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig60_HTML.jpg)

图 3-60

新的 x 现在是下一个卷积层的新输入

相同的过程再次重复，除了 X 的新值(图 [3-61](#Fig61) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig61_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig61_HTML.jpg)

图 3-61

重复同样的过程，得到 x 的新值

现在你得到了新的输出 X’(图 [3-62](#Fig62) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig62_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig62_HTML.jpg)

图 3-62

你再一次把 x 重新定义为 x”。然后，该过程会针对网络中的任意多个层继续进行

这个新的输出 X″然后是 X 的新值，并且该过程继续。

这是代码其余部分背后的相同逻辑，旧的激活层的输出现在是 x 的新定义。这个新的 x 然后进入下一层，在通过一层后应用一个函数，然后该数据成为 x 的新定义，依此类推。

因此

```py
x = x.view(-1, 12*12*64)

```

执行与 Keras 示例中展平层相同的功能。

现在你可以继续训练你的数据了(图 [3-63](#Fig63) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig63_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig63_HTML.png)

图 3-63

初始化模型、损失函数和优化器，然后开始训练过程

这可能需要一段时间，但您应该会看到类似图 [3-64](#Fig64) 的内容。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig64_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig64_HTML.jpg)

图 3-64

培训过程的输出

训练完成后，您可以测试您的模型并找到 AUC 分数(图 [3-65](#Fig65) )。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig65_HTML.png](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig65_HTML.png)

图 3-65

评估模型并生成 AUC 分数的代码

结果输出如图 [3-66](#Fig66) 所示。

![../images/483137_1_En_3_Chapter/483137_1_En_3_Fig66_HTML.jpg](../images/483137_1_En_3_Chapter/483137_1_En_3_Fig66_HTML.jpg)

图 3-66

测试集上生成的准确性分数和模型的 AUC 分数

现在你可以更多地了解如何在 PyTorch 中创建和训练你自己的 CNN。PyTorch 比 Keras 难学一点，Keras 的目标是让一切都变得可读和简单，抽象出所有更复杂的代码。TensorFlow 和 PyTorch 都是低级 API，由于缺乏抽象，需要编写更多的代码，但在控制一切方面提供了更大的灵活性。在这两者之间，如果您使用 PyCharm 中的调试工具，PyTorch 更容易调试。尽管 TensorFlow 和 PyTorch 都在较大的数据集上运行得更快，但最终都是个人喜好的问题。

如果您想进一步探索 PyTorch，请查看附录 B，其中我们介绍了一种更精确的创建模型、训练和测试的方法，以及 PyTorch 提供的一般功能。附录 B 还将 PyTorch 应用于第 [7](7.html) 章中的模型，这些模型是在 Keras 中完成的。

如果您想在访问附录 B 后了解更多关于 PyTorch 的信息，请查阅 PyTorch 官方文档。

## 摘要

近年来，深度学习已经彻底改变了许多领域。由于深度学习，我们现在有了自动驾驶汽车，在检测某些癌症方面击败了专业人士的模型，语言之间的即时翻译等。因此，深度学习也对异常检测领域做出了巨大贡献也就不足为奇了。

在本章中，我们讨论了什么是深度学习，什么是人工神经网络。您探索了两个流行的框架，Keras 和 PyTorch，通过将它们应用于 MNIST 数据集的图像分类任务。

在接下来的章节中，我们将看看以下类型的深度学习模型在异常检测中的应用:**自动编码器**、**受限波尔兹曼机器、RNN/LSTM** 网络和**时间卷积网络**。

在下一章中，我们将看看使用**自动编码器**的**无监督异常检测**。