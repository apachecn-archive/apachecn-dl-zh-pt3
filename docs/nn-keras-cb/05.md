        

# 五、迁移学习

在上一章中，我们学习了在给定的图像中识别图像所属的类别。在这一章中，我们将了解 CNN 的一个缺点，以及我们如何使用某些预先训练好的模型来克服它。

在本章中，我们将介绍以下配方:

*   使用 CNN 对图像中的人进行性别分类
*   使用基于 VGG16 架构的模型对图像中的人进行性别分类
*   可视化神经网络中间层的输出
*   使用基于 VGG19 架构的模型对图像中的人进行性别分类
*   使用基于 ResNet 架构的模型对 a 进行性别分类
*   使用基于 inception 架构的模型对 a 进行性别分类
*   检测人脸图像中的关键点

        

# 使用 CNN 对图像中的人进行性别分类

为了理解 CNN 的一些局限性，让我们看一个例子，在这个例子中，我们试图识别给定的图像是否包含猫或狗的图像。

        

# 做好准备

通过以下步骤，我们将直观地了解 CNN 如何预测图像中出现的对象类别:

*   卷积滤波器由图像的某些部分激活:
    *   例如，如果图像具有特定的图案，例如包含圆形结构，则特定的滤镜可能会激活
*   池层确保图像转换得到处理:
    *   这确保了即使图像很大，随着池化操作次数的增加，图像的大小也会变小，然后可以检测到对象，因为现在期望对象位于图像的较小部分中(因为它被多次池化)
*   最终的展平图层展平通过各种卷积和池化操作提取的所有图案

让我们假设一个场景，其中训练数据集中的图像数量很少。在这种情况下，模型没有足够的数据点来对测试数据集进行归纳。

此外，假设卷积从零开始学习各种特征，如果训练数据集包含具有较大形状(宽度和高度)的图像，则在模型开始适合训练数据集的顶部之前，可能需要许多时期。

因此，在下一节中，我们将编写以下构建 CNN 的场景，其中有几幅图像(约 1，700 幅图像)，并在不同形状的图像上测试准确性:

*   图像尺寸为 300 X 300 时，10 个时期内的精度
*   图像尺寸为 50 X 50 时，10 个时期内的精度

        

# 怎么做...

在本节中，我们将获取一个数据集并执行分类分析，其中一个场景中的图像大小为 300 x 300，而另一个场景中的图像大小为 50 x 50。(代码实现时请参考 GitHub 中的`Transfer_learning.ipynb`文件。)

        

# 场景 1–大图像

1.  获取数据集。对于这一分析，我们将继续使用我们在第四章、*的[性别分类案例研究中下载的男性与女性分类数据集，构建深度卷积神经网络](750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml)*:

```py
$ wget https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2017/04/a943287.csv 
```

```py
import pandas as pd, numpy as np
from skimage import io
# Location of file is /content/a943287.csv
# be sure to change to location of downloaded file on your machine
data = pd.read_csv('/content/a943287.csv')
```

```py
data_male = data[data['please_select_the_gender_of_the_person_in_the_picture']=="male"].reset_index(drop='index')
data_female = data[data['please_select_the_gender_of_the_person_in_the_picture']=="female"].reset_index(drop='index')
final_data = pd.concat([data_male[:1000],data_female[:1000]],axis=0).reset_index(drop='index')
```

2.  提取图像路径，然后准备输入和输出数据:

```py
x = []
y = []
for i in range(final_data.shape[0]):
     try:
         image = io.imread(final_data.loc[i]['image_url'])
         if(image.shape==(300,300,3)):
             x.append(image)
             y.append(final_data.loc[i]['please_select_the_gender_of_the_person_in_the_picture'])
     except:
         continue
```

3.  这些图像的示例如下:

![](Images/45f228ee-b2bc-43e1-af97-beb22587efb2.png)

请注意，所有图像的大小都是 300 x 300 x 3。

4.  创建输入和输出数据集数组:

```py
x2 = []
y2 = []
for i in range(len(x)):
      x2.append(x[i])
      img_label = np.where(y[i]=="male",1,0)
      y2.append(img_label)
```

在前面的步骤中，我们循环遍历所有的图像(一次一个)，将图像读入一个数组(在这次迭代中，如果没有这一步，我们可能已经成功了。但是，在下一个调整图像大小的场景中，我们将在这一步中调整图像的大小。此外，我们存储每个图像的标签。

5.  准备输入数组，以便可以将它传递给 CNN。此外，准备输出数组:

```py
x2 = np.array(x2)
x2 = x2.reshape(x2.shape[0],x2.shape[1],x2.shape[2],3)
```

这里，我们将数组列表转换成一个 numpy 数组，这样它就可以传递给神经网络。

缩放输入数组并创建输入和输出数组:

```py
X = np.array(x2)/255
Y = np.array(y2)
```

6.  创建训练和测试数据集:

```py
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.1, random_state=42)
```

7.  定义模型并编译它:

```py
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Conv2D
from keras.layers.pooling import MaxPooling2D
from keras.optimizers import SGD
from keras import backend as K

model = Sequential()
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',input_shape=(300,300,3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(256, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(512, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(Flatten())
model.add(Dense(100, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
```

在前面的代码中，我们构建了一个具有多层卷积、池化和丢弃的模型。此外，我们通过展平层传递最终漏失的输出，然后在将隐藏层连接到输出层之前，将展平的输出连接到 512 节点隐藏层。

该模型的摘要如下:

![](Images/d240df6a-7253-411a-bfd8-ee5b457bb19b.png)

在下面的代码中，我们编译模型以减少二进制交叉熵损失，如下所示:

```py
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
```

8.  符合模型:

```py
history = model.fit(X_train, y_train, batch_size=32,epochs=10,verbose=1,validation_data = (X_test, y_test))
```

在前面的步骤中，您可以看到模型没有在增加的时期内进行训练，如下图所示(该图的代码与我们在[第 2 章](2846fd6b-a2c2-4ebe-b36a-17f061b4cfa4.xhtml)、*构建深度前馈神经网络*中的*缩放输入数据*部分看到的代码相同，可以在本章的 GitHub 资源库中找到):

![](Images/d79e7aad-4074-4f56-a06d-feaf5337835d.png)

在上图中，您可以看到模型几乎没有学到任何东西，因为损失变化不大。此外，准确率停留在 51%附近(这大致是原始数据集中男性和女性图像的分布)。

        

# 场景 2–较小的图像

在这种情况下，我们将修改模型中的以下内容:

*   输入图像大小:
    *   我们将把尺寸从 300 X 300 缩小到 50 X 50
*   模型架构:
    *   架构的结构与我们在场景 1**中看到的一样——大图**

1.  使用缩小的图像尺寸(50 X 50 X 3)的输入和输出标签创建数据集。为此，我们将从场景 1 的*步骤 4* 继续:

```py
import cv2
x2 = []
y2 = []
for i in range(len(x)):
  img = cv2.resize(x[i],(50,50))
  x2.append(img)
  img_label = np.where(y[i]=="male",1,0)
  y2.append(img_label)
```

2.  为训练、测试数据集创建输入和输出数组:

```py
x2 = np.array(x2)
x2 = x2.reshape(x2.shape[0],x2.shape[1],x2.shape[2],3)
X = np.array(x2)/255
Y = np.array(y2)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.1, random_state=42)
```

3.  构建和编译模型:

```py
model = Sequential()
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',input_shape=(50,50,3)))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(256, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(512, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(Flatten())
model.add(Dense(100, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
```

该模型的摘要如下:

![](Images/55212a46-0138-4e44-9c56-04382a0bad47.png)

4.  符合模型:

```py
history = model.fit(X_train, y_train, batch_size=32,epochs=10,verbose=1,validation_data = (X_test, y_test))
```

在增加的时期内，跨训练和测试数据集的模型训练的准确性和损失如下:

![](Images/46cf59b8-b7ce-4522-8730-c564a321597a.png)

请注意，虽然最初在训练和测试数据集中准确性增加，损失稳步下降，但随着时间的增加，模型开始过度拟合(专门化)训练数据，在测试数据集上的准确性约为 76%。

由此，我们可以看到，当输入大小较小时，CNN 工作，因此滤波器必须从图像的较小部分学习。然而，随着图像尺寸的增加，CNN 经历了一段艰难的学习时间。

鉴于我们已经发现图像大小对模型准确性有影响，在新的场景中，让我们使用积极的池来确保较大的图像(300 x 300 形状)快速缩小到较小的图像。

        

# 场景 3–大图像上的积极池化

在下面的代码中，我们将保留我们已经完成的分析，直到场景 1 中的步骤 6。然而，唯一的变化将是模型架构；在下面的模型架构中，我们使用了比场景 1 中更激进的池。

在下面的架构中，与池大小较低的场景相比，在每层中使用较大的池窗口可以确保我们在更大的区域中捕获激活。该模型的架构如下:

```py
model = Sequential()
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',input_shape=(300,300,3)))
model.add(MaxPooling2D(pool_size=(3, 3)))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(3, 3)))
model.add(Conv2D(256, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(3, 3)))
model.add(Conv2D(512, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(Flatten())
model.add(Dense(100, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
```

请注意，在此体系结构中，池大小是 3 x 3，而不是 2 x 2，就像我们在前面的场景中一样:

![](Images/754dcf89-b23c-4d37-adec-f09749e89b81.png)

一旦我们在输入和输出阵列上拟合了模型，训练和测试数据集上的精度变化和损失如下:

```py
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
history = model.fit(X_train, y_train, batch_size=32,epochs=10,verbose=1,validation_data = (X_test, y_test))
```

以下是上述代码的输出:

![](Images/97fe1ba4-eb37-476a-9328-3b3f225d9952.png)

我们可以看到，测试数据对图像中的性别进行正确分类的准确率约为 70%。

但是，您可以看到在训练数据集上有相当多的过度拟合(因为在训练数据集上损失稳步下降，而在测试数据集上则没有)。

        

# 使用基于 VGG16 架构的模型对图像中的人进行性别分类

在前面关于使用 CNN 进行性别分类的章节中，我们看到，当我们从头开始构建 CNN 模型时，我们可能会遇到以下一些情况:

*   传递的图像数量不足以让模型学习
*   当图像很大时，卷积可能无法学习图像中的所有特征

第一个问题可以通过对大型数据集进行分析来解决。第二个问题可以通过在更大的数据集上训练更大的网络来解决。

然而，尽管我们能够执行所有这些操作，但我们往往没有执行此类分析所需的大量数据。在这种情况下，使用预先训练好的模型进行迁移学习可以解决问题。

ImageNet 是一项流行的竞赛，参与者被要求预测图像的各种类型，其中图像具有各种大小，并且还包含多种类型的对象。

有多个研究团队在这场比赛中竞争，以提出一个能够预测多个类别图像的模型，其中一个数据集中有数百万张图像。假设有数百万张图像，有限数据集的第一个问题就解决了。此外，鉴于研究团队已经建立了巨大的网络，提出学习各种特征的卷积的问题也得到解决。

因此，我们可以重用在不同数据集上构建的卷积，其中卷积正在学习预测图像中的各种特征，然后将它们通过隐藏层，以便我们可以预测特定数据集的图像类别。不同的团队开发了多种预训练模型。我们将在这里浏览 VGG16。

        

# 做好准备

在本节中，让我们试着了解如何利用 VGG16 预训练网络进行性别分类练习。

VGG16 型号的架构如下:

![](Images/c55e5a9b-e05e-4c31-9943-aeb2a6ae4c83.png)

请注意，该模型的架构与我们在“使用 CNN 进行性别分类”一节中训练的模型非常相似。主要区别是这个模型更深(更多隐藏层)。此外，VGG16 网络的权重是通过对数百万幅图像进行训练而获得的。

我们将确保在训练我们的模型对图像中的性别进行分类时，VGG16 权重不会更新。在性别分类练习中传递图像的输出(其形状为 300×300×3)在形状上是 9×9×512。

我们将保持原始网络中的权重，提取 9×9×512 的输出，将其通过另一个卷积池操作，将其展平，将其连接到一个隐藏层，然后将其通过 sigmoid 激活以确定图像是男性还是女性。

本质上，通过使用 VGG16 模型的卷积和池层，我们使用了在更大的数据集上训练的过滤器。最终，我们将为我们试图预测的对象微调这些卷积和池层的输出。

        

# 怎么做...

有了这个策略，让我们按如下方式编写我们的解决方案(在实现代码时，请参考 GitHub 中的`Transfer_learning.ipynb`文件):

1.  导入预训练模型:

```py
from keras.applications import vgg16
from keras.utils.vis_utils import plot_model
from keras.applications.vgg16 import preprocess_input
vgg16_model = vgg16.VGG16(include_top=False, weights='imagenet',input_shape=(300,300,3))
```

请注意，我们排除了 VGG16 模型中的最后一层。这是为了确保我们针对我们试图解决的问题对 VGG16 模型进行微调。此外，假设我们的输入图像形状是 300 X 300 X 3，我们在下载 VGG16 模型时指定相同的形状。

2.  预处理该组图像。该预处理步骤确保以预训练模型可以作为输入的方式处理图像。例如，在下面的代码中，我们对其中一个名为`img`的图像进行预处理:

```py
from keras.applications.vgg16 import preprocess_input
img = preprocess_input(img.reshape(1,224,224,3))
```

我们按照 VGG16 中的预处理要求，使用`preprocess_input`方法对图像进行预处理。

3.  创建输入和输出数据集。在这个练习中，我们将从使用 CNN 的性别分类场景 1 的步骤 3 的结尾继续。在这里，创建输入和输出数据集的过程与我们已经完成的过程相同，只是对使用 VGG16 模型提取要素进行了微小的修改。

我们将通过`vgg16_model`传递每个图像，这样我们将把`vgg16_model`的输出作为处理后的输入。此外，我们将对输入进行预处理，如下所示:

```py
import cv2
x2_vgg16 = []
for i in range(len(x)):
    img = x[i]
    img = preprocess_input(img.reshape(1,300,300,3))
```

现在，我们将预处理后的输入传递给 VGG16 模型以提取特征，如下所示:

```py
    img_new = vgg16_model.predict(img.reshape(1,300,300,3))
    x2_vgg16.append(img_new)
```

在前面的代码中，除了通过 VGG16 模型传递图像之外，我们还将输入值存储在一个列表中。

4.  将输入和输出转换为 NumPy 数组，并创建训练和测试数据集:

```py
x2_vgg16 = np.array(x2_vgg16)
x2_vgg16= x2_vgg16.reshape(x2_vgg16.shape[0],x2_vgg16.shape[2],x2_vgg16.shape[3],x2_vgg16.shape[4])
Y = np.array(y2)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x2_vgg16,Y, test_size=0.1, random_state=42)
```

5.  构建和编译模型:

```py
model_vgg16 = Sequential()
model_vgg16.add(Conv2D(512, kernel_size=(3, 3), activation='relu',input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3])))
model_vgg16.add(MaxPooling2D(pool_size=(2, 2)))
model_vgg16.add(Flatten())
model_vgg16.add(Dense(512, activation='relu'))
model_vgg16.add(Dropout(0.5))
model_vgg16.add(Dense(1, activation='sigmoid'))
model_vgg16.summary()
```

该模型的摘要如下:

![](Images/05928ce6-c892-492b-87f5-bdb7ba98730c.png)

编译模型:

```py
model_vgg16.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
```

6.  缩放输入数据时拟合模型:

```py
history_vgg16 = model_vgg16.fit(X_train/np.max(X_train), y_train, batch_size=16,epochs=10,verbose=1,validation_data = (X_test/np.max(X_train), y_test))
```

一旦我们拟合了模型，我们应该看到我们能够在前几个时期在测试数据集上获得大约 89%的准确度:

![](Images/1d5465bb-b9da-4905-a985-3d0a7c1d6b23.png)

相比之下，我们在使用 CNN 的性别分类部分建立的模型，在任何情况下，我们都无法在 10 个时期内达到 80%的分类准确率。

模型错误分类的一些图像示例如下:

![](Images/fcb824ca-4791-478c-9d65-7d2f63cec26c.png)

请注意，在前面的图片中，当输入图像是人脸的一部分，或者如果图像中的对象只占整个图像的很小一部分，或者如果标签提供不正确，则模型可能会误分类。

        

# 可视化神经网络中间层的输出

在上一节中，我们建立了一个模型，它学习从图像中分类性别，准确率为 89%。然而，截至目前，就过滤器正在学习的内容而言，它对我们来说是一个黑匣子。

在本节中，我们将学习如何提取模型中各种过滤器正在学习的内容。此外，我们将对比初始层中的过滤器正在学习的内容和最后几层中的要素正在学习的内容。

        

# 做好准备

为了理解如何提取各种过滤器正在学习的内容，让我们采用以下策略:

*   我们将选择一个图像来执行分析。
*   我们将选择第一个卷积，以了解第一个卷积中的各种滤波器正在学习什么。
*   计算第一层和输入图像中的权重卷积的输出:
    *   在这一步中，我们将提取模型的中间输出:
        *   我们将提取模型第一层的输出。
*   为了提取第一层的输出，我们将使用函数式 API:
    *   功能 API 的输入是输入图像，输出将是第一层的输出。
*   这将返回中间层跨所有通道(过滤器)的输出。
*   我们将在卷积的第一层和最后一层执行这些步骤。
*   最后，我们将可视化所有通道卷积运算的输出。
*   我们还将可视化所有图像中给定通道的输出。

        

# 怎么做...

在这一节中，我们将编写可视化过程的代码，显示滤波器在初始层和最终层的卷积滤波器中学习的内容。

我们将从*步骤 1* 到*步骤 4* 使用 CNN 菜谱的场景 1 在*性别分类中准备的数据进行重用(执行代码时请参考 GitHub 中的`Transfer_learning.ipynb`文件):*

1.  确定要显示中间输出的图像:

```py
plt.imshow(x[3])
plt.grid('off')
```

![](Images/009c041f-b3c5-40f0-973f-726d4f649b44.png)

2.  定义将图像作为输入，将第一个卷积层的输出作为输出的函数 API:

```py
from keras.applications.vgg16 import preprocess_input
model_vgg16.predict(vgg16_model.predict(preprocess_input(x[3].reshape(1,300,300,3)))/np.max(X_train))
from keras import models
activation_model = models.Model(inputs=vgg16_model.input,outputs=vgg16_model.layers[1].output)
activations = activation_model.predict(preprocess_input(x[3].reshape(1,300,300,3)))
```

我们已经定义了一个名为`activation_model`的中间模型，其中我们将感兴趣的图像作为输入传递，并提取第一层的输出作为模型的输出。

一旦我们定义了模型，我们将通过模型传递输入图像来提取第一层的激活。请注意，我们将不得不改变输入图像的形状，使其成为模型所期望的形状。

3.  让我们来看看输出中的前 36 个过滤器，如下所示:

```py
fig, axs = plt.subplots(6, 6, figsize=(10, 10))
fig.subplots_adjust(hspace = .5, wspace=.5)
first_layer_activation = activations[0]
for i in range(6):
  for j in range(6):
    try:
      axs[i,j].set_ylim((224, 0))
      axs[i,j].contourf(first_layer_activation[:,:,((6*i)+j)],6,cmap='viridis')
      axs[i,j].set_title('filter: '+str((6*i)+j))
      axs[i,j].axis('off')
    except:
      continue
```

4.  在前面的代码中，我们创建了一个 6 x 6 的帧，可以在其上绘制 36 幅图像。此外，我们循环通过`first_layer_activation`中的所有通道，并绘制第一层的输出，如下所示:

![](Images/ae741c21-6feb-427e-abbf-692edf9c8774.png)

在这里，我们可以看到某些滤波器提取原始图像的轮廓(例如，滤波器 0，4，7，10)。此外，某些过滤器已经学会仅识别几个方面，例如耳朵、眼睛和鼻子(例如，过滤器 30)。

5.  让我们验证我们的理解，即某些滤波器能够通过遍历 36 幅图像的滤波器 7 的输出来提取原始图像的轮廓，如下所示:

```py
activation_model = models.Model(inputs=vgg16_model.input,outputs=vgg16_model.layers[1].output)
activations = activation_model.predict(preprocess_input(np.array(x[:36]).reshape(36,300,300,3)))
fig, axs = plt.subplots(6, 6, figsize=(10, 10))
fig.subplots_adjust(hspace = .5, wspace=.5)
first_layer_activation = activations
for i in range(6):
  for j in range(6):
    try:
      axs[i,j].set_ylim((224, 0))
      axs[i,j].contourf(first_layer_activation[((6*i)+j),:,:,7],6,cmap='viridis')
      axs[i,j].set_title('filter: '+str((6*i)+j))
      axs[i,j].axis('off')
    except:
      continue
```

在前面的代码中，我们循环遍历前 36 个图像，并绘制所有 36 个图像的第一个卷积层的输出:

![](Images/1d4d36d6-33c2-45aa-9a20-ace2c48fa515.png)

注意，在所有图像中，第七个过滤器正在学习图像中的轮廓。

6.  我们来试着了解一下最后一个卷积层的滤镜在学习什么。为了理解最后一个卷积层在模型中的位置，让我们提取模型中的各个层:

```py
for i, layer in enumerate(model.layers):
     print(i, layer.name)
```

通过执行上述代码，将显示以下图层名称:

![](Images/2eec4bdc-3084-42c3-bb48-c6718498c6d0.png)

7.  请注意，最后一个卷积层是我们模型的第九个输出，可以提取如下:

```py
activation_model = models.Model(inputs=vgg16_model.input,outputs=vgg16_model.layers[-1].output)
activations = activation_model.predict(preprocess_input(x[3].reshape(1,300,300,3)))
```

由于在映像上执行了多次池化操作，映像的大小现在已经大幅缩小(缩小到 1，9，9，512)。最后一个卷积层中的各种过滤器正在学习的可视化如下:

![](Images/a9768459-b4fe-4a48-9f37-be7dcaf450fb.png)

注意，在该迭代中，不清楚理解最后卷积层的滤波器正在学习什么(因为轮廓不容易归因于原始图像的一部分)，因为这些比在第一卷积层中学习的轮廓更精细。

        

# 使用基于 VGG19 架构的模型对图像中的人进行性别分类

在上一节中，我们了解了 VGG16 的工作原理。VGG19 是 VGG16 的改进版本，具有更多的卷积和池操作。

        

# 做好准备

VGG19 模型的架构如下:

![](Images/6d17d7c6-4df1-4ad0-8e47-16e93e90cadd.png)

请注意，前面的架构有更多的层，以及更多的参数。

请注意，VGG16 和 VGG19 架构中的 16 和 19 代表每个网络中的层数。在我们通过 VGG19 网络传递每个图像后，一旦我们提取 9 x 9 x 512 输出，该输出将成为我们模型的输入。

此外，创建输入和输出数据集，然后构建、编译和拟合模型的过程将与我们在使用基于 VGG16 模型的架构配方的性别分类中看到的过程保持一致。

        

# 怎么做...

在本节中，我们将对 VGG19 预训练模型进行编码，如下所示(实现代码时请参考 GitHub 中的`Transfer_learning.ipynb`文件):

1.  准备输入和输出数据(我们将从使用 CNN 方法的*性别分类的场景 1 中的*步骤 3* 开始继续):*

```py
import cv2
x2 = []
for i in range(len(x)):
    img = x[i]
    img = preprocess_input(img.reshape(1,300,300,3))
    img_new = vgg19_model.predict(img.reshape(1,300,300,3))
    x2.append(img_new)
```

2.  将输入和输出转换为相应的数组，并创建训练和测试数据集:

```py
x2 = np.array(x2)
x2= x2.reshape(x2.shape[0],x2.shape[2],x2.shape[3],x2.shape[4])
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x2,Y, test_size=0.1, random_state=42)
```

3.  构建和编译模型:

```py
model_vgg19 = Sequential()
model_vgg19.add(Conv2D(512, kernel_size=(3, 3), activation='relu',input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3])))
model_vgg19.add(MaxPooling2D(pool_size=(2, 2)))
model_vgg19.add(Flatten())
model_vgg19.add(Dense(512, activation='relu'))
model_vgg19.add(Dropout(0.5))
model_vgg19.add(Dense(1, activation='sigmoid'))
model_vgg19.summary()
```

该模型的可视化如下:

![](Images/62ec64d6-36d0-41c4-9855-8692eb6c3594.png)

```py
model_vgg19.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
```

4.  缩放输入数据时拟合模型:

```py
history_vgg19 = model_vgg19.fit(X_train/np.max(X_train), y_train, batch_size=16,epochs=10,verbose=1,validation_data = (X_test/np.max(X_train), y_test))
```

让我们绘制训练和测试数据集的损失和准确性度量:

![](Images/42a19290-8e44-4746-92c9-acc35015a8f5.png)

我们应该注意，当我们使用 VGG19 架构时，我们能够在测试数据集上实现大约 89%的准确性，这与 VGG16 架构非常相似。

错误分类图像的示例如下:

![](Images/73d219f5-a4bd-412c-b609-880880dfe98e.png)

请注意，VGG19 似乎根据图像中人所占的空间进行了错误分类。此外，它似乎给出了更高的权重来预测长头发的男性是女性。

        

# 使用基于 Inception v3 架构的模型进行性别分类

在之前的配方中，我们基于 VGG16 和 VGG19 架构实现了性别分类。在这一部分中，我们将使用 Inception 架构来实现分类。

关于如何使用盗梦模型的直觉如下。

将会有物体占据图像大部分的图像。类似地，将会有物体占据整个图像的一小部分的图像。如果我们在两个场景中有相同大小的内核，我们会使模型难以学习-一些图像可能有较小的对象，而其他图像可能有较大的对象。

为了解决这个问题，我们将在同一层使用多种尺寸的过滤器。

在这种情况下，网络基本上会变宽而不是变深，如下所示:

![](Images/54cce575-4a32-480d-bedf-29f96bb1f82e.png)

在上图中，请注意我们正在给定层中执行多个过滤器的卷积。inception v1 模块有九个这样的线性堆叠的模块，如下所示:

![](Images/80aeb9f6-c85e-4033-83a1-15593f4127c7.png)

来源:http://joelouismmarino . github . io/images/blog _ images/blog _ Google net _ keras/Google net _ diagram . png

请注意，这个架构相当深，也相当宽。这很可能导致梯度消失的问题(正如我们在[第 2 章](2846fd6b-a2c2-4ebe-b36a-17f061b4cfa4.xhtml)、*构建深度前馈神经网络*中看到的批量标准化案例)。

为了解决渐变消失的问题，inception v1 有两个源自 inception 模块的辅助分类器。基于初始的网络试图最小化的总损失如下:

```py
total_loss = real_loss + 0.3 * aux_loss_1 + 0.3 * aux_loss_2
```

注意，辅助损失仅在训练期间使用，在预测过程中被忽略。

Inception v2 和 v3 是对 inception v1 架构的改进，在 v2 中，作者在卷积运算的基础上进行了优化，以更快地处理图像，而在 v3 中，作者在现有卷积的基础上添加了 7 x 7 卷积，以便它们可以连接在一起。

        

# 怎么做...

我们编写 inception v3 的过程与我们构建基于 VGG19 模型的分类器的方式非常相似(实现代码时请参考 GitHub 中的`Transfer_learning.ipynb`文件):

1.  下载预培训的初始模型:

```py
from keras.applications import inception_v3
from keras.applications.inception_v3 import preprocess_input
from keras.utils.vis_utils import plot_model
inception_model = inception_v3.InceptionV3(include_top=False, weights='imagenet',input_shape=(300,300,3))
```

请注意，我们需要一个形状至少为 300 x 300 的输入图像，以便 inception v3 预训练模型能够工作。

2.  创建输入和输出数据集(我们将从使用 CNN 方法的*性别分类场景 1 的步骤 3 继续):*

```py
import cv2
x2 = []
for i in range(len(x)):
    img = x[i]
    img = preprocess_input(img.reshape(1,300,300,3))
    img_new = inception_model.predict(img.reshape(1,300,300,3))
    x2.append(img_new)
```

3.  创建输入和输出数组，以及训练和测试数据集:

```py
x2 = np.array(x2)
x2= x2.reshape(x2.shape[0],x2.shape[2],x2.shape[3],x2.shape[4])
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x2,Y, test_size=0.1, random_state=42)
```

4.  构建和编译模型:

```py
model_inception_v3 = Sequential()
model_inception_v3.add(Conv2D(512, kernel_size=(3, 3), activation='relu',input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3])))
model_inception_v3.add(MaxPooling2D(pool_size=(2, 2)))
model_inception_v3.add(Flatten())
model_inception_v3.add(Dense(512, activation='relu'))
model_inception_v3.add(Dropout(0.5))
model_inception_v3.add(Dense(1, activation='sigmoid'))
model_inception_v3.summary()

model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
```

前面的模型可以如下所示:

![](Images/e6af6840-948a-4ae5-81f1-54292efc2e7e.png)

5.  缩放输入数据时拟合模型:

```py
history_inception_v3 = model_inception_v3.fit(X_train/np.max(X_train), y_train, batch_size=16,epochs=10,verbose=1,validation_data = (X_test/np.max(X_train), y_test)) 
```

准确度和损耗值的变化如下:

![](Images/24d5c932-1897-4540-af2c-c99ea772fbd1.png)

您应该注意到，这种情况下的准确率也在 90%左右。

        

# 使用基于 ResNet 50 架构的模型对图像中的人进行性别分类

从 VGG16 到 VGG19，我们增加了层数，一般来说，神经网络越深，其准确性就越好。然而，如果仅仅增加层数是一个诀窍，那么我们可以继续增加更多的层(同时注意避免过度拟合)到模型中，以获得更准确的结果。

不幸的是，事实证明这不是真的，渐变消失的问题就出现了。随着层数的增加，梯度在穿过网络时变得如此之小，以至于难以调整权重，并且网络性能恶化。

ResNet 的出现就是为了解决这个特定的场景。

想象一个场景，如果模型没有什么要学习的，卷积层什么也不做，只是将前一层的输出传递给下一层。但是，如果模型需要学习其他一些特征，卷积层会将前一层的输出作为输入，并学习执行分类时需要学习的其他特征。

术语残差是期望模型从一层到下一层学习的附加特征。

典型的 ResNet 架构如下所示:

![](Images/4e8186c6-17ab-4d77-9351-bca12d65e3f5.jpg)

资料来源:https://arxiv.org/pdf/1512.03385.pdf

请注意，除了该网络中的传统卷积层之外，我们还跳过了将前一层连接到下一层的连接。

此外，ResNet50 中的 50 来自于网络中共有 50 层。

        

# 怎么做...

ResNet50 架构构建如下(实现代码时请参考 GitHub 中的`Transfer_learning.ipynb`文件):

1.  下载预培训的初始模型:

```py
from keras.applications import resnet50
from keras.applications.resnet50 import preprocess_input
resnet50_model = resnet50.ResNet50(include_top=False, weights='imagenet',input_shape=(300,300,3))
```

请注意，我们需要一个形状至少为 224 x 224 的输入图像，ResNet50 预训练模型才能工作。

2.  创建输入和输出数据集(我们将从使用 CNN 方法的*性别分类的场景 1 中的*步骤 3* 继续):*

```py
import cv2
x2 = []
for i in range(len(x)):
    img = x[i]
    img = preprocess_input(img.reshape(1,300,300,3))
    img_new = resnet50_model.predict(img.reshape(1,300,300,3))
    x2.append(img_new)
```

3.  创建输入和输出数组，以及训练和测试数据集:

```py
x2 = np.array(x2)
x2= x2.reshape(x2.shape[0],x2.shape[2],x2.shape[3],x2.shape[4])
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x2,Y, test_size=0.1, random_state=42)
```

4.  构建和编译模型:

```py
model_resnet50 = Sequential()
model_resnet50.add(Conv2D(512, kernel_size=(3, 3), activation='relu',input_shape=(X_train.shape[1],X_train.shape[2],X_train.shape[3])))
model_resnet50.add(MaxPooling2D(pool_size=(2, 2)))
model_resnet50.add(Conv2D(512, kernel_size=(3, 3), activation='relu'))
model_resnet50.add(MaxPooling2D(pool_size=(2, 2)))
model_resnet50.add(Flatten())
model_resnet50.add(Dense(512, activation='relu'))
model_resnet50.add(Dropout(0.5))
model_resnet50.add(Dense(1, activation='sigmoid'))
model_resnet50.summary()

model_resnet50.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
```

该模型的摘要如下:

![](Images/a10c5097-edd5-4d36-ac78-482437730971.png)

5.  缩放输入数据时拟合模型:

```py
history_resnet50 = model_resnet50.fit(X_train/np.max(X_train), y_train, batch_size=32,epochs=10,verbose=1,validation_data = (X_test/np.max(X_train), y_test))
```

准确度和损耗值的变化如下:

![](Images/69d8c163-2be3-4ace-90ef-5e3bcb0fb8b6.png)

注意，前面的模型给出了 92%的准确度。

多个预训练模型在性别分类上的准确性水平没有显著差异，因为它们可能被训练来提取一般特征，但不一定是用于性别分类的特征。

        

# 检测人脸图像中的关键点

在这份食谱中，我们将学习检测人脸的关键点，即左右眼的边界、鼻子和嘴的四个坐标。

下面是两张带有要点的样图:

![](Images/a1d5652b-4dc0-4a50-81d8-212f94cd0d87.png)

请注意，我们期望检测的关键点在此图中被标为点。在人脸图像上总共检测到 68 个关键点，其中人脸的关键点包括-嘴、右眉、左眉、右眼、左眼、鼻子、下巴。

在本案例研究中，我们将利用在使用基于 VGG16 架构的模型部分的*图像性别分类中学习到的 VGG16 迁移学习技术来检测面部的关键点。*

        

# 做好准备

对于关键点检测任务，我们将在一个数据集上工作，在该数据集上我们注释我们想要检测的点。对于这个练习，输入将是我们想要检测关键点的图像，输出将是关键点的 *x* 和 *y* 坐标。数据集可以从这里下载:【https://github.com/udacity/P1_Facial_Keypoints[。](https://github.com/udacity/P1_Facial_Keypoints)

我们将遵循的步骤如下:

1.  下载数据集
2.  将图像调整为标准形状
    1.  调整图像大小时，确保关键点被修改，以便它们代表修改后(调整大小后)的图像

3.  通过 VGG16 模型传递调整后的图像
4.  创建输入和输出数组，其中输入数组是通过 VGG16 模型传递图像的输出，输出数组是修改后的面部关键点位置
5.  拟合使预测的和实际的面部关键点之间的差异的绝对误差值最小化的模型

        

# 怎么做...

我们讨论的策略编码如下(实现代码时请参考 GitHub 中的`Facial_keypoints.ipynb`文件):

1.  下载并导入数据集:

```py
$ git clone https://github.com/udacity/P1_Facial_Keypoints.git import pandas as pddata = pd.read_csv('/content/P1_Facial_Keypoints/data/training_frames_keypoints.csv')
```

检查此数据集。

![](Images/fa5e1b6c-254b-4440-88d1-6829589e3257.png)

总共有 137 列，其中第一列是图像的名称，其余 136 列表示相应图像的 68 个面部关键点的 x 和 y 坐标值。

2.  预处理数据集以提取图像、调整大小的图像、图像的 VGG16 特征、修改的关键点位置作为输出:

初始化将被追加以创建输入和输出数组的列表:

```py
import cv2, numpy as np
from copy import deepcopy
x=[]
x_img = []
y=[]
```

循环浏览图像并阅读它们:

```py
for i in range(data.shape[0]):
     img_path = '/content/P1_Facial_Keypoints/data/training/' + data.iloc[i,0]
     img = cv2.imread(img_path)
```

捕捉关键点值并存储它们

```py
 kp = deepcopy(data.iloc[i,1:].tolist())
 kp_x = (np.array(kp[0::2])/img.shape[1]).tolist()
 kp_y = (np.array(kp[1::2])/img.shape[0]).tolist()
 kp2 = kp_x + kp_y
```

调整图像大小

```py
img = cv2.resize(img,(224,224))
```

对图像进行预处理，使其能够通过 VGG16 模型并提取特征:

```py
preprocess_img = preprocess_input(img.reshape(1,224,224,3))
 vgg16_img = vgg16_model.predict(preprocess_img)
```

将输入和输出值附加到相应的列表中:

```py
 x_img.append(img)
 x.append(vgg16_img)
 y.append(kp2)
```

创建输入和输出数组:

```py
x = np.array(x)
x = x.reshape(x.shape[0],7,7,512)
y = np.array(y)
```

3.  构建和编译模型

```py
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
model_vgg16 = Sequential()
model_vgg16.add(Conv2D(512, kernel_size=(3, 3), activation='relu',input_shape=(x.shape[1],x.shape[2],x.shape[3])))
model_vgg16.add(MaxPooling2D(pool_size=(2, 2)))
model_vgg16.add(Flatten())
model_vgg16.add(Dense(512, activation='relu'))
model_vgg16.add(Dropout(0.5))
model_vgg16.add(Dense(y.shape[1], activation='sigmoid'))
model_vgg16.summary()
```

![](Images/e415b00a-b498-468b-b3ff-023188db2dd0.png)

编译模型:

```py
model_vgg16.compile(loss='mean_absolute_error',optimizer='adam')
```

4.  符合模型

```py
history = model_vgg16.fit(x/np.max(x), y, epochs=10, batch_size=32, verbose=1, validation_split = 0.1)
```

请注意，我们将输入数组除以输入数组的最大值，以便缩放输入数据集。训练和测试损失在增加的时期内的变化如下:

![](Images/07513580-4052-4792-9a44-5b2731f5769b.png)

5.  在测试图像上预测。在下面的代码中，我们对输入数组中倒数第二个图像进行预测(注意，因为`validation_split`是`0.1`，所以在训练时倒数第二个图像没有提供给模型)。我们确保通过`preprocess_input`方法传递我们的图像，然后通过`VGG16_model`，最后将`VGG16_model`的缩放版本输出到我们构建的`model_vgg16`:

```py
pred = model_vgg16.predict(vgg16_model.predict(preprocess_input(x_img[-2].reshape(1,224,224,3)))/np.max(x))
```

对测试图像的前述预测可以被可视化如下:

![](Images/cee845d9-96bf-44a9-a7fa-5fa46fbeecda.png)

我们可以看到，在测试图像上，关键点被非常准确地检测出来。