

# 三、带有内容的深度学习

在前几章中，我们讨论了密集网络，其中每一层都与相邻层完全相连。我们应用这些密集网络对 MNIST 手写字符数据集进行分类。在这种情况下，输入图像中的每个像素被分配给总共 784(28×28 像素)个输入神经元中的一个神经元。然而，这种策略没有利用每个图像的空间结构和关系。具体来说，这段代码将表示每个书写数字的位图转换为平面向量，其中空间局部性消失了:

```py

#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784
X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)
o

```

卷积神经网络(也称为 ConvNet)利用空间信息，因此非常适合对图像进行分类。这些网络使用了一种特殊的架构，其灵感来自视觉皮层生理实验的生物数据。如前所述，我们的视觉基于多个皮层水平，每一层都识别越来越多的结构化信息。首先，我们看到单个像素；然后从它们身上，我们认识到简单的几何形式。然后...越来越复杂的元素，如物体、人脸、人体、动物等等。

卷积神经网络确实令人着迷。在很短的时间内，它们变成了一种颠覆性的技术，打破了多个领域的所有最新成果，从文本到视频，再到语音，远远超出了最初设想的图像处理领域。

在本章中，我们将讨论以下主题:

*   深度卷积神经网络
*   图像分类



# 深度卷积神经网络— DCNN

一个**深度卷积神经网络** ( **DCNN** )由许多神经网络层组成。卷积层和池化层这两种不同类型的层通常是交替出现的。在网络中，每个过滤器的深度从左到右增加。最后一级通常由一个或多个完全连接的层组成:

![](assets/B06258_04_01.png)

ConvNets 之外有三个关键的直觉:

*   局部感受野
*   共享权重
*   联营

我们来复习一下。



# 局部感受野

如果我们想保留空间信息，那么用像素矩阵来表示每个图像是很方便的。然后，编码局部结构的简单方法是将相邻输入神经元的子矩阵连接成属于下一层的单个隐藏神经元。这个隐藏的神经元代表一个局部感受域。请注意，这种操作被命名为卷积，它给出了这种网络的名称。

当然，我们可以通过重叠子矩阵来编码更多的信息。例如，让我们假设每个单个子矩阵的大小是 5×5，并且这些子矩阵用于 28×28 像素的 MNIST 图像。然后我们将能够在下一个隐藏层中生成 23×23 个局部感受野神经元。事实上，在接触图像的边界之前，子矩阵仅滑动 23 个位置是可能的。在 Keras 中，每个子矩阵的大小被称为**步长**，这是一个超参数，可以在我们构建网络的过程中进行微调。

让我们定义从一层到另一层的特征图。当然，我们可以有多个从每个隐藏层独立学习的特征地图。例如，我们可以从处理 MINST 图像的 28×28 个输入神经元开始，然后在下一个隐藏层中调用每个 23×23 个神经元大小的 *k* 特征图(同样步长为 5×5)。



# 共享权重和偏见

让我们假设我们想要通过获得独立于其在输入图像中的放置位置来检测相同特征的能力，来远离行中的像素表示。一个简单的直觉是对隐藏层中的所有神经元使用相同的权重和偏差集。这样，每一层将学习一组从图像导出的位置无关的潜在特征。

假设输入图像在具有 *tf* (TensorFlow)排序的三个通道上具有形状 *(256，256)* ，这被表示为 *(256，256，3)* 。注意，在 th (Theano)模式下，通道的尺寸(深度)位于索引*1*；在 *tf* (TensoFlow)模式下，位于索引 *3* 。

在 Keras 中，如果我们想要添加一个卷积层，其具有输出 32 的维度和每个滤波器 3×3 的扩展，我们将写:

```py

model = Sequential()
model.add(Conv2D(32, (3, 3), input_shape=(256, 256, 3))

```

或者，我们会写:

```py

model = Sequential()
model.add(Conv2D(32, kernel_size=3, input_shape=(256, 256, 3))

```

这意味着我们正在对一个 256 x 256 的图像应用一个 3 x 3 的卷积，该图像有三个输入通道(或输入滤波器)，产生 32 个输出通道(或输出滤波器)。

下图提供了卷积的一个示例:

![](assets/B06258_04_02.png)

# 池层

让我们假设我们想要总结一个特征图的输出。同样，我们可以使用从单个特征地图生成的输出的空间邻接性，并将子矩阵的值聚合到单个输出值中，该输出值综合描述了与该物理区域相关联的*含义*。



# 最大池化

一个简单而常见的选择是 *max-pooling* ，它简单地输出在该区域观察到的最大激活。在 Keras 中，如果我们想要定义一个大小为 2 x 2 的最大池层，我们将编写:

```py

model.add(MaxPooling2D(pool_size = (2, 2)))

```

下图显示了最大池的一个示例:

![](assets/B06258_04_03.png)

# 平均池

另一种选择是平均池，它简单地将一个区域聚合成该区域中观察到的激活的平均值。

请注意，Keras 实现了大量的池层，完整的列表可从:[https://keras.io/layers/pooling/](https://keras.io/layers/pooling/)获得。简而言之，所有的池操作只不过是对给定区域的汇总操作。



# 内容摘要

到目前为止，我们已经描述了 ConvNets 的基本概念。CNN 在一个维度上对音频和文本数据沿时间维度应用卷积和池化操作，在两个维度上对图像沿(高×宽)维度应用卷积和池化操作，在三个维度上对视频沿(高×宽×时间)维度应用卷积和池化操作。对于图像，在输入体积上滑动过滤器会产生一个图，该图给出了过滤器对于每个空间位置的响应。换句话说，一个 ConvNet 具有多个堆叠在一起的过滤器，这些过滤器学习识别特定的视觉特征，而与图像中的位置无关。这些视觉特征在网络的初始层是简单的，然后在网络的更深处变得越来越复杂。



# DCNN — LeNet 的一个例子

Yann le Cun 提出(关于更多信息，参考 Y. LeCun 和 Y. Bengio 的《用于图像、语音和时间序列的卷积网络》, brain theory neural Networks，vol. 3361，1995)一族名为 LeNet 的卷积网络，其被训练用于识别对简单几何变换和失真具有鲁棒性的 MNIST 手写字符。这里的关键直觉是让低层交替卷积运算和最大池运算。卷积运算基于精心选择的局部感受野，具有多个特征图的共享权重。然后，更高的级别是基于具有隐藏层和 softmax 作为输出层的传统 MLP 的完全连接的层。



# Keras 中的 LeNet 代码

为了定义 LeNet 码，我们使用卷积 2D 模块，它是:

```py

keras.layers.convolutional.Conv2D(filters, kernel_size, padding='valid')

```

这里，`filters`是要使用的卷积核的数量(例如，输出的维数)，`kernel_size`是整数或两个整数的元组/列表，指定 2D 卷积窗口的宽度和高度(可以是单个整数，以指定所有空间维度的相同值)，`padding='same'`表示使用填充。有两个选项:`padding='valid'`意味着卷积只在输入和滤波器完全重叠的地方计算，因此输出小于输入，而`padding='same'`意味着我们有一个与输入大小相同的*输出，输入周围的区域用零填充。*

此外，我们使用一个`MaxPooling2D`模块:

```py

keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=(2, 2))

```

这里，`pool_size=(2, 2)`是两个整数的元组，表示图像被垂直和水平缩小的因子。所以 *(2，2)* 会在每个维度上对图像进行二等分，`strides=(2, 2)`是用于处理的步幅。

现在，让我们回顾一下代码。首先，我们导入一些模块:

```py

from keras import backend as K
from keras.models import Sequential
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dense
from keras.datasets import mnist
from keras.utils import np_utils
from keras.optimizers import SGD, RMSprop, Adam
import numpy as np
import matplotlib.pyplot as plt

```

然后我们定义 LeNet 网络:

```py

#define the ConvNet
class LeNet:
    @staticmethod
    def build(input_shape, classes):
         model = Sequential()
         # CONV => RELU => POOL

```

我们有一个 ReLU 激活的第一卷积阶段，随后是最大池。我们的网络将学习 20 个卷积滤波器，每个滤波器的大小为 5×5。输出尺寸与输入形状的尺寸相同，因此它将是 28 x 28。注意，由于`Convolution2D`是我们管道的第一级，我们还需要定义它的`input_shape`。max-pooling 操作实现了在层上滑动的滑动窗口，并以垂直和水平两个像素的步长取每个区域的最大值:

```py

model.add(Convolution2D(20, kernel_size=5, padding="same",
input_shape=input_shape))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))
# CONV => RELU => POOL

```

接着是具有 ReLU 激活的第二卷积阶段，同样是通过最大池。在这种情况下，我们将学习的卷积滤波器数量从之前的 20 个增加到 50 个。在更深的层中增加过滤器的数量是深度学习中常用的技术:

```py

model.add(Conv2D(50, kernel_size=5, border_mode="same"))
model.add(Activation("relu"))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))

```

然后，我们有一个非常标准的扁平化和一个 500 个神经元的密集网络，后面是一个 softmax 分类器，有 10 个类别:

```py

# Flatten => RELU layers
model.add(Flatten())
model.add(Dense(500))
model.add(Activation("relu"))
# a softmax classifier
model.add(Dense(classes))
model.add(Activation("softmax"))
return model

```

恭喜你，你刚刚定义了第一个深度学习网络！让我们看看它的视觉效果:

![](assets/B06258_04_04.png)

现在我们需要一些额外的代码来训练网络，但这与我们已经在第一章、*神经网络基础*中描述的非常相似。这一次，我们还显示了打印损失的代码:

```py

# network and training
NB_EPOCH = 20
BATCH_SIZE = 128
VERBOSE = 1
OPTIMIZER = Adam()
VALIDATION_SPLIT=0.2
IMG_ROWS, IMG_COLS = 28, 28 # input image dimensions
NB_CLASSES = 10 # number of outputs = number of digits
INPUT_SHAPE = (1, IMG_ROWS, IMG_COLS)
# data: shuffled and split between train and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()
k.set_image_dim_ordering("th")
# consider them as float and normalize
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
# we need a 60K x [1 x 28 x 28] shape as input to the CONVNET
X_train = X_train[:, np.newaxis, :, :]
X_test = X_test[:, np.newaxis, :, :]
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')
# convert class vectors to binary class matrices
y_train = np_utils.to_categorical(y_train, NB_CLASSES)
y_test = np_utils.to_categorical(y_test, NB_CLASSES)
# initialize the optimizer and model
model = LeNet.build(input_shape=INPUT_SHAPE, classes=NB_CLASSES)
model.compile(loss="categorical_crossentropy", optimizer=OPTIMIZER,
metrics=["accuracy"])
history = model.fit(X_train, y_train,
batch_size=BATCH_SIZE, epochs=NB_EPOCH,
verbose=VERBOSE, validation_split=VALIDATION_SPLIT)
score = model.evaluate(X_test, y_test, verbose=VERBOSE)
print("Test score:", score[0])
print('Test accuracy:', score[1])
# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

```

现在让我们运行代码。正如你所看到的，时间有了显著的增加，我们的深度网络中的每次迭代现在需要大约 134 秒，而在[第 1 章](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml)、*神经网络基础*中定义的网络需要大约 1-2 秒。但是准确率达到了一个新的高峰，达到了 99.06%:

![](assets/B06258_04_05.png)

让我们绘制模型精度和模型损失，我们知道我们可以仅通过 4 - 5 次迭代来达到 99.2%的类似精度:

| ![](assets/B06258_04_06.png) | ![](assets/B06258_04_07.png) |

在下面的截图中，我们展示了我们的模型所达到的最终精度:

![](assets/B06258_04_08.png)

让我们看一些 MNIST 的图片来理解 99.2%这个数字有多好！例如，人类书写 9 的方式有很多种，其中一种出现在下图中。这同样适用于 3、7、4 和 5。这个图表中的数字 **1** 很难辨认，可能连人类都会有问题:

![](assets/B06258_04_09.png)

我们可以在下面的图表中总结我们不同的模型到目前为止所取得的进展。我们的简单网络以 92.22%的准确率开始，这意味着 100 个手写字符中大约有 8 个没有被正确识别。然后，通过达到 99.20%的准确率，我们利用深度学习架构获得了 7%的准确率，这意味着 100 个手写字符中大约有 1 个被错误识别:

![](assets/B06258_04_10.png)

# 理解深度学习的力量

为了更好地了解深度学习和 ConvNet 的强大功能，我们可以进行的另一项测试是减少训练集的规模，并观察随之而来的性能衰减。一种方法是将 50，000 个示例的训练集分成两个不同的集合:

*   用于训练我们的模型的适当的训练集将逐渐减少它的大小(5，900，3，000，1，800，600 和 300 个)样本
*   用于评估我们的模型被训练得有多好的验证集将由剩余的例子组成

我们的测试集总是固定的，由 10，000 个例子组成。

通过这种设置，我们将刚刚定义的深度学习 ConvNet 与在[第 1 章](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml)、*神经网络基础*中定义的第一个神经网络示例进行比较。正如我们在下图中看到的，我们的深度网络总是优于简单网络，并且当提供用于训练的示例数量逐渐减少时，差距越来越明显。通过 5900 个训练样本，深度学习网络的准确率为 96.68%，而简单网络的准确率为 85.56%。更重要的是，只有 300 个训练样本，我们的深度学习网络仍然有 72.44%的准确率，而简单网络显示出 48.26%的显著下降。所有实验仅运行四次训练迭代。这证实了深度学习取得的突破性进展。乍一看，从数学的角度来看，这可能是令人惊讶的，因为深度网络有更多的未知数(权重)，所以人们会认为我们需要更多的数据点。然而，保留空间信息、添加卷积、池化和特征地图是 ConvNets 的创新，并且经过了数百万年的优化(因为该组织受到了视觉皮层的启发):

![](assets/B06258_04_11.png)

有关 MNIST 的最新结果列表，请访问:[http://rodrigob . github . io/are _ we _ there _ yet/build/classification _ datasets _ results . html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)。截至 2017 年 1 月，最好的结果误差率为 0.21%。



# 利用深度学习识别 CIFAR-10 图像

CIFAR-10 数据集包含 60，000 幅 32 x 32 像素的彩色图像，分为 10 类的 3 个通道。每个类包含 6000 幅图像。训练集包含 50，000 幅图像，而测试集提供 10，000 幅图像。这张取自 CIFAR 储存库([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))的图片描述了 10 个类别中的几个随机例子:

![](assets/B06258_04_12.png)

目标是识别以前未见过的图像，并将它们分配到 10 个类别中的一个。让我们定义一个合适的深网。

首先，我们导入一些有用的模块，定义一些常量，并加载数据集:

```py

from keras.datasets import cifar10
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.optimizers import SGD, Adam, RMSprop
import matplotlib.pyplot as plt

# CIFAR_10 is a set of 60K images 32x32 pixels on 3 channels
IMG_CHANNELS = 3
IMG_ROWS = 32
IMG_COLS = 32

#constant
BATCH_SIZE = 128
NB_EPOCH = 20
NB_CLASSES = 10
VERBOSE = 1
VALIDATION_SPLIT = 0.2
OPTIM = RMSprop()

#load dataset
(X_train, y_train), (X_test, y_test) = cifar10.load_data()
print('X_train shape:', X_train.shape)
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

```

现在让我们做一个一次性编码，并使图像正常化:

```py

# convert to categorical
Y_train = np_utils.to_categorical(y_train, NB_CLASSES)
Y_test = np_utils.to_categorical(y_test, NB_CLASSES)

# float and normalization
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255

```

我们的网络将学习 32 个卷积滤波器，每个滤波器的大小为 3×3。输出维度与输入形状的维度相同，因此它将是 32×32，并且激活是 ReLU，这是引入非线性的简单方式。之后，我们有一个最大池操作，池大小为 2 x 2，丢弃率为 25%:

```py

# network
model = Sequential()
model.add(Conv2D(32, (3, 3), padding='same',
input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

```

深层管道中的下一阶段是具有 512 个单元和 ReLU 激活的密集网络，随后是 50%的丢弃率和具有 10 个输出类别的 softmax 层，每个类别一个:

```py

model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(NB_CLASSES))
model.add(Activation('softmax'))
model.summary()

```

定义了网络之后，我们就可以训练模型了。在这种情况下，除了训练集和测试集之外，我们还拆分数据并计算验证集。训练用于构建我们的模型，验证用于选择性能最佳的方法，而测试集用于检查我们的最佳模型在新的未知数据上的性能:

```py

# train
model.compile(loss='categorical_crossentropy', optimizer=OPTIM,
metrics=['accuracy'])
model.fit(X_train, Y_train, batch_size=BATCH_SIZE,
epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT,
verbose=VERBOSE)
score = model.evaluate(X_test, Y_test,
batch_size=BATCH_SIZE, verbose=VERBOSE)
print("Test score:", score[0])
print('Test accuracy:', score[1])

```

在这种情况下，我们保存深层网络的架构:

```py

#save model
model_json = model.to_json()
open('cifar10_architecture.json', 'w').write(model_json)
And the weights learned by our deep network on the training set
model.save_weights('cifar10_weights.h5', overwrite=True)

```

让我们运行代码。我们的网络通过 20 次迭代达到了 66.4%的测试精度。我们还打印精度和损耗图，并使用`model.summary()`转储网络:

![](assets/B06258_04_13-1.png)

在下图中，我们报告了我们的网络在训练和测试数据集上实现的准确性和损失:

| ![](assets/B06258_04_14.png) | ![](assets/B06258_04_15.png) |



# 借助更深的网络提高 CIFAR-10 的性能

提高性能的一种方法是用多个卷积运算定义更深的网络。在本例中，我们有一系列模块:

*conv+conv+maxpool+dropout+conv+conv+maxpool*

后面跟着一个标准的*密+漏+密*。所有的激活功能都是 ReLU。

让我们看看新网络的代码:

```py

model = Sequential()
model.add(Conv2D(32, (3, 3), padding='same',
input_shape=(IMG_ROWS, IMG_COLS, IMG_CHANNELS)))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(64, 3, 3))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(NB_CLASSES))
model.add(Activation('softmax'))

```

恭喜你！你已经定义了一个更深的网络。让我们运行代码！首先，我们转储网络，然后我们运行 40 次迭代，达到 76.9%的准确度:

![](assets/B06258_04_16.png)

在下面的截图中，我们将看到 40 次迭代后达到的精度:

![](assets/B06258_04_17-2.png)

因此，相对于之前更简单更深入的网络，我们提高了 10.5%。为了完整起见，让我们也报告训练期间的准确度和损失，如下所示:

| ![](assets/B06258_04_18.png) | ![](assets/B06258_04_19-1.png) |



# 通过数据扩充提高 CIFAR-10 的性能

另一种提高性能的方法是为我们的训练生成更多的图像。关键的直觉是，我们可以采用标准的 CIFAR 训练集，并用多种类型的变换来扩充该训练集，包括旋转、重新缩放、水平/垂直翻转、缩放、频道转换等等。让我们看看代码:

```py

from keras.preprocessing.image import ImageDataGenerator
from keras.datasets import cifar10
import numpy as np
NUM_TO_AUGMENT=5

#load dataset
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

# augumenting
print("Augmenting training set images...")
datagen = ImageDataGenerator(
rotation_range=40,
width_shift_range=0.2,
height_shift_range=0.2,
zoom_range=0.2,
horizontal_flip=True,
fill_mode='nearest')

```

`rotation_range`是随机旋转图片的度数(`0` - `180`)。`width_shift`和`height_shift`是随机垂直或水平平移图片的范围。`zoom_range`用于随机缩放图片。`horizontal_flip`用于随机水平翻转一半图像。`fill_mode`用于填充旋转或移位后可能出现的新像素的策略:

```py

xtas, ytas = [], []
for i in range(X_train.shape[0]):
num_aug = 0
x = X_train[i] # (3, 32, 32)
x = x.reshape((1,) + x.shape) # (1, 3, 32, 32)
for x_aug in datagen.flow(x, batch_size=1,
save_to_dir='preview', save_prefix='cifar', save_format='jpeg'):
if num_aug >= NUM_TO_AUGMENT:
break
xtas.append(x_aug[0])
num_aug += 1

```

增强后，我们将从标准 CIFAR-10 集开始生成更多的训练图像:

![](assets/B06258_04_20.png)

现在我们可以将这种直觉直接用于训练。使用之前定义的相同的 ConvNet，我们简单地生成更多的增强图像，然后进行训练。为了提高效率，生成器与模型并行运行。这允许在 CPU 上进行图像增强，同时在 GPU 上进行训练。代码如下:

```py

#fit the dataget
datagen.fit(X_train)

# train
history = model.fit_generator(datagen.flow(X_train, Y_train,
batch_size=BATCH_SIZE), samples_per_epoch=X_train.shape[0],
epochs=NB_EPOCH, verbose=VERBOSE)
score = model.evaluate(X_test, Y_test,
batch_size=BATCH_SIZE, verbose=VERBOSE)
print("Test score:", score[0])
print('Test accuracy:', score[1])

```

每次迭代现在都更昂贵，因为我们有更多的训练数据。因此，让我们仅运行 50 次迭代，并看到我们达到了 78.3%的准确度:

![](assets/B06258_04_21.png)

下图总结了我们实验中获得的结果:

![](assets/B06258_04_22.png)

有关 CIFAR-10 的最新结果列表，请访问:[http://rodrigob . github . io/are _ we _ there _ yet/build/classification _ datasets _ results . html](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)。截至 2017 年 1 月，最好的结果准确率为 96.53%。



# 使用 CIFAR-10 进行预测

现在让我们假设我们想要使用我们刚刚为 CIFAR-10 训练的深度学习模型来进行图像的批量评估。因为我们保存了模型和重量，所以不需要每次都进行训练:

```py

import numpy as np
import scipy.misc
from keras.models import model_from_json
from keras.optimizers import SGD

#load model
model_architecture = 'cifar10_architecture.json'
model_weights = 'cifar10_weights.h5'
model = model_from_json(open(model_architecture).read())
model.load_weights(model_weights)

#load images
img_names = ['cat-standing.jpg', 'dog.jpg']
imgs = [np.transpose(scipy.misc.imresize(scipy.misc.imread(img_name), (32, 32)),
(1, 0, 2)).astype('float32')
for img_name in img_names]
imgs = np.array(imgs) / 255

# train
optim = SGD()
model.compile(loss='categorical_crossentropy', optimizer=optim,
metrics=['accuracy'])

# predict
predictions = model.predict_classes(imgs)
print(predictions)

```

现在让我们得到对一个![](assets/B06258_04_23-2.jpg)和一个![](assets/B06258_04_24-1.jpg)的预测。

我们得到类别`3`(猫)和`5`(狗)作为输出，正如预期的那样:

![](assets/B06258_04_25-2.png)

# 用于大规模图像识别的非常深的卷积网络

2014 年，展示了对图像识别的一项有趣贡献(有关更多信息，请参考 K. Simonyan 和 A. Zisserman 于 2014 年发表的文章:*用于大规模图像识别的超深度卷积网络*)。该论文表明，*通过将深度推进到 16-19 个重量层*，可以实现对现有技术配置的显著改进。论文中的一个型号命名为 *D* 或 VGG-16，有 16 个深层。Java Caffe([http://caffe.berkeleyvision.org/](http://caffe.berkeleyvision.org/))中的实现已用于在 ImageNet ils vrc-2012([http://image-net.org/challenges/LSVRC/2012/](http://image-net.org/challenges/LSVRC/2012/))数据集上训练模型，该数据集包括 1000 个类的图像，并被分成三组:训练(130 万个图像)、验证(5 万个图像)和测试(10 万个图像)。每个图像在三个通道上都是(224 x 224)的。该模型在 ILSVRC-2012-val 上实现了 7.5%的前 5 名误差，在 ILSVRC-2012-test 上实现了 7.4%的前 5 名误差。

根据 ImageNet 网站:

该竞赛的目标是使用大型手写标记 ImageNet 数据集(1000 万张标记图像，描述了 10，000 多个对象类别)的子集作为训练，来估计照片的内容，以便进行检索和自动注释。测试图像将没有初始注释——没有分割或标签——并且算法将必须产生指定图像中存在什么对象的标签。

在 Caffe 中实现的模型学习到的权重已在 Keras 中直接转换(有关更多信息，请参考:[https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3](https://gist.github.com/baraldilorenzo/07d7802847aaad0a35d3))，并可用于预加载到 Keras 模型中，Keras 模型接下来将按照本文所述实现:

```py

from keras.models import Sequential
from keras.layers.core import Flatten, Dense, Dropout
from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D
from keras.optimizers import SGD
import cv2, numpy as np

# define a VGG16 network
def VGG_16(weights_path=None):
model = Sequential()
model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(256, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(512, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Conv2D(512, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
model.add(Flatten())
#top layer of the VGG net
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1000, activation='softmax'))
if weights_path:
model.load_weights(weights_path)
return model

```



# 用 VGG 16 网识别猫

现在让我们测试一下![](assets/B06258_04_26.jpg)的图像:

```py

im = cv2.resize(cv2.imread('cat.jpg'), (224, 224)).astype(np.float32)
im = im.transpose((2,0,1))
im = np.expand_dims(im, axis=0)

# Test pretrained model
model = VGG_16('/Users/gulli/Keras/codeBook/code/data/vgg16_weights.h5')
optimizer = SGD()
model.compile(optimizer=optimizer, loss='categorical_crossentropy')
out = model.predict(im)
print np.argmax(out)

```

当代码被执行时，类`285`被返回，它对应于(更多信息请参考:[https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a))埃及猫:

![](assets/B06258_04_27-2.png)

# 利用 Keras 内置的 VGG-16 网络模块

Keras 应用是预先构建和预先训练的深度学习模型。当实例化一个模型时，权重被自动下载并存储在`~/.keras/models/`。使用内置代码非常简单:

```py

from keras.models import Model
from keras.preprocessing import image
from keras.optimizers import SGD
from keras.applications.vgg16 import VGG16
import matplotlib.pyplot as plt
import numpy as np
import cv2

# prebuild model with pre-trained weights on imagenet
model = VGG16(weights='imagenet', include_top=True)
sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(optimizer=sgd, loss='categorical_crossentropy')

# resize into VGG16 trained images' format
im = cv2.resize(cv2.imread('steam-locomotive.jpg'), (224, 224))
im = np.expand_dims(im, axis=0)

# predict
out = model.predict(im)
plt.plot(out.ravel())
plt.show()
print np.argmax(out)
#this should print 820 for steaming train

```

现在，让我们考虑一列火车:

![](assets/B06258_04_28.jpg)

就像我爷爷开的那种。如果我们运行代码，我们得到结果`820`，这是*蒸汽火车*的图像网络代码。同样重要的是，所有其他类的支持都很弱，如下图所示:

![](assets/B06258_04_29-1.png)

最后，请注意 VGG-16 只是 Keras 中预先构建的模块之一。可在:[https://keras.io/applications/](https://keras.io/applications/)获得经过预培训的 Keras 车型的完整列表。



# 循环利用预先构建的深度学习模型来提取特征

一个非常简单的想法是使用 VGG-16 和更一般的 DCNN 进行特征提取。此代码通过从特定图层提取要素来实现这一思想:

```py

from keras.applications.vgg16 import VGG16
from keras.models import Model
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
import numpy as np

# pre-built and pre-trained deep learning VGG16 model
base_model = VGG16(weights='imagenet', include_top=True)
for i, layer in enumerate(base_model.layers):
     print (i, layer.name, layer.output_shape)

# extract features from block4_pool block
model =
Model(input=base_model.input, output=base_model.get_layer('block4_pool').output)
img_path = 'cat.jpg'
img = image.load_img(img_path, target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x = preprocess_input(x)

# get the features from this block
features = model.predict(x)

```

现在你可能想知道为什么我们要从 DCNN 的中间层提取特征。关键的直觉是，随着网络学会将图像分类，每一层都学会识别进行最终分类所必需的特征。较低层识别较低阶特征，例如颜色和边缘，而较高层将这些较低阶特征组合成较高阶特征，例如形状或对象。因此，中间层具有从图像中提取重要特征的能力，并且这些特征更可能有助于不同种类的分类。这有多重优势。首先，我们可以依靠公开的大规模培训，并将这种学习转移到新的领域。第二，可以节省时间进行昂贵的大型训练。第三，即使我们的领域没有大量的训练样本，我们也能提供合理的解决方案。我们也能为手头的任务得到一个好的开始网络形状，而不是猜测它。



# 非常深入的 inception-v3 网络用于迁移学习

迁移学习是一种非常强大的深度学习技术，在不同领域有着广泛的应用。直觉很简单，可以用类比来解释。假设你想学一门新语言，比如说西班牙语；然后从你已经知道的不同语言开始，比如英语，会很有用。

按照这种思路，计算机视觉研究人员现在通常使用预训练的 CNN 来生成新任务的表示，其中数据集可能不足以从头开始训练整个 CNN。另一种常见的策略是采用预先训练的 ImageNet 网络，然后针对新任务对整个网络进行微调。

Inception-v3 net 是 Google 开发的一个非常深入的 ConvNet。Keras 实现了下图中描述的完整网络，并在 ImageNet 上进行了预培训。此型号的默认输入大小为三个通道的 299 x 299:

![](assets/B06258_04_59.png)

这个框架例子的灵感来自于在[https://keras.io/applications/](https://keras.io/applications/)提供的一个方案。我们假设在不同于 ImageNet 的域中有一个训练数据集 *D* 。 *D* 输入 1024 个特征，输出 200 个类别。让我们看一段代码:

```py

from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing import image
from keras.models import Model
from keras.layers import Dense, GlobalAveragePooling2D
from keras import backend as K

# create the base pre-trained model
base_model = InceptionV3(weights='imagenet', include_top=False)

```

我们使用一个经过训练的 inception-v3；我们不包括顶级型号，因为我们想在 *D* 上进行微调。顶层是具有 1，024 个输入的密集层，其中最后一个输出层是具有 200 个输出类别的 softmax 密集层。`x = GlobalAveragePooling2D()(x)`用于将输入转换为正确的形状，以供密集层处理。事实上，`base_model.output`张量对于`dim_ordering="th"`具有*(样本，通道，行，列)*的形状，对于`dim_ordering="tf"`具有*(样本，行，列，通道)*的形状，但是 dense 需要它们作为*(样本，通道)*和`GlobalAveragePooling2D`在*(行，列)*上的平均值。所以如果你看最后四层(这里是`include_top=True`，你会看到这些形状:

```py

# layer.name, layer.input_shape, layer.output_shape
('mixed10', [(None, 8, 8, 320), (None, 8, 8, 768), (None, 8, 8, 768), (None, 8, 8, 192)], (None, 8, 8, 2048))
('avg_pool', (None, 8, 8, 2048), (None, 1, 1, 2048))
('flatten', (None, 1, 1, 2048), (None, 2048))
('predictions', (None, 2048), (None, 1000))

```

当您执行`include_top=False,`操作时，您正在移除最后三个图层并暴露`mixed10`图层，因此`GlobalAveragePooling2D`图层会将*(无，8，8，2048)* 转换为*(无，2048)* ，其中*(无，2048)* 张量中的每个元素都是*(无，8，8，2048)*【8】*子张量中每个对应的平均值*

```py

*# add a global spatial average pooling layer* x = base_model.output
x = GlobalAveragePooling2D()(x)*# let's add a fully-connected layer as first layer* x = Dense(1024, activation='relu')(x)*# and a logistic layer with 200 classes as last layer* predictions = Dense(200, activation='softmax')(x)*# model to train* model = Model(input=base_model.input, output=predictions)

```

所有卷积级别都是预先训练的，因此我们在整个模型的训练期间冻结它们:

```py

*# that is, freeze all convolutional InceptionV3 layers* for layer in base_model.layers: layer.trainable = False

```

然后，该模型被编译和训练几个时期，以便训练顶层:

```py

*# compile the model (should be done *after* setting layers to non-trainable)* model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

*# train the model on the new data for a few epochs* model.fit_generator(...)

```

然后我们冻结盗梦空间的顶层，并微调一些盗梦空间层。在本例中，我们决定冻结前 172 层(要调整的超参数):

```py

*# we chose to train the top 2 inception blocks, that is, we will freeze* *# the first 172 layers and unfreeze the rest:* for layer in 
model.layers[:172]: layer.trainable = False for layer in 
model.layers[172:]: layer.trainable = True

```

然后重新编译该模型以进行微调优化。为了使这些修改生效，我们需要重新编译模型:

```py

*# we use SGD with a low learning rate* from keras.optimizers
import SGD
model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')

*# we train our model again (this time fine-tuning the top 2 inception blocks)* *# alongside the top Dense layers* model.fit_generator(...)

```

现在我们有了一个新的深度网络，它重用了标准的 Inception-v3 网络，但它是通过迁移学习在一个新的域 *D* 上训练的。当然，为了达到良好的精度，有许多参数需要微调。然而，我们现在通过迁移学习重新使用一个非常大的预训练网络作为起点。这样做，我们可以通过重用 Keras 中已经可用的内容来节省在我们的机器上进行培训的需求。



# 摘要

在这一章中，我们学习了如何使用深度学习神经网络来高精度地识别 MNIST 手写字符。然后，我们使用 CIFAR 10 数据集建立了 10 个类别的深度学习分类器，并使用 ImageNet 数据集建立了 1，000 个类别的精确分类器。此外，我们还调查了如何使用大型深度学习网络(如 VGG16)和非常深度的网络(如 InceptionV3)。本章最后讨论了迁移学习，以适应在大型数据集上训练的预建模型，使它们能够在新的领域很好地工作。

在下一章中，我们将介绍用于复制看起来像人类生成的数据的合成数据的生成性对抗网络；我们将展示 WaveNet，这是一种深度神经网络，用于高质量地再现人类语音和乐器。