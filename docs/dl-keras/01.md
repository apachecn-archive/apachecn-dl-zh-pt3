

# 一、神经网络基础

人工神经网络(简称为 *nets* )代表了一类机器学习模型，其灵感大致来自于对哺乳动物中枢神经系统的研究。每个网络由几个相互连接的*神经元*组成，组织在*层*，当特定条件发生时，它们交换信息(用行话来说，它们*触发*)。最初的研究开始于 20 世纪 50 年代末，引入了感知器(有关更多信息，请参考文章:*感知器:大脑中信息存储和组织的概率模型*，作者:F. Rosenblatt，心理评论，第 65 卷，第 386 - 408 页，1958)，一种用于简单操作的双层网络，并在 20 世纪 60 年代末随着*反向传播算法*的引入而进一步扩展。 用于有效的多层网络训练(根据 P. J. Werbos 的文章:*时间反向传播:它做什么和如何做*，IEEE 学报，第 78 卷，第 1550 - 1560 页，1990 年，以及 G. E. Hinton、S. Osindero 和 Y. W. Teh 的文章*深度信念网络的快速学习算法*，神经计算，第 18 卷，第 1527-1527 页) 一些研究认为，这些技术的根源可以追溯到比通常引用的更早的时候(有关更多信息，请参考文章:*神经网络中的深度学习:概述*，J. Schmidhuber，第 61 卷，第 85 - 117 页，2015 年)。直到 20 世纪 80 年代，其他更简单的方法变得更加相关，神经网络才成为密集学术研究的主题。然而，从 2000 年代中期开始，由于 G. Hinton 提出的突破性快速学习算法(欲了解更多信息，请参考文章:*反向传播的根源:从有序导数到神经网络和政治预测*、*神经网络*，S. Leven，第 9 卷，1996 年和*通过反向传播误差学习表示*、D. E. Rumelhart、G. E. Hinton

这些改进为现代*深度学习*开辟了道路，这是一类以大量神经元层为特征的神经网络，能够基于渐进的抽象级别学习相当复杂的模型。几年前人们称之为 3-5 层的*深*，现在已经涨到 100-200 了。

这种通过渐进抽象的学习类似于在人脑中进化了数百万年的视觉模型。人类的视觉系统确实被组织成不同的层次。我们的眼睛与大脑中被称为**视觉皮层 V1** 的区域相连，该区域位于我们大脑的后下部。这个区域是许多哺乳动物共有的，具有辨别视觉方向、空间频率和颜色的基本属性和微小变化的作用。据估计，V1 由大约 1.4 亿个神经元组成，神经元之间有 100 亿个连接。然后，V1 与其他区域 V2、V3、V4、V5 和 V6 相连，进行越来越复杂的图像处理，并识别更复杂的概念，如形状、面部、动物等等。这种分层组织是数亿年来大量尝试的结果。据估计，人类皮质神经元约有 160 亿个，人类皮质约有 10%-25%用于视觉(有关更多信息，请参考文章:*The Human Brain in Numbers:A linear Scaled up Primate Brain*，作者 S. Herculano-Houzel，第 3 卷，2009)。深度学习从人类视觉系统的这种基于层的组织中获得了一些灵感:早期的人工神经元层学习图像的基本属性，而更深的层学习更复杂的概念。

这本书通过提供用 Keras 编码的工作网络，涵盖了神经网络的几个主要方面，Keras 是一个极简高效的 Python 库，用于在谷歌的 TensorFlow(有关更多信息，请参考[https://www.tensorflow.org/](https://www.tensorflow.org/))或蒙特利尔大学的 Theano(有关更多信息，请参考[http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/))后端上运行的深度学习计算。那么，我们开始吧。

在本章中，我们将讨论以下主题:

*   感知器
*   多层感知器
*   激活功能
*   梯度下降
*   随机梯度下降
*   反向传播



# 感知器

感知器是一种简单的算法，给定输入向量 *x* 的 *m* 值( *x [1]* ， *x [2]* )，...， *x [n]* )通常称为输入特征或简称为特征，输出 *1* (是)或 *0* (否)。数学上，我们定义一个函数:

![](assets/image_01_005.png)

这里， *w* 是权重的向量， *wx* 是点积![](assets/image_01_008.jpg)，而 *b* 是偏差。如果你还记得初等几何， *wx + b* 定义了一个边界超平面，它根据分配给 *w* 和 *b* 的值改变位置。如果 *x* 位于直线上方，那么答案为正，否则为负。非常简单的算法！感知无法表达一个*也许*的答案。它可以回答*是* ( *1* )或*否* ( *0* )如果我们明白如何定义 *w* 和 *b* 的话，那就是下面几段将要讨论的训练过程。



# Keras 代码的第一个例子

Keras 最初的构建块是一个模型，最简单的模型叫做**序列**。顺序 Keras 模型是神经网络层的线性管道(堆栈)。这段代码定义了一个带有`12`人工神经元的单层，它需要`8`个输入变量(也称为特性):

```py

from keras.models import Sequential
model = Sequential()
model.add(Dense(12, input_dim=8, kernel_initializer='random_uniform'))

```

每个神经元可以用特定的权重初始化。Keras 提供了一些选择，其中最常见的如下所示:

*   `random_uniform`:权重初始化为均匀随机的小值( *-0.05* ， *0.05* )。换句话说，在给定区间内的任何值都有可能被抽取。
*   `random_normal`:根据高斯函数初始化权重，平均值为零，标准偏差为 *0.05* 。对于那些不熟悉高斯曲线的人，想象一下对称的*钟形曲线*形状。
*   `zero`:所有权重初始化为零。

完整名单可在 https://keras.io/initializations/获得。



# 多层感知器——网络的第一个例子

在本章中，我们定义了具有多个线性层的网络的第一个示例。历史上，感知器是一个只有一个线性层的模型的名字，因此，如果它有多个层，你可以称它为**多层感知器** ( **MLP** )。下图显示了一个具有一个输入层、一个中间层和一个输出层的通用神经网络。

![](assets/B06258_01_02.png)

在上图中，第一层中的每个节点接收一个输入，并根据预定义的本地决策边界触发。然后，第一层的输出被传递到第二层，其结果被传递到由一个单个神经元组成的最终输出层。有趣的是，这种分层组织与我们之前讨论的人类视觉模式有些相似。

*网*是密集的，意味着一层中的每个神经元都连接到位于前一层的所有神经元和下一层的所有神经元。



# 感知器训练中的问题及解决方法

让我们考虑单个神经元。权重 *w* 和偏差 *b* 的最佳选择是什么？理想情况下，我们希望提供一组训练示例，并让计算机调整权重和偏差，以使输出中产生的误差最小化。为了更具体一点，让我们假设我们有一组猫的图片和另一组不包含猫的图片。为简单起见，假设每个神经元只看一个输入像素值。当计算机处理这些图像时，我们希望我们的神经元调整其权重和偏差，以便我们越来越少的图像被错误地识别为非猫。这种方法看起来非常直观，但它要求权重(和/或偏差)的微小变化仅引起输出的微小变化。

如果我们有一个大的输出跳跃，我们不能*渐进地*学习(而不是尝试所有可能的方向——一个被称为穷举搜索的过程——不知道我们是否在改进)。毕竟，孩子是一点一点学的。不幸的是，感知器没有显示这种一点一点的行为。一个感知器要么是 *0* 要么是 *1* ，这是一个巨大的跳跃，它不会帮助它学习，如下图所示:

![](assets/B06258_01_03.png)

我们需要不同的，更平滑的东西。我们需要一个从 *0* 到 *1* 连续变化的函数。从数学上来说，这意味着我们需要一个连续的函数来计算导数。



# 激活函数— sigmoid

sigmoid 函数定义如下:

![](assets/image_01_068.jpg)

如下图所示，当输入在![](assets/image_01_020.jpg)中变化时，在 *(0，1)* 中输出变化很小。数学上，函数是连续的。典型的 sigmoid 函数如下图所示:

![](assets/B06258_01_05.png)

神经元可以使用 sigmoid 来计算非线性函数![](assets/image_01_022.png)。注意，如果![](assets/image_01_023.png)很大且为正，那么![](assets/image_01_024.jpg)，所以![](assets/image_01_025.jpg)，而如果![](assets/image_01_026.jpg)很大且为负![](assets/image_01_027.jpg)，所以![](assets/image_01_028.jpg)。换句话说，具有 sigmoid 激活的神经元具有类似于感知器的行为，但变化是渐进的，并且输出值，如 *0.5539* 或 *0.123191* ，是完全合法的。从这个意义上说，一个乙状结肠神经元可以回答*也许是*。



# 激活功能— ReLU

sigmoid 不是唯一一种用于神经网络的平滑激活函数。最近，一个叫做**整流线性单元** ( **ReLU** )的非常简单的函数变得非常流行，因为它产生了非常好的实验结果。一个 ReLU 简单定义为![](assets/image_01_029.jpg)，非线性函数如下图所示。如下图所示，对于负值，函数为零，对于正值，函数线性增长:

![](assets/B06258_01_06.png)              

# 激活功能

Sigmoid 和 ReLU 在神经网络行话中一般称为*激活函数*。在 Keras 部分的*测试不同的优化器中，我们将看到这些逐渐的变化，典型的 sigmoid 和 ReLU 函数，是开发一个学习算法的基本构件，该算法通过逐渐减少我们的网络所犯的错误来逐渐适应。使用激活函数σ和( *x [1]* ， *x [2]* )的例子，...、 *x [m]* 输入向量、 *w [1]* 、 *w [2]* 、*...*、 *w [m]* )权重向量、 *b* 偏差、σ总和如下图所示:*

![](assets/B06258_01_07.png)

Keras 支持许多激活功能，完整列表可在[https://keras.io/activations/](https://keras.io/activations/)获得。



# 一个真实的例子——识别手写数字

在本节中，我们将构建一个可以识别手写数字的网络。为了实现这一目标，我们使用了 MNIST(更多信息，请参考[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))，这是一个由 60，000 个样本的训练集和 10，000 个样本的测试集组成的手写数字数据库。训练示例由人类用正确的答案进行注释。例如，如果手写数字是数字 3，那么 3 只是与该示例相关联的标签。

在机器学习中，当有正确答案的数据集可用时，我们说我们可以执行一种形式的*监督学习*。在这种情况下，我们可以使用训练示例来调整我们的网络。测试示例也有与每个数字相关的正确答案。然而，在这种情况下，想法是假装标签是未知的，让网络进行预测，然后稍后，重新考虑标签，以评估我们的神经网络已经学会如何识别数字。因此，毫不奇怪，测试示例只是用来测试我们的网络。

每个 MNIST 图像都是灰度级的，由 28 x 28 像素组成。下图显示了这些数字的子集:

![](assets/B06258_01_08.png)

# 一键编码— OHE

在许多应用中，将分类(非数字)特征转换成数字变量是很方便的。例如，在*【0-9】*中具有值 *d* 的分类特征数字可以被编码成具有 *10 个*位置的二进制向量，该向量总是具有 *0* 值，除了第 *d* 个位置存在 *1* 之外。这种类型的表示被称为**一键编码** ( **OHE** )，当学习算法专门用于处理数值函数时，在数据挖掘中非常常见。



# 在 Keras 中定义简单的神经网络

这里，我们使用 Keras 来定义一个识别 MNIST 手写数字的网络。我们从一个非常简单的神经网络开始，然后逐步改进它。

Keras 提供了合适的库来加载数据集，并将其分成用于微调我们的网络的训练集`X_train` *、*，以及用于评估性能的测试集 *`X_test`、*。数据转换为`float32`以支持 GPU 计算，并归一化为*【0，1】*。此外，我们将真实标签分别加载到`Y_train`和`Y_test`中，并对它们执行一次热编码。让我们看看代码:

```py

from __future__ import print_function
import numpy as np
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Activation
from keras.optimizers import SGD
from keras.utils import np_utils
np.random.seed(1671) # for reproducibility

# network and training
NB_EPOCH = 200
BATCH_SIZE = 128
VERBOSE = 1
NB_CLASSES = 10 # number of outputs = number of digits
OPTIMIZER = SGD() # SGD optimizer, explained later in this chapter
N_HIDDEN = 128
VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION

# data: shuffled and split between train and test sets
#
(X_train, y_train), (X_test, y_test) = mnist.load_data()
#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784
RESHAPED = 784
#
X_train = X_train.reshape(60000, RESHAPED)
X_test = X_test.reshape(10000, RESHAPED)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
# normalize
#
X_train /= 255
X_test /= 255
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')
# convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(y_train, NB_CLASSES)
Y_test = np_utils.to_categorical(y_test, NB_CLASSES)

```

输入层有一个神经元与图像中的每个像素相关联，总共有 *28 x 28 = 784* 个神经元，MNIST 图像中的每个像素对应一个神经元。

通常，与每个像素相关联的值在范围*【0，1】*内被归一化(这意味着每个像素的强度除以 255，即最大强度值)。输出是 10 个类，每个类对应一个数字。

最后一层是具有激活函数 softmax 的单个神经元，它是 sigmoid 函数*的推广。* Softmax *将*任意实值的 k 维向量挤压成 *(0，1)* 范围内的 k 维实值向量。在我们的例子中，它用 10 个神经元聚集了前一层提供的 10 个答案:

```py

# 10 outputs
# final stage is softmax
model = Sequential()
model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))
model.add(Activation('softmax'))
model.summary()

```

一旦我们定义了模型，我们就必须编译它，以便它可以被 Keras 后端(无论是 Theano 还是 TensorFlow)执行。在编译过程中有几个选择:

*   我们需要选择*优化器*，它是在我们训练模型时用来更新权重的特定算法
*   我们需要选择优化器使用的*目标函数*来导航权重空间(通常，目标函数被称为*损失函数*，优化的过程被定义为损失*最小化*的过程)
*   我们需要评估训练好的模型

目标函数的一些常见选择(Keras 目标函数的完整列表位于[https://keras.io/objectives/](https://keras.io/objectives/))如下:

*   **MSE** :这是预测值和真实值之间的均方误差。数学上，如果![](assets/image_01_042.jpg)是 *n 个*预测值的向量， *Y* 是 *n 个*观测值的向量，那么它们满足以下等式:

![](assets/image_01_043.png)

这些目标函数对每次预测的所有错误进行平均，如果预测值远离真实值，则平方运算会使该距离更加明显。

*   **二元交叉熵**:这是二元对数损失。假设我们的模型预测 *p* 而目标是 *t* ，那么二元交叉熵定义如下:

![](assets/image_01_044.png)

该目标函数适用于二进制标签预测。

*   **分类交叉熵**:这是多类对数损失。如果目标是 *t [i，j]并且预测是 *p [i，j]，那么分类交叉熵是这样的:**

![](assets/image_01_047.png)

该目标函数适用于多类标签预测。这也是与 softmax 激活相关联的默认选择。

指标的一些常见选择(Keras 指标的完整列表位于[https://keras.io/metrics/](https://keras.io/metrics/))如下:

*   **准确性**:这是相对于目标的正确预测的比例
*   **精度**:这表示有多少选定项目与多标签分类相关
*   **回忆**:这表示有多少选择的项目与多标签分类相关

度量类似于目标函数，唯一的区别是它们不用于训练模型，而仅用于评估模型。在 Keras 中编译模型很容易:

```py

model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])

```

一旦模型被编译，就可以用`fit()`函数对其进行训练，该函数指定了几个参数:

*   `epochs`:这是模型暴露于训练集的次数。在每次迭代中，优化器都试图调整权重，以使目标函数最小化。
*   `batch_size`:这是在优化器执行权重更新之前观察到的训练实例的数量。

在 Keras 中训练一个模型非常简单。假设我们想要迭代`NB_EPOCH`步:

```py

history = model.fit(X_train, Y_train,
batch_size=BATCH_SIZE, epochs=NB_EPOCH,
verbose=VERBOSE, validation_split=VALIDATION_SPLIT)

```

我们保留了部分训练集进行验证。关键思想是，我们保留一部分训练数据，用于在训练时测量验证的性能。对于任何机器学习任务来说，这都是一个很好的实践，我们将在所有的例子中采用。

一旦模型被训练，我们就可以在包含新的未知例子的测试集上评估它。这样，我们就可以得到目标函数所能达到的最小值和评价指标所能达到的最佳值。

注意，训练集和测试集当然是严格分离的。在已经用于训练的例子上评估模型是没有意义的。学习本质上是一个过程，旨在概括看不见的观察结果，而不是记忆已知的东西:

```py

score = model.evaluate(X_test, Y_test, verbose=VERBOSE)
print("Test score:", score[0])
print('Test accuracy:', score[1])

```

所以，恭喜你，你刚刚在 Keras 中定义了你的第一个神经网络。几行代码，你的电脑就能识别手写数字。让我们运行代码，看看性能如何。



# 运行简单的 Keras 网络并建立基线

因此，让我们看看当我们运行下面截图中的代码时会发生什么:

![](assets/B06258_01_12.png)

首先，网络架构被转储，我们可以看到所使用的不同类型的层，它们的输出形状，它们需要优化多少参数，以及它们是如何连接的。然后，对 48，000 个样本训练网络，并保留 12，000 个样本用于验证。一旦神经模型建立起来，就要对 10，000 个样本进行测试。正如你所看到的，Keras 在内部使用 TensorFlow 作为计算的后端系统。现在，我们不深入了解训练是如何进行的，但是我们可以注意到程序运行了 200 次迭代，每一次，精确度都有所提高。当训练结束时，我们在测试集上测试我们的模型，并实现了大约 92.36%的训练准确率、92.27%的验证准确率和 92.22%的测试准确率。

这意味着十个手写字符中有不到一个没有被正确识别。我们当然可以做得更好。在下面的截图中，我们可以看到测试的准确性:

![](assets/B06258_01_12a.png)

# 用隐含层改进 Keras 中的简单网

我们在训练上有 92.36%的基线准确率，在验证上有 92.27%的基线准确率，在测试上有 92.22%的基线准确率。这是一个很好的起点，但我们当然可以改进它。让我们看看怎么做。

第一个改进是在网络中增加额外的层。因此，在输入层之后，我们有了第一个密集层，其中有`N_HIDDEN`个神经元和一个激活函数`relu`。这个附加层被认为是*隐藏的*，因为它不直接连接到输入或输出。在第一个隐藏层之后，我们有第二个隐藏层，也是由`N_HIDDEN`神经元组成，接着是一个有 10 个神经元的输出层，当相对数字被识别时，每个神经元都会触发。以下代码定义了这个新网络:

```py

from __future__ import print_function
import numpy as np
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Activation
from keras.optimizers import SGD
from keras.utils import np_utils
np.random.seed(1671) # for reproducibility
# network and training
NB_EPOCH = 20
BATCH_SIZE = 128
VERBOSE = 1
NB_CLASSES = 10 # number of outputs = number of digits
OPTIMIZER = SGD() # optimizer, explained later in this chapter
N_HIDDEN = 128
VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION
# data: shuffled and split between train and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()
#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784
RESHAPED = 784
#
X_train = X_train.reshape(60000, RESHAPED)
X_test = X_test.reshape(10000, RESHAPED)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
# normalize
X_train /= 255
X_test /= 255
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')
# convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(y_train, NB_CLASSES)
Y_test = np_utils.to_categorical(y_test, NB_CLASSES)
# M_HIDDEN hidden layers
# 10 outputs
# final stage is softmax
model = Sequential()
model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))
model.add(Activation('relu'))
model.add(Dense(N_HIDDEN))
model.add(Activation('relu'))
model.add(Dense(NB_CLASSES))
model.add(Activation('softmax'))
model.summary()
model.compile(loss='categorical_crossentropy',
optimizer=OPTIMIZER,
metrics=['accuracy'])
history = model.fit(X_train, Y_train,
batch_size=BATCH_SIZE, epochs=NB_EPOCH,
verbose=VERBOSE, validation_split=VALIDATION_SPLIT)
score = model.evaluate(X_test, Y_test, verbose=VERBOSE)
print("Test score:", score[0])
print('Test accuracy:', score[1])

```

让我们运行代码，看看这个多层网络会得到什么结果。还不错。通过添加两个隐藏层，我们在训练集上达到了 94.50%，在验证上达到了 94.63%，在测试上达到了 94.41%。这意味着相对于之前的网络，我们在测试中获得了额外的 2.2%的准确性。然而，我们将迭代次数从 200 次大幅减少到了 20 次。那很好，但是我们想要更多。

如果你愿意，你可以自己玩，看看如果你只添加一个隐藏层而不是两个，或者如果你添加两个以上的层会发生什么。我把这个实验作为练习。以下屏幕截图显示了前面示例的输出:

![](assets/B06258_01_13.png)

# 进一步改善辍学学生的简单网络

现在我们的基线是训练集的 94.50%，验证的 94.63%，测试的 94.41%。第二个改进非常简单。我们决定以丢弃概率随机丢弃一些在我们内部密集的隐藏层网络中传播的值。在机器学习中，这是一种众所周知的正则化形式。令人惊讶的是，这种随机丢弃一些值的想法可以提高我们的性能:

```py

from __future__ import print_function
import numpy as np
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import SGD
from keras.utils import np_utils
np.random.seed(1671) # for reproducibility
# network and training
NB_EPOCH = 250
BATCH_SIZE = 128
VERBOSE = 1
NB_CLASSES = 10 # number of outputs = number of digits
OPTIMIZER = SGD() # optimizer, explained later in this chapter
N_HIDDEN = 128
VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION
DROPOUT = 0.3
# data: shuffled and split between train and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()
#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784
RESHAPED = 784
#
X_train = X_train.reshape(60000, RESHAPED)
X_test = X_test.reshape(10000, RESHAPED)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
# normalize
X_train /= 255
X_test /= 255
# convert class vectors to binary class matrices
Y_train = np_utils.to_categorical(y_train, NB_CLASSES)
Y_test = np_utils.to_categorical(y_test, NB_CLASSES)
# M_HIDDEN hidden layers 10 outputs
model = Sequential()
model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))
model.add(Activation('relu'))
model.add(Dropout(DROPOUT))
model.add(Dense(N_HIDDEN))
model.add(Activation('relu'))
model.add(Dropout(DROPOUT))
model.add(Dense(NB_CLASSES))
model.add(Activation('softmax'))
model.summary()
model.compile(loss='categorical_crossentropy',
optimizer=OPTIMIZER,
metrics=['accuracy'])
history = model.fit(X_train, Y_train,
batch_size=BATCH_SIZE, epochs=NB_EPOCH,
verbose=VERBOSE, validation_split=VALIDATION_SPLIT)
score = model.evaluate(X_test, Y_test, verbose=VERBOSE)
print("Test score:", score[0])
print('Test accuracy:', score[1])

```

让我们像以前一样运行代码 20 次迭代，我们将看到这个网络在训练上达到 91.54%的准确率，在验证上达到 94.48%，在测试上达到 94.25%。

![](assets/B06258_01_14.png)

注意训练精度还是要在测试精度之上，否则我们训练的时间不够长。因此，让我们尝试将时期的数量显著增加到 250，我们在训练上获得 98.1%的准确性，在验证上获得 97.73%的准确性，在测试上获得 97.7%的准确性:

![](assets/B06258_01_15.png)

当历元数增加时，观察训练集和测试集的准确度如何增加是有用的。正如您在下图中看到的，这两条曲线在大约 250 个历元处接触，因此，在该点之后无需进一步训练:

| ![](assets/B06258_01_16.png) | ![](assets/B06258_01_17.png) |

注意，经常观察到，在内部隐藏层中具有随机丢失的网络可以更好地概括测试集中包含的看不见的例子。直觉上，人们可以认为这是每个神经元变得更有能力，因为它知道它不能依赖于它的邻居。在测试过程中，没有人掉线，所以我们现在正在使用我们所有高度调谐的神经元。简而言之，这通常是一个很好的方法来测试一个网络在采用某些下降函数时的表现。



# 在 Keras 中测试不同的优化器

我们已经定义并使用了一个网络；开始给出关于网络如何被训练的直觉是有用的。让我们来关注一种流行的训练技术，叫做**梯度下降** ( **GD** )。想象一个单一变量 *w* 中的一般成本函数 *C(w)* ，如下图所示:

![](assets/B06258_01_18.png)

梯度下降可以被看作是一个徒步旅行者，他的目标是爬下一座山进入一个山谷。山代表函数 *C* ，谷代表最小值*C[min]。徒步者有一个起点*w[0]。徒步旅行者一点一点地移动。在每一步 *r* ，梯度是最大增加的方向。在数学上，该方向是在步骤 *r* 到达的点*w[r]t【16】处评估的偏导数![](assets/image_01_062.jpg)的值。因此，通过采取相反的方向，![](assets/image_01_064.jpg)，徒步旅行者可以向山谷移动。在每一步，徒步旅行者可以在下一步之前决定腿的长度。这是梯度下降行话中的*学习率* ![](assets/image_01_065.jpg)。注意，如果![](assets/image_01_066.jpg)太小，那么徒步旅行者将移动缓慢。然而，如果![](assets/image_01_066.jpg)太高，那么徒步旅行者可能会错过山谷。***

现在你应该记住，一个 sigmoid 是一个连续函数，它是可以计算导数的。可以证明，sigmoid 如下所示:

![](assets/image_01_068.png)

它有以下导数:

![](assets/image_01_069.png) 

ReLU 在 *0* 中不可微。然而，我们可以通过选择一阶导数为 *0* 或 *1* 来将 *0* 中的一阶导数扩展为整个域上的函数。ReLU ![](assets/image_01_070.png)的逐点导数如下:

![](assets/B06258_01_070.png)

一旦我们有了导数，就有可能用梯度下降技术来优化网络。Keras 使用其后端(TensorFlow 或 Theano)代表我们计算导数，因此我们不需要担心实现或计算它。我们只需选择激活函数，Keras 代表我们计算它的导数。

神经网络本质上是具有数千个、有时数百万个参数的多个函数的组合。每个网络层计算一个函数，该函数的误差应该被最小化，以便提高在学习阶段观察到的精度。当我们讨论反向传播时，我们会发现最小化游戏比我们的玩具例子要复杂一些。但是，还是基于同样的下一个山谷的直觉。

Keras 实现了梯度下降的快速变体，称为**随机梯度下降** ( **SGD** )和两种更高级的优化技术，称为 **RMSprop** 和 **Adam** 。RMSprop 和 Adam 除了 SGD 的加速度分量之外，还包括动量(一个速度分量)的概念。这以更多的计算为代价实现了更快的收敛。Keras 支持的优化器的完整列表在[https://keras.io/optimizers/](https://keras.io/optimizers/)上。到目前为止，SGD 是我们的默认选择。现在让我们试试另外两个。非常简单，我们只需要修改几行代码:

```py

from keras.optimizers import RMSprop, Adam
...
OPTIMIZER = RMSprop() # optimizer,

```

就是这样。我们来测试一下，如下图截图所示:

![](assets/B06258_01_19.png)

正如您在前面的截图中看到的，RMSprop 比 SDG 更快，因为我们能够在训练上实现 97.97%的准确率，在验证上实现 97.59%的准确率，在测试上实现 97.84%的准确率，仅用 20 次迭代就改进了 SDG。为了完整起见，让我们看看精度和损失如何随历元数而变化，如下图所示:

| ![](assets/B06258_01_20.png) | ![](assets/B06258_01_21.png) |

好了，让我们试试另一个优化器，`Adam()`。这非常简单，如下所示:

```py

OPTIMIZER = Adam() # optimizer

```

正如我们所见，亚当略胜一筹。通过 Adam，我们在 20 次迭代中实现了 98.28%的训练准确率、98.03%的验证准确率和 97.93%的测试准确率，如下图所示:

| ![](assets/B06258_01_22.png) | ![](assets/B06258_01_23.png) |

这是我们的第五个变体，记住我们最初的基线是 92.36%。

到目前为止，我们取得了进步；然而，现在收获越来越难。请注意，我们是在 30%的辍学率下进行优化的。为了完整起见，仅报告选择`Adam()`作为优化器的其他压差值的测试精度可能是有用的，如下图所示:

![](assets/B06258_01_24.png)

# 增加纪元的数量

让我们再做一次尝试，将用于训练的历元数从 20 个增加到 200 个。不幸的是，这个选择增加了我们 10 倍的计算时间，但是它没有给我们带来任何好处。实验不成功，但是我们学到了，多花点时间学习，不一定会有进步。学习更多的是采用聪明的技术，而不一定是花费在计算上的时间。让我们跟踪下图中的第六个变体:

![](assets/B06258_01_25.png)

# 控制优化器学习率

我们还可以尝试改变优化器的学习参数。正如您在下图中看到的，最佳值接近于 *0.001* ，这是 optimer 的默认学习率。很好！亚当开箱即用:

![](assets/B06258_01_26.png)

# 增加内部隐藏神经元的数量

我们可以再做一次尝试，即改变内部隐藏神经元的数量。我们报告了越来越多的隐藏神经元的实验结果。我们可以在下图中看到，通过增加模型的复杂性，运行时间会显著增加，因为要优化的参数越来越多。然而，随着网络的增长，我们通过增加网络规模获得的收益越来越少:

![](assets/B06258_01_27.png)

在下图中，我们显示了随着隐藏神经元数量的增加，每次迭代所需的时间:

![](assets/B06258_01_28.png)

下图显示了隐藏神经元数量增长时的精确度:

![](assets/B06258_01_29.png)

# 增加批量计算的规模

梯度下降试图最小化训练集中提供的所有示例的成本函数，同时最小化输入中提供的所有特征的成本函数。随机梯度下降是一种更便宜的变体，它只考虑`BATCH_SIZE`个例子。那么，让我们通过改变这个参数来看看行为是什么。如您所见，`BATCH_SIZE=128`达到了最佳精度值:

![](assets/B06258_01_30.png)

# 总结用于识别手写图表的实验运行

所以，让我们总结一下:通过五种不同的变体，我们能够将我们的性能从 92.36%提高到 97.93%。首先，我们在 Keras 中定义了一个简单的层网络。然后，我们通过添加一些隐藏层来提高性能。在此之后，我们通过在网络中添加一些随机退出的测试集，并通过试验不同类型的优化器，提高了测试集的性能。下表总结了当前的结果:

| **型号/精度** | **训练** | **验证** | **测试** |
| **简单** | 92.36% | 92.37% | 92.22% |
| **两个隐藏(128)** | 94.50% | 94.63% | 94.41% |
| **辍学(30%)** | 98.10% | 97.73% | 97.7% (200 个时代) |
| **RMSprop** | 97.97% | 97.59% | 97.84% (20 个时期) |
| **亚当** | 98.28% | 98.03% | 97.93% (20 个时期) |

然而，接下来的两个实验并没有提供显著的改善。增加内部神经元的数量会创建更复杂的模型，需要更昂贵的计算，但它只能提供边际收益。如果我们增加训练时期的数量，我们会得到相同的体验。最后一个实验是为我们的优化器改变`BATCH_SIZE`。



# 采用正则化避免过拟合

直观上，一个好的机器学习模型应该在训练数据上实现低误差。从数学上讲，这相当于在给定建立的机器学习模型的情况下，最小化训练数据上的损失函数。这由下面的公式表示。：

![](assets/image_01_084.png)

然而，这可能还不够。为了捕捉由训练数据固有表达的所有关系，模型可能变得过于复杂。这种复杂性的增加可能会带来两个负面后果。首先，一个复杂的模型可能需要大量的时间来执行。第二，复杂模型可以在训练数据上实现非常好的性能-因为训练数据中的所有内在关系都被记住了，但是在验证数据上没有那么好的性能-因为模型不能在新的看不见的数据上概括。再说一遍，学习更多的是概括而不是记忆。下图显示了验证集和训练集上的典型损失函数递减。然而，在某个点上，由于过度拟合，验证损失开始增加:

![](assets/B06258_01_31.png)

根据经验，如果在训练过程中，我们看到验证的损失在最初减少后增加，那么我们就有一个模型复杂性的问题，使训练过度。事实上，过度拟合是机器学习中用来简洁描述这种现象的词。

为了解决过拟合问题，我们需要一种方法来捕捉模型的复杂性，即模型可以有多复杂。解决办法是什么？模型只不过是一个权重向量。因此，模型的复杂性可以方便地表示为非零权重的数量。换句话说，如果我们有两个模型， *M1* 和 *M2* ，在损失函数方面达到几乎相同的性能，那么我们应该选择具有最少非零权重的最简单的模型。我们可以使用一个超参数 *⅄ > =0* 来控制简单模型的重要性，如下式所示:

![](assets/image_01_086.png)

机器学习中使用三种不同类型的正则化:

*   **L1 正则化**(也称为**套索**):模型的复杂度表示为权重绝对值的和
*   **L2 正则化**(也称为**岭**):模型的复杂度表示为权重的平方和
*   **弹性网络正则化**:模型的复杂性通过前面两种技术的结合来捕捉

注意，正则化的相同思想可以独立地应用于权重、模型和激活。

因此，尝试正则化是提高网络性能的一种好方法，尤其是在明显过度拟合的情况下。这组实验留给有兴趣的读者作为练习。

请注意，Keras 支持 l1、l2 和弹性网正则化。添加正则化很容易；例如，这里我们有一个用于内核的`l2`正则化子(权重 *W* ):

```py

from keras import regularizers model.add(Dense(64, input_dim=64, kernel_regularizer=regularizers.l2(0.01)))

```

可用参数的完整描述可在:[https://keras.io/regularizers/](https://keras.io/regularizers/)获得。



# 超参数调谐

前面的实验给了我们微调网络的机会。然而，适用于这个例子的不一定适用于其他例子。对于一个给定的网，确实有多个参数可以优化(比如`hidden neurons`、`BATCH_SIZE`的个数、`epochs`的个数，根据网本身的复杂程度还有很多)。

超参数调整是找到使成本函数最小化的那些参数的最佳组合的过程。关键思想是，如果我们有 *n* 个参数，那么我们可以想象它们定义了一个具有 *n* 维的空间，目标是找到这个空间中对应于成本函数的最优值的点。实现这个目标的一种方法是在这个空间中创建一个网格，并系统地检查每个网格顶点的成本函数所假定的值是多少。换句话说，参数被划分到桶中，并且通过强力方法检查不同的值组合。



# 预测产量

当一个网络被训练后，它当然可以用于预测。在喀拉斯，这很简单。我们可以使用以下方法:

```py

# calculate predictions
predictions = model.predict(X)

```

对于给定的输入，可以计算几种类型的输出，包括一种方法:

*   `model.evaluate()`:用于计算损耗值
*   `model.predict_classes()`:用于计算类别输出
*   `model.predict_proba()`:用于计算类别概率



# 反向传播的实用概述

多层感知器通过称为反向传播的过程从训练数据中学习。这个过程可以被描述为一种一旦发现错误就逐步纠正错误的方法。让我们看看这是如何工作的。

请记住，每个神经网络层都有一组关联的权重，用于确定一组给定输入的输出值。除此之外，记住一个神经网络可以有多个隐藏层。

一开始，所有的权重都是随机分配的。然后，为训练集中的每个输入激活网络:值从输入阶段通过隐藏阶段向前传播*到输出阶段，在输出阶段进行预测(注意，我们保持下图简单，仅用绿色虚线表示几个值，但实际上，所有值都通过网络向前传播):*

![](assets/B06258_01_32.png)

因为我们知道训练集中的真实观察值，所以可以计算预测中的误差。回溯的关键直觉是将误差传播回去，并使用适当的优化算法(例如梯度下降)来调整神经网络权重，目的是减少误差(同样为了简单起见，仅表示几个误差值):

![](assets/B06258_01_33.png)

从输入到输出的正向传播和误差的反向传播的过程被重复几次，直到误差低于预定的阈值。整个过程如下图所示:

![](assets/B06258_01_34.png)

特征表示输入，标签在这里用于驱动学习过程。该模型以这样的方式更新，即损失函数逐渐最小化。在神经网络中，真正重要的不是单个神经元的输出，而是每一层中调整的集体权重。因此，网络逐步调整其内部权重，使预测增加正确预测的标签数量。当然，使用正确的集合特征和有质量标记的数据是在学习过程中最小化偏差的基础。



# 走向深度学习方法

在玩手写数字识别的时候，我们得出了一个结论，越接近 99%的准确率，越难提高。如果我们想有更多的改进，我们肯定需要一个新的想法。我们遗漏了什么？想想吧。

最基本的直觉是，到目前为止，我们丢失了与图像的局部空间性相关的所有信息。具体来说，这段代码将位图(表示每个书写的数字)转换为平面向量，空间局部性消失了:

```py

#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784
X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)

```

然而，这并不是我们大脑的工作方式。请记住，我们的视觉是基于多个皮层层次的，每一层都识别越来越多的结构化信息，但仍保持局部性。首先我们看到单个像素，然后从中我们识别简单的几何形状，然后是越来越复杂的元素，如物体、人脸、人体、动物等等。

在[第 3 章](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml)、*用 ConvNets* 进行深度学习中，我们将看到一种被称为**卷积神经网络** ( **CNN** )的特定类型的深度学习网络已经被开发出来，它同时考虑了保留图像中(以及更一般地，任何类型的信息中)的空间局部性的思想和通过渐进抽象级别进行学习的思想:一层，你只能学习简单的模式；有了不止一层，你就可以学习多种模式。在讨论 CNN 之前，我们需要讨论 Keras 架构的一些方面，并对一些额外的机器学习概念进行实际介绍。这将是下一章的主题。



# 摘要

在本章中，您学习了神经网络的基础知识，更具体地说，什么是感知器，什么是多层感知器，如何在 Keras 中定义神经网络，如何在建立好的基线后逐步改进指标，以及如何微调超参数的空间。除此之外，您现在还可以直观地了解一些有用的激活函数(sigmoid 和 ReLU)是什么，以及如何使用基于梯度下降、随机梯度下降或更复杂方法(如 Adam 和 RMSprop)的反向传播算法来训练网络。

在下一章，我们将看到如何在 AWS、微软 Azure、谷歌云以及你自己的机器上安装 Keras。除此之外，我们将概述 Keras APIs。