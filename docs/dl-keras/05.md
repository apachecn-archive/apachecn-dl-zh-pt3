

# 五、词嵌入

维基百科将词嵌入定义为一组语言建模和特征学习技术的统称，这些技术在**自然语言处理** ( **NLP** )中，来自词汇表的单词或短语被映射到实数的向量。

词嵌入是一种将文本中的单词转换为数字向量的方法，以便它们可以被要求向量作为数字输入的标准机器学习算法分析。

你已经在[第一章](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml)、*神经网络基础*中学习了一种叫做**一键编码**的词嵌入。一键编码是最基本的嵌入方法。概括地说，一键编码通过词汇大小的向量来表示文本中的单词，其中只有对应于该单词的条目是 1，而所有其他条目都是 0。

一键编码的一个主要问题是没有办法表示单词之间的相似性。在任何给定的语料库中，您都会期望诸如(*猫*、*狗*)、(*刀*、*勺*)等单词具有一些相似性。向量之间的相似性使用点积来计算，点积是向量元素之间的逐元素乘法的总和。在独热编码向量的情况下，语料库中任意两个词之间的点积总是零。

为了克服一次性编码的局限性，NLP 社区借用了来自**信息检索** ( **IR** )的技术，使用文档作为上下文来矢量化文本。值得注意的技术有 TF-IDF(【https://en.wikipedia.org/wiki/Tf%E2%80%93idf】)、**潜在语义分析**、**、【https://en.wikipedia.org/wiki/Latent_semantic_analysis】、)、话题建模([、](https://en.wikipedia.org/wiki/Topic_model))。然而，这些表示捕捉到了稍微不同的以文档为中心的语义相似性概念。**

词嵌入技术的发展在 2000 年就开始了。词嵌入不同于先前的基于信息检索的技术，因为它们使用单词作为它们的上下文，从人类理解的角度来看，这导致了更自然形式的语义相似性。今天，词嵌入是用于各种 NLP 任务的矢量化文本的选择技术，例如文本分类、文档聚类、词性标注、命名实体识别、情感分析等等。

在本章中，我们将了解词嵌入的两种具体形式，GloVe 和 word2vec，统称为单词的分布式表示。这些嵌入已被证明更有效，并在深度学习和 NLP 社区中被广泛采用。

我们还将学习在 Keras 代码中生成自己的嵌入的不同方法，以及如何使用和微调预先训练的 word2vec 和 GloVe 模型。

在本章中，我们将讨论以下主题:

*   在上下文中建立单词的各种分布表示
*   构建利用嵌入来执行 NLP 任务的模型，如句子解析和情感分析



# 分布式表示

分布式表示试图通过考虑单词与上下文中其他单词的关系来捕捉单词的含义。这一想法是从 J. R. Firth(欲了解更多信息，请参考文章:*Document Embedding with Paragraph Vectors*，作者:Andrew M. Dai、Christopher Olah 和 Quoc V. Le，arXiv:1507.07998，2015)的这段引文中捕捉到的，这位语言学家首先提出了这一想法:

从一个字所交的朋友，你就可以知道这个字。

考虑下面的一对句子:

巴黎是法国的首都。
*柏林是德国的首都。*

即使假设你没有世界地理知识(或者英语知识)，你仍然会毫不费力地得出这样的结论:单词对( *Paris* 、 *Berlin* )和( *France* 、 *Germany* )以某种方式相关联，并且每对单词中的对应单词以同样的方式相互关联，即:

*法国巴黎；德国柏林*

因此，分布式表示的目的是找到一个通用变换函数φ来将每个单词转换成其相关向量，使得以下形式的关系成立:

![](assets/paris-framce-eqn.png)

换句话说，分布式表示旨在将单词转换成向量，其中向量之间的相似性与单词之间的语义相似性相关。

最广为人知的词嵌入是 word2vec 和 GloVe，我们将在后续章节中更详细地介绍它们。



# word2vec

word2vec 模型组是由托马斯·米科洛夫领导的谷歌研究团队在 2013 年创建的。这些模型是无监督的，将大量文本作为输入，并产生单词的向量空间。word2vec 嵌入空间的维数通常低于 one-hot 嵌入空间的维数，one-hot 嵌入空间是词汇表的大小。与独热嵌入空间的稀疏嵌入相比，嵌入空间也更加密集。

word2vec 的两种架构如下:

*   **连续包话** ( **CBOW** )
*   **Skip-gram**

在 CBOW 架构中，给定周围单词的窗口，模型预测当前单词。此外，上下文单词的顺序不影响预测(即单词袋假设)。在 skip-gram 体系结构的情况下，该模型在给定中心单词的情况下预测周围的单词。根据作者的说法，CBOW 速度更快，但 skip-gram 在预测不常用词方面做得更好。

值得注意的一件有趣的事情是，即使 word2vec 创建了用于深度学习 NLP 模型的嵌入，我们将讨论的 word2vec 的两种风格，也是最近最成功和公认的模型，都是浅层神经网络。



# 跳格 word2vec 模型

跳过语法模型被训练成在给定当前单词的情况下预测周围的单词。为了理解跳格 word2vec 模型是如何工作的，请考虑下面的例句:

我喜欢绿鸡蛋和火腿。

假设窗口大小为三，这个句子可以分解成以下几组(上下文，单词)对:

*(【我，绿】，爱)*
*(【爱，蛋】，绿)*
*(【绿，和】，蛋)*
*...*

由于 skip-gram 模型预测给定中心词的上下文词，我们可以将前面的数据集转换为(输入，输出)对之一。也就是说，给定一个输入单词，我们期望 skip-gram 模型预测输出单词:

*(爱，我)，(爱，绿)，(绿，爱)，(绿，蛋)，(蛋，绿)，(蛋，和)，...*

我们还可以通过将每个输入单词与词汇表中的某个随机单词配对来生成额外的负样本。例如:

*(爱，山姆)，(爱，斑马)，(绿色，东西)，...*

最后，我们为我们的分类器生成正例和负例:

*((爱，我)，1)，((爱，绿)，1)，...，((爱，山姆)，0)，((爱，斑马)，0)，...*

我们现在可以训练一个分类器，它接受一个单词向量和一个上下文向量，并学习根据它看到的是正样本还是负样本来预测 1 或 0。这个经过训练的网络的可交付结果是词嵌入层的权重(下图中的灰色框):

![](assets/word2vec-skipgram.png)

skip-gram 模型可以在 Keras 中构建，如下所示。假设词汇表大小设置为`5000`，输出嵌入大小为`300`，窗口大小为`1`。窗口大小为 1 意味着单词的上下文是紧邻左侧和右侧的单词。我们首先处理导入，并将变量设置为初始值:

```py

from keras.layers import Merge
from keras.layers.core import Dense, Reshape
from keras.layers.embeddings import Embedding
from keras.models import Sequential

vocab_size = 5000
embed_size = 300

```

然后我们为单词创建一个顺序模型。这个模型的输入是词汇表中的单词 ID。嵌入权重最初被设置为小的随机值。在训练期间，模型将使用反向传播来更新这些权重。下一层将输入整形为嵌入大小:

```py

word_model = Sequential()
word_model.add(Embedding(vocab_size, embed_size,
                         embeddings_initializer="glorot_uniform",
                         input_length=1))
word_model.add(Reshape((embed_size, )))

```

我们需要的另一个模型是上下文单词的顺序模型。对于我们的每个跳词对，我们有一个对应于目标单词的上下文单词，因此该模型与单词模型相同:

```py

context_model = Sequential()
context_model.add(Embedding(vocab_size, embed_size,
                  embeddings_initializer="glorot_uniform",
                  input_length=1))
context_model.add(Reshape((embed_size,)))

```

两个模型的输出都是一个大小为(`embed_size`)的向量。使用点积将这些输出合并为一个输出，并输入到密集层，该层有一个包裹在 sigmoid 激活层中的输出。你已经在[第一章](c2484fb4-248d-49ed-8166-06aff812e5e9.xhtml)、*神经网络基础*中看到了 sigmoid 激活函数。您应该记得，它会调制输出，因此高于 0.5 的数字会迅速趋向于 1 并变平，低于 0.5 的数字会迅速趋向于 0 并变平:

```py

model = Sequential()
model.add(Merge([word_model, context_model], mode="dot"))
model.add(Dense(1, init="glorot_uniform", activation="sigmoid"))
model.compile(loss="mean_squared_error", optimizer="adam")

```

使用的损失函数是`mean_squared_error`；这个想法是最小化正面例子的点积，最大化反面例子的点积。如果您还记得，点积将两个向量的相应元素相乘，并将结果相加，这将导致相似向量的点积高于不相似向量，因为前者有更多的重叠元素。

Keras 提供了一个方便的函数来提取已经转换为单词索引列表的文本的跳格。下面是一个使用这个函数来提取生成的 56 个跳跃图中的前 10 个(包括正的和负的)的例子。

我们首先声明必要的导入和要分析的文本:

```py

from keras.preprocessing.text import *
from keras.preprocessing.sequence import skipgrams

text = "I love green eggs and ham ."

```

下一步是声明`tokenizer`并根据它运行文本。这将生成单词标记列表:

```py

tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])

```

`tokenizer`创建一个字典，将每个唯一的单词映射到一个整数 ID，并使其在`word_index`属性中可用。我们提取它并创建一个双向查找表:

```py

word2id = tokenizer.word_index
id2word = {v:k for k, v in word2id.items()}

```

最后，我们将输入的单词列表转换为 id 列表，并将其传递给`skipgrams`函数。然后，我们打印生成的 56 个(对，标签)跳格元组中的前 10 个:

```py

wids = [word2id[w] for w in text_to_word_sequence(text)]
pairs, labels = skipgrams(wids, len(word2id))
print(len(pairs), len(labels))
for i in range(10):
    print("({:s} ({:d}), {:s} ({:d})) -> {:d}".format(
          id2word[pairs[i][0]], pairs[i][0], 
          id2word[pairs[i][1]], pairs[i][1], 
          labels[i]))

```

代码的结果如下所示。请注意，您的结果可能会有所不同，因为 skip-gram 方法是从正例的可能性池中随机抽取结果的。此外，用于生成负样本的负采样过程包括从文本中随机配对任意标记。随着输入文本大小的增加，更有可能出现不相关的词对。在我们的例子中，因为我们的文本非常短，所以它也有可能最终产生正面的例子。

```py

(and (1), ham (3)) -> 0
(green (6), i (4)) -> 0
(love (2), i (4)) -> 1
(and (1), love (2)) -> 0
(love (2), eggs (5)) -> 0
(ham (3), ham (3)) -> 0
(green (6), and (1)) -> 1
(eggs (5), love (2)) -> 1
(i (4), ham (3)) -> 0
(and (1), green (6)) -> 1 

```

这个例子的代码可以在本章源代码下载的`skipgram_example.py`中找到。



# CBOW word2vec 模型

现在让我们看看 CBOW word2vec 模型。回想一下，CBOW 模型在给定上下文单词的情况下预测中心单词。因此，在以下示例的第一个元组中，CBOW 模型需要预测输出单词 *love* ，给定上下文单词 *I* 和 *green* :

*([我，绿]，爱)([爱，蛋]，绿)([绿，和]，蛋)...*

像 skip-gram 模型一样，CBOW 模型也是一个分类器，它将上下文单词作为输入，并预测目标单词。该架构比跳格模型更简单。模型的输入是上下文单词的单词 id。这些单词 id 被送入一个公共嵌入层，该层用小的随机权重初始化。嵌入层将每个单词 ID 转换成大小为(`embed_size`)的向量。因此，这一层将输入上下文的每一行转换成一个大小为(`2*window_size`、`embed_size`)的矩阵。这然后被馈送到 lambda 层，该层计算所有嵌入的平均值。这个平均值然后被馈送到一个密集层，该层为每一行创建一个大小为(`vocab_size`)的密集向量。密集层上的激活函数是 softmax，它以概率的形式报告输出向量上的最大值。具有最大概率的 ID 对应于目标单词。

CBOW 模型的可交付结果是来自嵌入层的权重，在下图中以灰色显示:

![](assets/word2vec-cbow.png)

该模型对应的 Keras 代码如下所示。再次假设词汇表大小为`5000`，嵌入大小为`300`，上下文窗口大小为`1`。我们的第一步是设置所有的导入和这些值:

```py

from keras.models import Sequential
from keras.layers.core import Dense, Lambda
from keras.layers.embeddings import Embedding
import keras.backend as K

vocab_size = 5000
embed_size = 300
window_size = 1

```

然后，我们构建一个序列模型，添加一个嵌入层，其权重用小的随机值初始化。注意，这个嵌入层的`input_length`等于上下文单词的数量。因此，每个上下文单词都被馈送到这一层，并将在反向传播期间联合更新权重。该层的输出是上下文词嵌入的矩阵，该矩阵由 lambda 层平均为单个向量(每行输入)。最后，密集层会将每一行转换成大小为(`vocab_size`)的密集向量。目标单词是其 ID 在密集输出向量中具有最大值的单词:

```py

model = Sequential()
model.add(Embedding(input_dim=vocab_size, output_dim=embed_size, 
                    embeddings_initializer='glorot_uniform',
                    input_length=window_size*2))
model.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=  (embed_size,)))
model.add(Dense(vocab_size, kernel_initializer='glorot_uniform', activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer="adam")

```

这里使用的损失函数是`categorical_crossentropy`，这是有两个或更多(在我们的例子中是`vocab_size`)类别的情况下的常见选择。

该示例的源代码可以在本章源代码下载中的`keras_cbow.py`文件中找到。



# 从模型中提取 word2vec 嵌入

如前所述，尽管两个 word2vec 模型都可以归结为一个分类问题，但我们对分类问题本身并不感兴趣。相反，我们感兴趣的是这种分类过程的副作用，即，将单词从词汇表转换为其密集的低维分布式表示的权重矩阵。

有很多例子可以说明这些分布式表示如何表现出令人惊讶的句法和语义信息。例如，如 Tomas Mikolov 在 NIPS 2013 上的演示中的下图所示(有关更多信息，请参考 T. Mikolov、I. Sutskever、K. Chen、G. S. Corrado、J. Dean、Q. Le 和 T. Strohmann 在 NIPS 2013 上撰写的文章:*使用神经网络学习文本表示*)，连接具有相似含义但相反性别的单词的向量在简化的 2D 空间中近似平行，我们通常可以通过算术运算获得非常直观的结果本演示提供了许多其他示例。

![](assets/word2vec_regularities.png)

直观上，训练过程将足够的信息传递给内部编码，以预测在输入单词的上下文中出现的输出单词。因此，代表单词的点在这个空间中移动，更靠近与其共现的单词。这导致相似的单词聚集在一起。与这些相似的词同时出现的词也以相似的方式聚集在一起。结果，连接表示语义相关点的点的向量倾向于在分布式表示中展示这些规律性。

Keras 提供了一种从训练好的模型中提取权重的方法。对于 skip-gram 示例，可以如下提取嵌入权重:

```py

merge_layer = model.layers[0]
word_model = merge_layer.layers[0]
word_embed_layer = word_model.layers[0]
weights = word_embed_layer.get_weights()[0]

```

类似地，CBOW 示例的嵌入权重可以使用以下一行代码提取:

```py

weights = model.layers[0].get_weights()[0]

```

在这两种情况下，权重矩阵的形状是`vocab_size`和`embed_size`。为了计算词汇表中单词的分布式表示，您需要通过将单词索引的位置设置为大小为(`vocab_size`)的零向量中的 1 来构建一个独热向量，并将其与矩阵相乘以获得大小为(`embed_size`)的嵌入向量。

来自 Christopher Olah 所做工作的词嵌入的可视化(关于更多信息，请参考文章:*利用段落向量的文档嵌入*，作者:Andrew M. Dai、Christopher Olah 和 Quoc V. Le，arXiv:1507.07998，2015)如下所示。这是一个可视化的词嵌入减少到二维和可视化与 T-SNE。使用 WordNet synset 集群选择构成实体类型的单词。如您所见，对应于相似实体类型的点往往会聚集在一起:

![](assets/word_embeddings_colah.png)

该示例的源代码可以在源代码下载中的`keras_skipgram.py`中找到。



# 使用 word2vec 的第三方实现

在过去的几节中，我们已经广泛地介绍了 word2vec。至此，您已经理解了 skip-gram 和 CBOW 模型是如何工作的，以及如何使用 Keras 构建这些模型的自己的实现。然而，word2vec 的第三方实现很容易获得，除非您的用例非常复杂或不同，否则只使用一个这样的实现而不是使用您自己的实现是有意义的。

gensim 库提供了 word2vec 的实现。尽管这是一本关于 Keras 而不是 gensim 的书，但我们包括了对此的讨论，因为 Keras 不提供对 word2vec 的任何支持，并且将 gensim 实现集成到 Keras 代码中是非常常见的做法。

gensim 的安装相当简单，在 gensim 安装页面([https://radimrehurek.com/gensim/install.html](https://radimrehurek.com/gensim/install.html))上有详细描述。

以下代码显示了如何使用 gensim 构建 word2vec 模型，并使用 text8 语料库中的文本对其进行训练，该语料库可从以下网址下载:[http://mattmahoney.net/dc/text8.zip](http://mattmahoney.net/dc/text8.zip)。text8 语料库是一个包含大约 1700 万个来自维基百科文本的单词的文件。维基百科的文本被清理掉了标记、标点和非 ASCII 文本，清理后的文本的前 1 亿个字符成为了 text8 语料库。这个语料库通常被用作 word2vec 的示例，因为它可以快速训练并产生良好的结果。首先，我们像往常一样设置导入:

```py

from gensim.models import KeyedVectors
import logging
import os

```

然后我们从文本语料库中读入单词，并把这些单词分成 50 个单词的句子。gensim 库提供了一个内置的 text8 处理程序来做类似的事情。因为我们想说明如何用任何(最好是大的)可能适合或不适合内存的语料库生成模型，所以我们将向您展示如何使用 Python 生成器生成这些句子。

`Text8Sentences`类将从 text8 文件中生成由`maxlen`个单词组成的句子。在这种情况下，我们确实将整个文件接收到内存中，但是在遍历文件目录时，生成器允许我们一次将部分数据加载到内存中，对它们进行处理，然后将它们提供给调用者:

```py

class Text8Sentences(object):
  def __init__(self, fname, maxlen):
    self.fname = fname
    self.maxlen = maxlen

  def __iter__(self):
    with open(os.path.join(DATA_DIR, "text8"), "rb") as ftext:
      text = ftext.read().split(" ")
      sentences, words = [], []
      for word in text:
        if len(words) >= self.maxlen:
          yield words
          words = []
          words.append(word)
          yield words

```

然后，我们设置呼叫者代码。gensim word2vec 使用 Python 日志来报告进度，所以我们首先启用它。下一行声明了一个`Text8Sentences`类的实例，下一行用数据集中的句子训练模型。我们已经选择嵌入向量的大小为`300`，并且我们只考虑在语料库中出现最少 30 次的单词。窗口大小默认为`5`，所以我们会考虑 *w [i-5]* ， *w [i-4]* ， *w [i-3]* ， *w [i-2]* ， *w [i-1]* ， *w [i+1] 默认情况下，创建的 word2vec 模型是 CBOW，但是您可以通过在参数中设置`sg=1`来更改它:*

```py

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)

DATA_DIR = "../data/"
sentences = Text8Sentences(os.path.join(DATA_DIR, "text8"), 50)
model = word2vec.Word2Vec(sentences, size=300, min_count=30)

```

word2vec 实现将对数据进行两次传递，首先生成词汇表，然后构建实际的模型。运行时，您可以在控制台上看到它的进度:

![](assets/ss-5-1.png)

一旦创建了模型，我们应该将结果向量标准化。根据文档，这可以节省大量内存。一旦模型完成训练，我们可以选择将其保存到磁盘:

```py

model.init_sims(replace=True)
model.save("word2vec_gensim.bin")

```

可以使用以下调用将保存的模型取回内存:

```py

model = Word2Vec.load("word2vec_gensim.bin")

```

我们现在可以查询该模型，以找到它知道的所有单词:

```py

>>> model.vocab.keys()[0:10]
['homomorphism',
'woods',
'spiders',
'hanging',
'woody',
'localized',
'sprague',
'originality',
'alphabetic',
'hermann']

```

我们可以找到给定单词的实际矢量嵌入:

```py

>>> model["woman"]
 array([ -3.13099056e-01, -1.85702944e+00, 1.18816841e+00,
 -1.86561719e-01, -2.23673001e-01, 1.06527400e+00,
 &mldr;
 4.31755871e-01, -2.90115297e-01, 1.00955181e-01,
 -5.17173052e-01, 7.22485244e-01, -1.30940580e+00], dtype=”float32”)

```

我们还可以找到与某个单词最相似的单词:

```py

>>> model.most_similar("woman")
 [('child', 0.7057571411132812),
 ('girl', 0.702182412147522),
 ('man', 0.6846336126327515),
 ('herself', 0.6292711496353149),
 ('lady', 0.6229539513587952),
 ('person', 0.6190367937088013),
 ('lover', 0.6062309741973877),
 ('baby', 0.5993420481681824),
 ('mother', 0.5954475402832031),
 ('daughter', 0.5871444940567017)]

```

我们可以提供寻找单词相似性的提示。例如，以下命令返回类似于`woman`和`king`但不同于`man`的前 10 个单词:

```py

>>> model.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)
 [('queen', 0.6237582564353943),
 ('prince', 0.5638638734817505),
 ('elizabeth', 0.5557916164398193),
 ('princess', 0.5456407070159912),
 ('throne', 0.5439794063568115),
 ('daughter', 0.5364126563072205),
 ('empress', 0.5354889631271362),
 ('isabella', 0.5233952403068542),
 ('regent', 0.520746111869812),
 ('matilda', 0.5167444944381714)]

```

我们还可以找到单个单词之间的相似之处。为了感觉单词在嵌入空间中的位置如何与其语义相关，让我们看下面的单词对:

```py

>>> model.similarity("girl", "woman")
 0.702182479574
 >>> model.similarity("girl", "man")
 0.574259909834
 >>> model.similarity("girl", "car")
 0.289332921793
 >>> model.similarity("bus", "car")
 0.483853497748

```

如你所见，`girl`和`woman`比`girl`和`man`更相似，`car`和`bus`比`girl`和`car`更相似。这非常符合我们人类对这些词的直觉。

该示例的源代码可以在源代码下载中的`word2vec_gensim.py`中找到。



# 探险手套

Jeffrey Pennington、Richard Socher 和 Christopher Manning 创建了单词表示的全局向量(GloVe ),更多信息请参考文章:*GloVe:Global Vectors for Word Representation*，作者 J. Pennington、R. Socher 和 C. Manning，2014 年自然语言处理经验方法会议(EMNLP)会议录，第 1532-1543 页，2013 年)。作者将 GloVe 描述为一种无监督的学习算法，用于获得单词的矢量表示。在来自语料库的聚集的全局单词-单词共现统计上执行训练，并且所得的表示展示了单词向量空间的有趣的线性子结构。

GloVe 与 word2vec 的不同之处在于，word2vec 是一个预测模型，而 GloVe 是一个基于计数的模型。第一步是构建在训练语料库中同时出现的(单词，上下文)对的大矩阵。该矩阵的每个元素表示由行表示的单词在由列表示的上下文(通常是单词序列)中出现的频率，如下图所示:

![](assets/glove-matfact.png)

GloVe 过程将共现矩阵转换成一对(单词、特征)和(特征、上下文)矩阵。这个过程被称为**矩阵分解**，并使用**随机梯度下降** ( **SGD** )，一种迭代数值方法来完成。等式形式的重写:

![](assets/matfact-eqn.png)

这里， *R* 是原共生矩阵。我们首先用随机值填充 *P* 和 *Q* ，并试图通过将它们相乘来重构矩阵*R’*。重构的矩阵 *R'* 和原始矩阵 *R* 之间的差异告诉我们需要改变 *P* 和 *Q* 的值多少，以将 *R'* 移近 *R* ，从而最小化重构误差。这重复多次，直到 SGD 收敛并且重建误差低于指定阈值。在这一点上，(词，特征)矩阵是手套嵌入。为了加速这个过程，SGD 经常以并行模式使用，正如 *HOGWILD 中所概述的！*纸。

需要注意的一点是，基于预测神经网络的模型(如 word2vec)和基于计数的模型(如 GloVe)在意图上非常相似。它们都建立了一个向量空间，其中一个单词的位置受其相邻单词的影响。神经网络模型从单词共现的单个例子开始，基于计数的模型从语料库中所有单词之间的聚合共现统计开始。最近的几篇论文已经证明了这两种模型之间的相关性。

在本书中，我们不会更详细地介绍手套向量的生成。尽管 GloVe 通常比 word2vec 显示出更高的准确性，并且如果使用并行化，训练起来更快，但是 Python 工具不如 word2vec 成熟。截至本文撰写之时，唯一可用的工具是 GloVe-Python 项目([https://github.com/maciejkula/glove-python](https://github.com/maciejkula/glove-python))，它为 GloVe on Python 提供了一个玩具实现。



# 使用预先训练的嵌入

一般来说，只有当你有大量非常专业的文本时，你才会从头开始训练你自己的 word2vec 或 GloVe 模型。到目前为止，嵌入最常见的用例是在您的网络中以某种方式使用预先训练的嵌入。您在网络中使用嵌入的三种主要方式如下:

*   从头开始学习嵌入
*   从预先训练的手套/word2vec 模型中微调学习嵌入
*   从预先训练的手套/word2vec 模型中查找嵌入

在第一种选择中，嵌入权重被初始化为小的随机值，并使用反向传播来训练。您可以在 Keras 的 skip-gram 和 CBOW 模型示例中看到这一点。在网络中使用 Keras 嵌入层时，这是默认模式。

在第二个选项中，从预先训练的模型构建权重矩阵，并用该权重矩阵初始化嵌入层的权重。网络将使用反向传播来更新这些权重，但是由于良好的起始权重，模型将更快地收敛。

第三种选择是从预先训练的模型中查找词嵌入，并将您的输入转换为嵌入向量。然后你可以在转换后的数据上训练任何机器学习模型(也就是说，甚至不一定是深度学习网络)。如果预先训练的模型在与目标域相似的域上被训练，这通常非常有效，并且是最便宜的选择。

对于英语文本的一般使用，可以使用谷歌的 word2vec 模型，该模型从谷歌新闻数据集中训练了超过 100 亿个单词。词汇量大约为 300 万单词，嵌入的维数为 300。Google 新闻模型(约 1.5 GB)可以从这里下载:[https://drive . Google . com/file/d/0 b 7 xkcwpi 5 kdynlnuttlss 21 pqmm/edit？usp =分享](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)。

类似地，可以从 GloVe 网站下载一个预训练模型，该模型使用来自英语维基百科和 gigaword 语料库的 60 亿个标记进行训练。词汇表的大小约为 400，000 个单词，下载提供了 50、100、200 和 300 维的向量。模型大小约为 822 MB。这里是这个模型的直接下载网址([http://nlp.stanford.edu/data/glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip))。基于普通爬行和 Twitter 的较大模型也可从同一位置获得。

在接下来的几节中，我们将看看如何以列出的三种方式使用这些预训练模型。



# 从头开始学习嵌入

在这个例子中，我们将训练一个一维**卷积神经网络** ( **CNN** )来将句子分类为肯定或否定。你已经在[第三章](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml)、*用 ConvNets 深度学习*看到了如何用二维 CNN 对图像进行分类。回想一下，CNN 通过加强相邻层神经元之间的局部连通性来利用图像中的空间结构。

句子中的单词呈现线性结构，就像图像呈现空间结构一样。传统的(非深度学习)NLP 语言建模方法包括创建单词*n*-grams(【https://en.wikipedia.org/wiki/N-gram】[)](https://en.wikipedia.org/wiki/N-gram)，以利用单词之间固有的线性结构。一维 CNN 做类似的事情，学习卷积滤波器，每次对句子进行几个词的操作，max 将结果汇集起来，创建一个向量，代表句子中最重要的思想。

还有一类神经网络，叫做**循环神经网络** ( **RNN** )，专门用来处理序列数据，包括文本，也就是单词序列。RNNs 中的处理不同于 CNN 中的处理。我们将在下一章学习 RNNs。

在我们的示例网络中，输入文本被转换为单词索引序列。注意，我们已经使用了**自然语言工具包** ( **NLTK** )将文本解析成句子和单词。我们也可以使用正则表达式来做到这一点，但是 NLTK 提供的统计模型在解析方面比正则表达式更强大。如果您正在使用词嵌入，很可能您也正在使用 NLP，在这种情况下，您可能已经安装了 NLTK。

这个链接([http://www.nltk.org/install.html](http://www.nltk.org/install.html))有帮助你在你的机器上安装 NLTK 的信息。您还需要安装 NLTK 数据，这是一些 NLTK 标准的经过训练的语料库。NLTK 数据的安装说明可在此处获得:【http://www.nltk.org/data.html】T2。

单词索引序列被输入到一个固定大小的嵌入层数组中(在我们的例子中，是最长句子中的单词数)。默认情况下，嵌入层被初始化为随机值。嵌入层的输出连接到 1D 卷积层，该卷积层以 256 种不同的方式卷积(在我们的例子中)单词三元组(本质上，它对词嵌入应用不同的学习到的线性权重组合)。这些特征然后由全局最大池层汇集成单个池字。这个向量(256)然后被输入到密集层，该密集层输出向量(2)。softmax 激活将返回一对概率，一个对应于积极情绪，另一个对应于消极情绪。网络如下图所示:

![](assets/umich_conv1d.png)

让我们来看看如何使用 Keras 对此进行编码。首先我们申报进口商品。在常量之后，您会注意到我将`random.seed`的值设置为`42`。这是因为我们希望在运行之间得到一致的结果。由于权重矩阵的初始化是随机的，初始化的差异会导致输出的差异，因此这是一种控制以下情况的方法:

```py

from keras.layers.core import Dense, Dropout, SpatialDropout1D
from keras.layers.convolutional import Conv1D
from keras.layers.embeddings import Embedding
from keras.layers.pooling import GlobalMaxPooling1D
from kera
s.models import Sequential
from keras.preprocessing.sequence import pad_sequences
from keras.utils import np_utils
from sklearn.model_selection import train_test_split
import collections
import matplotlib.pyplot as plt
import nltk
import numpy as np

np.random.seed(42)

```

我们声明我们的常数。对于本章中的所有后续示例，我们将对 Kaggle 上的 UMICH SI650 情感分类竞赛中的句子进行分类。该数据集大约有 7000 个句子，被标记为 *1* 表示肯定，而 *0* 表示否定。`INPUT_FILE`定义了这个句子和标签文件的路径。文件的格式是一个情感标签( *0* 或 *1* )后跟一个标签，再跟一个句子。

`VOCAB_SIZE`设置表示我们将只考虑文本中的前 5000 个标记。`EMBED_SIZE`设置是网络中嵌入层将生成的嵌入的大小。`NUM_FILTERS`是我们将为我们的卷积层训练的卷积滤波器的数量，`NUM_WORDS`是每个滤波器的大小，也就是我们一次将卷积多少个字。`BATCH_SIZE`和`NUM_EPOCHS`是每次提供给网络的记录数量，以及在训练期间我们将遍历整个数据集的次数:

```py

INPUT_FILE = "../data/umich-sentiment-train.txt"
VOCAB_SIZE = 5000
EMBED_SIZE = 100
NUM_FILTERS = 256
NUM_WORDS = 3
BATCH_SIZE = 64
NUM_EPOCHS = 20

```

在下一个模块中，我们首先阅读输入的句子，并用语料库中最常用的单词构建我们的词汇。然后，我们使用这个词汇表将输入的句子转换成单词索引列表:

```py

counter = collections.Counter()
fin = open(INPUT_FILE, "rb")
maxlen = 0
for line in fin:
    _, sent = line.strip().split("t")
    words = [x.lower() for x in   nltk.word_tokenize(sent)]
    if len(words) > maxlen:
        maxlen = len(words)
    for word in words:
        counter[word] += 1
fin.close()

word2index = collections.defaultdict(int)
for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):
    word2index[word[0]] = wid + 1
vocab_size = len(word2index) + 1
index2word = {v:k for k, v in word2index.items()}

```

我们将每个句子填充到预定的长度`maxlen`(在这种情况下是训练集中最长句子的字数)。我们还使用 Keras 实用函数将标签转换为分类格式。最后两步是处理文本输入的标准工作流程，我们会反复看到:

```py

xs, ys = [], []
fin = open(INPUT_FILE, "rb")
for line in fin:
    label, sent = line.strip().split("t")
    ys.append(int(label))
    words = [x.lower() for x in nltk.word_tokenize(sent)]
    wids = [word2index[word] for word in words]
    xs.append(wids)
fin.close()
X = pad_sequences(xs, maxlen=maxlen)
Y = np_utils.to_categorical(ys)

```

最后，我们将数据分成一个 *70/30* 训练和测试集。现在，数据已准备好输入网络:

```py

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)

```

我们定义了本节前面描述的网络:

```py

model = Sequential()
model.add(Embedding(vocab_size, EMBED_SIZE, input_length=maxlen)
model.add(SpatialDropout1D(Dropout(0.2)))
model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS,
activation="relu"))
model.add(GlobalMaxPooling1D())
model.add(Dense(2, activation="softmax"))

```

然后我们编译这个模型。由于我们的目标是二元的(正或负)，我们选择`categorical_crossentropy`作为损失函数。对于优化器，我们选择`adam`。然后，我们使用我们的训练集训练该模型，使用 64 的批量大小和 20 个时期的训练:

```py

model.compile(loss="categorical_crossentropy", optimizer="adam",
              metrics=["accuracy"])
history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,
                    epochs=NUM_EPOCHS,
                    validation_data=(Xtest, Ytest))

```

代码的输出如下所示:

![](assets/ss-5-4.png)

如你所见，网络在测试集上给了我们 98.6%的准确率。

这个例子的源代码可以在本章源代码下载的`learn_embedding_from_scratch.py`中找到。



# 微调从 word2vec 学到的嵌入

在这个例子中，我们将使用与我们用来从头学习嵌入的网络相同的网络。在代码方面，唯一的主要区别是加载 word2vec 模型并为嵌入层构建权重矩阵的额外代码块。

和往常一样，我们从导入开始，并为可重复性设置一个随机种子。除了我们之前看到的导入之外，还有一个额外的从 gensim 导入 word2vec 模型:

```py

from gensim.models import KeyedVectors
from keras.layers.core import Dense, Dropout, SpatialDropout1D
from keras.layers.convolutional import Conv1D
from keras.layers.embeddings import Embedding
from keras.layers.pooling import GlobalMaxPooling1D
from keras.models import Sequential
from keras.preprocessing.sequence import pad_sequences
from keras.utils import np_utils
from sklearn.model_selection import train_test_split
import collections
import matplotlib.pyplot as plt
import nltk
import numpy as np

np.random.seed(42)

```

接下来是设置常数。这里唯一的不同是我们将`NUM_EPOCHS`的设置从`20`减少到了`10`。回想一下，用来自预训练模型的值初始化矩阵往往会将它们设置为收敛更快的好值:

```py

INPUT_FILE = "../data/umich-sentiment-train.txt"
WORD2VEC_MODEL = "../data/GoogleNews-vectors-negative300.bin.gz"
VOCAB_SIZE = 5000
EMBED_SIZE = 300
NUM_FILTERS = 256
NUM_WORDS = 3
BATCH_SIZE = 64
NUM_EPOCHS = 10

```

下一个块从数据集中提取单词并创建最常用术语的词汇表，然后再次解析数据集以创建填充单词列表的列表。它还将标签转换为分类格式。最后，它将数据分为训练集和测试集。该模块与前面的示例相同，并在那里进行了深入解释:

```py

counter = collections.Counter()
fin = open(INPUT_FILE, "rb")
maxlen = 0
for line in fin:
   _, sent = line.strip().split("t")
   words = [x.lower() for x in nltk.word_tokenize(sent)]
   if len(words) > maxlen:
       maxlen = len(words)
   for word in words:
       counter[word] += 1
fin.close()

word2index = collections.defaultdict(int)
for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):
    word2index[word[0]] = wid + 1
vocab_sz = len(word2index) + 1
index2word = {v:k for k, v in word2index.items()}

xs, ys = [], []
fin = open(INPUT_FILE, "rb")
for line in fin:
    label, sent = line.strip().split("t")
    ys.append(int(label))
    words = [x.lower() for x in nltk.word_tokenize(sent)]
    wids = [word2index[word] for word in words]
    xs.append(wids)
fin.close()
X = pad_sequences(xs, maxlen=maxlen)
Y = np_utils.to_categorical(ys)

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3,
     random_state=42)

```

下一个块从预先训练的模型加载 word2vec 模型。这个模型使用大约 100 亿字的谷歌新闻文章进行训练，词汇量为 300 万。我们加载它，并从中查找词汇中单词的嵌入向量，并将嵌入向量写入我们的权重矩阵`embedding_weights`。这个权重矩阵的行对应于词汇表中的单词，每行的列构成单词的嵌入向量。

`embedding_weights`矩阵的维数是`vocab_sz`和`EMBED_SIZE`。`vocab_sz`比词汇表中唯一术语的最大数量多 1，额外的伪标记`_UNK_`表示词汇表中没有的单词。

请注意，我们词汇表中的一些单词可能在 Google News word2vec 模型中不存在，因此当我们遇到这样的单词时，它们的嵌入向量仍然是全零的默认值:

```py

# load word2vec model
word2vec = Word2Vec.load_word2vec_format(WORD2VEC_MODEL, binary=True)
embedding_weights = np.zeros((vocab_sz, EMBED_SIZE))
for word, index in word2index.items():
    try:
        embedding_weights[index, :] = word2vec[word]
    except KeyError:
        pass

```

我们定义我们的网络。该模块与上一个示例的不同之处在于，我们使用在前一模块中构建的`embedding_weights`矩阵来初始化嵌入层的权重:

```py

model = Sequential()
model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen,
          weights=[embedding_weights]))
model.add(SpatialDropout1D(Dropout(0.2)))
model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS,
                        activation="relu"))
model.add(GlobalMaxPooling1D())
model.add(Dense(2, activation="softmax"))

```

然后，我们用分类交叉熵损失函数和 Adam 优化器编译我们的模型，并用批量 64 训练网络 10 个时期，然后评估训练的模型:

```py

model.compile(optimizer="adam", loss="categorical_crossentropy",
              metrics=["accuracy"])
history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,
                    epochs=NUM_EPOCHS,
                    validation_data=(Xtest, Ytest))

score = model.evaluate(Xtest, Ytest, verbose=1)
print("Test score: {:.3f}, accuracy: {:.3f}".format(score[0], score[1]))

```

运行代码的输出如下所示:

```py

((4960, 42), (2126, 42), (4960, 2), (2126, 2))
 Train on 4960 samples, validate on 2126 samples
 Epoch 1/10
 4960/4960 [==============================] - 7s - loss: 0.1766 - acc: 0.9369 - val_loss: 0.0397 - val_acc: 0.9854
 Epoch 2/10
 4960/4960 [==============================] - 7s - loss: 0.0725 - acc: 0.9706 - val_loss: 0.0346 - val_acc: 0.9887
 Epoch 3/10
 4960/4960 [==============================] - 7s - loss: 0.0553 - acc: 0.9784 - val_loss: 0.0210 - val_acc: 0.9915
 Epoch 4/10
 4960/4960 [==============================] - 7s - loss: 0.0519 - acc: 0.9790 - val_loss: 0.0241 - val_acc: 0.9934
 Epoch 5/10
 4960/4960 [==============================] - 7s - loss: 0.0576 - acc: 0.9746 - val_loss: 0.0219 - val_acc: 0.9929
 Epoch 6/10
 4960/4960 [==============================] - 7s - loss: 0.0515 - acc: 0.9764 - val_loss: 0.0185 - val_acc: 0.9929
 Epoch 7/10
 4960/4960 [==============================] - 7s - loss: 0.0528 - acc: 0.9790 - val_loss: 0.0204 - val_acc: 0.9920
 Epoch 8/10
 4960/4960 [==============================] - 7s - loss: 0.0373 - acc: 0.9849 - val_loss: 0.0221 - val_acc: 0.9934
 Epoch 9/10
 4960/4960 [==============================] - 7s - loss: 0.0360 - acc: 0.9845 - val_loss: 0.0194 - val_acc: 0.9929
 Epoch 10/10
 4960/4960 [==============================] - 7s - loss: 0.0389 - acc: 0.9853 - val_loss: 0.0254 - val_acc: 0.9915
 2126/2126 [==============================] - 1s
 Test score: 0.025, accuracy: 0.993

```

经过 10 个时期的训练后，该模型在测试集上的准确率为 99.3%。这是对前一个例子的改进，在前一个例子中，我们在 20 个时期后获得了 98.6%的准确度。

这个例子的源代码可以在本章源代码下载的`finetune_word2vec_embeddings.py`中找到。



# 微调从手套学习的嵌入

使用预训练手套嵌入的微调与使用预训练 word2vec 嵌入的微调非常相似。事实上，除了为嵌入层构建权重矩阵的代码块之外，所有代码都是相同的。因为我们已经看过这个代码两次了，所以我将只关注从手套嵌入中构建权重矩阵的代码块。

手套嵌入物有多种口味。我们使用在来自英语维基百科和 gigaword 语料库的 60 亿个标记上预先训练的模型。该模型的词汇大小约为 400，000，下载提供了 50、100、200 和 300 维的向量。我们将使用来自 300 维模型的嵌入。

对于上一个示例，我们需要在代码中更改的唯一一件事是使用下面的代码块替换实例化 word2vec 模型并加载嵌入矩阵的代码块。如果我们使用向量大小不是 300 的模型，那么我们也需要更新`EMBED_SIZE`。

向量以空格分隔的文本格式提供，因此第一步是将代码读入字典`word2emb`。这类似于我们前面示例中实例化 Word2Vec 模型的代码行:

```py

GLOVE_MODEL = "../data/glove.6B.300d.txt"
word2emb = {}
fglove = open(GLOVE_MODEL, "rb")
for line in fglove:
    cols = line.strip().split()
    word = cols[0]
    embedding = np.array(cols[1:], dtype="float32")
    word2emb[word] = embedding
fglove.close()

```

然后，我们实例化大小为(`vocab_sz`和`EMBED_SIZE`)的嵌入权重矩阵，并填充来自`word2emb`字典的向量。在词汇表中找到但不在手套模型中找到的单词的向量保持设置为全零:

```py

embedding_weights = np.zeros((vocab_sz, EMBED_SIZE))
for word, index in word2index.items():
    try:
        embedding_weights[index, :] = word2emb[word]
    except KeyError:
        pass

```

这个程序的完整代码可以在 GitHub 上本书的代码库中的`finetune_glove_embeddings.py`中找到。运行的输出如下所示:

![](assets/ss-5-2.png)

这使我们在 10 个时期内获得了 99.1%的准确性，这几乎与我们使用 word2vec `embedding_weights`微调网络所获得的结果一样好。

这个例子的源代码可以在本章源代码下载的`finetune_glove_embeddings.py`中找到。



# 查找嵌入

我们的最终策略是从预先训练的网络中查找嵌入。在当前示例中，最简单的方法是将嵌入层的`trainable`参数设置为`False`。这确保了反向传播不会更新嵌入层上的权重:

```py

model.add(Embedding(vocab_sz, EMBED_SIZE, input_length=maxlen,
                     weights=[embedding_weights],
                     trainable=False))
model.add(SpatialDropout1D(Dropout(0.2)))

```

用 word2vec 和 GloVe 示例设置该值，在 10 个训练周期后，我们的准确率分别为 98.7%和 98.9%。

然而，一般来说，这不是您在代码中使用预训练嵌入的方式。通常，它涉及到对数据集进行预处理，通过在一个预先训练的模型中查找单词来创建单词向量，然后使用这些数据来训练其他模型。第二个模型将不包含嵌入层，甚至可能不是深度学习网络。

下面的例子描述了一个密集网络，该网络将大小为`100`的向量作为其输入，表示一个句子，并输出表示积极或消极情绪的`1`或`0`。我们的数据集仍然是来自 UMICH S1650 情感分类竞赛的数据集，大约有 7000 个句子。

如前所述，大部分代码都是重复的，所以我们只解释新的或需要解释的部分。
我们从导入开始，为可重复性设置随机种子，并设置一些常量值。为了为每个句子创建 100 维向量，我们为句子中的单词添加了 100 维向量，因此我们选择了`glove.6B.100d.txt`文件:

```py

from keras.layers.core import Dense, Dropout, SpatialDropout1D
from keras.models import Sequential
from keras.preprocessing.sequence import pad_sequences
from keras.utils import np_utils
from sklearn.model_selection import train_test_split
import collections
import matplotlib.pyplot as plt
import nltk
import numpy as np

np.random.seed(42)

INPUT_FILE = "../data/umich-sentiment-train.txt"
GLOVE_MODEL = "../data/glove.6B.100d.txt"
VOCAB_SIZE = 5000
EMBED_SIZE = 100
BATCH_SIZE = 64
NUM_EPOCHS = 10

```

下一个块读取句子并创建一个词频表。由此，选择最常见的 5，000 个标记，并创建查找表(从单词到单词索引，反之亦然)。此外，我们为词汇表中不存在的标记创建一个伪标记`_UNK_`。使用这些查找表，我们将每个句子转换为单词 id 序列，填充这些序列，以便所有序列都具有相同的长度(训练集中一个句子中的最大单词数)。我们还将标签转换为分类格式:

```py

counter = collections.Counter()
fin = open(INPUT_FILE, "rb")
maxlen = 0
for line in fin:
    _, sent = line.strip().split("t")
    words = [x.lower() for x in nltk.word_tokenize(sent)]
    if len(words) > maxlen:
        maxlen = len(words)
    for word in words:
        counter[word] += 1
fin.close()

word2index = collections.defaultdict(int)
for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):
     word2index[word[0]] = wid + 1
vocab_sz = len(word2index) + 1
index2word = {v:k for k, v in word2index.items()}
index2word[0] = "_UNK_"

ws, ys = [], []
fin = open(INPUT_FILE, "rb")
for line in fin:
    label, sent = line.strip().split("t")
    ys.append(int(label))
    words = [x.lower() for x in nltk.word_tokenize(sent)]
    wids = [word2index[word] for word in words]
    ws.append(wids)
fin.close()
W = pad_sequences(ws, maxlen=maxlen)
Y = np_utils.to_categorical(ys)

```

我们将手套向量加载到字典中。如果我们想在这里使用 word2vec，我们所要做的就是用 gensim `Word2Vec.load_word2vec_format()`调用替换这个块，并替换下面的块来查找 word2vec 模型，而不是`word2emb`字典:

```py

word2emb = collections.defaultdict(int)
fglove = open(GLOVE_MODEL, "rb")
for line in fglove:
    cols = line.strip().split()
    word = cols[0]
    embedding = np.array(cols[1:], dtype="float32")
    word2emb[word] = embedding
fglove.close()

```

下一个块从单词 ID 矩阵`W`中查找每个句子的单词，并用相应的嵌入向量填充矩阵`E`。这些嵌入向量然后被添加以创建句子向量，该句子向量被写回到`X`矩阵中。该代码块的输出是大小为(`num_records`和`EMBED_SIZE`)的矩阵`X`:

```py

X = np.zeros((W.shape[0], EMBED_SIZE))
for i in range(W.shape[0]):
    E = np.zeros((EMBED_SIZE, maxlen))
    words = [index2word[wid] for wid in W[i].tolist()]
    for j in range(maxlen):
         E[:, j] = word2emb[words[j]]
    X[i, :] = np.sum(E, axis=1)

```

现在，我们已经使用预训练模型对数据进行了预处理，并准备使用它来训练和评估我们的最终模型。让我们像往常一样将数据分成 *70/30* 训练/测试:

```py

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=42)

```

我们将为完成情感分析任务而训练的网络是一个简单的密集网络。我们用分类交叉熵损失函数和 Adam 优化器来编译它，并用我们从预训练嵌入中构建的句子向量来训练它。最后，我们在 30%测试集上评估该模型:

```py

model = Sequential()
model.add(Dense(32, input_dim=100, activation="relu"))
model.add(Dropout(0.2))
model.add(Dense(2, activation="softmax"))

model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
history = model.fit(Xtrain, Ytrain, batch_size=BATCH_SIZE,
                    epochs=NUM_EPOCHS,
                    validation_data=(Xtest, Ytest))

score = model.evaluate(Xtest, Ytest, verbose=1)
print("Test score: {:.3f}, accuracy: {:.3f}".format(score[0], score[1]))

```

使用手套嵌入的代码输出如下所示:

![](assets/ss-5-3.png)

当用 100 维手套嵌入进行预处理时，在 10 个时期的训练后，密集网络在测试集上给我们 96.5%的准确度。用 word2vec 嵌入(300 维固定)预处理后，网络在测试集上给出了 98.5%的准确率。

这个例子的源代码可以在本章源代码下载的`transfer_glove_embeddings.py`(对于手套例子)和`transfer_word2vec_embeddings.py`(对于 word2vec 例子)中找到。



# 摘要

在本章中，我们学习了如何将文本中的单词转换成矢量嵌入，并保留单词的分布语义。我们现在也有了一种直觉，为什么词嵌入表现出这种行为，以及为什么词嵌入对于处理文本数据的深度学习模型是有用的。

然后我们看了两个流行的词嵌入方案，word2vec 和 GloVe，并理解了这些模型是如何工作的。我们还研究了使用 gensim 从数据中训练我们自己的 word2vec 模型。

最后，我们了解了在我们的网络中使用嵌入的不同方式。首先是从零开始学习嵌入，作为训练我们网络的一部分。第二个是将预训练的 word2vec 和 GloVe 模型中的嵌入权重导入到我们的网络中，并在我们训练网络时对它们进行微调。第三是在我们的下游应用中使用这些预先训练的权重。

在下一章中，我们将学习循环神经网络，这是一类为处理文本等序列数据而优化的网络。