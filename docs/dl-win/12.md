# 12.强化学习的基础

在第 [1](01.html) 章中，我们简要介绍了强化学习的概念。正如我们在那里讨论的，强化学习是训练机器学习模型的方法之一。

强化学习是游戏人工智能编程和 AlphaZero 和 OpenAI Five(见附录 1)等模型背后的主要概念，也是机器人领域的应用。

在强化学习中，人工智能系统——通常被称为*代理*——被引入到一个环境中，并被赋予一个要实现的目标。代理还被给予一组可以用来改变环境状态的可能动作。代理的任务是使用那些动作来实现期望的目标状态。代理可以执行任何可能的操作。根据行动对实现预期目标的适当程度，代理人将获得奖励或惩罚。通过学习最大化奖励或最小化惩罚，代理人将最终学习实现目标所需的步骤(图 [12-1](#Fig1) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig1_HTML.png](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig1_HTML.png)

图 12-1

强化学习系统的工作流程

因为没有给代理一套完整的标记数据来训练，也没有给代理一些关于动作的反馈，所以强化学习介于监督学习和非监督学习之间。

如果我们要试验强化学习，我们需要一个框架来定义环境、要实现的目标、行动以及行动的奖励机制。

幸运的是，有一个专门为此开发的框架:OpenAI Gym。

## 什么是 OpenAI 健身房？

OpenAI Gym 是 OpenAI 开发的开源框架，提供工具来训练强化学习算法。

OpenAI 提供了一组内置环境，其中包含经典的强化学习问题，以及它们定义的动作、状态和奖励机制(图 [12-2](#Fig2) )。Gym 还允许您添加第三方或自定义环境。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig2_HTML.png](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig2_HTML.png)

图 12-2

OpenAI 健身房的一些可用环境

对于内置环境，Gym 还提供了环境、动作和结果的渲染/可视化(图 [12-3](#Fig3) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig3_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig3_HTML.jpg)

图 12-3

OpenAI 渲染山地车问题

在提供环境的同时，OpenAI Gym 并不限制你使用任何框架来进行强化学习模型的实际训练。因此，你可以使用 TensorFlow/Keras，或任何其他你熟悉的机器学习框架，用它来训练出模型。

## 建立开放式健身房

OpenAI Gym 以 PIP 包的形式提供。虽然最初 OpenAI Gym 只支持 Linux 和 Mac OS，但现在对 Windows 的支持更好了。大多数内置的健身房环境现在都可以在 Windows 上工作。

Note

一些高级环境，比如 MuJoCo(**Mu**LTI-**Jo**int dynamics with**Co**ntact)环境，需要极其特殊的依赖设置以及专有许可才能使用。因此，我们将在这里跳过它们。

我们将首先使用 pip 安装最小软件包(图 [12-4](#Fig4) ):

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig4_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig4_HTML.jpg)

图 12-4

安装健身房的最小包装

```
pip install gym

```

这将让您访问算法，玩具文本，和经典的控制环境。 <sup>[1](#Fn1)</sup>

接下来，我们可以通过运行(图 [12-5](#Fig5) )来安装 Atari 环境:

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig5_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig5_HTML.jpg)

图 12-5

安装 Atari 环境

```
pip install gym[atari]

```

最后，让我们安装 Box2D 环境。

为了让 Box2D 工作，我们需要安装 Swig 二进制文件。我们可以使用 conda 安装它(图 [12-6](#Fig6) ):

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig6_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig6_HTML.jpg)

图 12-6

使用 Conda 安装 swig 二进制文件

```
conda install swig

```

这允许我们安装 Box2D 环境(图 [12-7](#Fig7) ):

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig7_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig7_HTML.jpg)

图 12-7

安装 Box2D 环境

```
pip install gym[box2d]

```

我们现在可以通过启动其中一个环境来测试 OpenAI Gym 是否正确安装。我们将使用经典控制环境集中的 CartPole-v1 环境。

我们将创建一个名为`CartPole.py`的新代码文件，并添加以下代码:

```
01: import gym
02: env = gym.make('CartPole-v1')
03: observation = env.reset()
04: for step_index in range(1000):
05:     env.render()
06:     action = env.action_space.sample() # take a random action
07:     observation, reward, done, info = env.step(action)
08:     print("Step {}:".format(step_index))
09:     print("Action: {}".format(action))
10:     print("Observation: {}".format(observation))
11:     print("Reward: {}".format(reward))
12:     print("Is Done?: {}".format(done))
13:     print("Info: {}".format(info))
14: observation = env.reset()
15: env.close()

```

在这里，我们正在初始化 CartPole-v1 环境，并运行它 1000 步。

`env.action_space.sample()`函数将从该环境允许的动作列表中返回一个随机动作。我们通过将它传递给`env.step()`函数来执行这个动作，该函数将返回四个参数:

*   **观察:**环境的当前状态

*   **奖励:**行动的奖励或惩罚

*   **完成:**模拟是否达到完成状态；要么目标已经达到，要么任务已经失败，需要重新启动

*   **信息:**环境为调试目的提供的任何附加信息(代理不应将此信息用于培训)

运行该代码将导致渲染 CartPole-v1 环境，并且在控制台中打印每一步的结果(图 [12-8](#Fig8) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig8_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig8_HTML.jpg)

图 12-8

通过运行钢管环境测试开放式健身房

## 解决横竿问题

现在让我们仔细看看横竿问题，看看我们如何建立一个强化学习模型来解决它。

在横拉杆问题中，有一条无摩擦轨道，在这条轨道上有一辆小车。一根杆以这样的方式连接到该推车上，即该杆可以围绕其连接到推车上的枢轴点自由旋转。CartPole 问题的目标是通过改变小车的速度来防止杆子倒下(图 [12-9](#Fig9) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig9_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig9_HTML.jpg)

图 12-9

南极环境的要素

您可以对环境采取的唯一两个操作是 0(向左推车)或 1(向右推车)。

如果: <sup>[2](#Fn2)</sup> ，模拟将失败

*   杆的角度超过了`±` 12 `°`

*   购物车的位置超出显示区域(位置超过 2.4)

*   步数超过 500 步。

观察结果返回一个由四个值组成的数组，它们是小车位置(-2.4 到+2.4)、小车速度(-无穷大到+无穷大)、磁极角度(-41.8 `° to + 41.8°)`)和尖端的磁极速度(-无穷大到+无穷大)(图 [12-10](#Fig10) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig10_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig10_HTML.jpg)

图 12-10

来自南极环境的观测

每走一步，奖励将是+1(也就是说，你能保持杆子垂直的时间越长，越好)。

记住所有这些，让我们开始建立一个模型。

我们将从一个新的代码文件开始，命名为`CartPole_Train.py`，并导入必要的包:

```
1: import gym
2: import random
3: import numpy as np
4: import tensorflow as tf
5: from tensorflow.keras.models     import Sequential
6: from tensorflow.keras.layers     import Dense
7: from tensorflow.keras.optimizers import Adam
8: import tensorflow.keras.utils as np_utils
9: import matplotlib.pyplot as plt

```

然后，我们将定义我们的培训参数:

```
11: env = gym.make('CartPole-v1')
12: env.reset()
13: goal_steps = 500
14: score_requirement = 50
15: intial_games = 20000

```

在这里，我们将首先玩 20，000 个游戏，并列出在失败前导致模拟中至少 50 个步骤的动作。我们将定义一个函数`model_data_preparation()`来迭代和收集这些步骤数据:

```
17: def model_data_preparation():
18:     training_data = []
19:     accepted_scores = []
20:     for game_index in range(intial_games):
21:         score = 0
22:         game_memory = []
23:         previous_observation = []
24:         for step_index in range(goal_steps):
25:             action = random.randrange(0, 2)
26:             observation, reward, done, info = env.step(action)
27:
28:             if len(previous_observation) > 0:
29:                 game_memory.append([previous_observation, action])
30:
31:             previous_observation = observation
32:             score += reward
33:             if done:
34:                 break
35:
36:         if score >= score_requirement:
37:             accepted_scores.append(score)
38:             for data in game_memory:
39:                 output = np_utils.to_categorical(data[1], 2)
40:                 training_data.append([data[0], output])
41:
42:         env.reset()
43:
44:     print(accepted_scores)
45:
46:     return training_data
47:
48: training_data = model_data_preparation()

```

然后，我们构建一个简单的模型，并通过使用那些成功的初始游戏的步骤序列来训练它:

```
50: def build_model(input_size, output_size):
51:     model = Sequential()
52:     model.add(Dense(128, input_dim=input_size, activation="relu"))
53:     model.add(Dense(52, activation="relu"))
54:     model.add(Dense(output_size, activation="linear"))
55:     model.compile(loss='mse', optimizer=Adam())
56:
57:     return model
58:
59: def train_model(training_data):
60:     data_x = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))
61:     data_y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))
62:     model = build_model(input_size=len(data_x[0]), output_size=len(data_y[0]))
63:
64:     model.fit(data_x, data_y, epochs=20)
65:     return model
66:
67: trained_model = train_model(training_data)

```

经过训练后，该模型将能够根据先前步骤的序列作为输入，预测下一步要采取的步骤。

然后我们用这个训练过的模型运行 100 个游戏。如果模型能够运行游戏超过 400 步而没有失败，我们将认为它是一次成功的运行:

```
069: scores = []
070: choices = []
071: success_count = 0
072: for each_game in range(100):
073:     score = 0
074:     prev_obs = []
075:     print('Game {} playing'.format(each_game))
076:     for step_index in range(goal_steps):
077:         # Keep the below line uncommented if you want to see how our bot is playing the game.
078:         env.render()
079:         if len(prev_obs)==0:
080:             action = random.randrange(0,2)
081:         else:
082:             action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])
083:
084:         choices.append(action)
085:         new_observation, reward, done, info = env.step(action)
086:         prev_obs = new_observation
087:         score += reward
088:         if done:
089:             print('Final step count: {}'.format(step_index + 1))
090:             if (step_index + 1) > 400:
091:                 # if achieved more than 400 steps, consider successful
092:                 success_count += 1
093:             break

094:
095:     env.reset()
096:     scores.append(score)
097: env.close()
098:
099: print(scores)
100: # since we ran 100 games, success count is equal to percentage
101: print('Success percentage: {}%'.format(success_count))
102:
103: print('Average Score:',sum(scores)/len(scores))
104: print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))
105:
106: # draw the histogram of scores
107: plt.hist(scores, bins=5)
108: plt.show()

```

在 100 场比赛结束时，我们将打印出成功率和平均分数，并显示 100 场比赛的分数直方图。

Note

使用 env.render()会显著降低模拟速度。因此，如果不需要直观地检查模拟，最好不要调用 render 方法。

运行我们的代码，我们将能够看到经过训练的模型能够通过对手推车应用适当的速度变化来保持杆直，从而实现目标(图 [12-11](#Fig11) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig11_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig11_HTML.jpg)

图 12-11

受过训练的横竿模型

在 100 场比赛中，51%达到了我们的 400 步或以上的成功条件，平均得分为 351.77(图 [12-12](#Fig12) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig12_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig12_HTML.jpg)

图 12-12

我们的钢管模型的成功率

虽然乍一看，这似乎不是一个很好的结果，但查看分数直方图可以看出，我们的模型正在向成功标准倾斜(图 [12-13](#Fig13) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig14_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig14_HTML.jpg)

图 12-14

测试山地车环境

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig13_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig13_HTML.jpg)

图 12-13

我们的 CartPole 模型的分数直方图

由于我们使用一组初始游戏来检索步骤序列作为训练数据，因此模型仅通过一轮训练无法获得更高的成功率是正常的。您可以尝试使用几种不同的方法来提高成功率:

1.  增加用于收集训练数据的初始游戏计数

2.  调整初始训练数据的分数要求

3.  调整或尝试不同的模型结构

4.  使用第一轮训练的输出作为训练数据来训练新模型

5.  通过多轮训练走得更远。

## 解决登山车问题

我们刚刚解决的横竿问题是强化学习中最简单的问题之一。现在让我们加快一点，尝试一个稍微复杂一点的问题:登山车问题。

让我们先创建一个脚本来看看 MountainCar 环境:

```
01: import gym
02: env = gym.make('MountainCar-v0')
03: observation = env.reset()
04: for step_index in range(1000):
05:     env.render()
06:     action = env.action_space.sample()
07:     observation, reward, done, info = env.step(action)
08:     print("Step {}:".format(step_index))
09:     print("action: {}".format(action))
10:     print("observation: {}".format(observation))
11:     print("reward: {}".format(reward))
12:     print("done: {}".format(done))
13:     print("info: {}".format(info))
14:     if done:
15:         break

16: observation = env.reset()
17: env.close()

```

这就像我们为 CartPole 环境所做的一样，不同之处在于 MountainCar-v0 被用作环境名称。这将像我们之前所做的那样渲染带有随机动作的登山车环境(图 [12-14](#Fig14) )。

在登山车问题中，你需要把一辆车推上有旗子标记的陡峭山顶。汽车在接近谷底的地方启动。在环境的左边有一个不太陡的山，你可以用它来聚集足够的冲力去爬更陡的山。

您可以采取的操作有向左推(0)、向右推(2)或不推(1)。球门的位置为 0.5(图 [12-15](#Fig15) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig15_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig15_HTML.jpg)

图 12-15

山地车环境的要素

如果你走超过 200 步才能达到目标，模拟就会失败。 <sup>[3](#Fn3)</sup>

观察结果返回两个值的数组，它们是汽车的位置(-1.2 到+0.6)和汽车的速度(-0.07 到+0.07)。

在模拟开始时，汽车将处于-0.6 和-0.4 之间的随机位置，没有初始速度。

每一步的回报都是-1(也就是说，达到目标的步骤越少越好)。爬左边的山不会有惩罚，因为有时候为了达到目标是需要的。

让我们创建一个新的代码文件，命名为 MountainCar_Train.py，并导入必要的包:

```
1: import gym
2: import random
3: import numpy as np
4: import tensorflow as tf
5: from tensorflow.keras.models     import Sequential
6: from tensorflow.keras.layers     import Dense
7: from tensorflow.keras.optimizers import Adam
8: import tensorflow.keras.utils as np_utils
9: import matplotlib.pyplot as plt

```

我们的训练参数就像我们用来解决横翻筋斗问题的一样。但是在这里，我们将分数要求指定为-198。我们将在下一步中了解原因:

```
11: env = gym.make('MountainCar-v0')
12: env.reset()
13: goal_steps = 200
14: score_requirement = -198
15: intial_games = 20000

```

正如我们所讨论的，在登山车问题中，每走一步，奖励值为-1。因此，登山车游戏的最低分数是-199(如果达到 200 步，游戏就会结束)。为了从最初的游戏中筛选出可接受的步骤数据，我们需要一种方法来确定朝着目标前进的游戏。由于目标的位置是 0.5，汽车的初始位置在-0.6 和-0.4 之间，我们选择了至少一次已经达到位置-0.2(在大山坡的中途)的游戏。这使得我们的分数要求-198 或更高。

因此，数据准备功能将如下所示:

```
17: def model_data_preparation():
18:     training_data = []
19:     accepted_scores = []
20:     for game_index in range(intial_games):
21:         score = 0
22:         game_memory = []
23:         previous_observation = []
24:         for step_index in range(goal_steps):
25:             action = random.randrange(0, 3)
26:             observation, reward, done, info = env.step(action)
27:
28:             if len(previous_observation) > 0:
29:                 game_memory.append([previous_observation, action])
30:
31:             previous_observation = observation

32:
33:             if observation[0] > -0.2:
34:                 reward = 1
35:
36:             score += reward
37:             if done:
38:                 break
39:
40:         if score >= score_requirement:
41:             accepted_scores.append(score)
42:             for data in game_memory:
43:                 output = np_utils.to_categorical(data[1], 3)
44:                 training_data.append([data[0], output])
45:
46:         env.reset()
47:
48:     print(accepted_scores)
49:
50:     return training_data
51:
52: training_data = model_data_preparation()

```

模型构建和训练步骤与我们在横竿问题上所做的相同:

```
54: def build_model(input_size, output_size):
55:     model = Sequential()
56:     model.add(Dense(128, input_dim=input_size, activation="relu"))
57:     model.add(Dense(52, activation="relu"))
58:     model.add(Dense(output_size, activation="linear"))
59:     model.compile(loss='mse', optimizer=Adam())
60:
61:     return model
62:
63: def train_model(training_data):
64:     data_x = np.array([i[0] for i in training_data]).reshape(-1, len(training_data[0][0]))
65:     data_y = np.array([i[1] for i in training_data]).reshape(-1, len(training_data[0][1]))
66:     model = build_model(input_size=len(data_x[0]), output_size=len(data_y[0]))
67:
68:     model.fit(data_x, data_y, epochs=20)
69:     return model
70:
71: trained_model = train_model(training_data)

```

像以前一样，我们使用来自训练模型的步骤预测运行 100 个游戏。如果游戏能够在不到 200 步的时间内实现目标，我们认为它是成功的:

```
073: scores = []
074: choices = []
075: success_count = 0
076: for each_game in range(100):
077:     score = 0
078:     prev_obs = []
079:     print('Game {} playing'.format(each_game))
080:     for step_index in range(goal_steps):
081:         # Uncomment below line if you want to see how our bot is playing the game.
082:         # env.render()
083:         if len(prev_obs)==0:
084:             action = random.randrange(0, 3)
085:         else:
086:             action = np.argmax(trained_model.predict(prev_obs.reshape(-1, len(prev_obs)))[0])
087:
088:         choices.append(action)
089:         new_observation, reward, done, info = env.step(action)
090:         prev_obs = new_observation
091:         score += reward
092:         if done:
093:             print('Final step count: {}'.format(step_index + 1))
094:             if (step_index + 1) < 200:
095:                 # if goal achieved in less than 200 steps, consider successful
096:                 success_count += 1
097:             break
098:
099:     env.reset()
100:     scores.append(score)
101:
102: print(scores)
103:
104: # since we ran 100 games, success count is equal to percentage

105: print('Success percentage: {}%'.format(success_count))
106: print('Average Score:', sum(scores)/len(scores))
107: print('choice 0:{}  choice 1:{}  choice 2:{}'.format(choices.count(0)/len(choices), choices.count(1)/len(choices), choices.count(2)/len(choices)))
108:
109: # draw the histogram of scores
110: plt.hist(scores, bins=5)
111: plt.show()

```

如果我们运行我们的模型，你可以看到，一旦经过训练，它可以将汽车推到期望的目标位置(图 [12-16](#Fig16) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig16_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig16_HTML.jpg)

图 12-16

登山车到达目标

分数直方图显示，相当一部分游戏达到了 120 步左右的目标，比我们的目标 198 好得多(图 [12-17](#Fig17) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig17_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig17_HTML.jpg)

图 12-17

我们的登山车模型的分数直方图

我们现在达到了 99%的成功率(图 [12-18](#Fig18) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig18_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig18_HTML.jpg)

图 12-18

我们登山车模型的成功率

## 接下来你能做什么？

我们现在已经探索了如何在 OpenAI 健身房的两个环境中应用强化学习的基础知识:CartPole 和 MountainCar。虽然这两个是用强化学习解决的一些最简单的问题，但是我们在这里学习应用的概念对于复杂得多的问题也是一样的。像 OpenAI Five(见附录 1)这样的尖端模型是建立在相同的概念之上的。

OpenAI 健身房还有很多其他环境。一旦你完成了经典的控制环境，尝试一下我们安装的其他环境，比如 Atari(图 [12-19](#Fig19) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig19_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig19_HTML.jpg)

图 12-19

雅达利突击-v0 环境

另一个选项是 Box2D 环境之一(图 [12-20](#Fig20) )。

![../images/502073_1_En_12_Chapter/502073_1_En_12_Fig20_HTML.jpg](../images/502073_1_En_12_Chapter/502073_1_En_12_Fig20_HTML.jpg)

图 12-20

Box2D 两足行走者-v3 环境

<aside aria-label="Footnotes" class="FootnoteSection" epub:type="footnotes">Footnotes [1](#Fn1_source)

健身房(开放健身房环境)， [`https://gym.openai.com/envs/#classic_control`](https://gym.openai.com/%2523classic_control) ，【2020 年 4 月 2 日】。

  [2](#Fn2_source)

Github，“CartPole 概览”， [`https://github.com/openai/gym/wiki/CartPole-v0`](https://github.com/openai/gym/wiki/CartPole-v0) ，【2020 年 2 月 8 日】。

  [3](#Fn3_source)

Github，“MountainCar 概述”，[https://github . com/open ai/gym/wiki/mountain car-v 0](https://github.com/openai/gym/wiki/MountainCar-v0)，【2020 年 5 月 1 日】。

 </aside>