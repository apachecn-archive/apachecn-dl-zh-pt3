<title>B15777_Solution_Final_SZ_ePub</title> <link href="css/epub.css" rel="stylesheet" type="text/css">

# 附录

## 关于

本节旨在帮助学生完成书中介绍的活动。它包括学生完成和实现本书目标的详细步骤。

# 1。Keras 机器学习简介

## 活动 1.01:向模型添加正则化

在本活动中，我们将利用 scikit-learn 软件包中的相同逻辑回归模型。然而，这一次，我们将在模型中添加正则化，并搜索最佳正则化参数——这个过程通常被称为`hyperparameter tuning`。训练模型后，我们将测试预测，并将模型评估指标与基线模型和未进行正则化的模型产生的指标进行比较:

1.  加载来自*练习 1.03* 的特征数据、*适当表示的数据*，以及来自*练习 1.02* 、*的目标数据清理数据* :

    ```
    import pandas as pd
    feats = pd.read_csv('../data/OSI_feats_e3.csv')
    target = pd.read_csv('../data/OSI_target_e2.csv')
    ```

2.  Create a `test` and `train` dataset. Train the data using the training dataset. This time, however, use part of the `training` dataset for validation in order to choose the most appropriate hyperparameter.

    我们将再次使用`test_size = 0.2`，这意味着数据的`20%`将被保留用于测试。我们的验证集的大小将取决于我们有多少个验证折叠。如果我们做`10-fold cross-validation`，这等同于保留`training`数据集的`10%`来验证我们的模型。每个折叠将使用`training`数据集的不同`10%`，所有折叠的平均误差用于比较具有不同超参数的模型。给`random_state`变量分配一个随机值:

    ```
    from sklearn.model_selection import train_test_split
    test_size = 0.2
    random_state = 13
    X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=test_size, random_state=random_state)
    ```

3.  Check the dimensions of the DataFrames:

    ```
    print(f'Shape of X_train: {X_train.shape}')
    print(f'Shape of y_train: {y_train.shape}')
    print(f'Shape of X_test: {X_test.shape}')
    print(f'Shape of y_test: {y_test.shape}')
    ```

    上述代码产生以下输出:

    ```
    Shape of X_train: (9864, 68)
    Shape of y_train: (9864, 1)
    Shape of X_test: (2466, 68)
    Shape of y_test: (2466, 1)
    ```

4.  Next, instantiate the models. Try two types of regularization parameters, `l1` and `l2`, with 10-fold cross-validation. Iterate our regularization parameter from 1x10-2 to 1x106 equally in the logarithmic space to observe how the parameters affect the results:

    ```
    import numpy as np
    from sklearn.linear_model import LogisticRegressionCV
    Cs = np.logspace(-2, 6, 9)
    model_l1 = LogisticRegressionCV(Cs=Cs, penalty='l1', cv=10, solver='liblinear', random_state=42, max_iter=10000)
    model_l2 = LogisticRegressionCV(Cs=Cs, penalty='l2', cv=10, random_state=42, max_iter=10000)
    ```

    注意

    对于带有`l1`正则化参数的逻辑回归模型，只能使用`liblinear`解算器。

5.  Next, fit the models to the training data:

    ```
    model_l1.fit(X_train, y_train['Revenue'])
    model_l2.fit(X_train, y_train['Revenue'])
    ```

    下图显示了上述代码的输出:

    ![Figure 1.38: Output of the fit command indicating all of the model training parameters
    ](image/B15777_01_38.jpg)

    图 1.38:指示所有模型训练参数的 fit 命令的输出

6.  Here, we can see what the value of the regularization parameter was for the two different models. The regularization parameter is chosen according to which produced a model with the lowest error:

    ```
    print(f'Best hyperparameter for l1 regularization model: {model_l1.C_[0]}')
    print(f'Best hyperparameter for l2 regularization model: {model_l2.C_[0]}')
    ```

    上述代码产生以下输出:

    ```
    Best hyperparameter for l1 regularization model: 1000000.0
    Best hyperparameter for l2 regularization model: 1.0
    ```

    注意

    `C_`属性只有在模型被训练后才可用，因为它是在交叉验证过程的最佳参数被确定后设置的。

7.  为了评估模型的性能，对`test`集进行预测，我们将把它与`true`值进行比较:

    ```
    y_pred_l1 = model_l1.predict(X_test)
    y_pred_l2 = model_l2.predict(X_test)
    ```

8.  To compare these models, calculate the evaluation metrics. First, look at the accuracy of the model:

    ```
    from sklearn import metrics
    accuracy_l1 = metrics.accuracy_score(y_pred=y_pred_l1, y_true=y_test)
    accuracy_l2 = metrics.accuracy_score(y_pred=y_pred_l2, y_true=y_test)
    ```

    上述代码产生以下输出:

    ```
    Accuracy of the model with l1 regularization is 89.2133%
    Accuracy of the model with l2 regularization is 89.2944%
    ```

9.  Also, look at the other evaluation metrics:

    ```
    precision_l1, recall_l1, fscore_l1, _ = metrics.precision_recall_fscore_support(y_pred=y_pred_l1, y_true=y_test, average='binary')
    precision_l2, recall_l2, fscore_l2, _ = metrics.precision_recall_fscore_support(y_pred=y_pred_l2, y_true=y_test, average='binary')
    ```

    上述代码产生以下输出:

    ```
    l1
    Precision: 0.7300
    Recall: 0.4078
    fscore: 0.5233
    l2
    Precision: 0.7350
    Recall: 0.4106
    fscore: 0.5269
    ```

10.  Observe the values of the coefficients once the model has been trained:

    ```
    coef_list = [f'{feature}: {coef}' for coef, feature in sorted(zip(model_l1.coef_[0], X_train.columns.values.tolist()))]
    for item in coef_list:
        print(item)
    ```

    注意

    `coef_`属性只有在模型被训练后才可用，因为它是在交叉验证过程的最佳参数被确定后设置的。

    下图显示了上述代码的输出:

    ![Figure 1.39: The feature column names and the value of their respective coefficients 
    for the model with l1 regularization
    ](image/B15777_01_39.jpg)

    图 1.39:使用 l1 正则化的模型的特征列名称及其各自系数的值

11.  Do the same for the model with an `l2` regularization parameter type:

    ```
    coef_list = [f'{feature}: {coef}' for coef, feature in sorted(zip(model_l2.coef_[0], X_train.columns.values.tolist()))]
    for item in coef_list:
        print(item)
    ```

    下图显示了上述代码的输出:

![Figure 1.40: The feature column names and the value of their respective coefficients 
for the model with l2 regularization
](image/B15777_01_40.jpg)

图 1.40:使用 l2 正则化的模型的特征列名称及其各自系数的值

# 2。机器学习对深度学习

## 活动 2.01:使用 Keras 创建逻辑回归模型

在本活动中，我们将使用 Keras 库创建一个基本模型。我们将建立的模型将把网站用户分为会从网站购买产品的用户和不会从网站购买产品的用户。为此，我们将利用与之前相同的在线购物购买意向数据集，并尝试预测与我们在*第 1 章*、【Keras 机器学习简介中预测的变量相同的变量。

执行以下步骤来完成本练习:

1.  从“开始”菜单打开一个 Jupyter 笔记本来执行此活动。载入上一章的在线购物购买意向数据集。这应该是准备用于训练目的的预处理数据集。我们将使用 pandas 库进行数据加载，因此导入`pandas`库:

    ```
    import pandas as pd
    feats = pd.read_csv('../data/OSI__feats.csv')
    target = pd.read_csv('../data/OSI_target.csv')
    ```

2.  出于本练习的目的，我们将不执行任何进一步的预处理。正如我们在上一章所做的，我们将把数据集分成训练和测试，并把测试留到最后评估我们的模型。我们将通过设置`test_size=0.2`参数保留数据的`20%`用于测试，并且我们将创建一个`random_state`参数，以便我们可以重新创建结果:

    ```
    from sklearn.model_selection import train_test_split
    test_size = 0.2
    random_state = 42
    X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=test_size, random_state=random_state)
    ```

3.  在`numpy`和`tensorflow`中设置一个种子，用于再现性。通过初始化`Sequential`类的模型开始创建模型:

    ```
    from keras.models import Sequential
    import numpy as np
    from tensorflow import random
    np.random.seed(random_state)
    random.set_seed(random_state)
    model = Sequential()
    ```

4.  要向模型添加一个完全连接的层，添加一个`Dense`类的层。这里，我们包括层中节点的数量。在我们的例子中，这将是一个，因为我们正在执行二元分类，我们想要的输出是`zero`或`one`。此外，指定输入尺寸，这仅在模型的第一层完成。它指示输入数据的格式。传递特征的数量:

    ```
    from keras.layers import Dense
    model.add(Dense(1, input_dim=X_train.shape[1]))
    ```

5.  将 sigmoid 激活函数添加到前一层的输出中，以复制`logistic regression`算法:

    ```
    from keras.layers import Activation
    model.add(Activation('sigmoid'))
    ```

6.  一旦我们有了正确顺序的所有模型组件，我们必须编译模型，以便配置所有的学习过程。使用`adam`优化器，一个`binary_crossentropy`用于损失，并通过将参数传递到`metrics`参数中来跟踪模型的准确性:

    ```
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    ```

7.  Print the model summary to verify the model is as we expect it to be:

    ```
    print(model.summary())
    ```

    下图显示了上述代码的输出:

    ![Figure 2.19: A summary of the model
    ](image/B15777_02_19.jpg)

    图 2.19:模型的总结

8.  Next, fit the model using the `fit` method of the `model` class. Provide the training data, as well as the number of epochs and how much data to use for validation after each epoch:

    ```
    history = model.fit(X_train, y_train['Revenue'], epochs=10, validation_split=0.2, shuffle=False)
    ```

    下图显示了上述代码的输出:

    ![Figure 2.20: Using the fit method on the model
    ](image/B15777_02_20.jpg)

    图 2.20:在模型上使用拟合方法

9.  The values for the loss and accuracy have been stored within the `history` variable. Plot the values for each using the loss and accuracy we tracked after each epoch:

    ```
    import matplotlib.pyplot as plt
    %matplotlib inline
    # Plot training and validation accuracy values
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    plt.show()
    # Plot training and validation loss values
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    plt.show()
    ```

    下面的图显示了前面代码的输出:

    ![Figure 2.21: The loss and accuracy while fitting the model
    ](image/B15777_02_21.jpg)

    图 2.21:拟合模型时的损失和准确性

10.  Finally, evaluate the model on the test data we held out from the beginning, which will give an objective evaluation of the performance of the model:

    ```
    test_loss, test_acc = model.evaluate(X_test, y_test['Revenue'])
    print(f'The loss on the test set is {test_loss:.4f} and the accuracy is {test_acc*100:.3f}%')
    ```

    前面代码的输出可以在下面找到。在这里，模型预测测试数据集中用户的购买意向，并通过与`y_test`中的真实值进行比较来评估性能。在测试数据集上评估模型会产生损失和准确性值，我们可以打印出来:

    ```
    2466/2466 [==============================] - 0s 15us/step
    The loss on the test set is 0.3632 and the accuracy is 86.902%
    ```

# 3。使用 Keras 进行深度学习

## 活动 3.01:建立一个单层神经网络来执行二元分类

在此活动中，我们将比较逻辑回归模型和不同节点大小和不同激活函数的单层神经网络的结果。我们将使用的数据集表示飞机螺旋桨检查的标准化测试结果，而类表示它们是否通过了手动视觉检查。当给定自动化测试结果时，我们将创建模型来预测手动检查的结果。按照以下步骤完成本活动:

1.  加载所有需要的包:

    ```
    # import required packages from Keras
    from keras.models import Sequential 
    from keras.layers import Dense, Activation 
    import numpy as np
    import pandas as pd
    from tensorflow import random
    from sklearn.model_selection import train_test_split
    # import required packages for plotting
    import matplotlib.pyplot as plt 
    import matplotlib
    %matplotlib inline 
    import matplotlib.patches as mpatches
    # import the function for plotting decision boundary
    from utils import plot_decision_boundary
    ```

2.  设置`seed` :

    ```
    # define a seed for random number generator so the result will be reproducible
    seed = 1
    ```

3.  Load the simulated dataset and print the size of `X` and `Y` and the number of examples:

    ```
    # load the dataset, print the shapes of input and output and the number of examples
    feats = pd.read_csv(' ../data/outlier_feats.csv')
    target = pd.read_csv(' ../data/outlier_target.csv')
    print("X size = ", feats.shape)
    print("Y size = ", target.shape)
    print("Number of examples = ", feats.shape[0]
    ```

    **预期产量**:

    ```
    X size = (3359, 2)
    Y size = (3359, 1)
    Number of examples = 3359
    ```

4.  Plot the dataset. The x and y coordinates of each point will be the two input features. The color of each record represents the `pass`/`fail` result:

    ```
    class_1=plt.scatter(feats.loc[target['Class']==0,'feature1'], feats.loc[target['Class']==0,'feature2'], c="red", s=40, edgecolor='k')
    class_2=plt.scatter(feats.loc[target['Class']==1,'feature1'], feats.loc[target['Class']==1,'feature2'], c="blue", s=40, edgecolor='k')
    plt.legend((class_1, class_2),('Fail','Pass'))
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    ```

    下图显示了上述代码的输出:

    ![Figure 3.19: Simulated training data points
    ](image/B15777_03_19.jpg)

    图 3.19:模拟训练数据点

5.  构建`logistic regression`模型，这将是一个没有隐藏层的单节点序列模型，并且有一个`sigmoid activation`函数:

    ```
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential()
    model.add(Dense(1, activation='sigmoid', input_dim=2)) 
    model.compile(optimizer='sgd', loss='binary_crossentropy')
    ```

6.  Fit the model to the training data:

    ```
    model.fit(feats, target, batch_size=5, epochs=100, verbose=1, validation_split=0.2, shuffle=False)
    ```

    **预期产量**:

    `100`个时期= `0.3537`后验证集的损失:

    ![Figure 3.20: The loss details of the last 5 epochs out of 100
    ](image/B15777_03_20.jpg)

    图 3.20:100 个时段中最后 5 个时段的损失详情

7.  在训练数据上绘制决策边界:

    ```
    plot_decision_boundary(lambda x: model.predict(x), feats, target) 
    plt.title("Logistic Regression")
    ```

8.  The following image shows the output of the preceding code:![Figure 3.21: The decision boundary of the logistic regression model
    ](image/B15777_03_21.jpg)

    图 3.21:逻辑回归模型的决策边界

    逻辑回归模型的线性决策边界显然不能捕捉两个类之间的循环决策边界，并将所有结果预测为通过的结果。

9.  创建一个神经网络，其中一个隐藏层包含三个节点和一个`relu activation function`，一个输出层包含一个节点和一个`sigmoid activation function`。最后，编译模型:

    ```
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential() 
    model.add(Dense(3, activation='relu', input_dim=2))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='sgd', loss='binary_crossentropy')
    ```

10.  Fit the model to the training data:

    ```
    model.fit(feats, target, batch_size=5, epochs=200, verbose=1, validation_split=0.2, shuffle=False)
    ```

    **预期产量**:

    在`200`个时期= `0.0260`之后，在验证集上评估的损失:

    ![Figure 3.22: The loss details of the last 5 epochs out of 200
    ](image/B15777_03_22.jpg)

    图 3.22:200 个时段中最后 5 个时段的损失详情

11.  Plot the decision boundary that was created:

    ```
    plot_decision_boundary(lambda x: model.predict(x), feats, target) 
    plt.title("Decision Boundary for Neural Network with hidden layer size 3")
    ```

    下图显示了上述代码的输出:

    ![Figure 3.23: The decision boundary for the neural network with a hidden layer size 
    of 3 and a ReLU activation function
    ](image/B15777_03_23.jpg)

    图 3.23:隐含层大小为 3 且具有 ReLU 激活函数的神经网络的决策边界

    具有三个处理单元而不是一个处理单元极大地提高了模型捕捉两个类之间的非线性边界的能力。请注意，与上一步相比，损失值大幅下降。

12.  创建一个神经网络，其中一个隐藏层有六个节点和一个`relu activation function`，一个输出层有一个节点和一个`sigmoid activation function`。最后，编译模型:

    ```
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential() 
    model.add(Dense(6, activation='relu', input_dim=2))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='sgd', loss='binary_crossentropy')
    ```

13.  Fit the model to the training data:

    ```
    model.fit(feats, target, batch_size=5, epochs=400, verbose=1, validation_split=0.2, shuffle=False)
    ```

    **预期产量**:

    `400`个时期后的损失= `0.0231`:

    ![Figure 3.24: The loss details of the last 5 epochs out of 400
    ](image/B15777_03_24.jpg)

    图 3.24:400 个时段中最后 5 个时段的损失详情

14.  Plot the decision boundary:

    ```
    plot_decision_boundary(lambda x: model.predict(x), feats, target) 
    plt.title("Decision Boundary for Neural Network with hidden layer size 6")
    ```

    下图显示了上述代码的输出:

    ![Figure 3.25: The decision boundary for the neural network with a hidden layer size 
    ](image/B15777_03_25.jpg)

    图 3.25:隐含层大小为 6 和 ReLU 激活函数的神经网络的决策边界

    通过将隐藏层中的单元数量加倍，模型的决策边界变得更接近真正的圆形，并且与前一步骤相比，损失值降低得更多。

15.  创建一个神经网络，其中一个隐藏层包含三个节点和一个`tanh activation function`，一个输出层包含一个节点和一个`sigmoid activation function`。最后，编译模型:

    ```
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential() 
    model.add(Dense(3, activation='tanh', input_dim=2))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='sgd', loss='binary_crossentropy')
    ```

16.  Fit the model to the training data:

    ```
    model.fit(feats, target, batch_size=5, epochs=200, verbose=1, validation_split=0.2, shuffle=False)
    ```

    **预期产量**:

    `200`个时期后的损失= `0.0426`:

    ![Figure 3.26: The loss details of the last 5 epochs out of 200
    ](image/B15777_03_26.jpg)

    图 3.26:200 个时段中最后 5 个时段的损失详情

17.  Plot the decision boundary:

    ```
    plot_decision_boundary(lambda x: model.predict(x), X_train, y_train) 
    plt.title("Decision Boundary for Neural Network with hidden layer size 3")
    ```

    下图显示了上述代码的输出:

    ![Figure 3.27: The decision boundary for the neural network with a hidden layer size 
    of 3 and the tanh activation function
    ](image/B15777_03_27.jpg)

    图 3.27:隐含层大小为 3 和双曲正切激活函数的神经网络的决策边界

    使用`tanh`激活功能消除了决策边界中的锐边。换句话说，它使得决策边界更加平滑。然而，该模型并没有表现得更好，因为我们可以看到损失值的增加。当我们在测试数据集上进行评估时，我们获得了类似的损失和准确性分数，尽管之前提到过`tanh`的学习参数比`relu`的学习参数慢。

18.  创建一个神经网络，其中一个隐藏层有六个节点和一个`tanh activation function`，一个输出层有一个节点和一个`sigmoid activation function`。最后，编译模型:

    ```
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential() 
    model.add(Dense(6, activation='tanh', input_dim=2))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='sgd', loss='binary_crossentropy')
    ```

19.  **Fit the model to the training data:**

    ```
    model.fit(X_train, y_train, batch_size=5, epochs=400, verbose=1, validation_split=0.2, shuffle=False)
    ```

    **预期产量**:

    `400`个时期后的损失= `0.0215`:

    ![Figure 3.28: The loss details of the last 5 epochs out of 400
    ](image/B15777_03_28.jpg)

    图 3.28:400 个时段中最后 5 个时段的损失详情

20.  Plot the decision boundary:

    ```
    plot_decision_boundary(lambda x: model.predict(x), X_train, y_train) 
    plt.title("Decision Boundary for Neural Network with hidden layer size 3")
    ```

    下图显示了上述代码的输出:

![Figure 3.29: The decision boundary for the neural network with a hidden layer size 
of 6 and the tanh activation function
](image/B15777_03_29.jpg)

图 3.29:隐含层大小为 6 和双曲正切激活函数的神经网络的决策边界

同样，使用`tanh`激活函数代替`relu`并向我们的隐藏层添加更多的节点，使决策边界上的曲线更加平滑，根据训练数据的准确性更好地拟合训练数据。我们应该小心不要给隐藏层添加太多的节点，因为我们可能会使数据溢出。这可以通过评估测试集来观察到，其中与具有三个节点的神经网络相比，具有六个节点的神经网络的准确性略有下降。

## 活动 3.02:利用神经网络进行晚期纤维化诊断

在本活动中，您将使用真实数据集，根据年龄、性别和身体质量指数等指标来预测患者是否患有晚期纤维化。该数据集由接受丙肝治疗剂量的 1385 名患者的信息组成。对于每个患者，`28`个不同的属性可用，以及一个只能取两个值的类别标签:`1`，表示晚期纤维化；和`0`，表示无晚期纤维化迹象。这是一个输入维数等于 28 的二元/两类分类问题。

在本活动中，您将实施不同的深度神经网络架构来执行此分类，绘制训练错误率和测试错误率的趋势，并确定最终分类器需要训练多少个时期。按照以下步骤完成本活动:

1.  导入所有必要的库，并使用 pandas `read_csv`函数:

    ```
    import pandas as pd
    import numpy as np
    from tensorflow import random
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from keras.models import Sequential
    from keras.layers import Dense
    import matplotlib.pyplot as plt 
    import matplotlib
    %matplotlib inline)
    X = pd.read_csv('../data/HCV_feats.csv')
    y = pd.read_csv('../data/HCV_target.csv')
    ```

    加载数据集
2.  Print the number of `records` and `features` in the `feature` dataset and the number of unique classes in the `target` dataset:

    ```
    print("Number of Examples in the Dataset = ", X.shape[0])
    print("Number of Features for each example = ", X.shape[1]) 
    print("Possible Output Classes = ", y['AdvancedFibrosis'].unique())
    ```

    **预期产量**:

    ```
    Number of Examples in the Dataset = 1385
    Number of Features for each example = 28
    Possible Output Classes = [0 1]
    ```

3.  Normalize the data and scale it. Following this, split the dataset into the `training` and `test` sets:

    ```
    # 
    seed = 1
    np.random.seed(seed)
    sc = StandardScaler()
    X = pd.DataFrame(sc.fit_transform(X), columns=X.columns)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)
    # Print the information regarding dataset sizes
    print(X_train.shape)
    print(y_train.shape)
    print(X_test.shape)
    print(y_test.shape)
    print ("Number of examples in training set = ", X_train.shape[0])
    print ("Number of examples in test set = ", X_test.shape[0])
    ```

    **预期产量**:

    ```
    (1108, 28)
    (1108, 1)
    (277, 28)
    (277, 1)
    Number of examples in training set = 1108
    Number of examples in test set = 277
    ```

4.  Implement a deep neural network with one hidden layer of size 3 and a `tanh activation function`, an output layer with one node, and a `sigmoid activation function`. Finally, compile the model and print out a summary of the model:

    ```
    # 
    np.random.seed(seed)
    random.set_seed(seed)
    # define the keras model
    classifier = Sequential()
    classifier.add(Dense(units = 3, activation = 'tanh', input_dim=X_train.shape[1]))
    classifier.add(Dense(units = 1, activation = 'sigmoid'))
    classifier.compile(optimizer = 'sgd', loss = 'binary_crossentropy', metrics = ['accuracy'])
    classifier.summary()
    ```

    下图显示了上述代码的输出:

    ![Figure 3.30: The architecture of the neural network
    ](image/B15777_03_30.jpg)

    图 3.30:神经网络的架构

5.  将模型拟合到训练数据:

    ```
    history=classifier.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_split=0.1, shuffle=False)
    ```

6.  Plot the `training error rate` and `test error rate` for every epoch:

    ```
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    ```

    **预期产量**:

    ![Figure 3.31: A plot of the training error rate and test error rate while training the model 
    ](image/B15777_03_31.jpg)

    图 3.31:训练模型时的训练错误率和测试错误率的图表

7.  Print the values of the best accuracy that was reached on the training set and on the test set, as well as the `loss` and `accuracy` that was evaluated on the `test` dataset.

    ```
    print(f"Best Accuracy on training set = {max(history.history['accuracy'])*100:.3f}%")
    print(f"Best Accuracy on validation set = {max(history.history['val_accuracy'])*100:.3f}%") 
    test_loss, test_acc = classifier.evaluate(X_test, y_test['AdvancedFibrosis'])
    print(f'The loss on the test set is {test_loss:.4f} and the accuracy is {test_acc*100:.3f}%')
    ```

    下图显示了上述代码的输出:

    ```
    Best Accuracy on training set = 52.959%
    Best Accuracy on validation set = 58.559%
    277/277 [==============================] - 0s 25us/step
    The loss on the test set is 0.6885 and the accuracy is 55.235%
    ```

8.  Implement a deep neural network with two hidden layers of sizes `4` and `2` with a `tanh activation function`, an output layer with one node, and a `sigmoid activation function`. Finally, compile the model and print out a summary of the model:

    ```
    np.random.seed(seed)
    random.set_seed(seed)
    # define the keras model
    classifier = Sequential()
    classifier.add(Dense(units = 4, activation = 'tanh', input_dim = X_train.shape[1]))
    classifier.add(Dense(units = 2, activation = 'tanh'))
    classifier.add(Dense(units = 1, activation = 'sigmoid'))
    classifier.compile(optimizer = 'sgd', loss = 'binary_crossentropy', metrics = ['accuracy'])
    classifier.summary()
    ```

    ![Figure 3.32: The architecture of the neural network
    ](image/B15777_03_32.jpg)

    图 3.32:神经网络的架构

9.  将模型拟合到训练数据:

    ```
    history=classifier.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_split=0.1, shuffle=False)
    ```

10.  Plot training and test error plots with two hidden layers of size 4 and 2\. Print the best accuracy that was reached on the training and test sets:

    ```
    # plot training error and test error plots 
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    ```

    **预期产量**:

    ![Figure 3.33: A plot of the training error and test error rates while training the model
    ](image/B15777_03_33.jpg)

    图 3.33:训练模型时的训练错误和测试错误率的图表

11.  Print the values of the best accuracy that was achieved on the `training` set and on the `test` set, as well as the `loss` and `accuracy` that was evaluated on the test dataset.

    ```
    print(f"Best Accuracy on training set = {max(history.history['accuracy'])*100:.3f}%")
    print(f"Best Accuracy on validation set = {max(history.history['val_accuracy'])*100:.3f}%") 
    test_loss, test_acc = classifier.evaluate(X_test, y_test['AdvancedFibrosis'])
    print(f'The loss on the test set is {test_loss:.4f} and the accuracy is {test_acc*100:.3f}%')
    ```

    下图显示了上述代码的输出:

    ```
    Best Accuracy on training set = 57.272%
    Best Accuracy on test set = 54.054%
    277/277 [==============================] - 0s 41us/step
    The loss on the test set is 0.7016 and the accuracy is 49.819%
    ```

# 4。使用 Keras 包装器通过交叉验证评估您的模型

## 活动 4.01:使用交叉验证对高级纤维化诊断分类器进行模型评估

在这个活动中，我们将使用本主题中所学的知识，使用`k-fold cross-validation`来训练和评估一个深度学习模型。我们将使用之前活动中产生最佳测试错误率的模型，目标是将交叉验证错误率与训练集/测试集方法错误率进行比较。我们将使用的数据集是丙型肝炎数据集，其中我们将建立一个分类模型来预测哪些患者会出现晚期纤维化。按照以下步骤完成本活动:

1.  Load the dataset and print the number of records and features in the dataset, as well as the number of possible classes in the target dataset:

    ```
    # Load the dataset
    import pandas as pd
    X = pd.read_csv('../data/HCV_feats.csv')
    y = pd.read_csv('../data/HCV_target.csv')
    # Print the sizes of the dataset
    print("Number of Examples in the Dataset = ", X.shape[0])
    print("Number of Features for each example = ", X.shape[1]) 
    print("Possible Output Classes = ", y['AdvancedFibrosis'].unique())
    ```

    下面是预期的输出:

    ```
    Number of Examples in the Dataset = 1385
    Number of Features for each example = 28
    Possible Output Classes = [0 1]
    ```

2.  定义返回 Keras 模型的函数。首先，为 Keras 导入必要的库。在函数内部，实例化序列模型并添加两个密集层，第一个是`size 4`，第二个是`size 2`，两个都有`tanh activation`函数。使用`sigmoid activation`功能添加输出层。编译模型并从函数中返回模型:

    ```
    from keras.models import Sequential
    from keras.layers import Dense
    # Create the function that returns the keras model
    def build_model():
        model = Sequential()
        model.add(Dense(4, input_dim=X.shape[1], activation='tanh'))
        model.add(Dense(2, activation='tanh'))
        model.add(Dense(1, activation='sigmoid'))
        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
        return model
    ```

3.  使用`StandardScaler`功能缩放训练数据。设置种子，以便模型可重现。定义`n_folds`、`epochs`和`batch_size`超参数。然后，用 scikit-learn 构建 Keras 包装器，定义`cross-validation`迭代器，执行`k-fold cross-validation`，存储分数:

    ```
    # import required packages
    import numpy as np
    from tensorflow import random
    from keras.wrappers.scikit_learn import KerasClassifier
    from sklearn.model_selection import StratifiedKFold
    from sklearn.model_selection import cross_val_score
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    X = pd.DataFrame(sc.fit_transform(X), columns=X.columns)
    # define a seed for random number generator so the result will be reproducible
    seed = 1
    np.random.seed(seed)
    random.sed_seed(seed)
    # determine the number of folds for k-fold cross-validation, number of epochs and batch size
    n_folds = 5
    epochs = 100
    batch_size = 20
    # build the scikit-learn interface for the keras model
    classifier = KerasClassifier(build_fn=build_model, epochs=epochs, batch_size=batch_size, verbose=1, shuffle=False)
    # define the cross-validation iterator
    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)
    # perform the k-fold cross-validation and store the scores in results
    results = cross_val_score(classifier, X, y, cv=kfold)
    ```

4.  For each of the folds, print the accuracy stored in the `results` parameter:

    ```
    # print accuracy for each fold
    for f in range(n_folds):
        print("Test accuracy at fold ", f+1, " = ", results[f])
    print("\n")
    # print overall cross-validation accuracy plus the standard deviation of the accuracies
    print("Final Cross-validation Test Accuracy:", results.mean())
    print("Standard Deviation of Final Test Accuracy:", results.std())
    ```

    下面是预期的输出:

    ```
    Test accuracy at fold 1 = 0.5198556184768677
    Test accuracy at fold 2 = 0.4693140685558319
    Test accuracy at fold 3 = 0.512635350227356
    Test accuracy at fold 4 = 0.5740072131156921
    Test accuracy at fold 5 = 0.5523465871810913
    Final Cross-Validation Test Accuracy: 0.5256317675113678
    Standard Deviation of Final Test Accuracy: 0.03584760640500936
    ```

## 活动 4.02:使用交叉验证为高级纤维化诊断分类器进行模型选择

在本活动中，我们将通过交叉验证模型选择和超参数选择来改进丙型肝炎数据集的分类器。按照以下步骤完成本活动:

1.  导入所有必需的包并加载数据集。使用`StandardScaler`函数:

    ```
    # import the required packages
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.wrappers.scikit_learn import KerasClassifier
    from sklearn.model_selection import StratifiedKFold
    from sklearn.model_selection import cross_val_score
    import numpy as np
    from tensorflow import random
    # Load the dataset
    X = pd.read_csv('../data/HCV_feats.csv')
    y = pd.read_csv('../data/HCV_target.csv')
    sc = StandardScaler()
    X = pd.DataFrame(sc.fit_transform(X), columns=X.columns)
    ```

    缩放数据集
2.  Define three functions, each returning a different Keras model. The first model should have three hidden layers of `size 4`, the second model should have two hidden layers, the first of `size 4` and the second of `size 2`, and the third model should have two hidden layers of `size 8`. Use function parameters for the activation functions and optimizers so that they can be passed through to the model. The goal is to find out which of these three models leads to the lowest cross-validation error rate:

    ```
    # Create the function that returns the keras model 1
    def build_model_1(activation='relu', optimizer='adam'):
        # create model 1
        model = Sequential()
        model.add(Dense(4, input_dim=X.shape[1], activation=activation))
        model.add(Dense(4, activation=activation))
        model.add(Dense(4, activation=activation))
        model.add(Dense(1, activation='sigmoid'))
        # Compile model
        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
        return model
    # Create the function that returns the keras model 2
    def build_model_2(activation='relu', optimizer='adam'):
        # create model 2
        model = Sequential()
        model.add(Dense(4, input_dim=X.shape[1], activation=activation))
        model.add(Dense(2, activation=activation))
        model.add(Dense(1, activation='sigmoid'))
        # Compile model
        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
        return model
    # Create the function that returns the keras model 3
    def build_model_3(activation='relu', optimizer='adam'):
        # create model 3
        model = Sequential()
        model.add(Dense(8, input_dim=X.shape[1], activation=activation))
        model.add(Dense(8, activation=activation))
        model.add(Dense(1, activation='sigmoid'))
        # Compile model
        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])
        return model
    ```

    编写循环遍历三个模型并执行`5-fold cross-validation`的代码。设置种子，使模型可重现，并定义`n_folds`、`batch_size`和`epochs`超参数。存储训练模型时应用`cross_val_score`函数的结果:

    ```
    # define a seed for random number generator so the result will be reproducible
    seed = 2
    np.random.seed(seed)
    random.set_seed(seed)
    # determine the number of folds for k-fold cross-validation, number of epochs and batch size
    n_folds = 5
    batch_size=20
    epochs=100
    # define the list to store cross-validation scores
    results_1 = []
    # define the possible options for the model
    models = [build_model_1, build_model_2, build_model_3]
    # loop over models
    for m in range(len(models)):
        # build the scikit-learn interface for the keras model
        classifier = KerasClassifier(build_fn=models[m], epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False)
        # define the cross-validation iterator
        kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)
        # perform the k-fold cross-validation and store the scores in result
        result = cross_val_score(classifier, X, y, cv=kfold)
        # add the scores to the results list 
        results_1.append(result)
    # Print cross-validation score for each model
    for m in range(len(models)):
        print("Model", m+1,"Test Accuracy =", results_1[m].mean())
    ```

    这是预期的输出。**模型 3** 具有最好的交叉验证测试精度，如下图所示:

    ```
    Model 1 Test Accuracy = 0.4996389865875244
    Model 2 Test Accuracy = 0.5148014307022095
    Model 3 Test Accuracy = 0.5097472846508027
    ```

3.  Choose the model with the highest accuracy score and repeat *step 2* by iterating over the `epochs = [100, 200]` and `batches = [10, 20]` values and performing `5-fold cross-validation`:

    ```
    # define a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # determine the number of folds for k-fold cross-validation
    n_folds = 5
    # define possible options for epochs and batch_size
    epochs = [100, 200]
    batches = [10, 20]
    # define the list to store cross-validation scores
    results_2 = []
    # loop over all possible pairs of epochs, batch_size
    for e in range(len(epochs)):
        for b in range(len(batches)):
            # build the scikit-learn interface for the keras model
            classifier = KerasClassifier(build_fn=build_model_2, epochs=epochs[e], batch_size=batches[b], verbose=0)
            # define the cross-validation iterator
            kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)
            # perform the k-fold cross-validation. 
            # store the scores in result
            result = cross_val_score(classifier, X, y, cv=kfold)
            # add the scores to the results list 
            results_2.append(result)
    # Print cross-validation score for each possible pair of epochs, batch_size
    c = 0
    for e in range(len(epochs)):
        for b in range(len(batches)):
            print("batch_size =", batches[b],", epochs =", epochs[e], ", Test Accuracy =", results_2[c].mean())
            c += 1
    ```

    下面是预期的输出:

    ```
    batch_size = 10 , epochs = 100 , Test Accuracy = 0.5010830342769623
    batch_size = 20 , epochs = 100 , Test Accuracy = 0.5126353740692139
    batch_size = 10 , epochs = 200 , Test Accuracy = 0.5176895320416497
    batch_size = 20 , epochs = 200 , Test Accuracy = 0.5075812220573426
    ```

    `batch_size= 10`、`epochs=200`对具有最好的交叉验证测试准确性。

4.  Choose the batch size and epochs with the highest accuracy score and repeat *step 3* by iterating over the `optimizers = ['rmsprop', 'adam','sgd']` and `activations = ['relu', 'tanh']` values and performing `5-fold cross-validation`:

    ```
    # define a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # determine the number of folds for k-fold cross-validation, number of epochs and batch size
    n_folds = 5
    batch_size = 10
    epochs = 200
    # define the list to store cross-validation scores
    results_3 = []
    # define possible options for optimizer and activation
    optimizers = ['rmsprop', 'adam','sgd']
    activations = ['relu', 'tanh']
    # loop over all possible pairs of optimizer, activation
    for o in range(len(optimizers)):
        for a in range(len(activations)):
            optimizer = optimizers[o]
            activation = activations[a]
            # build the scikit-learn interface for the keras model
            classifier = KerasClassifier(build_fn=build_model_2, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=False)
            # define the cross-validation iterator
            kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)
            # perform the k-fold cross-validation. 
            # store the scores in result
            result = cross_val_score(classifier, X, y, cv=kfold)
            # add the scores to the results list 
            results_3.append(result)
    # Print cross-validation score for each possible pair of optimizer, activation
    c = 0
    for o in range(len(optimizers)):
        for a in range(len(activations)):
            print("activation = ", activations[a],", optimizer = ", optimizers[o], ", Test accuracy = ", results_3[c].mean())
            c += 1
    ```

    下面是预期的输出:

    ```
    activation =  relu , optimizer =  rmsprop , Test accuracy =  0.5234657049179077
    activation =  tanh , optimizer =  rmsprop , Test accuracy =  0.49602887630462644
    activation =  relu , optimizer =  adam , Test accuracy =  0.5039711117744445
    activation =  tanh , optimizer =  adam , Test accuracy =  0.4989169597625732
    activation =  relu , optimizer =  sgd , Test accuracy =  0.48953068256378174
    activation =  tanh , optimizer =  sgd , Test accuracy =  0.5191335678100586
    ```

5.  `activation='relu'`和`optimizer='rmsprop'`对具有最好的交叉验证测试准确性。此外，`activation='tanh'`和`optimizer='sgd'`对产生了第二好的性能。

## 活动 4.03:在交通量数据集上使用交叉验证的模型选择

在本活动中，您将再次使用交叉验证练习模型选择。这里，我们将使用一个模拟数据集，该数据集代表一个目标变量，该变量代表城市桥梁上每小时的车流量以及与交通数据相关的各种归一化要素，例如一天中的时间和前一天的交通量。我们的目标是建立一个模型，在给定各种特征的情况下，预测城市桥梁的交通量。按照以下步骤完成本活动:

1.  导入所有需要的包并加载数据集:

    ```
    # import the required packages
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.wrappers.scikit_learn import KerasRegressor
    from sklearn.model_selection import KFold
    from sklearn.model_selection import cross_val_score
    from sklearn.preprocessing import StandardScaler
    from sklearn.pipeline import make_pipeline
    import numpy as np
    from tensorflow import random
    ```

2.  Load the dataset, print the input and output size for the feature dataset, and print the possible classes in the target dataset. Also, print the range of the output:

    ```
    # Load the dataset
    # Load the dataset
    X = pd.read_csv('../data/traffic_volume_feats.csv')
    y = pd.read_csv('../data/traffic_volume_target.csv') 
    # Print the sizes of input data and output data
    print("Input data size = ", X.shape)
    print("Output size = ", y.shape)
    # Print the range for output
    print(f"Output Range = ({y['Volume'].min()}, { y['Volume'].max()})")
    ```

    下面是预期的输出:

    ```
    Input data size =  (10000, 10)
    Output size =  (10000, 1)
    Output Range = (0.000000, 584.000000)
    ```

3.  定义三个函数，每个函数返回一个不同的 Keras 模型。第一个模型应该有一个隐藏层`size 10`，第二个模型应该有两个隐藏层`size 10`，第三个模型应该有三个隐藏层`size 10`。为优化器使用函数参数，以便它们可以传递给模型。目标是找出这三个模型中哪一个导致最低的交叉验证错误率:

    ```
    # Create the function that returns the keras model 1
    def build_model_1(optimizer='adam'):
        # create model 1
        model = Sequential()
        model.add(Dense(10, input_dim=X.shape[1], activation='relu'))
        model.add(Dense(1))
        # Compile model
        model.compile(loss='mean_squared_error', optimizer=optimizer)
        return model
    # Create the function that returns the keras model 2
    def build_model_2(optimizer='adam'):
        # create model 2
        model = Sequential()
        model.add(Dense(10, input_dim=X.shape[1], activation='relu'))
        model.add(Dense(10, activation='relu'))
        model.add(Dense(1))
        # Compile model
        model.compile(loss='mean_squared_error', optimizer=optimizer)
        return model
    # Create the function that returns the keras model 3
    def build_model_3(optimizer='adam'):
        # create model 3
        model = Sequential()
        model.add(Dense(10, input_dim=X.shape[1], activation='relu'))
        model.add(Dense(10, activation='relu'))
        model.add(Dense(10, activation='relu'))
        model.add(Dense(1))
        # Compile model
        model.compile(loss='mean_squared_error', optimizer=optimizer)
        return model
    ```

4.  Write the code that will loop over the three models and perform `5-fold cross-validation`. Set the seed so that the models are reproducible and define the `n_folds` hyperparameters. Store the results from applying the `cross_val_score` function when training the models:

    ```
    # define a seed for random number generator so the result will be reproducible
    seed = 1
    np.random.seed(seed)
    random.set_seed(seed)
    # determine the number of folds for k-fold cross-validation
    n_folds = 5
    # define the list to store cross-validation scores
    results_1 = []
    # define the possible options for the model
    models = [build_model_1, build_model_2, build_model_3]
    # loop over models
    for i in range(len(models)):
        # build the scikit-learn interface for the keras model
        regressor = KerasRegressor(build_fn=models[i], epochs=100, batch_size=50, verbose=0, shuffle=False)
        # build the pipeline of transformations so for each fold training set will be scaled 
        # and test set will be scaled accordingly.
        model = make_pipeline(StandardScaler(), regressor)
        # define the cross-validation iterator
        kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)
        # perform the k-fold cross-validation. 
        # store the scores in result
        result = cross_val_score(model, X, y, cv=kfold)
        # add the scores to the results list 
        results_1.append(result)
    # Print cross-validation score for each model
    for i in range(len(models)):
        print("Model ", i+1," test error rate = ", abs(results_1[i].mean()))
    ```

    以下是预期的输出:

    ```
    Model  1  test error rate =  25.48777518749237
    Model  2  test error rate =  25.30460816860199
    Model  3  test error rate =  25.390239462852474
    ```

    `Model 2`(两层神经网络)测试错误率最低。

5.  Choose the model with the lowest test error rate and repeat *step 4* while iterating over `epochs = [80, 100]` and `batches = [50, 25]` and performing `5-fold cross-validation`:

    ```
    # define a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # determine the number of folds for k-fold cross-validation
    n_folds = 5
    # define the list to store cross-validation scores
    results_2 = []
    # define possible options for epochs and batch_size
    epochs = [80, 100]
    batches = [50, 25]
    # loop over all possible pairs of epochs, batch_size
    for i in range(len(epochs)):
        for j in range(len(batches)):
            # build the scikit-learn interface for the keras model
            regressor = KerasRegressor(build_fn=build_model_2, epochs=epochs[i], batch_size=batches[j], verbose=0, shuffle=False)
            # build the pipeline of transformations so for each fold training set will be scaled 
            # and test set will be scaled accordingly.
            model = make_pipeline(StandardScaler(), regressor)
            # define the cross-validation iterator
            kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)
            # perform the k-fold cross-validation. 
            # store the scores in result
            result = cross_val_score(model, X, y, cv=kfold)
            # add the scores to the results list 
            results_2.append(result)
    # Print cross-validation score for each possible pair of epochs, batch_size
    c = 0
    for i in range(len(epochs)):
        for j in range(len(batches)):
            print("batch_size = ", batches[j],", epochs = ", epochs[i], ", Test error rate = ", abs(results_2[c].mean()))
            c += 1
    ```

    下面是预期的输出:

    ```
    batch_size = 50 , epochs = 80 , Test error rate = 25.270704221725463
    batch_size = 25 , epochs = 80 , Test error rate = 25.309741401672362
    batch_size = 50 , epochs = 100 , Test error rate = 25.095393986701964
    batch_size = 25 , epochs = 100 , Test error rate = 25.24592453837395
    ```

    `batch_size=5`和`epochs=100`对的测试错误率最低。

6.  Choose the model with the highest accuracy score and repeat *step 2* by iterating over `optimizers = ['rmsprop', 'sgd', 'adam']` and performing `5-fold cross-validation`:

    ```
    # define a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # determine the number of folds for k-fold cross-validation
    n_folds = 5
    # define the list to store cross-validation scores
    results_3 = []
    # define the possible options for the optimizer
    optimizers = ['adam', 'sgd', 'rmsprop']
    # loop over optimizers
    for i in range(len(optimizers)):
        optimizer=optimizers[i]
        # build the scikit-learn interface for the keras model
        regressor = KerasRegressor(build_fn=build_model_2, epochs=100, batch_size=50, verbose=0, shuffle=False)
        # build the pipeline of transformations so for each fold trainind set will be scaled 
        # and test set will be scaled accordingly.
        model = make_pipeline(StandardScaler(), regressor)
        # define the cross-validation iterator
        kfold = KFold(n_splits=n_folds, shuffle=True, random_state=seed)
        # perform the k-fold cross-validation. 
        # store the scores in result
        result = cross_val_score(model, X, y, cv=kfold)
        # add the scores to the results list 
        results_3.append(result)
    # Print cross-validation score for each optimizer
    for i in range(len(optimizers)):
        print("optimizer=", optimizers[i]," test error rate = ", abs(results_3[i].mean()))
    ```

    下面是预期的输出:

    ```
    optimizer= adam  test error rate =  25.391812739372256
    optimizer= sgd  test error rate =  25.140230269432067
    optimizer= rmsprop  test error rate =  25.217947859764102
    ```

7.  `optimizer='sgd'`具有最低的测试错误率，因此我们应该继续使用这个特定的模型。

# 5。提高模型准确性

## 活动 5.01:Avila 模式分类器上的权重正则化

在本活动中，您将构建一个 Keras 模型，根据给定的网络架构和超参数值对 Avila 模式数据集执行分类。目标是对模型应用不同类型的权重正则化，即`L1`和`L2`，并观察每种类型如何改变结果。按照以下步骤完成本活动:

1.  加载数据集并将数据集分割成`training set`和`test set` :

    ```
    # Load the dataset
    import pandas as pd
    X = pd.read_csv('../data/avila-tr_feats.csv')
    y = pd.read_csv('../data/avila-tr_target.csv')
    # Split the dataset into training set and test set with a 0.8-0.2 ratio
    from sklearn.model_selection import train_test_split
    seed = 1
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)
    ```

2.  定义一个有三个隐藏层的 Keras 序列模型，第一个是`size 10`，第二个是`size 6`，第三个是`size 4`。最后，编译模型:

    ```
    # define a seed for random number generator so the result will be reproducible
    import numpy as np
    from tensorflow import random
    np.random.seed(seed)
    random.set_seed(seed)
    # define the keras model
    from keras.models import Sequential
    from keras.layers import Dense
    model_1 = Sequential()
    model_1.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))
    model_1.add(Dense(6, activation='relu'))
    model_1.add(Dense(4, activation='relu'))
    model_1.add(Dense(1, activation='sigmoid'))
    model_1.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
    ```

3.  将模型拟合到训练数据以执行分类，保存训练过程的结果:

    ```
    history=model_1.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_data=(X_test, y_test), verbose=0, shuffle=False)
    ```

4.  Plot the trends in training error and test error by importing the necessary libraries for plotting the loss and validation loss and saving them in the variable that was created when the model was fit to the training process. Print out the maximum validation accuracy:

    ```
    import matplotlib.pyplot as plt 
    import matplotlib
    %matplotlib inline 
    # plot training error and test error
    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) 
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim(0,1)
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    # print the best accuracy reached on the test set
    print("Best Accuracy on Validation Set =", max(history.history['val_accuracy']))
    ```

    以下是预期的输出:

    ![Figure 5.13: A plot of the training error and validation error during training 
    for the model without regularization
    ](image/B15777_05_13.jpg)

    图 5.13:没有正则化的模型在训练过程中的训练误差和验证误差图

    验证损失随着训练损失不断减少。尽管没有正规化，这是一个相当好的训练过程的例子，因为偏差和方差相当低。

5.  Redefine the model, adding `L2 regularizers` with `lambda=0.01` to each hidden layer of the model. Repeat *steps 3* and *4* to train the model and plot the `training error` and `validation error`:

    ```
    # set up a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # define the keras model with l2 regularization with lambda = 0.01
    from keras.regularizers import l2
    l2_param = 0.01
    model_2 = Sequential()
    model_2.add(Dense(10, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(l2_param)))
    model_2.add(Dense(6, activation='relu', kernel_regularizer=l2(l2_param)))
    model_2.add(Dense(4, activation='relu', kernel_regularizer=l2(l2_param)))
    model_2.add(Dense(1, activation='sigmoid'))
    model_2.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
    # train the model using training set while evaluating on test set
    history=model_2.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_data=(X_test, y_test), verbose=0, shuffle=False)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim(0,1)
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    # print the best accuracy reached on the test set
    print("Best Accuracy on Validation Set =", max(history.history['val_accuracy']))
    ```

    以下是预期的输出:

    ![Figure 5.14: A plot of the training error and validation error during training for the model with L2 weight regularization (lambda=0.01)
    ](image/B15777_05_14.jpg)

    图 5.14:使用 L2 权重正则化(λ= 0.01)的模型在训练期间的训练误差和验证误差图

    如前面的图所示，测试误差在降低到一定量后几乎保持平稳。在训练过程结束时，训练误差和验证误差之间的差距(偏差)稍小，这表明训练示例的模型过拟合减少。

6.  Repeat the previous step with `lambda=0.1` for the `L2 parameter`—redefine the model with the new lambda parameter, fit the model to the training data, and repeat *step 4* to plot the training error and validation error:

    ```
    # set up a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    from keras.regularizers import l2
    l2_param = 0.1
    model_3 = Sequential()
    model_3.add(Dense(10, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(l2_param)))
    model_3.add(Dense(6, activation='relu', kernel_regularizer=l2(l2_param)))
    model_3.add(Dense(4, activation='relu', kernel_regularizer=l2(l2_param)))
    model_3.add(Dense(1, activation='sigmoid'))
    model_3.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
    # train the model using training set while evaluating on test set
    history=model_3.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_data=(X_test, y_test), verbose=0, shuffle=False)
    # plot training error and test error
    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim(0,1)
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    # print the best accuracy reached on the test set
    print("Best Accuracy on Validation Set =", max(history.history['val_accuracy']))
    ```

    以下是预期的输出:

    ![Figure 5.15: A plot of the training error and validation error during training for the model with L2 weight regularization (lambda=0.1)
    ](image/B15777_05_15.jpg)

    图 5.15:使用 L2 权重正则化(λ= 0.1)的模型在训练期间的训练误差和验证误差图

    训练和验证错误很快达到平稳状态，并且比我们使用较低的`L2 parameter`创建的模型要高得多，这表明我们对模型进行了太多的惩罚，以至于它不具有学习训练数据的底层功能的灵活性。接下来，我们将减少正则化参数的值，以防止它对模型造成太大的影响。

7.  Repeat the previous step, this time with `lambda=0.005`. Repeat *step 4* to plot the training error and validation error:

    ```
    # set up a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # define the keras model with l2 regularization with lambda = 0.05
    from keras.regularizers import l2
    l2_param = 0.005
    model_4 = Sequential()
    model_4.add(Dense(10, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(l2_param)))
    model_4.add(Dense(6, activation='relu', kernel_regularizer=l2(l2_param)))
    model_4.add(Dense(4, activation='relu', kernel_regularizer=l2(l2_param)))
    model_4.add(Dense(1, activation='sigmoid'))
    model_4.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
    # train the model using training set while evaluating on test set
    history=model_4.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_data=(X_test, y_test), verbose=0, shuffle=False)
    # plot training error and test error
    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim(0,1)
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    # print the best accuracy reached on the test set
    print("Best Accuracy on Validation Set =", max(history.history['val_accuracy'])) 
    ```

    以下是预期的输出:

    ![Figure 5.16: A plot of the training error and validation error during training 
    for the model with L2 weight regularization (lambda=0.005)
    ](image/B15777_05_16.jpg)

    图 5.16:使用 L2 权重正则化(λ= 0.005)的模型在训练期间的训练误差和验证误差图

    `L2 weight`正则化的值达到了最高的精度，该精度是在所有模型的验证数据上用`L2 regularization`进行评估的，但它比没有正则化的略低。同样，测试误差在降低到某个值后没有显著增加，这表明模型没有过度拟合训练示例。看起来带有`lambda=0.005`的`L2 weight regularization`在防止模型过度拟合的同时实现了最低的验证误差。

8.  Add `L1 regularizers` with `lambda=0.01` to the hidden layers of your model. Redefine the model with the new lambda parameter, fit the model to the training data, and repeat *step 4* to plot the training error and validation error:

    ```
    # set up a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # define the keras model with l1 regularization with lambda = 0.01
    from keras.regularizers import l1
    l1_param = 0.01
    model_5 = Sequential()
    model_5.add(Dense(10, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l1(l1_param)))
    model_5.add(Dense(6, activation='relu', kernel_regularizer=l1(l1_param)))
    model_5.add(Dense(4, activation='relu', kernel_regularizer=l1(l1_param)))
    model_5.add(Dense(1, activation='sigmoid'))
    model_5.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
    # train the model using training set while evaluating on test set
    history=model_5.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_data=(X_test, y_test), verbose=0, shuffle=True)
    # plot training error and test error
    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim(0,1)
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    # print the best accuracy reached on the test set
    print("Best Accuracy on Validation Set =", max(history.history['val_accuracy']))
    ```

    以下是预期的输出:

    ![Figure 5.17: A plot of the training error and validation error during training 
    for the model with L1 weight regularization (lambda=0.01)
    ](image/B15777_05_17.jpg)

    图 5.17:使用 L1 权重正则化(λ= 0.01)的模型在训练期间的训练误差和验证误差图

9.  Repeat the previous step with `lambda=0.005` for the `L1 parameter`—redefine the model with the new lambda parameter, fit the model to the training data, and repeat *step 4* to plot the `training error` and `validation error`:

    ```
    # set up a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # define the keras model with l1 regularization with lambda = 0.1
    from keras.regularizers import l1
    l1_param = 0.005
    model_6 = Sequential()
    model_6.add(Dense(10, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l1(l1_param)))
    model_6.add(Dense(6, activation='relu', kernel_regularizer=l1(l1_param)))
    model_6.add(Dense(4, activation='relu', kernel_regularizer=l1(l1_param)))
    model_6.add(Dense(1, activation='sigmoid'))
    model_6.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
    # train the model using training set while evaluating on test set
    history=model_6.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_data=(X_test, y_test), verbose=0, shuffle=False)
    # plot training error and test error
    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) 
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim(0,1)
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    # print the best accuracy reached on the test set
    print("Best Accuracy on Validation Set =", max(history.history['val_accuracy']))
    ```

    以下是预期的输出:

    ![Figure 5.18: The plot of the training error and validation error during training for the model with L1 weight regularization (lambda=0.005)
    ](image/B15777_05_18.jpg)

    图 5.18:使用 L1 权重正则化(λ= 0.005)的模型在训练过程中的训练误差和验证误差图

    看起来，由于`lambda=0.01`的值过于严格，并且阻止模型学习训练数据的基本功能，所以具有`lambda=0.005`的`L1 weight regularization`实现了更好的测试误差，同时防止了模型过度拟合。

10.  Add `L1` and `L2 regularizers` with an `L1` of `lambda=0.005` and an `L2` of `lambda = 0.005` to the hidden layers of your model. Then, repeat *step 4* to plot the training error and validation error:

    ```
    # set up a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # define the keras model with l1_l2 regularization with l1_lambda = 0.005 and l2_lambda = 0.005
    from keras.regularizers import l1_l2
    l1_param = 0.005
    l2_param = 0.005
    model_7 = Sequential()
    model_7.add(Dense(10, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l1_l2(l1=l1_param, l2=l2_param)))
    model_7.add(Dense(6, activation='relu', kernel_regularizer=l1_l2(l1=l1_param, l2=l2_param)))
    model_7.add(Dense(4, activation='relu', kernel_regularizer=l1_l2(l1=l1_param, l2=l2_param)))
    model_7.add(Dense(1, activation='sigmoid'))
    model_7.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
    # train the model using training set while evaluating on test set
    history=model_7.fit(X_train, y_train, batch_size = 20, epochs = 100, validation_data=(X_test, y_test), verbose=0, shuffle=True)

    # plot training error and test error
    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim(0,1)
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    # print the best accuracy reached on the test set
    print("Best Accuracy on Validation Set =", max(history.history['val_accuracy']))
    ```

    以下是预期的输出:

![Figure 5.19: A plot of the training error and validation error during training for the model with L1 lambda equal to 0.005 and L2 lambda equal to 0.005
](image/B15777_05_19.jpg)

图 5.19:L1λ等于 0.005，L2λ等于 0.005 的模型在训练期间的训练误差和验证误差图

虽然`L1`和`L2 regularization`成功地防止了模型过度拟合，但是模型中的方差非常低。然而，在验证数据上获得的准确度不如没有进行正则化训练的模型或单独用`L2 regularization` `lambda=0.005`或`L1 regularization` `lambda=0.005`参数训练的模型高。

## 活动 5.02:交通量数据集中的流失调整

在本活动中，您将从*活动 4.03* 、*中的模型开始，在*第 4 章*、*的交通量数据集*上使用交叉验证进行模型选择，使用 Keras Wrappers* 使用交叉验证评估您的模型。您将使用训练集/测试集方法来训练和评估模型，绘制训练错误和泛化错误的趋势，并观察模型是否过度拟合数据示例。然后，您将尝试通过使用 dropout 正则化来解决过度拟合问题，从而提高模型性能。具体来说，您将尝试找出应该向哪些层添加下降正则化，以及什么样的`rate`值将最大程度地改进该特定模型。按照以下步骤完成本练习:

1.  使用 pandas `read_csv`函数加载数据集，使用`train_test_split`将数据集分成训练集和测试集，并使用`StandardScaler` :

    ```
    # Load the dataset
    import pandas as pd
    X = pd.read_csv('../data/traffic_volume_feats.csv')
    y = pd.read_csv('../data/traffic_volume_target.csv')
    # Split the dataset into training set and test set with an 80-20 ratio
    from sklearn.model_selection import train_test_split
    seed=1
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=seed)
    ```

    缩放输入数据
2.  设置一个种子，以便可以复制模型。接下来，定义一个带有两个隐藏层`size 10`的 Keras 序列模型，两者都带有`ReLU activation`函数。添加一个没有激活函数的输出层，用给定的超参数编译模型:

    ```
    # define a seed for random number generator so the result will be reproducible
    import numpy as np
    np.random.seed(seed)
    random.set_seed(seed)
    from keras.models import Sequential
    from keras.layers import Dense
    # create model
    model_1 = Sequential()
    model_1.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))
    model_1.add(Dense(10, activation='relu'))
    model_1.add(Dense(1))
    # Compile model
    model_1.compile(loss='mean_squared_error', optimizer='rmsprop')
    ```

3.  用给定的超参数对训练数据训练模型:

    ```
    # train the model using training set while evaluating on test set
    history=model_1.fit(X_train, y_train, batch_size = 50, epochs = 200, validation_data=(X_test, y_test), verbose=0, shuffle=False) 
    ```

4.  Plot the trends for the `training error` and `test error`. Print the best accuracy that was reached for the training and validation set:

    ```
    import matplotlib.pyplot as plt 
    import matplotlib
    %matplotlib inline 
    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) 
    # plot training error and test error plots 
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim((0, 100))
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    # print the best accuracy reached on the test set
    print("Lowest error on training set = ", min(history.history['loss']))
    print("Lowest error on validation set = ", min(history.history['val_loss']))
    ```

    以下是预期的输出:

    ![Figure 5.20: A plot of the training error and validation error during training 
    for the model without regularization
    ](image/B15777_05_20.jpg)

    图 5.20:没有正则化的模型在训练期间的训练误差和验证误差图

    在训练误差和验证误差值中，训练误差和验证误差之间存在非常小的差距，这表明模型的方差低，这是好的。

5.  Redefine the model by creating the same model architecture. However, this time, add a dropout regularization with `rate=0.1` to the first hidden layer of your model. Repeat *step 3* to train the model on the training data and repeat *step 4* to plot the trends for the training and validation errors. Then, print the best accuracy that was reached on the validation set:

    ```
    # define a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    from keras.layers import Dropout
    # create model
    model_2 = Sequential()
    model_2.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))
    model_2.add(Dropout(0.1))
    model_2.add(Dense(10, activation='relu'))
    model_2.add(Dense(1))
    # Compile model
    model_2.compile(loss='mean_squared_error', optimizer='rmsprop')
    # train the model using training set while evaluating on test set
    history=model_2.fit(X_train, y_train, batch_size = 50, epochs = 200, validation_data=(X_test, y_test), verbose=0, shuffle=False)
    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim((0, 25000))
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    # print the best accuracy reached on the test set
    print("Lowest error on training set = ", min(history.history['loss']))
    print("Lowest error on validation set = ", min(history.history['val_loss']))
    ```

    以下是预期的输出:

    ![Figure 5.21: A plot of the training error and validation error during training 
    for the model with dropout regularization (rate=0.1) in the first layer
    ](image/B15777_05_21.jpg)

    图 5.21:在第一层中带有丢失正则化(比率=0.1)的模型的训练期间，训练误差和验证误差的图

    训练误差和验证误差之间有很小的差距；但是，验证误差低于训练误差，表明模型没有过度拟合训练数据。

6.  Repeat the previous step, this time adding dropout regularization with `rate=0.1` to both hidden layers of your model. Repeat *step 3* to train the model on the training data and repeat *step 4* to plot the trends for the training and validation errors. Then, print the best accuracy that was reached on the validation set:

    ```
    # define a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # create model
    model_3 = Sequential()
    model_3.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))
    model_3.add(Dropout(0.1))
    model_3.add(Dense(10, activation='relu'))
    model_3.add(Dropout(0.1))
    model_3.add(Dense(1))
    # Compile model
    model_3.compile(loss='mean_squared_error', optimizer='rmsprop')
    # train the model using training set while evaluating on test set
    history=model_3.fit(X_train, y_train, batch_size = 50, epochs = 200, validation_data=(X_test, y_test), verbose=0, shuffle=False)
    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim((0, 25000))
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    # print the best accuracy reached on the test set
    print("Lowest error on training set = ", min(history.history['loss']))
    print("Lowest error on validation set = ", min(history.history['val_loss']))
    ```

    以下是预期的输出:

    ![Figure 5.22: A plot of the training error and validation error during training for the model with dropout regularization (rate=0.1) in both layers
    ](image/B15777_05_22.jpg)

    图 5.22:在两个层中均有丢失正则化(比率=0.1)的模型的训练期间，训练误差和验证误差的曲线图

    此处，训练误差和验证误差之间的差距稍大，主要是由于模型的第二个隐藏层上的额外正则化导致了训练误差的增加。

7.  Repeat the previous step, this time adding dropout regularization with `rate=0.2` in the first layer and `rate=0.1` in the second layer of your model. Repeat *step 3* to train the model on the training data and repeat *step 4* to plot the trends for the training and validation errors. Then, print the best accuracy that was reached on the validation set:

    ```
    # define a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # create model
    model_4 = Sequential()
    model_4.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))
    model_4.add(Dropout(0.2))
    model_4.add(Dense(10, activation='relu'))
    model_4.add(Dropout(0.1))
    model_4.add(Dense(1))
    # Compile model
    model_4.compile(loss='mean_squared_error', optimizer='rmsprop')
    # train the model using training set while evaluating on test set
    history=model_4.fit(X_train, y_train, batch_size = 50, epochs = 200, validation_data=(X_test, y_test), verbose=0, shuffle=False)
    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim((0, 25000))
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    # print the best accuracy reached on the test set
    print("Lowest error on training set = ", min(history.history['loss']))
    print("Lowest error on validation set = ", min(history.history['val_loss'])
    ```

    以下是预期的输出:

![Figure 5.23: A plot of training errors and validation errors while training the model with dropout regularization, with rate=0.2 in the first layer and rate 0.1 in the second layer
](image/B15777_05_23.jpg)

图 5.23:在使用漏失正则化对模型进行训练时，训练误差和验证误差的曲线图，第一层中的比率=0.2，第二层中的比率= 0.1

由于正则化的增加，训练误差和验证误差之间的差距稍大。在这种情况下，原始模型中不存在过拟合。因此，正则化增加了训练和验证数据集的错误率。

## 活动 5.03:在 Avila 模式分类器上调整超参数

在本活动中，您将构建一个类似于前面活动中的 Keras 模型，但这一次，您还将向模型中添加正则化方法。然后，您将使用 scikit-learn 优化器对模型超参数执行调优，包括正则化子的超参数。按照以下步骤完成本活动:

1.  加载数据集并导入库:

    ```
    # Load The dataset
    import pandas as pd
    X = pd.read_csv('../data/avila-tr_feats.csv')
    y = pd.read_csv('../data/avila-tr_target.csv')
    ```

2.  定义一个函数，返回带有三个隐藏层的 Keras 模型，第一个是`size 10`，第二个是`size 6`，第三个是`size 4`，在每个隐藏层上应用`L2 weight regularization`和`ReLU activation`函数。用给定的参数编译模型并从模型返回:

    ```
    # Create the function that returns the keras model
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.regularizers import l2
    def build_model(lambda_parameter):
        model = Sequential()
        model.add(Dense(10, input_dim=X.shape[1], activation='relu', kernel_regularizer=l2(lambda_parameter)))
        model.add(Dense(6, activation='relu', kernel_regularizer=l2(lambda_parameter)))
        model.add(Dense(4, activation='relu', kernel_regularizer=l2(lambda_parameter)))
        model.add(Dense(1, activation='sigmoid'))
        model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
        return model
    ```

3.  设置一个种子，使用 scikit-learn 包装器来包装我们在上一步中创建的模型，并定义要扫描的超参数。最后，使用超参数网格对模型执行`GridSearchCV()`并拟合模型:

    ```
    from keras.wrappers.scikit_learn import KerasClassifier
    from sklearn.model_selection import GridSearchCV
    # define a seed for random number generator so the result will be reproducible
    import numpy as np
    from tensorflow import random
    seed = 1
    np.random.seed(seed)
    random.set_seed(seed)
    # create the Keras wrapper with scikit learn
    model = KerasClassifier(build_fn=build_model, verbose=0, shuffle=False)
    # define all the possible values for each hyperparameter
    lambda_parameter = [0.01, 0.5, 1]
    epochs = [50, 100]
    batch_size = [20]
    # create the dictionary containing all possible values of hyperparameters
    param_grid = dict(lambda_parameter=lambda_parameter, epochs=epochs, batch_size=batch_size)
    # perform 5-fold cross-validation for ??????? store the results
    grid_seach = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)
    results_1 = grid_seach.fit(X, y)
    ```

4.  Print the results for the best cross-validation score that's stored within the variable we created in the fit process. Iterate through all the parameters and print the mean of the accuracy across all the folds, the standard deviation of the accuracy, and the parameters themselves:

    ```
    print("Best cross-validation score =", results_1.best_score_)
    print("Parameters for Best cross-validation score=", results_1.best_params_)
    # print the results for all evaluated hyperparameter combinations
    accuracy_means = results_1.cv_results_['mean_test_score']
    accuracy_stds = results_1.cv_results_['std_test_score']
    parameters = results_1.cv_results_['params']
    for p in range(len(parameters)):
        print("Accuracy %f (std %f) for params %r" % (accuracy_means[p], accuracy_stds[p], parameters[p]))
    ```

    以下是预期的输出:

    ```
    Best cross-validation score = 0.7673058390617371
    Parameters for Best cross-validation score= {'batch_size': 20, 'epochs': 100, 'lambda_parameter': 0.01}
    Accuracy 0.764621 (std 0.004330) for params {'batch_size': 20, 'epochs': 50, 'lambda_parameter': 0.01}
    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, 'epochs': 50, 'lambda_parameter': 0.5}
    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, 'epochs': 50, 'lambda_parameter': 1}
    Accuracy 0.767306 (std 0.015872) for params {'batch_size': 20, 'epochs': 100, 'lambda_parameter': 0.01}
    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, 'epochs': 100, 'lambda_parameter': 0.5}
    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, 'epochs': 100, 'lambda_parameter': 1}
    ```

5.  Repeat *step 3* using `GridSearchCV()`, `lambda_parameter = [0.001, 0.01, 0.05, 0.1]`, `batch_size = [20]`, and `epochs = [100]`. Fit the model to the training data using `5-fold cross-validation` and print the results for the entire grid:

    ```
    # define a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # create the Keras wrapper with scikit learn
    model = KerasClassifier(build_fn=build_model, verbose=0, shuffle=False)
    # define all the possible values for each hyperparameter
    lambda_parameter = [0.001, 0.01, 0.05, 0.1]
    epochs = [100]
    batch_size = [20]
    # create the dictionary containing all possible values of hyperparameters
    param_grid = dict(lambda_parameter=lambda_parameter, epochs=epochs, batch_size=batch_size)
    # search the grid, perform 5-fold cross-validation for each possible combination, store the results
    grid_seach = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)
    results_2 = grid_seach.fit(X, y)
    # print the results for best cross-validation score
    print("Best cross-validation score =", results_2.best_score_)
    print("Parameters for Best cross-validation score =", results_2.best_params_)
    # print the results for the entire grid
    accuracy_means = results_2.cv_results_['mean_test_score']
    accuracy_stds = results_2.cv_results_['std_test_score']
    parameters = results_2.cv_results_['params']
    for p in range(len(parameters)):
        print("Accuracy %f (std %f) for params %r" % (accuracy_means[p], accuracy_stds[p], parameters[p]))
    ```

    以下是预期的输出:

    ```
    Best cross-validation score = 0.786385428905487
    Parameters for Best cross-validation score = {'batch_size': 20, 'epochs': 100, 'lambda_parameter': 0.001}
    Accuracy 0.786385 (std 0.010177) for params {'batch_size': 20, 'epochs': 100, 'lambda_parameter': 0.001}
    Accuracy 0.693960 (std 0.084994) for params {'batch_size': 20, 'epochs': 100, 'lambda_parameter': 0.01}
    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, 'epochs': 100, 'lambda_parameter': 0.05}
    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, 'epochs': 100, 'lambda_parameter': 0.1}
    ```

6.  重新定义一个函数，返回一个带有三个隐藏层的 Keras 模型，第一个是`size 10`，第二个是`size 6`，第三个是`size 4`，并在每个隐藏层上应用`dropout regularization`和`ReLU activation`函数。用给定的参数编译模型，从函数

    ```
    # Create the function that returns the keras model
    from keras.layers import Dropout
    def build_model(rate):
        model = Sequential()
        model.add(Dense(10, input_dim=X.shape[1], activation='relu'))
        model.add(Dropout(rate))
        model.add(Dense(6, activation='relu'))
        model.add(Dropout(rate))
        model.add(Dense(4, activation='relu'))
        model.add(Dropout(rate))
        model.add(Dense(1, activation='sigmoid'))
        model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])
        return model
    ```

    返回
7.  Use `rate = [0, 0.1, 0.2]` and `epochs = [50, 100]` and perform `GridSearchCV()` on the model. Fit the model to the training data using `5-fold cross-validation` and print the results for the entire grid:

    ```
    # define a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # create the Keras wrapper with scikit learn
    model = KerasClassifier(build_fn=build_model, verbose=0, shuffle=False)
    # define all the possible values for each hyperparameter
    rate = [0, 0.1, 0.2]
    epochs = [50, 100]
    batch_size = [20]
    # create the dictionary containing all possible values of hyperparameters
    param_grid = dict(rate=rate, epochs=epochs, batch_size=batch_size)
    # perform 5-fold cross-validation for 10 randomly selected combinations, store the results
    grid_seach = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)
    results_3 = grid_seach.fit(X, y)
    # print the results for best cross-validation score
    print("Best cross-validation score =", results_3.best_score_)
    print("Parameters for Best cross-validation score =", results_3.best_params_)
    # print the results for the entire grid
    accuracy_means = results_3.cv_results_['mean_test_score']
    accuracy_stds = results_3.cv_results_['std_test_score']
    parameters = results_3.cv_results_['params']
    for p in range(len(parameters)):
        print("Accuracy %f (std %f) for params %r" % (accuracy_means[p], accuracy_stds[p], parameters[p]))
    ```

    以下是预期的输出:

    ```
    Best cross-validation score= 0.7918504476547241
    Parameters for Best cross-validation score= {'batch_size': 20, 'epochs': 100, 'rate': 0}
    Accuracy 0.786769 (std 0.008255) for params {'batch_size': 20, 'epochs': 50, 'rate': 0}
    Accuracy 0.764717 (std 0.007691) for params {'batch_size': 20, 'epochs': 50, 'rate': 0.1}
    Accuracy 0.752637 (std 0.013546) for params {'batch_size': 20, 'epochs': 50, 'rate': 0.2}
    Accuracy 0.791850 (std 0.008519) for params {'batch_size': 20, 'epochs': 100, 'rate': 0}
    Accuracy 0.779291 (std 0.009504) for params {'batch_size': 20, 'epochs': 100, 'rate': 0.1}
    Accuracy 0.767306 (std 0.005773) for params {'batch_size': 20, 'epochs': 100, 'rate': 0.2}
    ```

8.  Repeat *step 5* using `rate = [0.0, 0.05, 0.1]` and `epochs = [100]`. Fit the model to the training data using `5-fold cross-validation` and print the results for the entire grid:

    ```
    # define a seed for random number generator so the result will be reproducible
    np.random.seed(seed)
    random.set_seed(seed)
    # create the Keras wrapper with scikit learn
    model = KerasClassifier(build_fn=build_model, verbose=0, shuffle=False)
    # define all the possible values for each hyperparameter
    rate = [0.0, 0.05, 0.1]
    epochs = [100]
    batch_size = [20]
    # create the dictionary containing all possible values of hyperparameters
    param_grid = dict(rate=rate, epochs=epochs, batch_size=batch_size)
    # perform 5-fold cross-validation for 10 randomly selected combinations, store the results
    grid_seach = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)
    results_4 = grid_seach.fit(X, y)
    # print the results for best cross-validation score
    print("Best cross-validation score =", results_4.best_score_)
    print("Parameters for Best cross-validation score =", results_4.best_params_)
    # print the results for the entire grid
    accuracy_means = results_4.cv_results_['mean_test_score']
    accuracy_stds = results_4.cv_results_['std_test_score']
    parameters = results_4.cv_results_['params']
    for p in range(len(parameters)):
        print("Accuracy %f (std %f) for params %r" % (accuracy_means[p], accuracy_stds[p], parameters[p]))
    ```

    以下是预期的输出:

    ```
    Best cross-validation score= 0.7862895488739013
    Parameters for Best cross-validation score= {'batch_size': 20, 'epochs': 100, 'rate': 0.0}
    Accuracy 0.786290 (std 0.013557) for params {'batch_size': 20, 'epochs': 100, 'rate': 0.0}
    Accuracy 0.786098 (std 0.005184) for params {'batch_size': 20, 'epochs': 100, 'rate': 0.05}
    Accuracy 0.772004 (std 0.013733) for params {'batch_size': 20, 'epochs': 100, 'rate': 0.1}
    ```

# 6。模型评估

## 活动 6.01:当我们改变训练/测试分割时，计算神经网络的精确度和零精确度

在本活动中，我们将看到`null accuracy`和`accuracy`会受到`train` / `test`分割的影响。要实现这一点，必须更改定义训练/测试分割的代码部分。我们将使用我们在*练习 6.02* 、*中使用的相同数据集，计算斯堪尼亚卡车数据*的 APS 故障的准确度和零准确度。按照以下步骤完成本活动:

1.  Import the required libraries. Load the dataset using the pandas `read_csv` function and look at the first `five` rows of the dataset:

    ```
    # Import the libraries
    import numpy as np
    import pandas as pd
    # Load the Data
    X = pd.read_csv("../data/aps_failure_training_feats.csv")
    y = pd.read_csv("../data/aps_failure_training_target.csv")
    # Use the head function to get a glimpse data
    X.head()
    ```

    下表显示了上述代码的输出:

    ![Figure 6.13: Initial five rows of the dataset
    ](image/B15777_06_13.jpg)

    图 6.13:数据集的前五行

2.  Change the `test_size` and `random_state` from `0.20` to `0.3` and `42` to `13`, respectively:

    ```
    # Split the data into training and testing sets
    from sklearn.model_selection import train_test_split
    seed = 13
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed)
    ```

    注意

    如果您使用不同的`random_state`，您可能会得到不同的`train` / `test`分割，这可能会产生稍微不同的最终结果。

3.  Scale the data using the `StandardScaler` function and use the scaler to scale the test data. Convert both into pandas DataFrames:

    ```
    # Initialize StandardScaler
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    # Transform the training data
    X_train = sc.fit_transform(X_train)
    X_train = pd.DataFrame(X_train, columns=X_test.columns)
    # Transform the testing data
    X_test = sc.transform(X_test)
    X_test = pd.DataFrame(X_test, columns = X_train.columns)
    ```

    注意

    `sc.fit_transform()`函数转换数据，数据也被转换成一个`NumPy`数组。我们以后可能需要数据作为 DataFrame 对象进行分析，所以`pd.DataFrame()`函数将数据重新转换成 DataFrame。

4.  导入构建神经网络架构所需的库:

    ```
    # Import the relevant Keras libraries
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.layers import Dropout
    From tensorflow import random
    ```

5.  启动`Sequential`类:

    ```
    # Initiate the Model with Sequential Class
    np.random.state(seed)
    random.set_seed(seed)
    model = Sequential()
    ```

6.  用`Dropout`向网络添加五个`Dense`层。设置第一个隐藏层，使其大小为`64`，丢失率为`0.5`，第二个隐藏层，使其大小为`32`，丢失率为`0.4`，第三个隐藏层，使其大小为`16`，丢失率为`0.3`，第四个隐藏层，使其大小为`8`，丢失率为`0.2`，最后一个隐藏层，使其大小为`4`，丢失率为`0.1`。将所有激活功能设置为`ReLU` :

    ```
    # Add the hidden dense layers and with dropout Layer
    model.add(Dense(units=64, activation='relu', kernel_initializer='uniform', input_dim=X_train.shape[1]))
    model.add(Dropout(rate=0.5))
    model.add(Dense(units=32, activation='relu', kernel_initializer='uniform', input_dim=X_train.shape[1]))
    model.add(Dropout(rate=0.4))
    model.add(Dense(units=16, activation='relu', kernel_initializer='uniform', input_dim=X_train.shape[1]))
    model.add(Dropout(rate=0.3))
    model.add(Dense(units=8, activation='relu', kernel_initializer='uniform', input_dim=X_train.shape[1]))
    model.add(Dropout(rate=0.2))
    model.add(Dense(units=4, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.1))
    ```

7.  Add an output `Dense` layer with a `sigmoid` activation function:

    ```
    # Add Output Dense Layer
    model.add(Dense(units=1, activation='sigmoid', kernel_initializer='uniform'))
    ```

    注意

    因为输出是二进制的，所以我们使用了`sigmoid`函数。如果输出是多类的(即超过两个类)，那么应该使用`softmax`函数。

8.  Compile the network and fit the model. The metric that's being used here is `accuracy`:

    ```
    # Compile the Model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    ```

    注意

    度量名称，在我们的例子中是`accuracy`，在前面的代码中定义。

9.  用`100`个时期、`20`个批量和`0.2` :

    ```
    # Fit the Model
    model.fit(X_train, y_train, epochs=100, batch_size=20, verbose=1, validation_split=0.2, shuffle=False)
    ```

    个验证分割来拟合模型
10.  Evaluate the model on the test dataset and print out the values for the `loss` and `accuracy`:

    ```
    test_loss, test_acc = model.evaluate(X_test, y_test)
    print(f'The loss on the test set is {test_loss:.4f} and the accuracy is {test_acc*100:.4f}%')
    ```

    上述代码产生以下输出:

    ```
    18000/18000 [==============================] - 0s 19us/step
    The loss on the test set is 0.0766 and the accuracy is 98.9833%
    ```

    模型返回的精度为`98.9833%`。但这就足够好了吗？我们只能通过与零精度比较来得到这个问题的答案。

11.  Now, compute the null accuracy. The `null accuracy` can be calculated using the `value_count` function of the `pandas` library, which we used in *Exercise 6.01*, *Calculating Null Accuracy on a Pacific Hurricanes Dataset*, of this chapter:

    ```
    # Use the value_count function to calculate distinct class values
    y_test['class'].value_counts()
    ```

    上述代码产生以下输出:

    ```
    0    17700
    1      300
    Name: class, dtype: int64
    ```

12.  Calculate the `null accuracy`:

    ```
    # Calculate the null accuracy
    y_test['class'].value_counts(normalize=True).loc[0]
    ```

    上述代码产生以下输出:

    ```
    0.9833333333333333
    ```

## 活动 6.02:计算 ROC 曲线和 AUC 分数

`ROC curve`和`AUC score`是一种简单评估二进制分类器性能的有效方法。在本活动中，我们将绘制模型的`ROC curve`并计算其`AUC score`。我们将使用在*练习 6.03* 、*中使用的相同数据集和相同模型，根据混淆矩阵*推导和计算指标。继续相同的 APS 故障数据，绘制`ROC curve`，计算模型的`AUC score`。按照以下步骤完成本活动:

1.  导入必要的库，并使用 pandas `read_csv`函数:

    ```
    # Import the libraries
    import numpy as np
    import pandas as pd
    # Load the Data
    X = pd.read_csv("../data/aps_failure_training_feats.csv")
    y = pd.read_csv("../data/aps_failure_training_target.csv")
    ```

    加载数据
2.  使用`train_test_split`函数将数据分成训练和测试数据集:

    ```
    from sklearn.model_selection import train_test_split
    seed = 42
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)
    ```

3.  使用`StandardScaler` 函数缩放特征数据，使其具有`0`的`mean`和`1`的`standard deviation`。将定标器安装在`training data`中，并将其应用于`test data` :

    ```
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    # Transform the training data
    X_train = sc.fit_transform(X_train)
    X_train = pd.DataFrame(X_train,columns=X_test.columns)
    # Transform the testing data
    X_test = sc.transform(X_test)
    X_test = pd.DataFrame(X_test,columns=X_train.columns)
    ```

4.  导入创建模型所需的 Keras 库。实例化一个`Sequential`类的 Keras 模型，并向模型添加五个隐藏层，包括每个层的 dropout。第一个隐藏层的大小应该为`64`并且辍学率应该为`0.5`。第二个隐藏层的大小应该为`32`，丢失率应该为`0.4`。第三个隐藏层的大小应该是`16`并且丢失率应该是`0.3`。第四个隐藏层的大小应该是`8`并且丢失率应该是`0.2`。最终的隐藏层的大小应该是`4`并且丢失率应该是`0.1`。所有隐藏层都应具有`ReLU activation`功能并设置`kernel_initializer = 'uniform'`。使用 sigmoid 激活函数向模型添加最终输出图层。通过在训练过程中计算精度度量来编译模型:

    ```
    # Import the relevant Keras libraries
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.layers import Dropout
    from tensorflow import random
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential()
    # Add the hidden dense layers with dropout Layer
    model.add(Dense(units=64, activation='relu', kernel_initializer='uniform', input_dim=X_train.shape[1]))
    model.add(Dropout(rate=0.5))
    model.add(Dense(units=32, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.4))
    model.add(Dense(units=16, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.3))
    model.add(Dense(units=8, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.2))
    model.add(Dense(units=4, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.1))
    # Add Output Dense Layer
    model.add(Dense(units=1, activation='sigmoid', kernel_initializer='uniform'))
    # Compile the Model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    ```

5.  通过用`batch_size=20`和`validation_split=0.2` :

    ```
    model.fit(X_train, y_train, epochs=100, batch_size=20, verbose=1, validation_split=0.2, shuffle=False)
    ```

    训练`100`个时期，使模型符合训练数据
6.  一旦模型完成了对训练数据的拟合，使用模型的`predict_proba`方法:

    ```
    y_pred_prob = model.predict_proba(X_test)
    ```

    创建一个变量，该变量是模型对测试数据的预测结果
7.  Import `roc_curve` from scikit-learn and run the following code:

    ```
    from sklearn.metrics import roc_curve
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)
    ```

    `fpr` =假阳性率(1 -特异性)

    `tpr` =真阳性率(灵敏度)

    `thresholds` =阈值`y_pred_prob`

8.  Run the following code to plot the `ROC curve` using `matplotlib.pyplot`:

    ```
    import matplotlib.pyplot as plt
    plt.plot(fpr, tpr)
    plt.title("ROC Curve for APS Failure")
    plt.xlabel("False Positive rate (1-Specificity)")
    plt.ylabel("True Positive rate (Sensitivity)")
    plt.grid(True)
    plt.show()
    ```

    下图显示了上述代码的输出:

    ![Figure 6.14: ROC curve of the APS failure dataset
    ](image/B15777_06_14.jpg)

    图 6.14:APS 故障数据集的 ROC 曲线

9.  Calculate the AUC score using the `roc_auc_score` function:

    ```
    from sklearn.metrics import roc_auc_score
    roc_auc_score(y_test,y_pred_prob)
    ```

    以下是上述代码的输出:

    ```
    0.944787151628455
    ```

    AUC 分数`94.4479%`表明我们的模型是优秀的，正如上面显示的一般可接受的`AUC score`。

# 7。卷积神经网络的计算机视觉

## 活动 7.01:用多层和使用 softmax 修改我们的模型

让我们尝试改进我们的图像分类算法的性能。有许多方法可以提高其性能，最直接的方法之一是在模型中添加多个人工神经网络层，我们将在本活动中了解这一点。我们还会将激活从 sigmoid 更改为 softmax。然后，我们可以将结果与之前的练习进行比较。按照以下步骤完成本活动:

1.  导入`numpy`库和必要的 Keras 库和类:

    ```
    # Import the Libraries 
    from keras.models import Sequential
    from keras.layers import Conv2D, MaxPool2d, Flatten, Dense
    import numpy as np
    from tensorflow import random
    ```

2.  现在，用`Sequential`类:

    ```
    # Initiate the classifier
    seed = 1
    np.random.seed(seed)
    random.set_seed(seed)
    classifier=Sequential()
    ```

    初始化模型
3.  Add the first layer of the CNN, set the input shape to `(64, 64, 3)`, the dimension of each image, and the activation function as a ReLU. Then, add `32` feature detectors of size `(3, 3)`. Add two additional convolutional layers with `32` feature detectors of size `(3, 3)`, also with `ReLU activation` functions:

    ```
    classifier.add(Conv2D(32,3,3,input_shape=(64,64,3),activation='relu'))
    classifier.add(Conv2D(32, (3, 3), activation = 'relu'))
    classifier.add(Conv2D(32, (3, 3), activation = 'relu'))
    ```

    `32, (3, 3)`表示有`32`个尺寸为`3x3`的特征检测器。作为一个好的做法，总是从`32`开始；以后可以加`64`或者`128`。

4.  现在，添加图像大小为`2x2` :

    ```
    classifier.add(MaxPool2D(2,2))
    ```

    的池层
5.  通过向`CNN model` :

    ```
    classifier.add(Flatten())
    ```

    添加展平层来展平池层的输出
6.  添加人工神经网络的第一个密集层。这里，`128`是输出的节点数。作为一个很好的做法，`128`很好上手。`activation`就是`relu`。作为一个好的实践，二的幂是首选:

    ```
    classifier.add(Dense(128,activation='relu')) 
    ```

7.  给同样大小的人工神经网络增加三层`128`，以及`ReLU activation`函数:

    ```
    classifier.add(Dense(128,activation='relu'))
    classifier.add(Dense(128,activation='relu'))
    classifier.add(Dense(128,activation='relu'))
    ```

8.  添加人工神经网络的输出层。将 sigmoid 函数替换为`softmax` :

    ```
    classifier.add(Dense(1,activation='softmax')) 
    ```

9.  用`Adam optimizer`编译网络，并在训练过程中计算精度:

    ```
    # Compile The network
    classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    ```

10.  创建培训和测试数据生成器。通过`1/255`重新缩放训练和测试图像，使所有值都在`0`和`1`之间。仅为训练数据发生器设置这些参数—`shear_range=0.2`、`zoom_range=0.2`和`horizontal_flip=True` :

    ```
    from keras.preprocessing.image import ImageDataGenerator
    train_datagen = ImageDataGenerator(rescale = 1./255,
                                       shear_range = 0.2,
                                       zoom_range = 0.2,
                                       horizontal_flip = True)
    test_datagen = ImageDataGenerator(rescale = 1./255)
    ```

11.  从`training set`文件夹创建一个训练集。`'../dataset/training_set'`是存放我们数据的文件夹。我们的 CNN 模型有一个`64x64`的图像大小，所以这里也应该传递相同的大小。`batch_size`是单批图像的数量，也就是`32`。`class_mode`被设置为`binary`，因为我们正在处理二元分类器:

    ```
    training_set = train_datagen.flow_from_directory('../dataset/training_set',
    target_size = (64, 64),
    batch_size = 32,
    class_mode = 'binary')
    ```

12.  将文件夹设置到测试图像的位置，即`'../dataset/test_set'` :

    ```
    test_set = test_datagen.flow_from_directory('../dataset/test_set',
    target_size = (64, 64),
    batch_size = 32,
    class_mode = 'binary')
    ```

    ，重复*步骤 6* 进行测试
13.  Finally, fit the data. Set the `steps_per_epoch` to `10000` and the `validation_steps` to `2500`. The following step might take some time to execute:

    ```
    classifier.fit_generator(training_set,
    steps_per_epoch = 10000,
    epochs = 2,
    validation_data = test_set,
    validation_steps = 2500,
    shuffle=False)
    ```

    上述代码产生以下输出:

    ```
    Epoch 1/2
    10000/10000 [==============================] - 2452s 245ms/step - loss: 8.1783 - accuracy: 0.4667 - val_loss: 11.4999 - val_accuracy: 0.4695
    Epoch 2/2
    10000/10000 [==============================] - 2496s 250ms/step - loss: 8.1726 - accuracy: 0.4671 - val_loss: 10.5416 - val_accuracy: 0.4691
    ```

    请注意，由于新的 softmax 激活功能，精度已降至`46.91%`。

## 活动 7.02:对新图像进行分类

在本练习中，您将尝试对另一张新图片进行分类，就像我们在之前的练习中所做的那样。图像还没有暴露给算法，所以我们将使用这个活动来测试我们的算法。您可以运行本章中的任何算法(尽管获得最高准确度的算法是首选)，然后使用该模型对您的图像进行分类。按照以下步骤完成本活动:

1.  运行本章中的一个算法。
2.  加载图像并进行处理。`'test_image_2.jpg'`是测试图像的路径。在保存数据集的代码中更改路径:

    ```
    from keras.preprocessing import image
    new_image = image.load_img('../test_image_2.jpg', target_size = (64, 64))
    new_image
    ```

3.  通过使用`img_to_array`函数将图像转换成`numpy`数组来处理图像。然后，使用 numpy 的`expand_dims`函数:

    ```
    new_image = image.img_to_array(new_image)
    new_image = np.expand_dims(new_image, axis = 0)
    ```

    沿第 0 轴添加一个额外的尺寸
4.  通过调用分类器的`predict`方法预测新图像:

    ```
    result = classifier.predict(new_image)
    ```

5.  Use the `class_indices` method with an `if…else` statement to map the 0 or 1 output of the prediction to a class label:

    ```
    if result[0][0] == 1:
        prediction = 'It is a flower'
    else:
        prediction = 'It is a car'
    print(prediction)
    ```

    上述代码产生以下输出:

    ```
    It is a flower
    ```

    `test_image_2`是一朵花的图像，被预言是一朵花。

# 8。迁移学习和预训练模型

## 活动 8.01:使用 VGG16 网络训练深度学习网络识别图像

使用`VGG16`网络预测给定的图像(`test_image_1`)。在开始之前，请确保您已经将图像(`test_image_1`)下载到您的工作目录中。按照以下步骤完成本活动:

1.  导入`numpy`库和必要的`Keras`库:

    ```
    import numpy as np
    from keras.applications.vgg16 import VGG16, preprocess_input
    from keras.preprocessing import image 
    ```

2.  Initiate the model (note that, at this point, you can also view the architecture of the network, as shown in the following code):

    ```
    classifier = VGG16()
    classifier.summary()
    ```

    `classifier.summary()`向我们展示了网络的架构。应该注意以下几点:它具有四维输入形状(`None, 224, 224, 3`)，并且它具有三个卷积层。

    输出的最后四层如下:

    ![Figure 8.16: The architecture of the network
    ](image/B15777_08_16.jpg)

    图 8.16:网络的架构

3.  Load the image. `'../Data/Prediction/test_image_1.jpg'` is the path of the image on our system. It will be different on your system:

    ```
    new_image = image.load_img('../Data/Prediction/test_image_1.jpg', target_size=(224, 224))
    new_image
    ```

    下图显示了上述代码的输出:

    ![Figure 8.17: The sample motorbike image
    ](image/B15777_08_17.jpg)

    图 8.17:示例摩托车图像

    目标尺寸应为`224x 224`，因为`VGG16`只接受(`224,224`)。

4.  Change the image into an array by using the `img_to_array` function:

    ```
    transformed_image = image.img_to_array(new_image)
    transformed_image.shape
    ```

    上述代码提供了以下输出:

    ```
    (224, 224, 3)
    ```

5.  The image should be in a four-dimensional form for `VGG16` to allow further processing. Expand the dimension of the image, as follows:

    ```
    transformed_image = np.expand_dims(transformed_image, axis=0)
    transformed_image.shape
    ```

    上述代码提供了以下输出:

    ```
    (1, 224, 224, 3)
    ```

6.  Preprocess the image:

    ```
    transformed_image = preprocess_input(transformed_image)
    transformed_image
    ```

    下图显示了上述代码的输出:

    ![Figure 8.18: Image preprocessing
    ](image/B15777_08_18.jpg)

    图 8.18:图像预处理

7.  Create the `predictor` variable:

    ```
    y_pred = classifier.predict(transformed_image)
    y_pred
    ```

    下图显示了上述代码的输出:

    ![Figure 8.19: Creating the predictor variable
    ](image/B15777_08_19.jpg)

    图 8.19:创建预测变量

8.  Check the shape of the image. It should be (`1,1000`). It's `1000` because, as we mentioned previously, the ImageNet database has `1000` categories of images. The predictor variable shows the probabilities of our image being one of those images:

    ```
    y_pred.shape
    ```

    上述代码提供了以下输出:

    ```
    (1, 1000)
    ```

9.  Print the top five probabilities of what our image is using the `decode_predictions` function and pass the function of the predictor variable, `y_pred`, and the number of predictions and corresponding labels to output:

    ```
    from keras.applications.vgg16 import decode_predictions
    decode_predictions(y_pred, top=5)
    ```

    上述代码提供了以下输出:

    ```
    [[('n03785016', 'moped', 0.8433369),
      ('n03791053', 'motor_scooter', 0.14188054),
      ('n03127747', 'crash_helmet', 0.007004856),
      ('n03208938', 'disk_brake', 0.0022349996),
      ('n04482393', 'tricycle', 0.0007717237)]]
    ```

    数组的第一列是内部代码。第二个是标签，第三个是图片是标签的概率。

10.  Transform the predictions into a human-readable format. We need to extract the most probable label from the output, as follows:

    ```
    label = decode_predictions(y_pred)
    # Most likely result is retrieved, for example, the highest probability
    decoded_label = label[0][0]
    # The classification is printed
    print('%s (%.2f%%)' % (decoded_label[1], decoded_label[2]*100 ))
    ```

    上述代码提供了以下输出:

    ```
    moped (84.33%)
    ```

    在这里，我们可以看到，我们有一个`84.33%`的概率，图片是一辆轻便摩托车，这是足够接近摩托车，可能代表了这样一个事实，即在 ImageNet 数据集中的摩托车被标记为轻便摩托车。

## 活动 8.02:使用 ResNet 进行图像分类

在本活动中，我们将使用另一个预先训练好的网络，称为`ResNet`。我们有一个位于`../Data/Prediction/test_image_4`的电视图像。我们将使用`ResNet50`网络来预测图像。按照以下步骤完成本活动:

1.  导入`numpy`库和必要的`Keras`库:

    ```
    import numpy as np
    from keras.applications.resnet50 import ResNet50, preprocess_input
    from keras.preprocessing import image 
    ```

2.  Initiate the ResNet50 model and print a summary of the model:

    ```
    classifier = ResNet50()
    classifier.summary()
    ```

    `classifier.summary()`向我们展示了网络的架构。应注意以下几点:

    ![Figure 8.20: The last four layers of the output
    ](image/B15777_08_20.jpg)

    图 8.20:输出的最后四层

    注意

    最后一层预测(`Dense`)具有`1000`值。这意味着`VGG16`总共有`1000`个标签，我们的形象将成为这些`1000`标签中的一个。

3.  Load the image. `'../Data/Prediction/test_image_4.jpg'` is the path of the image on our system. It will be different on your system:

    ```
    new_image = image.load_img('../Data/Prediction/test_image_4.jpg', target_size=(224, 224))
    new_image
    ```

    以下是上述代码的输出:

    ![Figure 8.21: A sample image of a television
    ](image/B15777_08_21.jpg)

    图 8.21:一台电视的样本图像

    目标尺寸应为`224x224`，因为`ResNet50`只接受(`224,224`)。

4.  使用`img_to_array`函数:

    ```
    transformed_image = image.img_to_array(new_image)
    transformed_image.shape
    ```

    将图像转换成数组
5.  图像必须是四维形式，以便`ResNet50`进行进一步处理。使用`expand_dims`功能:

    ```
    transformed_image = np.expand_dims(transformed_image, axis=0)
    transformed_image.shape
    ```

    沿第 0 轴扩展图像尺寸
6.  使用`preprocess_input`功能对图像进行预处理:

    ```
    transformed_image = preprocess_input(transformed_image)
    transformed_image
    ```

7.  使用分类器创建预测变量，使用其`predict`方法:

    ```
    y_pred = classifier.predict(transformed_image)
    y_pred
    ```

    预测图像
8.  Check the shape of the image. It should be (`1,1000`):

    ```
    y_pred.shape
    ```

    上述代码提供了以下输出:

    ```
    (1, 1000)
    ```

9.  Select the top five probabilities of what our image is using the `decode_predictions` function and by passing the predictor variable, `y_pred`, as the argument and the top number of predictions and corresponding labels:

    ```
    from keras.applications.resnet50 import decode_predictions
    decode_predictions(y_pred, top=5)
    ```

    上述代码提供了以下输出:

    ```
    [[('n04404412', 'television', 0.99673873),
      ('n04372370', 'switch', 0.0009829825),
      ('n04152593', 'screen', 0.00095111143),
      ('n03782006', 'monitor', 0.0006477369),
      ('n04069434', 'reflex_camera', 8.5398955e-05)]]
    ```

    数组的第一列是内部代码。第二个是标签，第三个是图片匹配标签的概率。

10.  Put the predictions in a human-readable format. Print the most probable label from the output from the result of the `decode_predictions` function:

    ```
    label = decode_predictions(y_pred)
    # Most likely result is retrieved, for example, the highest probability
    decoded_label = label[0][0]
    # The classification is printed 
    print('%s (%.2f%%)' % (decoded_label[1], decoded_label[2]*100 ))
    ```

    上述代码产生以下输出:

    ```
    television (99.67%)
    ```

# 9。递归神经网络序列建模

## 活动 9.01:使用 50 个单位(神经元)的 LSTM 预测亚马逊股价的趋势

在本活动中，我们将考察亚马逊最近 5 年的股价——从 2014 年 1 月 1 日到 2018 年 12 月 31 日。为此，我们将尝试使用`RNN`和`LSTM`预测公司 2019 年 1 月的未来趋势。我们有 2019 年 1 月的实际值，所以我们可以稍后将我们的预测与实际值进行比较。按照以下步骤完成本活动:

1.  导入所需的库:

    ```
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from tensorflow import random
    ```

2.  Import the dataset using the pandas `read_csv` function and look at the first five rows of the dataset using the `head` method:

    ```
    dataset_training = pd.read_csv('../AMZN_train.csv')
    dataset_training.head()
    ```

    下图显示了上述代码的输出:

    ![Figure 9.24: The first five rows of the dataset
    ](image/B15777_09_24.jpg)

    图 9.24:数据集的前五行

3.  We are going to make our prediction using the `Open` stock price; therefore, select the `Open` stock price column from the dataset and print the values:

    ```
    training_data = dataset_training[['Open']].values 
    training_data
    ```

    上述代码产生以下输出:

    ```
    array([[ 398.799988],
           [ 398.290009],
           [ 395.850006],
           ...,
           [1454.199951],
           [1473.349976],
           [1510.800049]])
    ```

4.  Then, perform feature scaling by normalizing the data using `MinMaxScaler` and setting the range of the features so that they have a minimum value of zero and a maximum value of one. Use the `fit_transform` method of the scaler on the training data:

    ```
    from sklearn.preprocessing import MinMaxScaler
    sc = MinMaxScaler(feature_range = (0, 1))
    training_data_scaled = sc.fit_transform(training_data)
    training_data_scaled
    ```

    上述代码产生以下输出:

    ```
    array([[0.06523313],
           [0.06494233],
           [0.06355099],
           ...,
           [0.66704299],
           [0.67796271],
           [0.69931748]])
    ```

5.  创建数据以从当前实例获取`60`时间戳。我们在这里选择`60`,因为它将为我们提供足够数量的先前实例，以便了解趋势；从技术上讲，这可以是任何数字，但`60`是最佳值。另外，这里的上限值是`1258`，它是训练集中的行(或记录)的索引或计数:

    ```
    X_train = []
    y_train = []
    for i in range(60, 1258):
        X_train.append(training_data_scaled[i-60:i, 0])
        y_train.append(training_data_scaled[i, 0])
    X_train, y_train = np.array(X_train), np.array(y_train)
    ```

6.  使用 NumPy 的`reshape`函数对数据进行整形，在`X_train`的末尾添加一个额外的维度:

    ```
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    ```

7.  导入以下库来构建 RNN:

    ```
    from keras.models import Sequential
    from keras.layers import Dense, LSTM, Dropout
    ```

8.  设置种子并启动顺序模型，如下:

    ```
    seed = 1
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential()
    ```

9.  向具有`50`个单元的网络添加一个`LSTM`层，将`return_sequences`参数设置为`True`，将`input_shape`参数设置为`(X_train.shape[1], 1)`。添加三个额外的`LSTM`层，每个层都有`50`个单元，并将前两个层的`return_sequences`参数设置为`True`。添加大小为 1 的最终输出层:

    ```
    model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))
    # Adding a second LSTM layer
    model.add(LSTM(units = 50, return_sequences = True))
    # Adding a third LSTM layer
    model.add(LSTM(units = 50, return_sequences = True))
    # Adding a fourth LSTM layer
    model.add(LSTM(units = 50))
    # Adding the output layer
    model.add(Dense(units = 1))
    ```

10.  使用`adam`优化器编译网络，并使用`Mean Squared Error`计算损耗。将模型拟合到批量为`32` :

    ```
    # Compiling the RNN
    model.compile(optimizer = 'adam', loss = 'mean_squared_error')
    # Fitting the RNN to the Training set
    model.fit(X_train, y_train, epochs = 100, batch_size = 32)
    ```

    的`100`时期的训练数据
11.  加载并处理测试数据(此处视为实际数据)，选择代表`Open`股票数据值的列:

    ```
    dataset_testing = pd.read_csv('../AMZN_test.csv')
    actual_stock_price = dataset_testing[['Open']].values
    actual_stock_price
    ```

12.  连接数据，因为我们将需要`60`以前的实例来获得每天的股票价格。因此，我们将需要训练和测试数据:

    ```
    total_data = pd.concat((dataset_training['Open'], dataset_testing['Open']), axis = 0)
    ```

13.  重塑和缩放输入以准备测试数据。请注意，我们预测的是一月份的月度趋势，它有`21`个金融日，因此为了准备测试集，我们将下限值作为`60`，上限值作为`81`。这确保了`21`的差异被保持:

    ```
    inputs = total_data[len(total_data) - len(dataset_testing) - 60:].values
    inputs = inputs.reshape(-1,1)
    inputs = sc.transform(inputs)
    X_test = []
    for i in range(60, 81):
        X_test.append(inputs[i-60:i, 0])
    X_test = np.array(X_test)
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
    predicted_stock_price = model.predict(X_test)
    predicted_stock_price = sc.inverse_transform(predicted_stock_price)
    ```

14.  Visualize the results by plotting the actual stock price and plotting the predicted stock price:

    ```
    # Visualizing the results
    plt.plot(actual_stock_price, color = 'green', label = 'Real Amazon Stock Price',ls='--')
    plt.plot(predicted_stock_price, color = 'red', label = 'Predicted Amazon Stock Price',ls='-')
    plt.title('Predicted Stock Price')
    plt.xlabel('Time in days')
    plt.ylabel('Real Stock Price')
    plt.legend()
    plt.show()
    ```

    请注意，您的结果可能与亚马逊的实际股价略有不同。

    **预期产量**:

![Figure 9.25: Real versus predicted stock prices
](image/B15777_09_25.jpg)

图 9.25:真实与预测的股票价格

如上图所示，预测价格和实际价格的趋势基本相同；这条线有相同的波峰和波谷。这是可能的，因为 LSTM 的能力，记住测序数据。传统的前馈神经网络不能预测这个结果。这才是`LSTM`和`RNNs`的真正威力。

## 活动 9.02:预测亚马逊的股票价格

在本活动中，我们将考察从 2014 年 1 月 1 日到 2018 年 12 月 31 日的过去 5 年中亚马逊的股价。为此，我们将尝试使用 RNNs 和 LSTM 预测公司 2019 年 1 月的未来趋势。我们有 2019 年 1 月的实际值，因此我们将能够在稍后将我们的预测与实际值进行比较。最初，我们使用 50 个单位(或神经元)的 LSTM 来预测亚马逊股价的趋势。在本活动中，我们还将添加 dropout 正则化，并将结果与*活动 9.01* 、*使用 50 个单位(神经元)*的 LSTM 预测亚马逊股价的走势进行比较。按照以下步骤完成本活动:

1.  导入所需的库:

    ```
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from tensorflow import random
    ```

2.  使用 pandas `read_csv`函数导入数据集，并使用`head`方法查看数据集的前五行:

    ```
    dataset_training = pd.read_csv('../AMZN_train.csv')
    dataset_training.head()
    ```

3.  We are going to make our prediction using the `Open` stock price; therefore, select the `Open` stock price column from the dataset and print the values:

    ```
    training_data = dataset_training[['Open']].values
    training_data
    ```

    上述代码产生以下输出:

    ```
    array([[ 398.799988],
           [ 398.290009],
           [ 395.850006],
           ...,
           [1454.199951],
           [1473.349976],
           [1510.800049]])
    ```

4.  Then, perform feature scaling by normalizing the data using `MinMaxScaler` and setting the range of the features so that they have a minimum value of `0` and a maximum value of one. Use the `fit_transform` method of the scaler on the training data:

    ```
    from sklearn.preprocessing import MinMaxScaler
    sc = MinMaxScaler(feature_range = (0, 1))
    training_data_scaled = sc.fit_transform(training_data)
    training_data_scaled
    ```

    上述代码产生以下输出:

    ```
    array([[0.06523313],
           [0.06494233],
           [0.06355099],
           ...,
           [0.66704299],
           [0.67796271],
           [0.69931748]])
    ```

5.  创建数据以从当前实例获取`60`时间戳。我们在这里选择`60`,因为它将为我们提供足够数量的先前实例，以便了解趋势；从技术上讲，这可以是任何数字，但`60`是最佳值。另外，这里的上限值是`1258`，它是训练集中的行(或记录)的索引或计数:

    ```
    X_train = []
    y_train = []
    for i in range(60, 1258):
        X_train.append(training_data_scaled[i-60:i, 0])
        y_train.append(training_data_scaled[i, 0])
    X_train, y_train = np.array(X_train), np.array(y_train)
    ```

6.  使用 NumPy 的`reshape`函数对数据进行整形，在`X_train`的末尾添加一个额外的维度:

    ```
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    ```

7.  导入以下 Keras 库来构建 RNN:

    ```
    from keras.models import Sequential
    from keras.layers import Dense, LSTM, Dropout
    ```

8.  设置种子并启动顺序模型，如下:

    ```
    seed = 1
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential()
    ```

9.  在有 50 个单元的网络中添加一个 LSTM 层，将`return_sequences`参数设置为`True`，将`input_shape`参数设置为`(X_train.shape[1], 1)`。用`rate=0.2`给模型添加辍学。添加三个额外的 LSTM 图层，每个图层都有`50`个单位，并将前两个图层的`return_sequences`参数设置为`True`。在每个`LSTM`层之后，用`rate=0.2`添加一个 dropout。添加大小为`1` :

    ```
    model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))
    model.add(Dropout(0.2))
    # Adding a second LSTM layer and some Dropout regularization
    model.add(LSTM(units = 50, return_sequences = True))
    model.add(Dropout(0.2))
    # Adding a third LSTM layer and some Dropout regularization
    model.add(LSTM(units = 50, return_sequences = True))
    model.add(Dropout(0.2))
    # Adding a fourth LSTM layer and some Dropout regularization
    model.add(LSTM(units = 50))
    model.add(Dropout(0.2))
    # Adding the output layer
    model.add(Dense(units = 1))
    ```

    的最终输出层
10.  使用`adam`优化器编译网络，并使用`Mean Squared Error`计算损耗。将模型拟合到批量为`32` :

    ```
    # Compiling the RNN
    model.compile(optimizer = 'adam', loss = 'mean_squared_error')
    # Fitting the RNN to the Training set
    model.fit(X_train, y_train, epochs = 100, batch_size = 32)
    ```

    的`100`时期的训练数据
11.  加载并处理测试数据(此处视为实际数据)，选择代表`Open`股票数据值的列:

    ```
    dataset_testing = pd.read_csv('../AMZN_test.csv')
    actual_stock_price = dataset_testing[['Open']].values
    actual_stock_price 
    ```

12.  连接数据，因为我们将需要`60`以前的实例来获得每天的股票价格。因此，我们将需要训练和测试数据:

    ```
    total_data = pd.concat((dataset_training['Open'], dataset_testing['Open']), axis = 0)
    ```

13.  重塑和缩放输入以准备测试数据。请注意，我们预测的是一月份的月度趋势，它有`21`个金融日，因此为了准备测试集，我们将下限值作为`60`，上限值作为`81`。这确保了`21`的差异被保持:

    ```
    inputs = total_data[len(total_data) - len(dataset_testing) - 60:].values
    inputs = inputs.reshape(-1,1)
    inputs = sc.transform(inputs)
    X_test = []
    for i in range(60, 81):
        X_test.append(inputs[i-60:i, 0])
    X_test = np.array(X_test)
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
    predicted_stock_price = model.predict(X_test)
    predicted_stock_price = sc.inverse_transform(predicted_stock_price)
    ```

14.  通过绘制实际股票价格和预测股票价格来可视化结果:

    ```
    # Visualizing the results
    plt.plot(actual_stock_price, color = 'green', label = 'Real Amazon Stock Price',ls='--')
    plt.plot(predicted_stock_price, color = 'red', label = 'Predicted Amazon Stock Price',ls='-')
    plt.title('Predicted Stock Price')
    plt.xlabel('Time in days')
    plt.ylabel('Real Stock Price')
    plt.legend()
    plt.show()
    ```

请注意，您的结果可能与实际股价略有不同。

**预期产量**:

![Figure 9.26: Real versus predicted stock prices
](image/B15777_09_26.jpg)

图 9.26:真实与预测的股票价格

在下图中，第一个图显示了活动 9.02 中正则化模型的预测输出，第二个图显示了活动 9.01 中未正则化的预测输出。如您所见，添加剔除正则化并不能精确拟合数据。因此，在这种情况下，最好不要使用正则化，或者使用辍学率较低的正则化:

![Figure 9.27: Comparing the results of Activity 9.01 and Activity 9.02
](image/B15777_09_27.jpg)

图 9.27:比较“活动 9.01”和“活动 9.02”的结果

## 活动 9.03:使用 LSTM 神经元数量不断增加的 LSTM(100 个单位)预测亚马逊股票价格的趋势

在本活动中，我们将考察从 2014 年 1 月 1 日到 2018 年 12 月 31 日的过去 5 年中亚马逊的股价。我们将尝试使用具有四个`LSTM`层的`RNNs`预测公司 2019 年 1 月的未来趋势，每个层具有`100`单位。我们有 2019 年 1 月的实际值，因此我们将能够在稍后将我们的预测与实际值进行比较。还可以将输出差异与*活动 9.01* 、*用 50 个单位(神经元)的 LSTM*预测亚马逊股价走势进行比较。按照以下步骤完成本活动:

1.  导入所需的库:

    ```
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from tensorflow import random
    ```

2.  使用 pandas `read_csv`函数导入数据集，并使用`head`方法查看数据集的前五行:

    ```
    dataset_training = pd.read_csv('../AMZN_train.csv')
    dataset_training.head()
    ```

3.  我们将使用`Open`股票价格进行预测；因此，从数据集中选择`Open`股票价格列，并打印值:

    ```
    training_data = dataset_training[['Open']].values
    training_data
    ```

4.  然后，通过使用`MinMaxScaler`标准化数据并设置特征范围来执行特征缩放，使其最小值为 0，最大值为 1。对训练数据使用定标器的`fit_transform`方法:

    ```
    from sklearn.preprocessing import MinMaxScaler
    sc = MinMaxScaler(feature_range = (0, 1))
    training_data_scaled = sc.fit_transform(training_data)
    training_data_scaled
    ```

5.  创建数据以从当前实例获取`60`时间戳。我们在这里选择`60`,因为它将为我们提供足够数量的先前实例，以便了解趋势；从技术上讲，这可以是任何数字，但`60`是最佳值。另外，这里的上限值是`1258`，它是训练集中的行(或记录)的索引或计数:

    ```
    X_train = []
    y_train = []
    for i in range(60, 1258):
        X_train.append(training_data_scaled[i-60:i, 0])
        y_train.append(training_data_scaled[i, 0])
    X_train, y_train = np.array(X_train), np.array(y_train)
    ```

6.  使用 NumPy 的`reshape`函数对数据进行整形，在`X_train`的末尾添加一个额外的维度:

    ```
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    ```

7.  导入以下 Keras 库来构建 RNN:

    ```
    from keras.models import Sequential
    from keras.layers import Dense, LSTM, Dropout
    ```

8.  设置种子并启动顺序模型:

    ```
    seed = 1
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential()
    ```

9.  向具有`100`个单位的网络添加一个 LSTM 图层，将`return_sequences`参数设置为`True`，将`input_shape`参数设置为`(X_train.shape[1], 1)`。添加三个额外的`LSTM`层，每个层都有`100`个单元，并将前两个层的`return_sequences`参数设置为`True`。添加大小为`1` :

    ```
    model.add(LSTM(units = 100, return_sequences = True, input_shape = (X_train.shape[1], 1)))
    # Adding a second LSTM layer
    model.add(LSTM(units = 100, return_sequences = True))
    # Adding a third LSTM layer
    model.add(LSTM(units = 100, return_sequences = True))
    # Adding a fourth LSTM layer
    model.add(LSTM(units = 100))
    # Adding the output layer
    model.add(Dense(units = 1))
    ```

    的最终输出层
10.  使用`adam`优化器编译网络，并使用`Mean Squared Error`计算损耗。将模型拟合到批量为`32` :

    ```
    # Compiling the RNN
    model.compile(optimizer = 'adam', loss = 'mean_squared_error')
    # Fitting the RNN to the Training set
    model.fit(X_train, y_train, epochs = 100, batch_size = 32)
    ```

    的`100`时期的训练数据
11.  加载并处理测试数据(此处视为实际数据)，并选择代表开仓股票数据值的列:

    ```
    dataset_testing = pd.read_csv('../AMZN_test.csv')
    actual_stock_price = dataset_testing[['Open']].values
    actual_stock_price
    ```

12.  连接数据，因为我们将需要`60`以前的实例来获得每天的股票价格。因此，我们将需要训练和测试数据:

    ```
    total_data = pd.concat((dataset_training['Open'], dataset_testing['Open']), axis = 0)
    ```

13.  重塑和缩放输入以准备测试数据。请注意，我们预测的是一月份的月度趋势，它有`21`个金融日，因此为了准备测试集，我们将下限值作为`60`，上限值作为`81`。这确保了`21`的差异被保持:

    ```
    inputs = total_data[len(total_data) - len(dataset_testing) - 60:].values
    inputs = inputs.reshape(-1,1)
    inputs = sc.transform(inputs)
    X_test = []
    for i in range(60, 81):
        X_test.append(inputs[i-60:i, 0])
    X_test = np.array(X_test)
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
    predicted_stock_price = model.predict(X_test)
    predicted_stock_price = sc.inverse_transform(predicted_stock_price)
    ```

14.  Visualize the results by plotting the actual stock price and plotting the predicted stock price:

    ```
    plt.plot(actual_stock_price, color = 'green', label = 'Actual Amazon Stock Price',ls='--')
    plt.plot(predicted_stock_price, color = 'red', label = 'Predicted Amazon Stock Price',ls='-')
    plt.title('Predicted Stock Price')
    plt.xlabel('Time in days')
    plt.ylabel('Real Stock Price')
    plt.legend()
    plt.show()
    ```

    请注意，您的结果可能与实际股价略有不同。

    **预期产量**:

![Figure 9.28: Real versus predicted stock prices
](image/B15777_09_28.jpg)

图 9.28:真实与预测的股票价格

因此，如果我们比较`LSTM`与`50`单位的结果(来自*活动 9.01* 、*使用 50 单位(神经元)的 LSTM 预测亚马逊股票价格的趋势*)以及本活动中`LSTM`与`100`单位的结果，我们会得到`100`单位的趋势。另外，请注意，当我们用`100`单元运行`LSTM`时，它比用`50`单元运行`LSTM`需要更多的计算时间。在这种情况下，需要考虑权衡:

![Figure 9.29: Comparing the real versus predicted stock price with 50 and 100 units](image/B15777_09_29.jpg)

图 9.29:50 和 100 单位的实际和预测股票价格的比较