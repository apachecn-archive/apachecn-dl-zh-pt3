

# 九、循环神经网络序列建模

概观

本章将向您介绍序列建模，即创建模型来预测序列中的下一个值或一系列值。本章结束时，你将能够建立序列模型，解释循环神经网络，描述消失梯度问题，并实现长期短期记忆(LSTM)架构。您将应用`RNNs`和`LSTM`架构来预测 Alphabet 和 Amazon 的未来股价值。

# 简介

在前一章中，我们学习了预训练网络，以及如何通过迁移学习将它们用于我们自己的应用。我们用`VGG16`和`ResNet50`进行了实验，这两个预先训练好的网络用于图像分类，并使用它们对新图像进行分类，并针对我们自己的应用进行微调。通过利用预先训练的网络，我们能够比我们在前面章节中训练的卷积神经网络更快地训练更精确的模型。

在传统的神经网络(以及前面章节中介绍的每种神经网络架构)中，数据从输入层开始依次通过网络，并通过隐藏层(如果有)到达输出层。信息通过网络一次，输出被认为是相互独立的，只取决于模型的输入。然而，在某些情况下，特定的输出依赖于系统先前的输出。

以一家公司的股票价格为例:任何一天结束时的产量都与前一天的产量相关。类似地，在自然语言处理(NLP)中，如果句子要有语法意义，则句子中的最终单词高度依赖于句子中的先前单词。`NLP`是顺序建模的一个具体应用，其中正在处理和分析的数据集是自然语言数据。一种特殊类型的神经网络，称为`Recurrent Neural Network` ( `RNN`，用于解决这类网络需要记住以前输出的问题。

本章介绍并探讨了`RNNs`的概念和应用。它还解释了`RNNs`如何不同于标准的前馈神经网络。你也将了解消失梯度问题和`Long-Short-Term-Memory` ( `LSTM`)网络。本章还向您介绍了顺序数据及其处理方式。我们将使用股票市场数据进行股票价格预测，以了解所有这些概念。

# 顺序记忆和顺序建模

如果我们分析**字母表**过去 6 个月的股价，如下图截图所示，可以看出有一个趋势。为了预测未来的股票价格，我们需要了解这种趋势，然后在记住这种趋势的同时进行数学计算:

![Figure 9.1: Alphabet's stock price over the last 6 months
](image/B15777_09_01.jpg)

图 9.1: Alphabet 过去 6 个月的股价

这种趋势与顺序记忆和顺序建模密切相关。如果你有一个模型可以记住以前的输出，然后根据以前的输出预测下一个输出，我们说这个模型有顺序记忆。

为处理这种顺序存储器而进行的建模被称为顺序建模。这不仅适用于股市数据，在 NLP 应用中也是如此；我们将在下一节研究 RNNs 时看一个这样的例子。

# 循环神经网络

是一类基于顺序记忆概念的神经网络。与传统的神经网络不同，`RNN`可以预测序列数据中的结果。目前，`RNN`是可用于处理顺序数据的最健壮的技术。

如果你有一部装有谷歌助手的智能手机，试着打开它，问一个问题:“联合国是什么时候成立的？”答案显示在下面的截图中:

![Figure 9.2: Google Assistant's output
](image/B15777_09_02.jpg)

图 9.2:谷歌助手的输出

现在，问第二个问题，“为什么会形成？”，如下所示:

![Figure 9.3: Google Assistant's contextual output
](image/B15777_09_03.jpg)

图 9.3:谷歌助手的上下文输出

现在，问第三个问题，“它的总部在哪里？”，您应该会得到以下答案:

![Figure 9.4: Google Assistant's output
](image/B15777_09_04.jpg)

图 9.4:谷歌助手的输出

这里要注意的一个有趣的事情是，我们在第一个问题中只提到了“联合国”。第二个和第三个问题，我们简单的分别问了助理`why it was formed`和`where the headquarters were`。谷歌助手明白，由于前一个问题是关于联合国的，所以接下来的问题也是在联合国的背景下。这对于一台机器来说，并不是一件简单的事情。

这台机器能够显示出预期的结果，因为它已经以序列的形式处理了数据。机器理解当前的问题与前一个问题相关，因此，本质上，它记住了前一个问题。

让我们考虑另一个简单的例子。假设我们要按以下顺序预测下一个数字:`7`、`8`、`9`和`?`。我们希望下一个输出是`9` + `1`。或者，如果我们提供序列、`3`、`6`、`9`和`?`，我们希望得到`9` + `3`作为输出。虽然在这两种情况下，最后一个数字是`9`，但是预测结果应该是不同的(即，当我们考虑先前值的上下文信息而不仅仅是最后一个值时)。这里的关键是记住从以前的值中获得的上下文信息。

在高层次上，这种能够记住先前状态的网络被称为循环网络。为了完全理解`RNNs`，让我们重新回顾一下传统的神经网络，也称为`feedforward neural networks`。这是一个神经网络，其中神经网络的连接不形成循环；也就是说，数据只向一个方向流动，如下图所示:

![Figure 9.5: A feedforward neural network
](image/B15777_09_05.jpg)

图 9.5:一个前馈神经网络

在`feedforward neural network`中，例如上图所示，输入层(左边的绿色圆圈)获取数据并将其传递给隐藏层(中间的蓝色圆圈表示权重)。稍后，隐藏图层中的数据被传递到输出图层(由右侧的红色圆圈表示)。基于阈值，数据被反向传播，但是在隐藏层中没有数据的循环流动。

在`RNN`中，网络的隐藏层允许数据和信息的循环。如下图所示，架构类似前馈神经网络；然而，在这里，数据和信息也是循环流动的:

![Figure 9.6: An RNN
](image/B15777_09_06.jpg)

图 9.6:RNN

这里，`RNN`的定义属性是，隐藏层不仅给出输出，而且还将输出的信息反馈给它自己。在深入研究 RNNs 之前，让我们讨论一下为什么我们需要`RNNs`以及为什么`Convolutional Neural Networks` ( `CNNs`)或普通的`Artificial Neural Networks` ( `ANNs`)在处理顺序数据时会有所欠缺。假设我们正在使用一个`CNN`来识别图像；首先，我们输入一张狗的图片，`CNN`会将图片标注为“狗”。然后，我们输入一个芒果的图像，CNN 会给图像贴上“芒果”的标签。让我们输入狗在时间`t`的图像，如下所示:

![Figure 9.7: An image of a dog with a CNN
](image/B15777_09_07.jpg)

图 9.7:一只带有 CNN 的狗的图像

现在，让我们输入时间`t + 1`的芒果图像，如下所示:

![Figure 9.8: An image of a mango with a CNN
](image/B15777_09_08.jpg)

图 9.8:一个带有 CNN 的芒果图片

这里，你可以清楚地看到狗图像在时间`t`的输出和芒果图像在时间`t + 1`的输出是完全相互独立的。因此，我们不需要我们的算法来记住先前的输出实例。然而，正如我们在 Google Assistant 例子中提到的，我们询问了`when the United Nations was formed`和`why it was formed`，算法必须记住前一个实例的输出才能处理顺序数据。`CNNs`或`ANNs`不能做到这一点，所以我们需要用`RNNs`来代替。

在`RNN`中，我们可以在多个时间实例上有多个输出。下图是一辆`RNN`的图示。它表示从时间`t – 1`到时间`t + n`的网络状态:

![Figure 9.9: An unfolded RNN at various timestamps
](image/B15777_09_09.jpg)

图 9.9:不同时间戳下展开的 RNN

在培训`RNNs`时，您可能会遇到一些与`RNNs`的独特架构相关的问题。它们关系到渐变的值，因为随着`RNN`深度的增加，渐变要么消失，要么爆炸，我们将在下一节学习。

## 消失梯度问题

如果有人问你“你昨晚吃了什么？”，记住并正确回答它们是相当容易的。现在，如果有人问你“在过去的 30 天里，你晚餐吃了什么？”，那么你也许能记住过去 3、4 天的菜单，但是之前几天的菜单就有点难记了。这种从过去回忆信息的能力是消失梯度问题的基础，我们将在这一节研究这个问题。简而言之，消失梯度问题指的是在一段时间内丢失或衰减的信息。

下图显示了`RNN`在不同时刻`t`的状态。顶部的点(红色)表示输出图层，中间的点(蓝色)表示隐藏图层，底部的点(绿色)表示输入图层:

![Figure: 9.10: Information decaying over time
](image/B15777_09_10.jpg)

图 9.10:信息随时间衰减

如果你在`t + 10`，你将很难记起在`t`(当天的前 10 天)你吃了什么晚餐菜单。此外，如果你在`t + 100`，假设你选择的晚餐没有模式，你很可能记不住 100 天前的晚餐菜单。在机器学习的上下文中，消失梯度问题是在使用基于梯度的学习方法和反向传播来训练人工神经网络时发现的一个困难。让我们回忆一下神经网络是如何工作的，如下所示:

1.  首先，我们用随机权重和偏差值初始化网络。
2.  我们得到一个预测的输出。将这个输出与实际输出进行比较，其差值称为成本。
3.  训练过程利用梯度，该梯度测量成本相对于权重或偏差的变化率。
4.  然后，我们试图通过在整个训练过程中反复调整权重和偏差来降低成本，直到获得尽可能低的值。

例如，如果你把一个球放在陡峭的地板上，那么这个球就会快速滚动；然而，如果你把球放在平面上，它会慢慢滚动，或者根本不滚动。同样，在深度神经网络中，当梯度较大时，模型学习很快。但是，如果梯度很小，那么模型的学习率就变得很低。记住，在任何一点，梯度都是到该点的所有梯度的乘积(也就是说，它遵循微积分链规则)。

此外，梯度通常是在`0`和`1`之间的一个小数字，并且在`0`和`1`之间的两个数字的乘积给你一个更小的数字。网络越深，网络初始层的梯度就越小。在某些情况下，它到达一个非常小的点，以至于在那个网络中没有训练发生；这就是消失梯度问题。下图显示了遵循微积分链规则的梯度:

![Figure 9.11: The vanishing gradient with cost, C, and the calculus chain rule
](image/B15777_09_11.jpg)

图 9.11:成本为 C 的消失梯度和微积分链规则

参考*图 9.10* ，假设我们在`t + 10`实例，我们得到一个将被反向传播到`t`的输出，这是 10 步之外。现在权重更新的时候会有 10 个梯度(本身就很小)，互相相乘，数字变得小到几乎可以忽略不计。这就是所谓的消失梯度。

## 爆炸梯度问题的简要说明

如果不是权重小，而是权重大于`1`，那么随后的乘法将指数地增加梯度；这就是所谓的爆炸梯度。爆炸渐变与消失渐变正好相反，因为在消失渐变的情况下，值变得太小，而在爆炸渐变的情况下，值变得非常大。结果，网络遭受重创，无法预测任何事情。我们没有像消失渐变那样频繁地遇到爆炸渐变问题，但是简单地了解一下什么是爆炸渐变是有好处的。

我们采用了一些方法来克服梯度消失或爆炸问题所带来的挑战。我们将学习的一种方法是`Long Short-Term Memory`，它通过长时间记忆信息来克服梯度问题。

# 长短期记忆(LSTM)

`LSTMs`是`RNNs`，其主要目标是克服消失梯度和爆炸梯度问题的缺点。该体系结构的构建使他们能够长时间记住数据和信息。

`LSTMs`旨在克服消失和爆炸梯度问题的限制。`LSTM`网络是一种特殊的`RNN`，能够学习长期依赖关系。它们旨在避免长期依赖问题；能够长时间记忆信息是它们的联系方式。下图显示了一个标准的循环网络，其中重复模块具有一个`tanh activation`功能。这是一个简单的`RNN`。在这种架构中，我们经常要面对渐变消失的问题:

![Figure 9.12: A simple RNN model
](image/B15777_09_12.jpg)

图 9.12:一个简单的 RNN 模型

`LSTM`架构类似于简单的`RNNs`，但是它们的重复模块有不同的组件，如下图所示:

![Figure 9.13: The LSTM model architecture
](image/B15777_09_13.jpg)

图 9.13:LSTM 模型架构

除了简单的`RNN`之外，`LSTM`还包括以下内容:

*   `Sigmoid activation`函数(σ)
*   数学计算函数(带+和 x 的黑色圆圈)
*   门控细胞(或门):

![Figure 9.14: An LSTM in detail
](image/B15777_09_14.jpg)

图 9.14:详细的 LSTM

简单的`RNN`和`LSTM`的主要区别在于门控细胞的存在。你可以把门想象成计算机的存储器，在那里可以写入、读取或存储信息。上图显示了一辆`LSTM`的详细图像。门中的单元(由黑色圆圈表示)决定存储什么以及何时允许读取或写入值。大门接受从`0`到`1`的任何信息；即如果为 0，则信息被屏蔽；如果是`1`，那么所有信息都流过。如果输入在`0`和`1`之间，那么只有部分信息流动。

除了这些输入门，网络的梯度取决于两个因素:权重和激活函数。这些门决定哪些信息需要保存在`LSTM`单元格中，哪些需要被遗忘或删除。这样，闸门就像水阀一样；也就是说，网络可以选择哪个阀门允许水流动，哪个阀门不允许水流动。

阀门的调节方式使得输出值永远不会产生梯度(消失或爆炸)问题。例如，如果该值变得太大，那么有一个遗忘门，它将遗忘该值，并且不再考虑将其用于计算。本质上，遗忘门所做的是将信息乘以`0`或`1`。如果信息需要进一步处理，那么遗忘门将信息乘以`1`，如果需要遗忘，那么它将信息乘以`0`。每个门都由一个 sigmoid 函数辅助，该函数压缩`0`和`1`之间的信息。为了更好地理解这一点，我们来看一些活动和练习。

注意

本章中的所有活动和练习将在 Jupyter 笔记本中展开。你可以在[https://packt.live/2vtdA8o](https://packt.live/2vtdA8o)下载这本书的 GitHub 库，以及所有准备好的模板。

## 练习 9.01:使用具有 50 个单位(神经元)的 LSTM 预测 Alphabet 的股价趋势

在本练习中，我们将考察 Alphabet 在 5 年内的股价，即从 2014 年 1 月 1 日到 2018 年 12 月 31 日。这样，我们将尝试使用`RNNs`来预测和预报公司 2019 年 1 月的未来趋势。我们有 2019 年 1 月的实际值，因此我们将能够在稍后将我们的预测与实际值进行比较。按照以下步骤完成本练习:

1.  导入所需的库:

    ```
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from tensorflow import random
    ```

2.  Import the dataset using the pandas `read_csv` function and look at the first five rows of the dataset using the `head` method:

    ```
    dataset_training = pd.read_csv('../GOOG_train.csv')
    dataset_training.head()
    ```

    下图显示了上述代码的输出:

    ![Figure 9.15: The first five rows of the GOOG_Training dataset
    ](image/B15777_09_15.jpg)

    图 9.15:GOOG _ Training 数据集的前五行

3.  We are going to make our prediction using the `Open` stock price; therefore, select the `Open` stock price column from the dataset and print the values:

    ```
    training_data = dataset_training[['Open']].values
    training_data
    ```

    上述代码产生以下输出:

    ```
    array([[ 555.647278],
           [ 555.418152],
           [ 554.42688 ],
           ...,
           [1017.150024],
           [1049.619995],
           [1050.959961]])
    ```

4.  Then, perform feature scaling by normalizing the data using `MinMaxScaler` and setting the range of the features so that they have a minimum value of `0` and a maximum value of one. Use the `fit_transform` method of the scaler on the training data:

    ```
    from sklearn.preprocessing import MinMaxScaler
    sc = MinMaxScaler(feature_range = (0, 1))
    training_data_scaled = sc.fit_transform(training_data)
    training_data_scaled
    ```

    上述代码产生以下输出:

    ```
    array([[0.08017394],
          [0.07987932],
          [0.07860471],
          ...,
          [0.67359064],
          [0.71534169],
          [0.71706467]])
    ```

5.  创建数据以从当前实例获取 60 个时间戳。我们在这里选择`60`,因为这将为我们提供足够数量的先前实例，以便我们可以了解趋势；从技术上讲，这可以是任何数字，但`60`是最佳值。另外，这里的上限值是`1258`，它是训练集中的行(或记录)的索引或计数:

    ```
    X_train = []
    y_train = []
    for i in range(60, 1258):
        X_train.append(training_data_scaled[i-60:i, 0])
        y_train.append(training_data_scaled[i, 0])
    X_train, y_train = np.array(X_train), np.array(y_train)
    ```

6.  Next, reshape the data to add an extra dimension to the end of `X_train` using NumPy's `reshape` function:

    ```
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    X_train
    ```

    下图显示了上述代码的输出:

    ![Figure 9.16: The data of a few timestamps from the current instance
    ](image/B15777_09_16.jpg)

    图 9.16:当前实例中几个时间戳的数据

7.  导入以下 Keras 库来构建`RNN` :

    ```
    from keras.models import Sequential
    from keras.layers import Dense, LSTM, Dropout
    ```

8.  设置种子并启动顺序模型，如下:

    ```
    seed = 1
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential()
    ```

9.  在有 50 个单元的网络中添加一个 LSTM 层，将`return_sequences`参数设置为`True`，将`input_shape`参数设置为`(X_train.shape[1], 1)`。添加三个额外的 LSTM 图层，每个图层有 50 个单位，并将前两个图层的`return_sequences`参数设置为`True`，如下:

    ```
    model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))
    # Adding a second LSTM layer
    model.add(LSTM(units = 50, return_sequences = True))
    # Adding a third LSTM layer
    model.add(LSTM(units = 50, return_sequences = True))
    # Adding a fourth LSTM layer
    model.add(LSTM(units = 50))
    # Adding the output layer
    model.add(Dense(units = 1))
    ```

10.  使用`adam`优化器编译网络，并使用`Mean Squared Error`计算损耗。将模型拟合到批量为`32` :

    ```
    # Compiling the RNN
    model.compile(optimizer = 'adam', loss = 'mean_squared_error')
    # Fitting the RNN to the Training set
    model.fit(X_train, y_train, epochs = 100, batch_size = 32)
    ```

    的`100`时期的训练数据
11.  Load and process the `test` data (which is treated as actual data here) and select the column representing the value of `Open` stock data:

    ```
    dataset_testing = pd.read_csv("../GOOG_test.csv")
    actual_stock_price = dataset_testing[['Open']].values
    actual_stock_price
    ```

    下图显示了上述代码的输出:

    ![Figure 9.17: The actual processed data
    ](image/B15777_09_17.jpg)

    图 9.17:实际处理的数据

12.  连接数据；我们将需要`60`以前的实例，以便获得每天的股票价格。因此，我们将需要训练和测试数据:

    ```
    total_data = pd.concat((dataset_training['Open'], dataset_testing['Open']), axis = 0)
    ```

13.  重塑和缩放输入以准备测试数据。请注意，我们预测的是一月份的月度趋势，它有`21`个金融日，因此为了准备测试集，我们将下限值设为 60，上限值设为 81。这确保了`21`的差异被保持:

    ```
    inputs = total_data[len(total_data) - len(dataset_testing) - 60:].values
    inputs = inputs.reshape(-1,1)
    inputs = sc.transform(inputs)
    X_test = []
    for i in range(60, 81):
        X_test.append(inputs[i-60:i, 0])
    X_test = np.array(X_test)
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
    predicted_stock_price = model.predict(X_test)
    predicted_stock_price = sc.inverse_transform(predicted_stock_price)
    ```

14.  Visualize the results by plotting the actual stock price and then plotting the predicted stock price:

    ```
    # Visualizing the results
    plt.plot(actual_stock_price, color = 'green', label = 'Real Alphabet Stock Price',ls='--')
    plt.plot(predicted_stock_price, color = 'red', label = 'Predicted Alphabet Stock Price',ls='-')
    plt.title('Predicted Stock Price')
    plt.xlabel('Time in days')
    plt.ylabel('Real Stock Price')
    plt.legend()
    plt.show()
    ```

    请注意，您的结果可能与 Alphabet 的实际股价略有不同。

    **预期产量**:

![Figure 9.18: The real versus predicted stock price
](image/B15777_09_18.jpg)

图 9.18:真实与预测的股票价格

这就结束了*练习 9.01* 、*使用具有 50 个单位(神经元)的 LSTM*预测 Alphabet 的股票价格趋势，其中我们在`LSTM`的帮助下预测了 Alphabet 的股票趋势。如前图所示，趋势已经被很好地捕捉到了。在下一个活动中，我们将通过预测亚马逊股票价格在过去 5 年中的趋势来测试我们的知识，并练习用`LSTM`层构建`RNNs`。

## 活动 9.01:使用 50 个单位(神经元)的 LSTM 预测亚马逊股价的趋势

在本活动中，我们将考察亚马逊最近 5 年的股价，即从 2014 年 1 月 1 日到 2018 年 12 月 31 日。为此，我们将尝试使用 RNN 和 LSTM 预测公司 2019 年 1 月的未来趋势。我们有 2019 年 1 月的实际值，所以我们可以稍后将我们的预测与实际值进行比较。按照以下步骤完成本活动:

1.  导入所需的库。
2.  从完整的数据集中，提取`Open`列，因为将对开盘价进行预测。从这本书的 GitHub 资源库下载数据集。你可以在 https://packt.live/2vtdA8o 找到数据集。
3.  将数据归一化到 0 和 1 之间。
4.  然后，创建时间戳。2019 年 1 月每一天的数值都将由之前的 60 天来预测；因此，如果使用从第 *n* 天到 12 月 31 日的值来预测 1 月 1 日，那么将使用第 *n* + *1* 天以及 1 月 1 日来预测 1 月 2 日，以此类推。
5.  将数据重塑为三维，因为网络需要三维数据。
6.  用四层`LSTM`的`50`单元(这里的单元指神经元)在`Keras`中建立一个`RNN`模型。第一步应该提供输入形状。注意最后的`LSTM`层总是加上`return_sequences=True`，所以不必显式定义。
7.  处理并准备测试数据，即 2019 年 1 月的实际数据。
8.  合并和处理训练和测试数据。
9.  将结果可视化。

实现这些步骤后，您应该会看到以下预期输出:

![Figure 9.19: Real versus predicted stock prices
](image/B15777_09_19.jpg)

图 9.19:真实与预测的股票价格

注意

这项活动的解决方案可在第 404 页找到。

现在，让我们试着通过调整我们的`LSTM`来提高性能。关于如何建造一个`LSTM`没有黄金标准；但是，为了提高性能，可以尝试以下排列组合:

*   用中等单位建造一个`LSTM`，比如`50`
*   建造一个拥有超过`100`个单位的`LSTM`
*   使用更多数据；也就是说，不是从`5`年开始，而是从`10`年开始
*   使用`100`单位应用正则化
*   使用`50`单位应用正则化
*   使用更多数据和`50`单位应用正则化

该列表可以有多种组合；无论哪种组合提供最佳结果，都可以被认为是该特定数据集的好算法。在下一个练习中，我们将通过向我们的`LSTM`层添加更多单元并观察性能来探索这些选项之一。

## 练习 9.02:使用有 100 个单位的 LSTM 预测 Alphabet 的股价趋势

在本练习中，我们将考察 Alphabet 在过去 5 年(从 2014 年 1 月 1 日到 2018 年 12 月 31 日)的股价。为此，我们将尝试使用 RNNs 预测公司 2019 年 1 月的未来趋势。我们有 2019 年 1 月的实际值，所以稍后我们会将我们的预测与实际值进行比较。这是与第一个练习相同的任务，但是现在我们使用 100 个单位。确保您将输出与*练习 9.01* 、*使用 50 个单位(神经元)的 LSTM*预测 Alphabet 股价的趋势进行比较。按照以下步骤完成本练习:

1.  导入所需的库:

    ```
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from tensorflow import random
    ```

2.  使用 pandas `read_csv`函数导入数据集，并使用`head`方法查看数据集的前五行:

    ```
    dataset_training = pd.read_csv('../GOOG_train.csv')
    dataset_training.head()
    ```

3.  我们将使用`Open`股票价格进行预测；因此，从数据集中选择`Open`股票价格列，并打印值:

    ```
    training_data = dataset_training[['Open']].values
    training_data
    ```

4.  然后，通过使用`MinMaxScaler`标准化数据并设置特征范围来执行特征缩放，使其最小值为 0，最大值为 1。对训练数据使用定标器的`fit_transform`方法:

    ```
    from sklearn.preprocessing import MinMaxScaler
    sc = MinMaxScaler(feature_range = (0, 1))
    training_data_scaled = sc.fit_transform(training_data)
    training_data_scaled
    ```

5.  创建数据以从当前实例获取`60`时间戳。我们在这里选择`60`,因为它将为我们提供足够数量的先前实例，以便了解趋势；从技术上讲，这可以是任何数字，但`60`是最佳值。另外，这里的上限值是`1258`，它是`training`集合

    ```
    X_train = []
    y_train = []
    for i in range(60, 1258):
        X_train.append(training_data_scaled[i-60:i, 0])
        y_train.append(training_data_scaled[i, 0])
    X_train, y_train = np.array(X_train), np.array(y_train)
    ```

    中的行(或记录)的索引或计数
6.  使用 NumPy 的`reshape`函数对数据进行整形，在`X_train`的末尾添加一个额外的维度:

    ```
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
    ```

7.  导入下面的`Keras`库来构建`RNN` :

    ```
    from keras.models import Sequential
    from keras.layers import Dense, LSTM, Dropout
    ```

8.  设置种子并启动顺序模型，如下:

    ```
    seed = 1
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential()
    ```

9.  向具有`50`个单元的网络添加一个`LSTM`层，将`return_sequences`参数设置为`True`，将`input_shape`参数设置为`(X_train.shape[1], 1)`。添加三个额外的`LSTM`层，每个层都有`50`个单元，并将前两个层的`return_sequences`参数设置为`True`。添加大小为`1` :

    ```
    model.add(LSTM(units = 100, return_sequences = True, input_shape = (X_train.shape[1], 1)))
    # Adding a second LSTM
    model.add(LSTM(units = 100, return_sequences = True))
    # Adding a third LSTM layer
    model.add(LSTM(units = 100, return_sequences = True))
    # Adding a fourth LSTM layer
    model.add(LSTM(units = 100))
    # Adding the output layer
    model.add(Dense(units = 1))
    ```

    的最终输出层
10.  使用`adam`优化器编译网络，并使用`Mean Squared Error`计算损耗。将模型拟合到批量为`32` :

    ```
    # Compiling the RNN
    model.compile(optimizer = 'adam', loss = 'mean_squared_error')
    # Fitting the RNN to the Training set
    model.fit(X_train, y_train, epochs = 100, batch_size = 32)
    ```

    的`100`时期的训练数据
11.  加载并处理测试数据(此处视为实际数据)，选择代表`Open`股票数据值的列:

    ```
    dataset_testing = pd.read_csv("../GOOG_test.csv")
    actual_stock_price = dataset_testing[['Open']].values
    actual_stock_price
    ```

12.  连接数据，因为我们将需要`60`以前的实例来获得每天的股票价格。因此，我们将需要训练和测试数据:

    ```
    total_data = pd.concat((dataset_training['Open'], dataset_testing['Open']), axis = 0)
    ```

13.  重塑和缩放输入以准备测试数据。请注意，我们预测的是一月份的月度趋势，它有`21`个金融日，因此为了准备测试集，我们将下限值作为`60`，上限值作为`81`。这确保了`21`的差异被保持:

    ```
    inputs = total_data[len(total_data) - len(dataset_testing) - 60:].values
    inputs = inputs.reshape(-1,1)
    inputs = sc.transform(inputs)
    X_test = []
    for i in range(60, 81):
        X_test.append(inputs[i-60:i, 0])
    X_test = np.array(X_test)
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
    predicted_stock_price = model.predict(X_test)
    predicted_stock_price = sc.inverse_transform(predicted_stock_price)
    ```

14.  Visualize the results by plotting the actual stock price and plotting the predicted stock price:

    ```
    # Visualizing the results
    plt.plot(actual_stock_price, color = 'green', label = 'Real Alphabet Stock Price',ls='--')
    plt.plot(predicted_stock_price, color = 'red', label = 'Predicted Alphabet Stock Price',ls='-')
    plt.title('Predicted Stock Price')
    plt.xlabel('Time in days')
    plt.ylabel('Real Stock Price')
    plt.legend()
    plt.show()
    ```

    **预期产量**:

![Figure 9.20: Real versus predicted stock price
](image/B15777_09_20.jpg)

图 9.20:真实与预测的股票价格

现在，如果我们比较*练习 9.01* 、*中的`LSTM`和`LSTM`中的*预测 Alphabet 股价的趋势，前者使用 50 个单位(神经元)，后者使用`50`个神经元，后者使用`100`个单位，我们可以看到，与亚马逊股价的情况不同，使用具有`100`个单位的`LSTM`可以更好地捕捉 Alphabet 股价的趋势:

![Figure 9.21: Comparing the output with the LSTM of Exercise 9.01
](image/B15777_09_21.jpg)

图 9.21:将输出与练习 9.01 的 LSTM 进行比较

因此，我们可以清楚地看到，带有`100`单位的`LSTM`比带有`50`单位的`LSTM`预测的趋势更准确。请记住，带有`100`单元的`LSTM`将需要更多的计算时间，但在这种情况下会提供更好的结果。除了通过添加更多单元来修改我们的模型，我们还可以添加正则化。以下活动将测试添加正则化是否可以使我们的 Amazon 模型更加准确。

## 活动 9.02:预测亚马逊的股票价格

在本活动中，我们将考察从 2014 年 1 月 1 日到 2018 年 12 月 31 日的过去 5 年中亚马逊的股价。为此，我们将尝试使用 RNNs 和 LSTM 预测公司 2019 年 1 月的未来趋势。我们有 2019 年 1 月的实际值，因此我们将能够在稍后将我们的预测与实际值进行比较。最初，我们使用 50 个单位(或神经元)的 LSTM 来预测亚马逊股价的趋势。在这里，我们还将添加 dropout 正则化，并将结果与*活动 9.01* 、*使用 50 个单位(神经元)*的 LSTM 预测亚马逊股价的趋势进行比较。按照以下步骤完成本活动:

1.  导入所需的库。
2.  从完整的数据集中，提取`Open`列，因为预测将基于开盘价。你可以从这本书的 GitHub 资源库下载数据集，该资源库位于 https://packt.live/2vtdA8o。
3.  将数据归一化到 0 和 1 之间。
4.  然后，创建时间戳。2019 年 1 月每一天的值将由之前的`60`天预测。因此，如果使用从第 *n* 天到 12 月 31 日的值来预测 1 月 1 日，那么将使用第*n+1*天和 1 月 1 日来预测 1 月 2 日，以此类推。
5.  将数据重塑为三维，因为网络需要三维数据。
6.  在 Keras 中建立一个具有四个 LSTM 层的 RNN，每个层具有`50`个单元(这里，单元指神经元)，并且在每个 LSTM 层之后有 20%的下降。第一步应该提供输入形状。请注意，最后的 LSTM 层总是添加`return_sequences=True`。
7.  处理和准备测试数据，这是 2019 年 1 月的实际数据。
8.  合并和处理训练和测试数据。
9.  最后，将结果可视化。

实现这些步骤后，您应该得到以下预期输出:

![Figure 9.22: Real versus predicted stock prices
](image/B15777_09_22.jpg)

图 9.22:真实与预测的股票价格

注意

这项活动的解决方案可在 408 页找到。

在下一个活动中，我们将尝试在每个`LSTM`层中用`100`个单元构建一个`RNN`，并将其与只使用`50`个单元的`RNN`的性能进行比较。

## 活动 9.03:使用 LSTM 神经元数量不断增加(100 个单位)的 LSTM 预测亚马逊股票价格的趋势

在本活动中，我们将考察从 2014 年 1 月 1 日到 2018 年 12 月 31 日的过去 5 年中亚马逊的股价。为此，我们将尝试使用 RNNs 预测公司 2019 年 1 月的未来趋势。我们有 2019 年 1 月的实际值，因此我们将能够在稍后将我们的预测与实际值进行比较。还可以将输出差异与 *Activity 9.01* 、*用 50 个单位(神经元)的 LSTM*预测亚马逊股价的走势进行比较。按照以下步骤完成本活动:

1.  导入所需的库。
2.  从完整的数据集中，提取`Open`列，因为预测将基于`Open`股票价值。
3.  将数据归一化到 0 和 1 之间。
4.  然后，创建时间戳。2019 年 1 月每一天的值将由之前的`60`天预测；因此，如果使用从第 n 天到 12 月 31 日的值来预测 1 月 1 日，那么将使用第 *n + 1* 天以及 1 月 1 日来预测 1 月 2 日，依此类推。
5.  将数据重塑为三维，因为网络需要三维数据。
6.  用 100 个单位(这里，单位指神经元)在喀拉斯建造一个 LSTM。第一步应该提供输入形状。注意最后的`LSTM`层总是加上`return_sequences=True`。编译模型并使其符合训练数据。
7.  处理和准备测试数据，这是 2019 年 1 月的实际数据。
8.  合并和处理训练和测试数据。
9.  将结果可视化。

实现这些步骤后，您应该得到以下预期输出:

![Figure 9.23: Real versus predicted stock prices
](image/B15777_09_23.jpg)

图 9.23:真实与预测的股票价格

注意

这项活动的解决方案可在第 413 页找到。

在这个活动中，我们创建了一个有四个`LSTM`层的`RNN`，每个层都有`100`单元。我们将其与*活动 9.02* 、*预测亚马逊股票价格的结果进行了比较，其中每层有`50`个单位。这两个模型之间的差异很小，因此，由于计算时间的减少以及过拟合训练数据的可能性较小，具有较少单元的模型是优选的。*

# 总结

在这一章中，我们通过使用 Google Assistant 检查一些现实生活中的案例，了解了顺序建模和顺序记忆。然后，我们学习了顺序建模如何与`RNNs`相关，以及`RNNs`如何不同于传统的前馈网络。我们详细了解了消失梯度问题，以及如何使用`LSTM`比简单的`RNN`更好地克服消失梯度问题。我们通过预测股票趋势，将所学知识应用于时间序列问题。

在这个研讨会中，我们学习了机器学习和 Python 的基础知识，同时也深入了解了如何应用 Keras 开发高效的深度学习解决方案。我们探讨了机器和深度学习的区别。我们通过建立一个逻辑回归模型开始了研讨会，首先用 scikit-learn，然后用 Keras。

然后，我们通过为各种现实场景创建预测模型，进一步探索了 Keras 及其不同的模型，例如将在线购物者分为有购买意向的人和没有购买意向的人。我们学习了如何评估、优化和改进模型以获得最大信息，从而创建在新的、看不见的数据上表现良好的健壮模型。

我们还通过用 scikit-learn 的包装器构建 Keras 模型来合并交叉验证，帮助那些熟悉 scikit-learn 工作流的人轻松利用 Keras 模型。然后，我们学习了如何应用`L1`、`L2`和`dropout regularization`技术来提高模型的准确性，并帮助防止我们的模型过度拟合训练数据。

接下来，我们通过应用零精度基线比较等技术和精度、`AUC-ROC`分数等评估指标来进一步探索模型评估，以了解我们的模型如何对分类任务进行评分。最终，这些先进的评估技术帮助我们了解在什么条件下我们的模型表现良好，以及哪里有改进的空间。

我们通过使用 Keras 创建一些高级模型结束了研讨会。我们通过建立具有各种参数的`CNN`模型来对图像进行分类，从而探索了计算机视觉。然后，我们使用预训练模型对新图像进行分类，并对这些预训练模型进行微调，以便我们可以将它们用于自己的应用。最后，我们介绍了序列建模，它用于对股票价格和自然语言处理等序列进行建模。我们通过创建具有`LSTM`层的`RNN`网络来预测真实股票数据的股票价格，并在每层中使用不同数量的单元以及剔除正则化对模型性能的影响来测试这一知识。

总的来说，我们已经全面了解了如何使用 Keras 来解决使用真实数据集的各种问题。我们涵盖了在线购物者的分类任务、丙型肝炎数据和斯堪尼亚卡车的故障数据，以及回归任务，例如在给定各种化学属性的情况下预测各种化学物质的水生毒性。我们还执行了图像分类任务，并建立了`CNN`模型来预测图像是花还是车，还建立了回归任务来预测未来的股票价格。通过使用本课程建立真实数据集的模型，您可以将您的学习和理解应用到自己的问题解决中，并创建自己的应用。