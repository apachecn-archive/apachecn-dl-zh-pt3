

# 5.提高模型精度

概观

本章介绍了神经网络的正则化概念。正则化旨在防止模型在训练过程中过度拟合训练数据，并在对新的未知数据测试模型时提供更准确的结果。您将学习利用不同的正则化技术(L1 和 L2 正则化以及剔除正则化)来提高模型性能。正则化是一个重要的组成部分，因为它可以防止神经网络过度拟合训练数据，并帮助我们建立健壮、准确的模型，这些模型在新的、看不见的数据上表现良好。本章结束时，你将能够在 scikit-learn 中实现网格搜索和随机搜索，并找到最佳超参数。

# 简介

在前一章中，我们继续发展我们的知识，通过实验交叉验证作为一种方法来测试各种超参数如何以无偏的方式表现，从而用神经网络创建精确的模型。我们利用留一法交叉验证，其中我们在训练过程中留下一条记录用于验证，并对数据集中的每条记录重复这一过程。然后，我们查看了 k 折叠交叉验证，其中我们将训练数据集分成`k`折叠，在`k-1`折叠上训练模型，并使用最终折叠进行验证。这些交叉验证方法允许我们用不同的超参数训练模型，并在无偏数据上测试它们的性能。

深度学习不仅仅是建立神经网络，使用可用的数据集训练它们，并报告`model accuracy`。它包括尝试理解您的模型和数据集，以及通过在许多方面改进它来超越基本模型。在这一章中，您将了解两组非常重要的技术，用于改进一般的机器学习模型，特别是深度学习模型。这些技术是正则化方法和超参数调整。

本章将进一步讨论`regularization methods`——具体来说，我们为什么需要它们以及它们如何帮助我们。然后，我们将介绍两种最重要和最常用的正则化技术。在这里，您将详细了解参数正则化及其两种变体，`L1`和 `L2`范数正则化。然后，您将了解一种专门为神经网络设计的正则化技术，称为辍学调节。您还将通过完成涉及真实数据集的活动，练习在 Keras 模型上实现这些技术。我们将通过简要介绍一些其他的正则化技术来结束我们对正则化的讨论，这些技术可能对您以后的工作有所帮助。

然后，我们将讨论超参数调整的重要性，特别是对深度学习模型而言，通过探索调整超参数的值如何显著影响模型准确性，以及在构建深度神经网络时调整许多需要它的超参数的挑战。您将在 scikit 中学习两种非常有用的方法——了解可用于在 Keras 模型上执行超参数调整的方法、每种方法的优点和缺点，以及如何将它们结合起来以从两者中获得最大收益。最后，您将通过完成一个活动来练习使用 scikit-learn 优化器实现 Keras 模型的超参数调优。

# 正规化

由于深度神经网络是高度灵活的模型，过度拟合是在训练它们时经常出现的问题。因此，成为深度学习专家的一个非常重要的部分是知道如何检测过度拟合，以及随后如何解决模型中的过度拟合问题。如果您的模型在训练数据上表现出色，但在新的、看不见的数据上表现不佳，则模型中的过度拟合将会很明显。

例如，如果您建立了一个模型来将狗和猫的图像分类到它们各自的类别中，并且您的图像分类器在训练过程中表现出很高的准确性，但在新的示例中表现不佳，则这表明您的模型过度拟合了训练数据。正则化技术是一组重要的方法，专门用于减少机器学习模型中的过拟合。

彻底理解正则化技术，并能够将它们应用到您的深度神经网络中，这是构建深度神经网络以解决现实问题的重要一步。在本节中，您将了解正则化的基本概念，这将为您提供后续部分所需的基础，在后续部分中，您将学习如何使用 Keras 实现各种类型的正则化方法。

## 正规化的需要

机器学习的主要目标是建立表现良好的模型，不仅是在他们接受训练的例子上，而且是在训练中没有包括的新例子上。一个好的机器学习模型是这样一种模型，它能找到产生训练样本的真正底层过程/函数的形式和参数，但不会捕捉与单个训练样本相关联的噪声。这种机器学习模型可以很好地推广到稍后由相同过程产生的新数据。

我们之前讨论的方法——例如将数据集分为训练集和测试集，以及交叉验证——都是为了评估训练模型的泛化能力而设计的。事实上，用来指测试集错误和交叉验证错误的术语是“泛化错误”这仅仅意味着没有在训练中使用的例子的错误率。同样，机器学习的主要目标是建立泛化错误率低的模型。

在*第三章*、*深度学习与 Keras* 中，我们讨论了机器学习模型的两个非常重要的问题:过拟合和欠拟合。我们说过，欠拟合是这样一种场景，其中估计的模型不够灵活或复杂，不足以捕捉与真实过程相关联的所有关系和模式。这是一个带有`high bias`的模型，在训练误差较高时检测。另一方面，过度拟合是用于估计真实过程的模型过于灵活或复杂的情况。这是一个带有`high variance`的模型，在训练误差和泛化误差差距较大时进行诊断。在下图中可以看到二元分类问题的这些场景的概述:

![Figure 5.1: Underfitting
](image/B15777_05_01.jpg)

图 5.1:欠拟合

正如你在上面看到的，欠拟合比过拟合问题小。事实上，通过使模型更加灵活/复杂，可以很容易地解决欠拟合问题。在深度神经网络中，这意味着改变网络的架构，通过添加更多层或增加层中的单元数量来使网络变得更大。

现在让我们看看下面的过度拟合图像:

![Figure 5.2: Overfitting
](image/B15777_05_02.jpg)

图 5.2:过度拟合

类似地，存在解决过度拟合的简单解决方案，例如使模型不那么灵活/复杂(同样，通过改变网络的架构)或者向网络提供更多的训练示例。然而，降低网络的复杂性有时会以大幅增加偏差或训练错误率为代价。出现这种情况的原因是，大多数时候，过拟合的原因不是模型的灵活性，而是训练样本太少。另一方面，提供更多的数据示例以减少过度拟合并不总是可能的。因此，在保持模型复杂性和训练样本数量固定的同时，找到减少泛化误差的方法既重要又具有挑战性。

现在，让我们来看看下面这张合适的图片:

![Figure 5.3: Right fit
](image/B15777_05_03.jpg)

图 5.3:合适

这就是为什么我们在建立高度灵活的机器学习模型(如深度神经网络)时需要正则化技术，以抑制模型的灵活性，使其不能过度适应个别例子。在下一节中，我们将描述正则化方法如何减少模型对训练数据的过度拟合，以减少模型中的方差。

## 通过正则化减少过拟合

方法试图以减少模型方差的方式修改学习算法。通过减少方差，正则化技术旨在减少泛化误差，同时不增加训练误差(或者至少不显著增加训练误差)。

正则化方法提供了某种限制，有助于模型的稳定性。有几种方法可以实现这一点。在深度神经网络上执行正则化的最常见方法之一是通过在权重上放置某种类型的惩罚项来保持权重较小。

保持较小的权重使得网络对单个数据示例中的噪声不太敏感。事实上，神经网络中的权重是决定每个处理单元对网络最终输出的影响大小的系数。如果这些单元的权重很大，这意味着它们中的每一个都会对输出产生重大影响。将每个处理单元引起的所有大的影响结合起来，将导致最终输出的许多波动。

另一方面，保持较小的权重会减少每个单元对最终输出的影响。事实上，通过保持权重接近零，一些单位将对输出几乎没有影响。训练大型神经网络(其中每个单元对输出影响很小或没有影响)相当于训练一个简单得多的网络，因此方差和过度拟合会减少。下图显示了正则化如何消除大型网络中某些单元的影响的示意图:

![Figure 5.4: Schematic view of how regularization zeroes out the effect of some units in a large network

](image/B15777_05_04.jpg)

图 5.4:正则化如何消除大型网络中某些单元的影响的示意图

上图是正则化过程的示意图。顶部网络显示的是未进行正则化的网络，而底部网络显示的是进行正则化的网络示例，其中白色单元表示对输出影响很小或没有影响的单元，因为它们已被正则化过程惩罚。

到目前为止，我们已经了解了正则化背后的概念。在下一节中，我们将研究深度学习模型最常见的正则化方法— `L1`、`L2`和辍学正则化—以及如何在 Keras 中实现它们。

# L1 和 L2 正规化

深度学习模型最常见的正则化类型是保持网络权重较小的类型。这种类型的正则化称为权重正则化，有两种不同的变体:L2 正则化和 L1 正则化。在本节中，您将详细了解这些正则化方法，以及如何在 Keras 中实现它们。此外，您将练习将它们应用于现实生活中的问题，并观察它们如何提高模型的性能。

## L1 和 L2 的正规化表述

在权重正则化中，损失函数中增加了一个惩罚项。这一项或者是权重的 L2 范数(平方值之和),或者是权重的 L1 范数(绝对值之和)。如果使用 L1 范数，那么它将被称为`L1 regularization`。如果使用 L2 范数，那么它将被称为`L2 regularization`。在每种情况下，总和乘以称为正则化参数(λ)的超参数。

因此，对于`L1 regularization`，公式如下:

*损失函数=旧损失函数+λ*权重绝对值之和*

对于`L2 regularization`，公式如下:

*损失函数=旧损失函数+λ*权重值的平方和*

`Lambda`可以取`0`和`1`之间的任意值，其中`lambda=0`表示完全不罚分(相当于一个没有正则化的网络)`lambda=1`表示全额罚分。

像所有其他超参数一样，可以通过尝试不同的值并观察哪个值提供更低的泛化误差来选择`lambda`的正确值。事实上，从没有正规化的网络开始并观察结果是一个很好的实践。然后，您应该使用递增的 lambda 值执行正则化，例如`0.001`、`0.01`、`0.1`、`0.5`、…，并观察每种情况下的结果，以便计算出对权重值进行多少惩罚适合于特定问题。

在正则化优化算法的每次迭代中，权重(w)变得越来越小。这就是为什么权重正则化通常被称为权重衰减。

到目前为止，我们只讨论了深度神经网络中的正则化权重。然而，你需要记住，同样的程序也可以应用于偏见。更准确地说，我们也可以通过向损失函数添加偏差惩罚项来再次更新损失函数，从而在神经网络的训练期间保持偏差值较小。

注意

如果通过在损失函数中增加两项(一项用于惩罚权重，一项用于惩罚偏差)来执行正则化，那么我们称之为参数正则化，而不是权重正则化。

然而，正则化偏差值在深度学习中并不常见。其原因是权重是神经网络的重要得多的参数。事实上，通常情况下，与仅正则化权重值相比，添加另一项来正则化偏差不会显著改变结果。

是机器学习中最常用的正则化技术。`L1 regularization`和`L2 regularization`的区别在于`L1`产生了更稀疏的权重矩阵，这意味着有更多的权重等于零，因此有更多的节点完全从网络中删除。`L2 regularization`则相反，更为微妙。它大幅降低权重，但同时留给你的权重更少，等于 0。也可以同时执行`L1`和`L2 regularization`。

现在你已经了解了`L1`和`L2 regularization`是如何工作的，你已经准备好在 Keras 的深度神经网络上实现`L1`和`L2 regularization`了。

## L1 和 L2 在喀拉斯的正规化实施

Keras 提供了一个正则化 API，可以用来将惩罚项添加到损失函数中，以便正则化深度神经网络的每一层中的权重或偏差。要定义罚项或`regularizer`，需要在`keras.regularizers`下定义期望的正则化方法。

例如，要用`lambda=0.01`定义一个`L1 regularizer`，您可以这样写:

```
from keras.regularizers import l1
keras.regularizers.l1(0.01)
```

类似地，要用`lambda=0.01`定义一个`L2 regularizer`，您可以这样写:

```
from keras.regularizers import l2
keras.regularizers.l2(0.01)
```

最后，用`lambda=0.01`来定义`L1`和`L2 regularizers`，你可以这样写:

```
from keras.regularizers import l1_l2
keras.regularizers.l1_l2(l1=0.01, l2=0.01)
```

这些`regularizers`中的每一个都可以应用于层中的权重和/或偏差。例如，如果我们想要将`L2 regularization`(带`lambda=0.01`)应用于具有八个节点的密集层的权重和偏差，我们可以这样写:

```
from keras.layers import Dense
from keras.regularizers import l2
model.add(Dense(8, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01)))
```

我们将在*活动 5.01* 、*中进一步练习在 Avila 模式分类器*上实现`L1`和`L2 regularization`，在该活动中，您将对糖尿病数据集的深度学习模型应用正则化，并观察结果与之前的活动相比有何变化。

注意

本章中的所有活动都将在 Jupyter 笔记本中展开。请从[https://packt.live/2OOBjqq](https://packt.live/2OOBjqq)下载这本书的 GitHub 资源库，以及所有准备好的模板。

## 活动 5.01:Avila 模式分类器上的权重正则化

Avila 数据集是从《T1》的`800`图像中提取的，这是一部 12 世纪的巨型拉丁文圣经。数据集由关于文本图像的各种特征组成，例如列间距离和文本的边距。数据集还包含一个类别标签，该标签指示图像的模式是否属于最频繁出现的类别。在本活动中，您将构建一个 Keras 模型，根据给定的网络架构和超参数值对此数据集执行分类。目标是对模型应用不同类型的权重正则化，并观察每种类型如何改变结果。

在本活动中，我们将使用`training set` / `test set`方法进行评估，原因有二。首先，由于我们要尝试几种不同的正则化方法，执行交叉验证将会花费很长时间。其次，我们希望绘制训练误差和测试误差的趋势，以便以直观的方式理解正则化如何防止模型过度拟合数据示例。

按照以下步骤完成本活动:

1.  使用`X = pd.read_csv('../data/avila-tr_feats.csv')`和`y = pd.read_csv('../data/avila-tr_target.csv')`从 GitHub 的`Chapter05`的`data`子文件夹中加载数据集。使用`sklearn.model_selection.train_test_split`方法将数据集分成训练集和测试集。保留测试集的数据示例`20%`。
2.  定义一个有三个隐藏层的 Keras 模型，第一个是`size 10`，第二个是`size 6`，第三个是`size 4`，来执行分类。将这些值用于超参数:`activation='relu'`、`loss='binary_crossentropy'`、`optimizer='sgd'`、`metrics=['accuracy']`、`batch_size=20`、`epochs=100`和`shuffle=False`。
3.  在训练集上训练模型，并用测试集对其进行评估。存储每次迭代的训练损失和测试损失。训练完成后，在`training error`和`test error`中绘制趋势图(将垂直轴的极限值改为(`0, 1`)，以便更好地观察损失的变化)。测试集的最小错误率是多少？
4.  将`L2 regularizers`和`lambda=0.01`添加到你的模型的隐藏层，然后重复训练。训练完成后，绘制训练误差和测试误差的趋势图。测试集的最小错误率是多少？
5.  对`lambda=0.1`和`lambda=0.005`重复上述步骤，针对`lambda`的每个值训练模型，并报告结果。在这个深度学习模型和这个数据集上执行`L2 regularization`时，`lambda`的哪个值是更好的选择？
6.  重复上一步，这次用`lambda=0.01`和`lambda=0.005`的`L1 regularizers`，为`lambda`的每个值训练模型，并报告结果。在这个深度学习模型和这个数据集上执行`L1 regularization`时，`lambda`的哪个值是更好的选择？
7.  将带有`L1 lambda=0.005`和`L2 lambda=0.005`的`L1_L2 regularizers`添加到你的模型的隐藏层，然后重复训练。训练完成后，绘制训练误差和测试误差的趋势图。测试集的最小错误率是多少？

实现这些步骤后，您应该得到以下预期输出:

![Figure 5.5: A plot of the training error and validation error during training for the model with L1 lambda ](image/B15777_05_05.jpg)

图 5.5:L1λ等于 0.005，L2λ等于 0.005 的模型的训练误差和验证误差图

注意

这项活动的解决方案可在第 354 页找到。

在本活动中，您练习了针对实际问题实现`L1`和`L2 weight regularizations`，并将正则化模型的结果与未正则化模型的结果进行了比较。在下一节中，我们将探讨一种不同的技术，称为辍学正则化的监管。

# 退学正规化

在本节中，您将了解辍学正规化是如何工作的，它如何有助于减少过度拟合，以及如何使用 Keras 实现它。最后，你将通过完成一个涉及真实数据集的活动来练习你所学到的关于辍学的知识。

## 辍学正规化的原则

退出正则化通过在训练期间从神经网络中随机移除节点来工作。更准确地说，dropout 在每个节点上设置了一个概率。这个概率指的是在学习算法的每次迭代中节点被包括在训练中的机会。假设我们有一个大型神经网络，其中每个节点都有一个`0.5`的退出机会。在这种情况下，在每次迭代中，学习算法为每个节点投掷硬币，以决定该节点是否将从网络中移除。下图说明了这一过程:

![Figure 5.6: Illustration of removing nodes from a deep neural network using dropout regularization
](image/B15777_05_06.jpg)

图 5.6:使用丢失正则化从深度神经网络中移除节点的图示

这个过程在每次迭代中重复；这意味着，在每次迭代中，随机选择的节点从网络中移除，这意味着参数更新过程是在不同的较小网络上完成的。例如，上图底部显示的网络将仅用于一次训练迭代。对于下一次迭代，一些其他随机选择的节点将从顶部网络中删除，因此删除这些节点后得到的网络将不同于图中的底部网络。

当在学习算法的迭代中选择移除/忽略一些节点时，这意味着它们根本不会参与该迭代中的参数更新过程。更准确地说，预测输出的前向传播、损耗计算和计算导数的后向传播都是在去掉一些节点的较小网络上进行的。因此，参数更新将仅在该迭代中出现在网络中的节点上进行；移除的节点的权重和偏差不会更新。

然而，重要的是要记住，为了在测试集或保留集上评估模型的性能，总是使用原始的完整网络。如果我们对删除了随机节点的网络进行评估，结果中会引入噪声，这是不希望的。

在`dropout regularization`中，`training`总是在从原始网络中移除随机选择的节点而产生的网络上执行。`Evaluation`总是使用原始网络执行。在下一节中，我们将了解为什么辍学正规化有助于防止过度拟合。

## 减少过度拟合和漏失

在这一节中，我们将讨论辍学作为一种正规化方法背后的概念。正如我们之前讨论的，正则化技术的目标是防止模型过度拟合数据。因此，我们将研究如何从神经网络中随机移除一部分节点来帮助减少方差和过度拟合。

为什么从网络中移除随机节点可以防止过度拟合，最明显的解释是，通过从网络中移除节点，我们在与原始网络相比更小的网络上执行训练。正如您之前所了解的，较小的神经网络提供的灵活性较小，因此网络过度适应数据的可能性较低。

辍学调整在减少过度适应方面做得如此之好还有另一个原因。通过随机移除深度神经网络中每一层的输入，整个网络对单个输入变得不那么敏感。我们知道，在训练神经网络时，权重会以最终模型将适合训练示例的方式进行更新。通过从训练过程中随机移除一些权重，dropout 迫使其他权重参与学习与该迭代中的训练示例相关的模式，因此最终权重值将更好地分散。

换句话说，不是一些权重更新太多以适应一些输入值，而是所有权重学习参与学习那些输入值，因此过度适应减少。这就是为什么与简单地使用较小的网络相比，执行 dropout 会产生更健壮的模型——在新的、看不见的数据上表现得更好。事实上，`dropout regularization`在更大的网络上工作得更好。

既然你已经了解了辍学的基本程序和其有效性背后的原因，我们可以继续在 Keras 中实现`dropout regularization`。

## 练习 5.01:Keras 的辍学实施

`Dropout regularization`在 Keras 中作为核心层提供。因此，您可以像向网络中添加图层一样向模型中添加 dropout。在 Keras 中定义 dropout 层时，需要提供`rate`超参数作为参数。`rate`可以取`0`和`1`之间的任何值，并确定要删除或忽略的输入单元部分。在本练习中，您将学习使用辍学层实现 Keras 深度学习模型的分步过程。

我们的模拟数据集代表了树木的各种度量，如高度、树枝数量和底部树干的周长。我们的目标是根据给定的测量值，将记录分为落叶树(等级值为`1`)或针叶树(等级值为`0`)类型。数据集由包含两个类的`10000`记录组成，代表两种树类型，每个数据示例都有`10`特征值。按照以下步骤完成本练习:

1.  首先，执行下面的代码块来加载数据集，并将数据集分成`training set`和`test set` :

    ```
    # Load the data
    import pandas as pd
    X = pd.read_csv('../data/tree_class_feats.csv')
    y = pd.read_csv('../data/tree_class_target.csv')
    # Split the dataset into training set and test set with a 80-20 ratio
    from sklearn.model_selection import train_test_split
    seed = 1
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)
    ```

2.  导入所有必要的依赖项。不用`dropout regularization`搭建四层 Keras 序列模型。构建网络，第一隐层 16 个单元，第二隐层`12`个单元，第三隐层`8`个单元，第四隐层`4`个单元，都具有`ReLU activation`功能。添加一个具有`sigmoid activation`功能的输出层:

    ```
    #Define your model
    from keras.models import Sequential
    from keras.layers import Dense, Activation
    import numpy as np
    from tensorflow import random
    np.random.seed(seed)
    random.set_seed(seed)
    model_1 = Sequential()
    model_1.add(Dense(16, activation='relu', input_dim=10))
    model_1.add(Dense(12, activation='relu'))
    model_1.add(Dense(8, activation='relu'))
    model_1.add(Dense(4, activation='relu'))
    model_1.add(Dense(1, activation='sigmoid'))
    ```

3.  Compile the model with `binary cross-entropy` as the `loss` function and `sgd` as the optimizer and train the model for `300` epochs with `batch_size=50` on the `training set`. Then, evaluate the trained model on the `test set`:

    ```
    model_1.compile(optimizer='sgd', loss='binary_crossentropy')
    # train the model
    model_1.fit(X_train, y_train, epochs=300, batch_size=50, verbose=0, shuffle=False)
    # evaluate on test set
    print("Test Loss =", model_1.evaluate(X_test, y_test))
    ```

    下面是预期的输出:

    ```
    2000/2000 [==============================] - 0s 23us/step
    Test Loss = 0.1697693831920624
    ```

    因此，在对模型进行`300`个时期的训练后，预测树种的测试误差率等于`16.98%`。

4.  Redefine the model with the same number of layers and same size in each layer as the prior model. However, add a `dropout regularization` of `rate=0.1` to the first hidden layer of your model and repeat the compilation, training, and evaluation steps of the model on the test data:

    ```
    # define the keras model with dropout in the first hidden layer
    from keras.layers import Dropout
    np.random.seed(seed)
    random.set_seed(seed)
    model_2 = Sequential()
    model_2.add(Dense(16, activation='relu', input_dim=10))
    model_2.add(Dropout(0.1))
    model_2.add(Dense(12, activation='relu'))
    model_2.add(Dense(8, activation='relu'))
    model_2.add(Dense(4, activation='relu'))
    model_2.add(Dense(1, activation='sigmoid'))
    model_2.compile(optimizer='sgd', loss='binary_crossentropy')
    # train the model
    model_2.fit(X_train, y_train, epochs=300, batch_size=50, verbose=0, shuffle=False)
    # evaluate on test set
    print("Test Loss =", model_2.evaluate(X_test, y_test))
    ```

    下面是预期的输出:

    ```
    2000/2000 [==============================] - 0s 29us/step
    Test Loss = 0.16891103076934816
    ```

    在网络的第一层加入一个`rate=0.1`的退出正则化后，测试错误率从`16.98%`降低到`16.89%`。

5.  Redefine the model with the same number of layers and the same size in each layer as the prior model. However, add a dropout regularization of `rate=0.2` to the first hidden layer and `rate=0.1` to the remaining layers of your model and repeat the compilation, training, and evaluation steps of the model on the test data:

    ```
    # define the keras model with dropout in all hidden layers
    np.random.seed(seed)
    random.set_seed(seed)
    model_3 = Sequential()
    model_3.add(Dense(16, activation='relu', input_dim=10))
    model_3.add(Dropout(0.2))
    model_3.add(Dense(12, activation='relu'))
    model_3.add(Dropout(0.1))
    model_3.add(Dense(8, activation='relu'))
    model_3.add(Dropout(0.1))
    model_3.add(Dense(4, activation='relu'))
    model_3.add(Dropout(0.1))
    model_3.add(Dense(1, activation='sigmoid'))
    model_3.compile(optimizer='sgd', loss='binary_crossentropy')
    # train the model
    model_3.fit(X_train, y_train, epochs=300, batch_size=50, verbose=0, shuffle=False)
    # evaluate on test set
    print("Test Loss =", model_3.evaluate(X_test, y_test))
    ```

    下面是预期的输出:

    ```
    2000/2000 [==============================] - 0s 40us/step
    Test Loss = 0.19390961921215058
    ```

    通过在第一层中保持`rate=0.2`的漏失正则化，同时向后续层添加`rate=0.1`的漏失正则化，测试错误率从`16.89%`增加到`19.39%`。像`L1`和`L2 regularizations`一样，增加过多的丢失会阻止模型学习与训练数据相关联的底层函数，并导致比没有丢失正则化时更高的偏差。

正如您在本练习中看到的，您还可以根据您认为这些层中可能发生的过度拟合程度，对不同的层应用不同速率的剔除。通常，我们不喜欢在输入层和输出层上执行丢弃。关于隐藏层，我们需要调整`rate`值并观察结果，以便决定什么值最适合特定问题。

在以下活动中，您将在 Keras 中对流量数据集练习实施深度学习模型和辍学正则化。

## Acti vity 5.02:交通量数据集上的流失正则化

在第 4 章、*的*活动 4.03* 、*使用交通量数据集*进行交叉验证的模型选择中，使用 Keras Wrappers* 使用交叉验证评估您的模型，您使用交通量数据集构建了一个模型，用于在给定与交通数据相关的各种标准化特征(例如一天中的时间和前一天的交通量等)的情况下预测城市桥梁的交通量。数据集包含`10000`条记录，对于每条记录，数据集中包含`10`个属性/特征。

在本活动中，您将从*活动 4.03* 、*中的模型开始，在*第 4 章*、*的交通量数据集*上使用交叉验证进行模型选择，使用 Keras Wrappers* 使用交叉验证评估您的模型。您将使用训练集/测试集方法来训练和评估模型，绘制训练错误和泛化错误的趋势，并观察模型是否过度拟合数据示例。然后，您将尝试通过使用 dropout 正则化来解决过度拟合问题，从而提高模型性能。具体来说，您将尝试找出应该向哪些层添加下降正则化，以及什么样的`rate`值将最大程度地改进该特定模型。按照以下步骤完成本活动:

1.  使用 pandas `read_csv`函数加载数据集。数据集也存储在`Chapter05` GitHub 存储库的**数据**子文件夹中。用一个`80-20`比率将数据集分成一个训练集和一个测试集。
2.  定义一个带有两个隐藏层`size 10`的 Keras 模型来预测交通量。将这些值用于以下超参数:`activation='relu'`、`loss='mean_squared_error'`、`optimizer='rmsprop'`、`batch_size=50`、`epochs=200`和`shuffle=False`。
3.  在训练集上训练模型，在测试集上评估。存储每次迭代的训练损失和测试损失。
4.  训练完成后，绘制训练误差和测试误差的趋势图。训练集和测试集的最低错误率是多少？
5.  将带有`rate=0.1`的 dropout 正则化添加到模型的第一个隐藏层，并重复训练过程(因为带有 dropout 的训练需要更长时间，所以训练`200`个时期)。训练完成后，绘制训练误差和测试误差的趋势图。训练集和测试集的最低错误率是多少？
6.  重复上一步，这一次用`rate=0.1`添加退出正则化到你的模型的两个隐藏层，训练模型并报告结果。
7.  重复上一步，这次在第一层使用`rate=0.2`，在第二层使用`0.1`，训练模型并报告结果。
8.  到目前为止，在这个深度学习模型和这个数据集上，哪个辍学正则化导致了最好的性能？

实现这些步骤后，您应该得到以下预期输出:

![Figure 5.7: A plot of training errors and validation errors while training the model with dropout regularization, with rate=0.2 in the first layer and rate=0.1 in the second layer
](image/B15777_05_07.jpg)

图 5.7:在使用漏失正则化对模型进行训练时，训练误差和验证误差的曲线图，其中第一层的 rate=0.2，第二层的 rate=0.1

注意

这项活动的解决方案可在第 369 页找到。

在本活动中，您学习了如何在 Keras 中实现下降正则化，并练习了如何在涉及交通量数据集的问题中使用它。`Dropout regularization`专为减少神经网络中的过度拟合而设计，通过在训练过程中从神经网络中随机移除节点来工作。这个过程产生了一个具有很好分布的权重值的神经网络，这导致了在单个数据示例中较少的过拟合。在下一节中，我们将讨论可用于防止模型过度拟合训练数据的其他正则化方法。

# 其他正则化方法

在本节中，您将简要了解一些常用的、已被证明在深度学习中有效的其他正则化技术。请记住，正则化是机器学习中一个广泛而活跃的研究领域。因此，在一章中涵盖所有可用的正则化方法是不可能的(也很可能是没有必要的，尤其是在一本关于应用深度学习的书中)。因此，在本节中，我们将简要介绍另外三种正则化方法，称为提前停止、数据扩充和添加噪声。你将了解他们的基本思想，并获得一些关于如何使用它们的提示和建议。

## 提前停止

在本章的前面，我们讨论了机器学习的主要假设是存在一个产生训练样本的真实函数或过程。但是，这个过程是未知的，没有显式的方法找到它。不仅没有办法找到确切的底层流程，而且选择一个具有适当灵活性或复杂性的模型来评估流程也是一个挑战。因此，一个好的实践是选择高度灵活的模型，例如深度神经网络，来对过程建模并仔细监控训练过程。

通过监控训练过程，我们可以训练模型，使其足以捕捉过程的形式，并且我们可以在它开始过度适应单个数据示例之前停止训练。这是提前停止背后的基本思想。我们在*第三章*、*深度学习与 Keras* 的*模型评估*部分简单讨论过提前停止的想法。我们说过，通过监测和观察训练期间`training error`和`test error`的变化，我们可以确定多少训练是太少，多少训练是太多。

下图显示了在数据集上训练高度灵活的模型时，训练误差和测试误差的变化。正如我们所看到的，训练需要在标记为“正确适应”的区域停止，以避免过度适应:

![Figure 5.8: Plot of the training error and test error while training a model
](image/B15777_05_08.jpg)

图 5.8:训练模型时的训练误差和测试误差图

在*第 3 章*、*使用 Keras 的深度学习*中，我们练习了存储和绘制训练误差和测试误差的变化，以便识别过度拟合。您了解了如何在训练 Keras 模型时提供验证集或测试集，并通过使用以下代码在每个训练时期存储每个验证集或测试集的指标值:

```
history=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs)
```

在本节中，您将学习如何在 Keras 中实现早期停止。这意味着当一个期望的指标——例如,`test error rate`—不再改善时，强制 Keras 模型停止训练。为此，您需要定义一个`EarlyStopping()`回调，并将其作为参数提供给`model.fit()`。

当定义一个`EarlyStopping()`回调时，您需要为它提供正确的参数。第一个参数是`monitor`，它决定了在训练期间将监控什么指标，以便执行提前停止。通常，`monitor='val_loss'`是一个很好的选择，意味着我们希望监控测试错误率。

同样，根据您为`monitor`选择的参数，您需要将`mode`参数设置为`'min'`或`'max'`。如果度量是错误/损失，我们希望将其最小化。例如，下面的代码块定义了一个`EarlyStopping()`回调函数，它在训练期间监控测试错误，并检测它是否不再减少了:

```
from keras.callbacks import EarlyStopping
es_callback = EarlyStopping(monitor='val_loss', mode='min')
```

如果错误率有很多波动或噪音，当损失开始增加时停止训练可能不是一个好主意。为此，我们可以将`patience`参数设置为多个时期，以便在停止训练过程之前给早期停止方法一些时间来监视更长时间的期望指标:

```
es_callback = EarlyStopping(monitor='val_loss', mode='min', patience=20)
```

如果`monitor`指标在过去`epoch`中没有出现最小的改善，或者`monitor`指标已经达到基线水平，我们也可以修改`EarlyStopping()`回调来停止训练过程:

```
es_callback = EarlyStopping(monitor='val_loss', mode='min', min_delta=1)
es_callback = EarlyStopping(monitor='val_loss', mode='min', baseline=0.2)
```

在定义了`EarlyStopping()`回调之后，您可以将它作为一个`callbacks`参数提供给`model.fit()`并训练模型。根据`EarlyStopping()`回调，训练将自动停止；

```
history=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, callbacks=[es_callback])
```

我们将在下一个练习中探讨如何在实践中实现提前停止。

## 练习 5.02:在 Keras 中实施提前停车

在本练习中，您将学习如何在 Keras 深度学习模型上实现早期停止。我们将使用的数据集是一个模拟数据集，它代表了树木的各种测量值，如高度、树枝数量和底部树干的周长。我们的目标是根据给定的测量值将记录分为落叶树或针叶树。

首先，执行下面的代码块来加载一个由`10000`记录组成的模拟数据集，该数据集由代表两个树种的两个类组成，落叶树种的类值为`1`，针叶树种的类值为`0`。每条记录都有`10`特征值。

目标是建立一个模型，以便在给定树的测量值时预测树的种类。现在，让我们来完成这些步骤:

1.  使用熊猫`read_csv`函数加载数据集，并使用`train_test_split`函数

    ```
    # Load the data
    import pandas as pd
    X = pd.read_csv('../data/tree_class_feats.csv')
    y = pd.read_csv('../data/tree_class_target.csv')
    # Split the dataset into training set and test set with an 80-20 ratio
    from sklearn.model_selection import train_test_split
    seed=1
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)
    ```

    在`80-20`分割中分割数据集
2.  导入所有必要的依赖项。构建一个三层 Keras 序列模型，不需要提前停止。第一层会有`16`单元，第二层会有`8`单元，第三层会有`4`单元，都有`ReLU activation`功能。添加带`sigmoid activation function` :

    ```
    # Define your model
    from keras.models import Sequential
    from keras.layers import Dense, Activation
    import numpy as np
    from tensorflow import random
    np.random.seed(seed)
    random.set_seed(seed)
    model_1 = Sequential()
    model_1.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))
    model_1.add(Dense(8, activation='relu'))
    model_1.add(Dense(4, activation='relu'))
    model_1.add(Dense(1, activation='sigmoid'))
    ```

    的`output layer`
3.  用作为二元交叉熵的`loss`函数和作为`SGD`的优化器编译模型。用`batch_size=50`为`300`时期训练模型，同时在每次迭代

    ```
    model_1.compile(optimizer='sgd', loss='binary_crossentropy')
    # train the model
    history = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=300, batch_size=50, verbose=0, shuffle=False)
    ```

    时存储`training error`和`test error`
4.  导入绘图所需的包:

    ```
    import matplotlib.pyplot as plt 
    import matplotlib
    %matplotlib inline
    ```

5.  Plot the `training error` and `test error` that are stored in the variable that was created during the fitting process:

    ```
    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) 
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.ylim(0,1)
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train loss', 'validation loss'], loc='upper right')
    ```

    下面是预期的输出:

    ![Figure 5.9: Plot of the training error and validation error while training the model 
    without early stopping
    ](image/B15777_05_09.jpg)

    图 5.9:在没有提前停止的情况下训练模型时，训练误差和验证误差的曲线图

    从前面的图中可以看出，为`300`时期训练模型会导致`training error`和`validation error`之间的差距增大，这表明过度拟合开始发生。

6.  Redefine the model by creating the model with the same number of layers and with the same number of units within each layer. This ensures the model is initialized in the same way. Add a callback `es_callback = EarlyStopping(monitor='val_loss', mode='min')` to the training process. Repeat *step 4* to plot the training error and validation error:

    ```
    #Define your model with early stopping on test error
    from keras.callbacks import EarlyStopping
    np.random.seed(seed)
    random.set_seed(seed)
    model_2 = Sequential()
    model_2.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))
    model_2.add(Dense(8, activation='relu'))
    model_2.add(Dense(4, activation='relu'))
    model_2.add(Dense(1, activation='sigmoid'))
    #  Choose the loss function to be binary cross entropy and the optimizer to be SGD for training the model
    model_2.compile(optimizer='sgd', loss='binary_crossentropy')
    # define the early stopping callback
    es_callback = EarlyStopping(monitor='val_loss', mode='min')
    # train the model
    history=model_2.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=300, batch_size=50, callbacks=[es_callback], verbose=0, shuffle=False)
    ```

    下面是预期的输出:

    ![Figure 5.10: Plot of training error and validation error while training the model 
    with early stopping (patience=0)
    ](image/B15777_05_10.jpg)

    图 5.10:训练提前停止模型(耐心=0)时的训练误差和验证误差图

    通过将带有`patience=0`的提前停止回调添加到模型中，训练过程在大约`39`个时期后自动停止。

7.  Repeat *step 5* while adding `patience=10` to your early stopping callback. Repeat *step 3* to plot the `training error` and `validation error`:

    ```
    #Define your model with early stopping on test error with patience=10
    from keras.callbacks import EarlyStopping
    np.random.seed(seed)
    random.set_seed(seed)
    model_3 = Sequential()
    model_3.add(Dense(16, activation='relu', input_dim=X_train.shape[1]))
    model_3.add(Dense(8, activation='relu'))
    model_3.add(Dense(4, activation='relu'))
    model_3.add(Dense(1, activation='sigmoid'))
    # Choose the loss function to be binary cross entropy and the optimizer to be SGD for training the model
    model_3.compile(optimizer='sgd', loss='binary_crossentropy')
    # define the early stopping callback
    es_callback = EarlyStopping(monitor='val_loss', mode='min', patience=10)
    # train the model
    history=model_3.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=300, batch_size=50, callbacks=[es_callback], verbose=0, shuffle=False)
    ```

    下面是预期的输出:

![Figure 5.11: Plot of training error and validation error while training the model with early stopping (patience=10)
](image/B15777_05_11.jpg)

图 5.11:训练提前停止模型(耐心=10)时的训练误差和验证误差图

通过将带有`patience=10`的提前停止回调添加到模型中，训练过程在大约`150`个时期后自动停止。

在本练习中，您学习了如何停止模型以防止 Keras 模型过度拟合训练数据。为此，您利用了`EarlyStopping`回调，并用它来训练模型。我们使用这个回调在验证损失增加的任何时候停止模型，并添加了一个`patience`参数，它在停止之前等待给定数量的时期。我们在一个涉及交通流量数据集的问题上练习使用这个回调来训练我们的 Keras 模型。在下一节中，我们将讨论可用于防止过度拟合的其他正则化方法。

## 数据增强

数据扩充是一种正则化技术，试图通过以一种廉价的方式在更多的训练示例上训练模型来解决过度拟合问题。在数据扩充中，可用数据以不同的方式进行转换，并作为新的训练数据提供给模型。这种类型的正则化已经被证明是有效的，特别是对于一些特定的应用，例如计算机视觉和语音处理中的对象检测/识别。

例如，在计算机视觉应用程序中，通过向数据集添加每个图像的镜像版本和旋转版本，您可以简单地将训练数据集的大小增加一倍或三倍。由这些转换生成的新训练样本显然不如原始训练样本好。然而，它们显示出在过度拟合方面改进了模型。

执行数据扩充的一个挑战性方面是选择要对数据执行的正确转换。根据数据集的类型和应用程序，需要仔细选择转换。

## 添加噪声

通过向数据添加噪声来正则化模型背后的基本思想与数据扩充正则化的思想相同。在小数据集上训练深度神经网络增加了网络记忆单个数据示例的机会，而不是捕捉输入和输出之间的关系。

这将导致以后新数据的性能很差，这表明模型过度拟合了训练数据。相比之下，在大型数据集上训练模型增加了模型捕获真实底层过程的机会，而不是记住单个数据点，因此减少了过度拟合的机会。

扩展训练数据并减少过度拟合的一种方法是通过向可用数据中注入噪声来生成新的数据示例。这种类型的正则化已经显示出将过拟合减少到与加权正则化技术相当的程度。

通过向训练数据添加单个示例的不同版本(每个版本都是通过向原始示例添加少量噪声而创建的)，我们可以确保模型不会符合数据中的噪声。此外，通过包括这些修改的示例来增加训练数据集的大小为模型提供了底层数据生成过程的更好表示，并且增加了模型学习真实过程的机会。

在深度学习应用程序中，您可以通过向隐藏层的权重或激活、网络的梯度甚至输出层添加噪声，以及向训练示例(输入层)添加噪声来提高模型性能。决定在深度神经网络中的何处添加噪声是另一个挑战，需要通过尝试不同的网络并观察结果来解决。

在 Keras 中，您可以轻松地将噪波定义为一个层，并将其添加到您的模型中。例如，要将`standard deviation`为`0.1`(平均值等于`0`)的`Gaussian noise`添加到您的模型中，您可以这样写:

```
from keras.layers import GaussianNoise
model.add(GaussianNoise(0.1))
```

以下代码将把`Gaussian noise`添加到模型的第一个隐藏层的输出/激活中:

```
model = Sequential()
model.add(Dense(4, input_dim=30, activation='relu'))
model.add(GaussianNoise(0.01))
model.add(Dense(4, activation='relu'))
model.add(Dense(4, activation='relu'))
model.add(Dense(1, activation='sigmoid')) 
```

在本节中，您学习了三种正则化方法:`early stopping`、`data augmentation`和`adding noise`。除了它们的基本概念和程序之外，您还了解了它们如何减少过度拟合，并获得了一些关于如何使用它们的提示和建议。在下一节中，您将学习如何使用 scikit-learn 提供的函数来调优超参数。通过这样做，我们可以将 Keras 模型合并到 scikit-learn 工作流中。

# 使用 scikit-learn 调整超参数

超参数调整是提高深度学习模型性能的一项非常重要的技术。在*第 4 章*、*使用 Keras 包装器通过交叉验证评估您的模型*中，您了解了如何使用带有 scikit-learn 的 Keras 包装器，这允许在 scikit-learn 工作流中使用 Keras 模型。因此，scikit-learn 中可用的不同通用机器学习和数据分析工具和方法可以应用于 Keras 深度学习模型。这些方法包括 scikit-learn 超参数优化器。

在前一章中，您学习了如何通过编写用户定义的函数来循环每个超参数的可能值，从而执行超参数调整。在本节中，您将学习如何通过使用 scikit-learn 中提供的各种超参数优化方法，以更简单的方式执行该操作。您还将通过完成一个涉及真实数据集的活动来练习应用这些方法。

## 使用 scikit-learn 进行网格搜索

到目前为止，我们已经确定，建立深度神经网络涉及对几个超参数做出决定。超参数列表包括隐藏层的数量、每个隐藏层中的单元数量、每个层的激活函数、网络的损失函数、优化器的类型及其参数、正则化器的类型及其参数、批量大小、时期的数量等。我们还观察到不同的超参数值会显著影响模型的性能。

因此，找到超参数的最佳值是成为深度学习专家的最重要和最具挑战性的部分之一。由于没有为每个数据集和每个问题挑选超参数的绝对规则，因此需要针对每个特定问题通过反复试验来决定超参数的值。使用不同的超参数训练和评估模型并根据模型性能决定最终超参数的过程称为超参数调整或超参数优化。

对于我们感兴趣的每个超参数，有一个范围或一组可能的值可以创建一个网格，如下图所示。因此，超参数调优可以看作是一个网格搜索问题；我们希望尝试网格中的每个单元(超参数的每个可能组合),并找到为模型带来最佳性能的一个单元:

![Figure 5.12: A hyperparameter grid created by some values for optimizer, batch_size, and epochs
](image/B15777_05_12.jpg)

图 5.12:由 optimizer、batch_size 和 epochs 的一些值创建的超参数网格

Scikit-learn 提供了一个名为`GridSearchCV()`的参数优化器来执行这种彻底的网格搜索。`GridSearchCV()`接收模型作为`estimator`参数，接收包含超参数所有可能值的字典作为`param_grid`参数。然后，它遍历网格中的每个点，使用该点的超参数值对模型执行交叉验证，并返回最佳交叉验证分数，以及导致该分数的超参数值。

在前一章中，您了解到为了在 scikit-learn 中使用 Keras 模型，您需要定义一个返回 Keras 模型的函数。例如，下面的代码块定义了一个 Keras 模型，我们希望稍后对其执行超参数调整:

```
from keras.models import Sequential
from keras.layers import Dense
def build_model():
    model = Sequential(optimizer)
    model.add(Dense(10, input_dim=X_train.shape[1], activation='relu'))
    model.add(Dense(10, activation='relu'))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer= optimizer)
    return model
```

下一步是定义参数网格。例如，假设我们想要调谐`optimizer=['rmsprop', 'adam', 'sgd', 'adagrad']`、`epochs = [100, 150]`、`batch_size = [1, 5, 10]`。为此，我们将编写以下代码:

```
optimizer = ['rmsprop', 'adam', 'sgd', 'adagrad']
epochs = [100, 150]
batch_size = [1, 5, 10]
param_grid = dict(optimizer=optimizer, epochs=epochs, batch_size= batch_size)
```

现在已经创建了超参数网格，我们可以创建包装器，以便为 Keras 模型构建接口，并将其用作执行网格搜索的估计器:

```
from keras.wrappers.scikit_learn import KerasRegressor
model = KerasRegressor(build_fn=build_model, verbose=0, shuffle=False)
from sklearn.model_selection import GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=10)
results = grid_search.fit(X, y)
```

前面的代码遍历网格中的每个单元格，并使用每个单元格中的超参数值执行 10 次交叉验证(这里，它执行`10-fold cross-validation` 4*2*3=24 次)。然后，它返回每个`24`单元格的交叉验证分数，以及产生最佳分数的单元格。

注意

对超参数的许多可能组合执行 k-fold 交叉验证肯定需要很长时间。出于这个原因，您可以通过将参数`n_jobs=-1`传递给`GridSearchCV()`来并行化这个过程，这将导致使用每个可用的处理器来执行网格搜索。该参数的默认值是`n_jobs=1`，这意味着没有并行化。

创建超参数格网只是迭代超参数以找到最佳选择的一种方式。另一种方法是简单地随机选择超参数，我们将在下一个主题中学习。

## 使用 scikit-learn 进行随机搜索

正如你可能已经意识到的，彻底的网格搜索可能不是调整深度学习模型的超参数的最佳选择，因为它不是非常有效。深度学习中有许多超参数，特别是如果你想为每个参数尝试大范围的值，彻底的网格搜索将需要太长时间才能完成。执行超参数优化的另一种方法是在网格上执行随机采样，并在一些随机选择的单元上执行 k-fold 交叉验证。Scikit-learn 提供了一个名为`RandomizedSearchCV()`的优化器来执行随机搜索，以达到超参数优化的目的。

例如，我们可以将上一节中的代码从穷举网格搜索改为随机搜索，如下所示:

```
from keras.wrappers.scikit_learn import KerasRegressor
model = KerasRegressor(build_fn=build_model, verbose=0)
from sklearn.model_selection import RandomizedSearchCV
grid_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, cv=10, n_iter=12)
results = grid_search.fit(X, y)
```

注意，`RandomizedSearchCV()`需要额外的`n_iter`参数，它决定了必须选择多少个随机单元格。这决定了将执行多少次 k-fold 交叉验证。因此，通过选择较小的数字，将考虑较少的超参数组合，并且该方法将花费较少的时间来完成。另外，请注意这里的`param_grid`参数被改为`param_distributions`。`param_distributions`参数可以接受一个字典，其中参数名作为键，参数列表或分布作为每个键的值。

可以认为`RandomizedSearchCV()`不如`GridSearchCV()`好，因为它没有考虑超参数的所有可能值和值的组合，这是合理的。因此，为深度学习模型执行超参数调整的一种聪明的方法是从许多超参数上的`RandomizedSearchCV()`开始，或者从更少的超参数上的`GridSearchCV()`开始，这些超参数之间的差距更大。

通过对许多超参数进行随机搜索，我们可以确定哪些超参数对模型的性能影响最大。它还可以帮助缩小重要超参数的范围。然后，您可以通过对较少数量的超参数和每个超参数的较小范围执行`GridSearchCV()`来完成您的超参数调整。这被称为超参数调谐的粗到细方法。

现在，您已经准备好练习使用 scikit-learn 优化器实现超参数调优了。在下一个活动中，您将尝试通过调整超参数来改进糖尿病数据集的模型。

## 活动 5 .03:在 Avila 模式分类器上调整超参数

阿维拉数据集是从阿维拉圣经的图像中提取的，阿维拉圣经是一部巨大的 12 世纪拉丁文圣经。数据集由关于文本图像的各种特征组成，例如文本的列间距离和边距。数据集还包含一个类别标签，该标签指示图像的模式是否属于最频繁出现的类别。在本活动中，您将构建一个类似于前面活动中的 Keras 模型，但这一次，您还将向模型中添加正则化方法。然后，您将使用 scikit-learn 优化器对模型超参数执行调优，包括正则化子的超参数。以下是您在本活动中需要完成的步骤:

1.  使用`X = pd.read_csv('../data/avila-tr_feats.csv')`和`y = pd.read_csv('../data/avila-tr_target.csv')`从 GitHub 的`Chapter05`文件夹的`data`子文件夹中加载数据集。
2.  定义一个函数，返回一个带有三个隐藏层的 Keras 模型，第一个是`size 10`，第二个是`size 6`，第三个是`size 4`，都带有`L2 weight regularizations`。使用这些值作为模型的超参数:`activation='relu'`、`loss='binary_crossentropy'`、`optimizer='sgd'`和`metrics=['accuracy']`。此外，确保将`L2 lambda`超参数作为参数传递给函数，以便我们稍后可以对其进行调优。
3.  为您的 Keras 模型创建包装器，并使用`cv=5`对其执行`GridSearchCV()`。然后，在参数网格中添加以下值:`lambda_parameter = [0.01, 0.5, 1]`、`epochs = [50, 100]`和`batch_size = [20]`。这可能需要一些时间来处理。参数搜索完成后，打印最佳交叉验证分数的准确度和超参数。您还可以打印每隔一个交叉验证分数，以及产生该分数的超参数。
4.  重复上一步，这次用`lambda_parameter = [0.001, 0.01, 0.05, 0.1]`、`epochs = [400]`和`batch_size = [10]`在更窄的范围内使用`GridSearchCV()`。可能需要一些时间来处理。
5.  重复上一步，但是从你的 Keras 模型中移除`L2 regularizers`，而不是在每个隐藏层添加带有`rate`参数的退出正则化。使用参数网格中的以下值对模型执行`GridSearchCV()`，并打印结果: `rate = [0, 0.2, 0.4]`、`epochs = [350, 400]`和`batch_size = [10]`。
6.  使用`rate = [0.0, 0.05, 0.1]`和`epochs=[400]`重复上一步。

实现这些步骤后，您应该会看到以下预期输出:

```
Best cross-validation score= 0.7862895488739013
Parameters for Best cross-validation score= {'batch_size': 20, 'epochs': 100, 'rate': 0.0}
Accuracy 0.786290 (std 0.013557) for params {'batch_size': 20, 'epochs': 100, 'rate': 0.0}
Accuracy 0.786098 (std 0.005184) for params {'batch_size': 20, 'epochs': 100, 'rate': 0.05}
Accuracy 0.772004 (std 0.013733) for params {'batch_size': 20, 'epochs': 100, 'rate': 0.1}
```

注意

这项活动的解决方案可在第 378 页找到。

在本活动中，我们学习了如何使用正则化器在 Keras 模型上实现超参数调整，以使用真实数据集执行分类。我们学习了如何使用 scikit-learn 优化器对模型超参数进行调优，包括正则化子的超参数。在本节中，我们通过创建超参数网格并遍历它们来实现超参数调优。这使我们能够使用 scikit-learn 工作流程找到最佳的超参数集。

# 总结

在这一章中，你学习了两组非常重要的技术来提高深度学习模型的准确性:正则化和超参数调整。您了解了正则化如何通过几种不同的方法来帮助解决过拟合问题，包括 L1 和 L2 范数正则化以及丢弃正则化(更常用的正则化技术)。您发现了超参数调整对机器学习模型的重要性，尤其是对深度学习模型的超参数调整的挑战。您甚至练习了使用 scikit-learn 优化器在 Keras 模型上执行超参数调优。

在下一章中，您将探索评估模型性能时准确性指标的局限性，以及其他指标(如`precision`、`sensitivity`、`specificity`和`AUC-ROC score`)，包括如何使用它们来更好地评估模型性能的质量。