<title>B15777_06_Final_SP_ePub</title> <link href="css/epub.css" rel="stylesheet" type="text/css">

# 6.模型评估

概观

本章将深入讨论模型评估。当标准技术不可行时，我们将讨论准确性的替代方法来评估模型的性能，尤其是在存在不平衡类的情况下。最后，我们将利用混淆矩阵、灵敏度、特异性、精确度、FPR、ROC 曲线和 AUC 分数来评估分类器的性能。在本章结束时，您将对精度和零精度有深入的了解，并将能够理解和应对不平衡数据集的挑战。

# 简介

在前一章中，我们介绍了神经网络的`regularization`技术。`Regularization`是一项重要的技术，用于应对模型如何过度拟合训练数据，并帮助模型在新的、看不见的数据示例上表现良好。我们讨论的正则化技术之一涉及`L1`和`L2`权重正则化，其中惩罚被添加到权重中。我们了解的另一个正则化技术是`dropout regularization`，在每次迭代中，一些层的单元被随机从模型拟合过程中移除。这两种正则化技术都旨在通过强烈影响单个权重或单位并允许它们进行概化来防止它们。

在本章中，我们将学习除了`accuracy`之外的一些不同的评估技术。对于任何一个数据科学家来说，建立模型后的第一步就是评估模型，而评估模型最简单的方法就是通过它的准确性。但是，在现实世界中，尤其是在存在类别高度不平衡的分类任务的情况下，例如预测飓风的出现、预测罕见疾病的出现或预测某人是否会拖欠贷款，使用模型的准确度分数来评估模型并不是最佳的评估技术。

本章探讨了不平衡数据集等核心概念，以及如何使用不同的评估技术来处理这些不平衡数据集。本章首先介绍准确性及其局限性。然后，我们将探讨`null accuracy`、`imbalanced datasets`、`sensitivity`、`specificity`、`precision`、`false positives`、`ROC curves`、`AUC scores`的概念。

# 精确度

为了正确理解准确性，让我们探讨一下模型评估。模型评估是模型开发过程不可或缺的一部分。一旦你建立并执行了你的模型，下一步就是评估你的模型。

一个模型建立在一个`training dataset`之上，在相同的训练数据集上评估一个模型的性能在数据科学中是一种不好的做法。一旦在训练数据集上对模型进行了训练，就应该在与训练数据集完全不同的数据集上对其进行评估。这个数据集被称为`test dataset`。目标应该始终是构建一个概化的模型，这意味着该模型应该在任何数据集上产生相似(但不相同)的结果，或者相对相似的结果。这只有在我们根据未知数据评估模型时才能实现。

模型评估过程需要一个可以量化模型性能的指标。模型评估最简单的标准是准确性。`Accuracy`是我们的模型预测正确的比例。这是计算`accuracy`的公式:

*准确度=(正确预测数)/(预测总数)*

例如，如果我们有`10`记录，并且`7`被正确预测，那么我们可以说我们模型的精度是`70%`。这被计算为`7/10` = `0.7`或`70%`。

`Null accuracy`是预测最频繁类所能达到的精度。如果我们不运行算法，只根据最频繁的结果预测准确度，那么根据这种预测计算的准确度称为`null accuracy`:

*空精度=(频繁出现类的实例总数)/(实例总数)*

看一下这个例子:

10 个实际结果:[1，0，0，0，0，0，0，1，0]。

`Prediction` : [0，0，0，0，0，0，0，0，0，0]

`Null accuracy` = 8/10 = 0.8 或 80%

所以，我们的零精度是`80%`，这意味着我们的时间是正确的`80%`。这意味着我们已经在不运行算法的情况下达到了`80%`精度。请始终记住，当空精度较高时，这意味着响应变量的分布偏向于频繁出现的类。

让我们做一个练习来找出数据集的零精度。数据集的空精度可以通过使用 pandas 库中的`value_count`函数找到。`value_count`函数返回包含唯一值计数的序列。

注意

本章所有练习和活动的 Jupyter 笔记本都可以在 GitHub 上的[https://packt.live/37jHNUR](https://packt.live/37jHNUR)获得。

## 练习 6.01:计算太平洋飓风数据集的零精度

我们有一个数据集记录是否在太平洋中观察到了一个`hurricane`，它有两列，`Date`和`hurricane`。`Date`列表示观察的日期，而`hurricane`列表示那天是否有飓风。`hurricane`值为`1`的行表示有飓风，而`0`表示没有飓风。按照以下步骤找到数据集的`null accuracy`:

1.  Open a Jupyter notebook. Import all the required libraries and load the `pacific_hurricanes.csv` file into the `data` folder from this book's GitHub repository:

    ```
    # Import the data
    import pandas as pd
    df = pd.read_csv("../data/pacific_hurricanes.csv")
    df.head() 
    ```

    以下是上述代码的输出:

    ![Figure 6.1: Data exploration of the pacific hurricanes dataset
    ](image/B15777_06_01.jpg)

    图 6.1:太平洋飓风数据集的数据探索

2.  Use the built-in `value_count` function from the pandas library to get the distribution for the data of the `hurricane` column. The `value_count` function shows the total instances of unique values:

    ```
    df['hurricane'].value_counts()
    ```

    上述代码产生以下输出:

    ```
    0 22435
    1 1842
    Name: hurricane, dtype: int64
    ```

3.  Use the `value_count` function and set the `normalize` parameter to `True`. To find the null accuracy, you will have to index the `pandas` series that was produced for index `0` to get the proportion of values related to no hurricanes occurring on a given day:

    ```
    df['hurricane'].value_counts(normalize=True).loc[0]
    ```

    上述代码产生以下输出:

    ```
    0.9241257156979857
    ```

    数据集的计算出的`null accuracy`为`92.4126%`。

在这里，我们可以看到我们的数据集具有非常高的空精度`92.4126%`。因此，如果我们只是建立一个预测所有结果的多数类的哑模型，我们的模型将是`92.4126%`准确的。在本章后面的*活动 6.01* 、*中，当我们改变训练/测试分割时计算神经网络的准确度和零准确度，*我们将看到当我们改变`test` / `train`分割时零准确度如何变化。

## 准确性的优势和局限性

**精度的优势如下:**

*   **易于使用**:精确度非常容易计算和理解，因为它只是一个简单的分数公式。
*   **与其他技术相比更受欢迎**:因为它是最容易计算的指标，所以也是最受欢迎的，并且被普遍认为是评估模型的第一步。大多数关于数据科学的入门书籍都将准确性作为一种评估指标来教授。
*   **适合比较不同的模型**:假设你正在尝试用不同的模型解决一个问题。你总是可以信任给出最高准确度的模型。

**精确度的限制如下**:

*   **没有反应变量分布**的表示:精确度没有给我们一个`response` / `dependent`变量分布的概念。如果我们在模型中获得了`80%`的精度，我们就不知道响应变量是如何分布的，也不知道数据集的零精度是多少。如果我们的数据集的零精度高于`70%`，那么`80%`精确模型就没什么用了。
*   **第一类和第二类误差** : `Accuracy`也没有给出模型的`type 1`和`type 2`误差的信息。一个`type 1`错误是当一个类是`negative`并且我们已经预测为`positive`时，而一个`type 2`错误是当一个类是正数并且我们已经预测为负数时。我们将在本章的后面研究这两个错误。在下一节中，我们将介绍不平衡数据集。对不平衡数据集进行分类的模型的准确度分数可能特别容易误导，这就是为什么其他评估度量对于模型评估是有用的。

# 不平衡的数据集

不平衡数据集是分类问题的一种特殊情况，在这种情况下，类之间的类分布会发生变化。在这样的数据集中，一个类占绝对优势。换句话说，不平衡数据集的`null accuracy`非常高。

考虑一个信用卡欺诈的例子。如果我们有一个信用卡交易的数据集，那么我们会发现，在所有的交易中，极少数交易是欺诈性的，而大多数交易是正常的交易。如果`1`代表欺诈交易，`0`代表正常交易，那么 0 会很多，1 几乎没有。数据集的`null accuracy`可能大于`99%`。这意味着多数阶级(在这种情况下，`0`)压倒性地大于少数阶级(在这种情况下，`1`)。这样的数据集是不平衡的数据集。考虑下图，它显示了一个总体不平衡的数据集`scatter plot`:

![Figure 6.2: A general imbalanced dataset scatter plot
](image/B15777_06_02.jpg)

图 6.2:总体不平衡数据集散点图

上图显示了不平衡数据集的概化散点图，其中星号表示少数类，圆圈表示多数类。正如我们所看到的，圆圈比星星多得多；这使得机器学习模型很难区分这两个类别。在下一节中，我们将介绍一些处理不平衡数据集的方法。

## 处理不平衡的数据集

在机器学习中，有两种方法可以克服不平衡数据集的缺点，如下所示:

*   **采样技术**:我们可以缓解数据集不平衡的一种方法是使用特殊的采样技术，通过这种技术，我们可以选择训练和测试数据，从而充分代表所有类别。有许多这样的技术—例如，对少数类进行过采样(意味着我们从少数类中获取更多样本)或对多数类进行欠采样(意味着我们从多数类中获取较小样本)。然而，如果数据高度不平衡，零精度高于`90%`，那么采样技术很难给出数据中多数-少数类的正确表示，我们的模型可能会过拟合。所以，最好的方法是修改我们的评估技术。
*   **修改模型评估技术**:当处理高度不平衡的数据集时，最好修改模型评估技术。这是获得好结果的最可靠的方法，这意味着使用这些方法可能会在新的、看不见的数据上获得好结果。除了准确性之外，还有许多评估指标可以修改来评估模型。要了解所有这些技术，理解混淆矩阵的概念是很重要的。

# 混乱矩阵

A `confusion matrix`描述了分类模型的性能。换句话说，混淆矩阵是总结分类器性能的一种方式。下表显示了混淆矩阵的基本表示，并显示了模型的预测结果与真实值的比较情况:

![Figure 6.3: Basic representation of a confusion matrix
](image/B15777_06_03.jpg)

图 6.3:混淆矩阵的基本表示

让我们回顾一下上表中使用的缩写的含义:

*   **TN** ( **真阴性**):这是最初为阴性并被预测为阴性的结果的计数。
*   **FP** ( **假阳性**):这是最初为阴性但预测为阳性的结果的计数。该错误也被称为**类型 1 错误**。
*   **FN** ( **假阴性**):这是最初为阳性但被预测为阴性的结果的计数。该错误也被称为**类型 2 错误**。
*   **TP** ( **真阳性**):这是最初为阳性并被预测为阳性的结果的计数。

目标是最大化上表中 **TN** 和 **TP** 框中的值，即真阴性和真阳性，最小化 **FN** 和 **FP** 框中的值，即假阴性和假阳性。

以下代码是混淆矩阵的一个示例:

```
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test,y_pred_class)
print(cm)
```

上述代码产生以下输出:

```
array([[89, 2],
       [13, 4]], dtype=int64)
```

所有机器学习和深度学习算法的目的都是最大化 TN 和 TP，最小化 FN 和 FP。以下示例代码计算 TN、FP、FN 和 TP:

```
# True Negative
TN = cm[0,0]
# False Negative
FN = cm[1,0]
# False Positives
FP = cm[0,1]
# True Positives
TP = cm[1,1]
```

注意

准确性不能帮助我们理解第一类和第二类错误。

## 根据混淆矩阵计算的指标

可以从`confusion matrix`中导出的指标有`sensitivity`、`specificity`、`precision`、`FP rate`、`ROC`和`AUC`:

*   **Sensitivity**: This is the number of positive predictions divided by the total actual number of positives. Sensitivity is also known as recall or true positive. In our case, it is the total number of patients classified as `1`, divided by the total number of patients who are actually `1`:

    *灵敏度= TP / (TP+FN)*

    敏感度是指当实际值为正时，预测正确的频率。在一些情况下，比如建立一个模型来预测病人在医院的再入院，我们需要我们的模型是高度敏感的。我们需要 1 被预测为`1`。如果一个`0`被预测为`1`，这是可以接受的，但是如果一个`1`被预测为`0`，这意味着一个被再次入院的病人被预测为不被再次入院，这将对医院造成严重的处罚。

*   **Specificity**: This is the number of negative predictions divided by the total number of actual negatives. To use the previous example, it would be readmission predicted as `0` divided by the total number of patients who were actually `0`. Specificity is also known as the true negative rate:

    *特异性= TN / (TN+FP)*

    特异性是指当实际值为负值时，预测正确的频率。有些情况下，比如垃圾邮件检测，我们需要我们的算法更加具体。该模型预测电子邮件何时是垃圾邮件，何时不是垃圾邮件。我们希望该模型一如既往地预测`0``0`，因为如果一封非垃圾邮件被归类为垃圾邮件，重要的邮件可能会最终出现在垃圾邮件文件夹中。敏感性可能会受到影响，因为一些垃圾邮件可能会到达我们的收件箱，但非垃圾邮件不应进入垃圾邮件文件夹。

    注意

    正如我们之前所讨论的，一个模型应该是敏感的还是特定的完全取决于业务问题。

*   **Precision**: This is the true positive prediction divided by the total number of positive predictions. Precision refers to how often we are correct when the value predicted is positive:

    *精度= TP / (TP+FP)*

*   **False Positive Rate** (**FPR**): The `FPR` is calculated as the ratio between the number of false-positive events and the total number of actual negative events. `FPR` refers to how often we are incorrect when the actual value is negative. `FPR` is also equal to `1` - specificity:

    *假阳性率= FP / (FP+TN)*

*   **接收机工作特性** ( **ROC** ) **曲线**:评估分类模型的另一个重要方法是使用`ROC curve`。A `ROC curve`是真阳性率(`sensitivity`)和`FPR` ( `1 - specificity`)之间的曲线。下图显示了一个`ROC curve`的例子:

![Figure 6.4: An example of ROC curve
](image/B15777_06_04.jpg)

图 6.4:ROC 曲线示例

为了决定多条曲线中哪条`ROC curve`是最好的，我们需要查看曲线左上方的空白区域——空间越小，结果越好。下图显示了多个`ROC curves`的示例:

![Figure 6.5: An example of multiple ROC curves
](image/B15777_06_05.jpg)

图 6.5:多重 ROC 曲线的例子

注意

红色曲线比蓝色曲线更好，因为它在左上角留下的空间更少。

一个模型的`ROC curve`告诉我们`sensitivity`和`specificity`之间的关系。

*   **Area Under Curve** (**AUC**): This is the area under the `ROC curve`. Sometimes, `AUC` is also written as `AUROC`, meaning the area under the `ROC curve`. Basically, `AUC` is a numeric value that represents the area under a `ROC curve`. The larger the area under the `ROC`, the better, and the bigger the `AUC score`, the better. The preceding plot shows us an example of an `AUC`.

    在上图中，红色曲线的`AUC`大于蓝色曲线的`AUC`，这意味着红色曲线的`AUC`优于蓝色曲线的 AUC。`AUC score`没有标准规则，但这里有一些普遍接受的值，以及它们与模型质量的关系:

![Figure 6.6: General acceptable AUC score
](image/B15777_06_06.jpg)

图 6.6:一般可接受的 AUC 分数

现在，我们已经了解了各种指标背后的理论，让我们完成一些活动和练习来实施我们所学的内容。

## 练习 6.02:计算 Scania Trucks 数据在 APS 失败时的精确度和零精确度

我们将在本练习中使用的数据集包含从日常使用中以某种方式出现故障的重型斯堪尼亚卡车上收集的数据。焦点系统是`Air pressure system` ( `APS`)，它产生压缩空气，用于卡车的各种功能，如制动和换档。数据集中的正类表示 APS 中特定组件的组件故障，而负类表示与 APS 无关的组件故障。

本练习的目的是预测哪些卡车因 APS 而出现故障，以便维修和维护技工在检查卡车故障原因以及卡车的哪个区域需要检查时，可以使用这些信息。

注意

这个练习的数据集可以从本书位于 https://packt.live/2SGEEsH 的 GitHub 知识库下载。

在整个练习中，由于内部数学运算的随机性，您可能会得到稍微不同的结果。

**数据预处理和探索性数据分析**:

1.  Import the required libraries. Load the dataset using the pandas `read_csv` function and explore the first `five` rows of the dataset:

    ```
    #import the libraries
    import numpy as np
    import pandas as pd
    # Load the Data
    X = pd.read_csv("../data/aps_failure_training_feats.csv")
    y = pd.read_csv("../data/aps_failure_training_target.csv")
    # use the head function view the first 5 rows of the data
    X.head()
    ```

    下表显示了上述代码的输出:

    ![Figure 6.7: First five rows of the patient readmission dataset
    ](image/B15777_06_07.jpg)

    图 6.7:患者再入院数据集的前五行

2.  Describe the feature values in the dataset using the `describe` method:

    ```
    # Summary of Numerical Data
    X.describe()
    ```

    下表显示了上述代码的输出:

    ![Figure 6.8: Numerical metadata of the patient readmission dataset
    ](image/B15777_06_08.jpg)

    图 6.8:患者再入院数据集的数字元数据

    注意

    自变量也称为解释变量，因变量也称为`response variables`。另外，记住 Python 中的索引是从`0`开始的。

3.  Explore `y` using the `head` function:

    ```
    y.head()
    ```

    下表显示了上述代码的输出:

    ![Figure 6.9: The first five rows of the y variable of the patient readmission dataset
    ](image/B15777_06_09.jpg)

    图 6.9:患者再入院数据集 y 变量的前五行

4.  使用 scikit-learn 库中的`train_test_split`函数将数据分成测试集和训练集。为了确保我们得到相同的结果，将`random_state`参数设置为`42`。数据用`80:20 ratio`分割，意思是数据的`80%`是`training data`，剩余的`20%`是`test data` :

    ```
    from sklearn.model_selection import train_test_split
    seed = 42
    X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.20, random_state=seed)
    ```

5.  Scale the training data using the `StandardScaler` function and use the scaler to scale the `test data`:

    ```
    # Initialize StandardScaler
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    # Transform the training data
    X_train = sc.fit_transform(X_train)
    X_train = pd.DataFrame(X_train,columns=X_test.columns)
    # Transform the testing data
    X_test = sc.transform(X_test)
    X_test = pd.DataFrame(X_test,columns=X_train.columns)
    ```

    注意

    `sc.fit_transform()`函数转换数据，数据被转换成一个`NumPy`数组。我们可能需要在 DataFrame 对象中进一步分析数据，所以`pd.DataFrame()`函数将数据重新转换成 DataFrame。

    这就完成了本练习的数据预处理部分。现在，我们需要建立一个神经网络，并计算出`accuracy`。

6.  导入创建神经网络架构所需的库:

    ```
    # Import the relevant Keras libraries
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.layers import Dropout
    from tensorflow import random
    ```

7.  启动`Sequential`类:

    ```
    # Initiate the Model with Sequential Class
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential()
    ```

8.  添加`Dense`类的`five`隐藏层，并在每层后添加`Dropout`。构建第一个隐藏层，使其大小为`64`，辍学率为`0.5`。第二隐藏层将具有尺寸`32`和丢失率`0.4`。第三个隐藏层的大小为`16`，丢失率为`0.3`。第四个隐藏层的大小为`8`，丢失率为`0.2`。最终隐藏层的大小为`4`，丢失率为`0.1`。每个隐藏层将有一个`ReLU activation`函数，内核初始化器将被设置为`uniform` :

    ```
    # Add the hidden dense layers and with dropout Layer
    model.add(Dense(units=64, activation='relu', kernel_initializer='uniform', input_dim=X_train.shape[1]))
    model.add(Dropout(rate=0.5))
    model.add(Dense(units=32, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.4))
    model.add(Dense(units=16, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.3))
    model.add(Dense(units=8, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.2))
    model.add(Dense(units=4, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.1))
    ```

9.  Add an output `Dense` layer with a `sigmoid` activation function:

    ```
    # Add Output Dense Layer
    model.add(Dense(units=1, activation='sigmoid', kernel_initializer='uniform'))
    ```

    注意

    因为输出是二进制的，所以我们使用了`sigmoid`函数。如果输出是多类的(即超过两个类)，那么应该使用`softmax`函数。

10.  编译网络并拟合模型。通过设置`metrics=['accuracy']` :

    ```
    # Compile the model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    ```

    计算训练过程中的精度
11.  用`100`个时期、`20`个批量和`20%` :

    ```
    #Fit the Model
    model.fit(X_train, y_train, epochs=100, batch_size=20, verbose=1, validation_split=0.2, shuffle=False)
    ```

    个验证分割来拟合模型
12.  Evaluate the model on the `test` dataset:

    ```
    test_loss, test_acc = model.evaluate(X_test, y_test)
    print(f'The loss on the test set is {test_loss:.4f} and the accuracy is {test_acc*100:.4f}%')
    ```

    上述代码产生以下输出:

    ```
    12000/12000 [==============================] - 0s 20us/step
    The loss on the test set is 0.0802 and the accuracy is 98.9917%
    ```

    模型返回的精度为`98.9917%`。但这就足够好了吗？我们只能通过与零精度的比较来得到这个问题的答案。

    **计算零位精度:**

13.  The null accuracy can be calculated using the `value_count` function of the pandas library, which we used in *Exercise 6.01*, *Calculating Null Accuracy on a Pacific Hurricanes Dataset*, of this chapter:

    ```
    # Use the value_count function to calculate distinct class values
     y_test['class'].value_counts()
    ```

    上述代码产生以下输出:

    ```
    0    11788
    1      212
    Name: class, dtype: int64
    ```

14.  Calculate the `null` accuracy:

    ```
    # Calculate the null accuracy 
    y_test['class'].value_counts(normalize=True).loc[0]
    ```

    上述代码产生以下输出:

    ```
    0.9823333333333333
    ```

    这里，我们已经获得了模型的零精度。在我们结束这个练习时，必须注意以下几点:我们的模型的精度大约是`98.9917%`。在理想条件下，`98.9917%`精度非常接近于`good`精度，但是在这里，`null accuracy`是`very high`，这有助于正确看待我们模型的性能。我们这款的`null accuracy`是`98.2333%`。由于模型的`null accuracy`是`so high`，所以`98.9917%`的`accuracy`并不重要，但肯定是值得尊敬的，并且在这种情况下`accuracy`不是评估算法的正确度量。

现在，当我们改变训练/测试分割时，让我们来看一下计算神经网络模型的准确性和零准确性的活动。

## 活动 6.01:当我们改变训练/测试分割时，计算神经网络的精确度和零精确度

训练/测试分割是一种随机抽样技术。在本练习中，我们将看到，更改`train` / `test`分割会影响我们的零精度和精度。要实现这一点，必须更改定义训练/测试分割的代码部分。我们将使用在*练习 6.02* 、*中使用的相同数据集，计算斯堪尼亚卡车数据*的 APS 故障的准确度和零准确度。按照以下步骤完成本活动:

1.  导入所有必要的依赖项并加载数据集。
2.  分别将`test_size`和`random_state`从`0.20`更改为`0.30`，将`42`更改为`13`。
3.  使用`StandardScaler`功能缩放数据。
4.  导入构建神经网络架构所需的库，并启动`Sequential`类。
5.  用`Dropout`添加`Dense`层。设置第一个隐藏层，使其大小为`64`，丢失率为`0.5`，第二个隐藏层，使其大小为`32`，丢失率为`0.4`，第三个隐藏层，使其大小为`16`，丢失率为`0.3`，第四个隐藏层，使其大小为`8`，丢失率为`0.2`，最后一个隐藏层，使其大小为`4`，丢失率为`0.1`。将所有激活功能设置为`ReLU`。
6.  添加一个带有`sigmoid`激活功能的输出`Dense`层。
7.  编译网络并使用精确度拟合模型。用 100 个历元和 20 个批量来拟合模型。
8.  将模型拟合到训练数据，同时保存拟合过程的结果。
9.  在测试数据集上评估模型。
10.  计算测试目标数据集中每个类的值的数量。
11.  Calculate the null accuracy using the pandas `value_count` function.

    注意

    在本练习中，由于内部数学运算的随机性，您可能会得到稍微不同的结果。

在这里，我们可以看到，当我们改变`train` / `test`分割时，精度和零精度也会改变。我们不会在本章中讨论任何采样技术，因为我们有一个非常不平衡的数据集，并且采样技术不会产生任何有成效的结果。

注意

这项活动的解决方案可在第 385 页找到。

让我们继续下一个练习，计算从混淆矩阵中得出的指标。

## 练习 6.03:根据混淆矩阵推导和计算指标

我们将在本练习中使用的数据集包含从日常使用中以某种方式出现故障的重型斯堪尼亚卡车上收集的数据。聚焦的系统是`Air Pressure System` ( `APS`)，它产生压缩空气，用于卡车的各种功能，如制动和换档。数据集中的正类表示 APS 中特定组件的组件故障，而负类表示与 APS 无关的组件故障。

本练习的目的是预测哪些卡车因 APS 而出现故障，这与我们在之前的练习中所做的很相似。我们将推导神经网络模型的灵敏度、特异性、精确度和假阳性率，以评估其性能。最后，我们将调整阈值并重新计算灵敏度和特异性。按照以下步骤完成本练习:

注意

这个练习的数据集可以从本书位于 https://packt.live/2SGEEsH 的 GitHub 知识库下载。

由于内部数学运算的随机性，您可能会得到稍微不同的结果。

1.  导入必要的库，并使用 pandas `read_csv`函数:

    ```
    # Import the libraries
    import numpy as np
    import pandas as pd
    # Load the Data
    X = pd.read_csv("../data/aps_failure_training_feats.csv")
    y = pd.read_csv("../data/aps_failure_training_target.csv")
    ```

    加载数据
2.  接下来，使用`train_test_split`函数将数据分成训练和测试数据集:

    ```
    from sklearn.model_selection import train_test_split
    seed = 42
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=seed)
    ```

3.  接下来，使用`StandardScaler`函数缩放特征数据，使其具有`0`的`mean`和`1`的`standard deviation`。将定标器安装到`training data`上，并将其应用到`test data` :

    ```
    from sklearn.preprocessing import StandardScaler
    sc = StandardScaler()
    # Transform the training data
    X_train = sc.fit_transform(X_train)
    X_train = pd.DataFrame(X_train,columns=X_test.columns)
    # Transform the testing data
    X_test = sc.transform(X_test)
    X_test = pd.DataFrame(X_test,columns=X_train.columns)
    ```

4.  接下来，导入创建模型所需的`Keras`库。实例化一个`Sequential`类的`Keras`模型，并在模型中添加五个隐藏层，包括每层的 dropout。第一个隐藏层的大小应该为`64`，丢失率应该为`0.5`。第二个隐藏层的大小应该是`32`并且丢失率应该是`0.4`。第三个隐藏层的大小应该是`16`并且丢失率应该是`0.3`。第四个隐藏层的大小应该是`8`并且丢失率应该是`0.2`。最终的隐藏层的大小应该为`4`并且丢失率应该为`0.1`。所有隐藏层应具有`ReLU activation`功能，并具有`kernel_initializer = 'uniform'`。用`sigmoid activation`功能给模型添加一个最终输出层。通过在训练过程中计算精度度量来编译模型:

    ```
    # Import the relevant Keras libraries
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.layers import Dropout
    from tensorflow import random
    np.random.seed(seed)
    random.set_seed(seed)
    model = Sequential()
    # Add the hidden dense layers and with dropout Layer
    model.add(Dense(units=64, activation='relu', kernel_initializer='uniform', input_dim=X_train.shape[1]))
    model.add(Dropout(rate=0.5))
    model.add(Dense(units=32, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.4))
    model.add(Dense(units=16, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.3))
    model.add(Dense(units=8, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.2))
    model.add(Dense(units=4, activation='relu', kernel_initializer='uniform'))
    model.add(Dropout(rate=0.1))
    # Add Output Dense Layer
    model.add(Dense(units=1, activation='sigmoid', kernel_initializer='uniform'))
    # Compile the Model
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    ```

5.  接下来，通过用`batch_size=20`和`validation_split=0.2` :

    ```
    Model.fit(X_train, y_train, epochs=100, batch_size=20, verbose=1, validation_split=0.2, shuffle=False)
    ```

    对`100`个时期进行训练，使模型符合训练数据
6.  一旦模型完成对`training data`的拟合，使用模型的`predict`和`predict_proba`方法:

    ```
    y_pred = model.predict(X_test)
    y_pred_prob = model.predict_proba(X_test)
    ```

    创建一个变量，该变量是模型对`test data`的预测结果
7.  Next, compute the predicted class by setting the value of the prediction on the `test set` to `1` if the value is above `0.5` and `0` if it's below `0.5`. Compute the `confusion matrix` using the `confusion_matrix` function from `scikit-learn`:

    ```
    from sklearn.metrics import confusion_matrix
    y_pred_class1 = y_pred > 0.5
    cm = confusion_matrix(y_test, y_pred_class1)
    print(cm)
    ```

    上述代码产生以下输出:

    ```
    [[11730  58]
     [   69 143]]
    ```

    始终使用`y_test`作为第一个参数，使用`y_pred_class1`作为第二个参数，这样您就总能得到正确的结果。

8.  Calculate the true negative (`TN`), false negative (`FN`), false positive (`FP`), and true positive (`TP`):

    ```
    # True Negative
    TN = cm[0,0]
    # False Negative
    FN = cm[1,0]
    # False Positives
    FP = cm[0,1]
    # True Positives
    TP = cm[1,1]
    ```

    注意

    按照这个顺序使用`y_test`和`y_pred_class1`是必要的，因为如果它们以相反的顺序使用，矩阵仍然会被正确地计算，但是会是不正确的。

9.  Calculate the `sensitivity`:

    ```
    # Calculating Sensitivity
    Sensitivity = TP / (TP + FN)
    print(f'Sensitivity: {Sensitivity:.4f}')
    ```

    上述代码产生以下输出:

    ```
    Sensitivity: 0.6745
    ```

10.  Calculate the `specificity`:

    ```
    # Calculating Specificity
    Specificity = TN / (TN + FP)
    print(f'Specificity: {Specificity:.4f}')
    ```

    上述代码产生以下输出:

    ```
    Specificity: 0.9951
    ```

11.  Calculate the `precision`:

    ```
    # Precision
    Precision = TP / (TP + FP)
    print(f'Precision: {Precision:.4f}')
    ```

    上述代码产生以下输出:

    ```
    Precision: 0.7114
    ```

12.  Calculate the `false positive rate`:

    ```
    # Calculate False positive rate
    False_Positive_rate = FP / (FP + TN)
    print(f'False positive rate: {False_Positive_rate:.4f}')
    ```

    上述代码产生以下输出:

    ```
    False positive rate: 0.0049
    ```

    下图显示了值的输出:

    ![Figure 6.10: Metrics summary
    ](image/B15777_06_10.jpg)

    图 6.10:指标总结

    注意

    灵敏度与特异性成反比。

    正如我们之前讨论的，我们的模型应该更敏感，但它看起来更具体，更不敏感。那么，我们如何解决这个问题呢？答案在于阈值概率。通过调整将因变量分类为`1`或`0`的阈值，可以提高模型的灵敏度。回想一下，最初，我们将`y_pred_class1`的值设置为大于`0.5`。让我们将阈值改为`0.3`，并重新运行代码来检查结果。

13.  转到*步骤 7* ，将阈值从`0.5`改为`0.3`，重新运行代码:

    ```
    y_pred_class2 = y_pred > 0.3
    ```

14.  Now, create a `confusion matrix` and calculate the `specificity` and `sensitivity`:

    ```
    from sklearn.metrics import confusion_matrix
    cm = confusion_matrix(y_test,y_pred_class2)
    print(cm)
    ```

    上述代码产生以下输出:

    ```
    [[11700  88]
     [   58 154]]
    ```

    下面是前面的`confusion matrix`加上一个`0.5`的`threshold`:

    ```
    [[11730  58]
     [   69 143]]
    ```

    注意

    永远记住，`y_test`的原始值应该作为第一个参数传递，`y_pred`作为第二个参数传递。

15.  计算`confusion matrix` :

    ```
    # True Negative
    TN = cm[0,0]
    # False Negative
    FN = cm[1,0]
    # False Positives
    FP = cm[0,1]
    # True Positives
    TP = cm[1,1]
    ```

    的各个分量
16.  Calculate the new `sensitivity`:

    ```
    # Calculating Sensitivity
    Sensitivity = TP / (TP + FN)
    print(f'Sensitivity: {Sensitivity:.4f}')
    ```

    上述代码产生以下输出:

    ```
    Sensitivity: 0.7264
    ```

17.  Calculate the `specificity`:

    ```
    # Calculating Specificity
    Specificity = TN / (TN + FP)
    print(f'Specificity: {Specificity:.4f}')
    ```

    上述代码产生以下输出:

    ```
    Specificity: 0.9925
    ```

    降低阈值后，`sensitivity`和`specificity`明显增加:

    ![Figure 6.11: Sensitivity and specificity comparison
    ](image/B15777_06_11.jpg)

    图 6.11:灵敏度和特异性比较

    因此，很明显，降低阈值会提高灵敏度。

18.  Visualize the data distribution. To understand why decreasing the threshold value increases the sensitivity, we need to see a histogram of our predicted probabilities. Recall that we created the `y_pred_prob` variable to predict the probabilities of the classifier:

    ```
    import matplotlib.pyplot as plt
    %matplotlib inline
    # histogram of class distribution
    plt.hist(y_pred_prob, bins=100)
    plt.title("Histogram of Predicted Probabilities")
    plt.xlabel("Predicted Probabilities of APS failure")
    plt.ylabel("Frequency")
    plt.show()
    ```

    下图显示了上述代码的输出:

![Figure 6.12: A histogram of the probabilities of patient readmission from the dataset
](image/B15777_06_12.jpg)

图 6.12:数据集中患者再次入院概率的直方图

该直方图清楚地显示，预测分类器的大多数概率位于从`0.0`到`0.1`的范围内，这确实非常低。除非我们将阈值设置得非常低，否则我们无法提高模型的灵敏度。此外，请注意，灵敏度与特异性成反比，因此当一个增加时，另一个减少。

阈值没有统一的值，尽管`0.5`的值通常被用作默认值。选择阈值的一种方法是绘制直方图，然后手动选择阈值。在我们的例子中，`0.1`和`0.7`之间的任何阈值都可以用作模型，因为这些值之间的预测很少，这可以从之前练习结束时生成的直方图中看出。

另一种选择阈值的方法是绘制`ROC curve`，它将真阳性率绘制为假阳性率的函数。根据您对每一项的容忍度，可以选择阈值。如果我们希望评估模型的性能，绘制`ROC curve`也是一种很好的技术，因为`ROC curve`下的面积是模型性能的直接度量。在下一个活动中，我们将使用`ROC curve`和`AUC score`来探索我们模型的性能。

## A 活动 6.02:计算 ROC 曲线和 AUC 分数

`ROC curve`和`AUC score`是一种简单评估二进制分类器性能的有效方法。在本活动中，我们将绘制模型的`ROC curve`并计算其`AUC score`。我们将使用在*练习 6.03* 、*中使用的相同数据集和相同模型，根据混淆矩阵*推导和计算指标。使用 APS 故障数据，计算`ROC curve`和`AUC score`。按照以下步骤完成本活动:

1.  导入所有必要的依赖项并加载数据集。
2.  使用`train_test_split`函数将数据分成训练和测试数据集。
3.  使用`StandardScaler`功能缩放训练和测试数据。
4.  导入构建神经网络架构所需的库，并启动`Sequential`类。用`Dropout`添加五个`Dense`层。设置第一个隐藏层，使其大小为`64`，丢失率为`0.5`，第二个隐藏层的大小为`32`，丢失率为`0.4`，第三个隐藏层的大小为`16`，丢失率为`0.3`，第四个隐藏层的大小为`8`，丢失率为`0.2`，最后一个隐藏层的大小为`4`，丢失率为`0.1`。将所有激活功能设置为`ReLU`。
5.  添加一个带有`sigmoid`激活功能的输出`Dense`层。编译网络，然后使用精确度拟合模型。用`100`纪元和`20`批量拟合模型。
6.  将模型拟合到训练数据，保存拟合过程的结果。
7.  创建一个表示测试数据集的预测类的变量。
8.  使用`sklearn.metrics`中的`roc_curve`函数计算假阳性率和真阳性率。假阳性率和真阳性率是三个回归变量中的第一和第二个。将真实值和预测值传递给函数。
9.  绘制 ROC 曲线，这是作为假阳性率函数的真阳性率。
10.  使用 sklearn.metrics 中的`roc_auc_score`计算 AUC 分数，同时传递模型的真实值和预测值。

实现这些步骤后，您应该得到以下输出:

```
0.944787151628455
```

注意

这项活动的解决方案可以在第 389 页找到。

在本次活动中，我们学习了如何使用 APS 故障数据集计算`ROC`和`AUC score`。我们还了解了特异性和敏感性如何随着不同的阈值而变化。

# 总结

在这一章中，我们深入讨论了模型评估和准确性。我们了解到当我们的数据集不平衡时，准确性并不是最合适的评估技术。我们还学习了如何使用 scikit-learn 计算混淆矩阵，以及如何导出其他指标，如灵敏度、特异性、精确度和假阳性率。

最后，我们了解了如何使用阈值来调整指标，以及`ROC curves`和`AUC scores`如何帮助我们评估我们的模型。在现实问题中，处理不平衡数据集是很常见的。信用卡欺诈检测、疾病预测和垃圾邮件检测等问题都存在不同比例的不平衡数据。

在下一章，我们将学习一种不同的神经网络结构(卷积神经网络),它在图像分类任务中表现良好。我们将通过将图像分为两类来测试性能，并使用不同的架构和激活功能进行实验。