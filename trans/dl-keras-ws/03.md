

# 3.使用 Keras 进行深度学习

概观

在本章中，您将尝试不同的神经网络架构。您将创建 Keras 顺序模型(构建单层和多层模型)并评估训练模型的性能。不同架构的网络将帮助你理解过度拟合和欠拟合。到本章结束时，你将已经探索了早期停止，可以用来打击过度适应训练数据。

# 简介

在前一章中，你学习了神经网络的数学，包括带有`scalars`、`vectors`、`matrices`和`tensors`的`linear transformations`。然后，您使用 Keras 实现了您的第一个神经网络，构建了一个逻辑回归模型，将网站用户分为愿意从该网站购物的用户和不愿意从该网站购物的用户。

在本章中，您将扩展使用 Keras 构建神经网络的知识。本章涵盖了深度学习的基础知识，并将为您提供必要的基础，以便您可以构建高度复杂的神经网络架构。我们将从将`logistic regression`模型扩展到简单的单层神经网络开始，然后进行到具有多个隐藏层的更复杂的神经网络。

在此过程中，您将了解神经网络的基本概念，包括用于预测、计算损失的`forward propagation`、用于计算损失相对于模型参数的导数的反向传播，以及用于了解模型最佳参数的梯度下降。您还将了解可用的各种选择，以便您可以根据`activation functions`、`loss functions`和`optimizers`来构建和训练神经网络。

此外，您将学习如何评估您的模型，同时了解过拟合和欠拟合等问题，同时了解它们如何影响您的模型的性能以及如何检测它们。您将了解在用于训练的相同数据集上评估模型的缺点，以及保留一部分可用数据集用于评估目的的替代方法。随后，您将学习如何比较数据集的这两个子集的模型错误率，这两个子集可用于检测模型中的高偏差和高方差等问题。最后，您将了解一种称为早期停止以减少过度拟合的技术，它也是基于将模型的错误率与数据集的两个子集进行比较。

# 建立你的第一个神经网络

在本节中，您将了解深度学习的表示和概念，例如前向传播-数据在网络中的传播，将输入值乘以每个节点的每个连接的权重，以及反向传播-计算损失函数相对于矩阵中权重的梯度，以及梯度下降-用于查找损失函数最小值的优化算法。我们不会深究这些概念，因为这不是本书所要求的。然而，这篇报道将从本质上帮助任何想要将深度学习应用于某个问题的人。

然后，我们将继续使用 Keras 实现神经网络。此外，我们将坚持最简单的情况，这是一个只有一个隐藏层的神经网络。您将学习如何在 Keras 中定义模型，选择超参数(在训练模型之前设置的模型参数)，然后训练您的模型。在本节结束时，您将有机会通过在 Keras 中实现神经网络来实践您所学的内容，以便您可以对数据集执行分类，并观察神经网络如何优于更简单的模型，如逻辑回归。

## 逻辑回归到深层神经网络

在*第 1 章*、*用 Keras* 介绍机器学习中，你了解了逻辑回归模型，然后在*第 2 章*、*机器学习对比深度学习*中，你如何使用 Keras 将其实现为序列模型。从技术上讲，逻辑回归涉及到一个非常简单的神经网络，只有一个隐层，其隐层只有一个节点。

下图概述了具有二维输入的逻辑回归模型。你在这个图像中看到的被称为深度学习世界中的一个节点或单元，用绿色圆圈表示。您可能已经注意到，逻辑回归术语和深度学习术语之间存在一些差异。在逻辑回归中，我们称模型的参数为系数和截距。在深度学习模型中，参数被称为权重(w)和偏差(b):

![Figure 3.1: Overview of the logistic regression model with a two-dimensional input
](image/B15777_03_01.jpg)

图 3.1:二维输入的逻辑回归模型概述

在每个节点/单元，输入乘以一些权重，然后将偏置项添加到这些加权输入的和中。这可以在上图中节点上方的计算中看到。`inputs`是`X1`和`X2`，`weights`是`W1`和`W2`，`bias`是`b`。接下来，非线性函数(例如，逻辑回归模型中的 sigmoid 函数)应用于加权输入的总和，偏差项用于计算节点的最终输出。在上图所示的计算中，这是`σ`。在深度学习中，非线性函数称为激活函数，节点的输出称为该节点的激活。

如下图所示，可以通过在层中堆叠逻辑回归节点/单元来构建单层神经网络。输入层`X1`和`X2`的每个值都被传递到隐藏层的所有三个节点:

![Figure 3.2: Overview of a single-layer neural network with a two-dimensional input 
and a hidden layer of size 3
](image/B15777_03_02.jpg)

图 3.2:具有二维输入和大小为 3 的隐藏层的单层神经网络的概述

也可以通过依次堆叠多层处理节点来构建多层神经网络，如下图所示。下图显示了一个具有二维输入的双层神经网络:

![Figure 3.3: Overview of a two-layer neural network with a two-dimensional input
](image/B15777_03_03.jpg)

图 3.3:具有二维输入的双层神经网络概述

前面两幅图像显示了表示神经网络的最常见方式。每个神经网络都由一个**输入层**、一个**输出层**和一个或多个**隐含层**组成。如果只有一个隐含层，则该网络称为浅层神经网络。另一方面，有很多隐层的神经网络称为深度神经网络，训练它们的过程称为**深度学习**。

*图 3.2* 显示了一个只有一个隐含层的神经网络，所以这将是一个浅层神经网络，而*图 3.3* 中的神经网络有两个隐含层，所以它是一个深层神经网络。输入层通常位于左侧。在*图 3.3* 的情况下，这些是特征`X1`和`X2`，它们被输入到第一个隐藏层，该层有三个节点。箭头表示应用于输入的权重值。在第二隐藏层，第一隐藏层的结果成为第二隐藏层的输入。第一个和第二个隐藏层之间的箭头表示权重。输出通常是最右边的层，在*图 3.3* 的情况下，由标记为`Y`的层表示。

注意

在某些资源中，您可能会看到一个网络，例如上图所示的网络，被称为四层网络。这是因为输入和输出图层与隐藏图层一起计算。然而，更常见的惯例是只计算隐藏层，因此我们之前提到的网络将被称为两层网络。

在深度学习设置中，输入层的节点数等于输入数据的特征数，输出层的节点数等于输出数据的维数。但是，您需要选择隐藏层中的节点数或隐藏层的大小。如果选择较大尺寸的层，模型将变得更加灵活，并且能够模拟更复杂的功能。这种灵活性的增加是以需要更多训练数据和更多计算来训练模型为代价的。要求开发者选择的参数称为`hyperparameters`，包括层数、每层节点数等参数。要选择的常见超参数包括要训练的历元数和要使用的损失函数。

在下一节中，我们将介绍在每个隐藏层之后应用的`activation functions`。

## 激活功能

除了图层的大小，您还需要为添加到模型中的每个隐藏图层选择一个激活函数，并对输出图层执行同样的操作。我们学习了逻辑回归模型中的 sigmoid 激活函数。然而，在 Keras 中构建神经网络时，您可以选择更多的激活函数选项。例如，对于二进制分类任务，sigmoid 激活函数是输出图层上激活函数的最佳选择，因为 sigmoid 函数的结果介于 0 和 1 之间。深度学习常用的激活函数有 sigmoid/logistic、tanh(双曲正切)和整流线性单位(ReLU)。

下图显示了一个`sigmoid activation function`:

![Figure 3.4: Sigmoid activation function
](image/B15777_03_04.jpg)

图 3.4:乙状结肠激活功能

下图显示了一个`tanh activation function`:

![Figure 3.5: tanh activation function
](image/B15777_03_05.jpg)

图 3.5: tanh 激活函数

下图显示了一个`ReLU activation function`:

![Figure 3.6: ReLU activation function
](image/B15777_03_06.jpg)

图 3.6: ReLU 激活功能

在*图 3.4* 和*图 3.5* 中可以看到，一个 sigmoid 函数的输出总是在`0`和`1`之间，tanh 的输出总是在`-1`和`1`之间。这使得`tanh`成为隐藏层的更好选择，因为它使每层的输出平均值接近于零。事实上，在构建`binary classifier`时，`sigmoid`只是输出层的`activation function`的一个好选择，因为它的输出可以解释为属于一个类的给定输入的概率。

因此，`tanh`和`ReLU`是隐藏层激活函数最常见的选择。事实证明，当使用`ReLU activation function`时，学习过程更快，因为对于大于`0`的输入，它有一个固定的导数(或斜率)，而在其他任何地方都有一个`0`的斜率。

注意

你可以在这里阅读更多关于 Keras 激活功能的选择:[https://packt.live/31KsmUk](https://packt.live/31KsmUk)。

## 进行预测的正向传播

神经网络通过执行前向传播来预测输出。前向传播需要在神经网络的每一层中对输入执行计算，直到到达输出层。最好通过一个例子来理解正向传播。

让我们逐一研究两层神经网络(如下图所示)的正向传播方程，其中输入数据是二维的，输出数据是一维的二进制类标签。层 1 和层 2 的激活函数将是 tanh，而输出层中的激活函数是 sigmoid。

下图将各图层的权重和偏差显示为具有适当索引的矩阵和向量。对于每一层，权重矩阵中的行数等于前一层中的节点数，列数等于该层中的节点数。

例如，`W1`有两行三列，因为层 1 的输入是输入层，`X`有两列，而层 1 有三个节点。同样，`W2`有三行三列，因为第 2 层的输入是第 1 层，它有两个节点，而第 2 层有五个节点。然而，偏差始终是一个向量，其大小等于该层中节点的数量。深度学习模型中的参数总数等于所有权重矩阵和偏差向量中的元素总数:

![Figure 3.7: A two-layer neural network
](image/B15777_03_07.jpg)

图 3.7:双层神经网络

根据前面图像中概述的神经网络执行所有正向传播步骤的示例如下。

**执行正向传播的步骤**:

1.  `X` is the network input to the network in the preceding image, so it is the input for the first hidden layer. First, the input matrix, `X`, is the matrix multiplied by the weight matrix for layer 1, `W1`, and the bias, `b1`, is added:

    *z1 = X*W1 + b1*

2.  Next, the layer 1 output is computed by applying an activation function to *z1*, which is the output of the previous step:

    *a1 = tanh(z1)*

3.  `a1` is the output of layer 1 and is called the `activation of layer 1`. The output of layer 1 is, in fact, the `input` for layer 2\. Next, the activation of layer 1 is the matrix multiplied by the weight matrix for layer 2, `W2`, and the bias, `b2`, is added:

    *z2 = a1 * W2 + b2*

4.  The layer 2 output/activation is computed by applying an activation function to `z2`:

    *a2 = tanh(z2)*

5.  The output of layer 2 is, in fact, the input for the next layer (the network output layer here). Following this, the activation of layer 2 is the matrix multiplied by the weight matrix for the output layer, `W3`, and the bias, `b3`, is added:

    *z3 = a2 * W3 + b3*

6.  Finally, the network output, Y, is computed by applying the sigmoid activation function to `z3`:

    *Y = sigmoid(z3)*

该模型中的参数总数等于`W1`、`W2`、`W3`、`b1`、`b2`、`b3`中的元素数之和。因此，可以通过对权重矩阵和偏差中的每个参数中的参数求和来计算参数的数量，这等于 6 + 15 + 5 + 3 + 5 + 1 = 35。这些都是深度学习过程中需要学习的参数。

既然我们已经了解了正向传播步骤，我们必须评估我们的模型，并将其与真实的目标值进行比较。为了实现这一点，我们将使用一个损失函数，这将在下一节中介绍。在这里，我们将了解一些可用于分类和回归任务的常见损失函数。

## 损失函数

当学习模型的最优参数(权重和偏差)时，我们需要定义一个函数来度量误差。该函数称为损失函数，它为我们提供了一种测量方法，用于测量网络预测的输出与数据集中的实际输出之间的差异。

损失函数可以用几种不同的方式定义，这取决于问题和目标。例如，对于分类问题，定义损失的一种常用方法是计算数据集中错误分类输入的比例，并将其用作模型出错的概率。另一方面，在回归问题的情况下，损失函数通常通过计算预测输出与其对应的实际输出之间的距离来定义，然后对数据集中的所有示例进行平均。

Keras 中一些常用损失函数的简要描述如下:

*   mean_squared_error 是回归问题的损失函数，它计算数据集中每个示例的`(real output – predicted output)^2`,然后返回它们的平均值。
*   mean_absolute_error 是回归问题的损失函数，它计算数据集中每个示例的`abs (real output – predicted output)`,然后返回它们的平均值。
*   mean_absolute_percentage_error 是回归问题的损失函数，它计算数据集中每个示例的`abs [(real output – predicted output) / real output]`,然后返回它们乘以 100%的平均值。
*   二元交叉熵是两类/二元分类问题的损失函数。通常，交叉熵损失用于计算模型的损失，其中输出是在`0`和`1`之间的概率数。
*   categorical_crossentropy is a loss function for multi-class (more than two classes) classification problems.

    注意

    你可以在这里阅读更多关于 Keras 损失函数的选择:[https://packt.live/39uJyQi](https://packt.live/39uJyQi)。

在训练过程中，我们不断改变模型参数，直到达到模型预测输出和实际输出之间的最小差异。这被称为优化过程，我们将在后面的章节中了解它是如何工作的。对于神经网络，我们使用反向传播来计算损失函数相对于权重的导数。

## 计算损失函数导数的反向传播算法

反向传播是从神经网络的输出层到输入层执行微积分链式法则的过程，以便计算损失函数相对于每层中的模型参数的导数。函数的导数就是该函数的斜率。我们对损失函数的斜率感兴趣，因为它为我们提供了模型参数需要改变的方向，以使损失值最小化。

微积分的链式法则规定，例如，如果`z`是`y`的函数，`y`是`x`的函数，那么`z`相对于`x`的导数可以通过将`z`相对于`y`的导数乘以`y`相对于`x`的导数得到。这可以这样写:

*dz/dx = dz/dy * dy/dx*

在深度神经网络中，损失函数是预测输出的函数。我们可以通过下面给出的等式来说明这一点:

*损失= L(y _ 预测)*

另一方面，根据前向传播方程，模型预测的输出是模型参数的函数，即每层中的权重和偏差。因此，根据微积分的链式法则，损失相对于模型参数的导数可以通过将损失相对于预测输出的导数乘以预测输出相对于模型参数的导数来计算。

在下一节中，我们将了解当给定损失函数相对于权重的导数时，如何修改最佳权重参数。

## 学习参数的梯度下降

在本节中，我们将了解深度学习模型如何学习其最佳参数。我们的目标是更新权重参数，使得损失函数最小化。这将是一个迭代过程，在这个过程中，我们继续更新权重参数，以使损失函数最小。这个过程称为学习参数，它是通过使用优化算法来完成的。机器学习中用于学习参数的一种非常常见的优化算法是梯度下降法。让我们看看梯度下降是如何工作的。

如果我们为模型参数的所有可能值绘制数据集中所有示例的平均损失，它通常是一个凸形(如下图所示)。在梯度下降中，我们的目标是找到图上的最小点(`Pt`)。该算法首先用一些随机值初始化模型参数(`P1`)。然后，它根据该点的参数计算损失和损失的导数。正如我们之前提到的，函数的导数实际上是函数的斜率。计算初始点的斜率后，我们就有了需要更新参数的方向。

超参数称为学习率(alpha)，它决定了算法将从初始点迈出多大的一步。选择适当的 alpha 值后，算法将参数从初始值更新为新值(如下图中的点`P2`所示)。如下图所示，`P2`更接近目标点，如果我们继续朝那个方向前进，最终会到达目标点，`Pt`。该算法在`P2`再次计算函数的斜率，并采取另一步骤。重复该过程，直到斜率等于零，因此没有提供进一步移动的方向:

![Figure 3.8: A schematic view of the gradient descent algorithm finding the set of parameters that minimize loss
](image/B15777_03_08.jpg)

图 3.8:梯度下降算法的示意图，该算法寻找使损失最小化的一组参数

梯度下降算法的伪代码如下:

```
Initialize all the weights (w) and biases (b) arbitrarily
Repeat Until converge {
Compute loss given w and b
Compute derivatives of loss with respect to w (dw), and with respect to b (db) using backpropagation
Update w to w – alpha * dw
Update b to b – alpha * db
}
```

总而言之，当训练深度神经网络时(在将参数初始化为一些随机值之后)，重复以下步骤:

1.  使用正向传播和当前参数来预测整个数据集的输出。
2.  使用预测的输出来计算所有示例的损耗。
3.  使用反向传播计算每层的权重和偏差的损失导数。
4.  使用导数值和学习率更新权重和偏差。

我们在这里讨论的是标准梯度下降算法，它使用整个数据集计算损失和导数，以便更新参数。还有另一种梯度下降法，称为随机梯度下降法(SGD ),每次仅使用一个子集或一批数据示例计算损失和导数；因此，它的学习过程比标准梯度下降更快。

注意

另一种常见的选择是称为 Adam 的优化算法。Adam 在训练深度学习模型时通常会胜过`SGD`。正如我们已经了解到的，`SGD`使用一个单一的超参数(称为学习率)来更新参数。然而，Adam 通过使用学习率、梯度的加权平均和梯度平方的加权平均来改进这个过程，以在每次迭代中更新参数。

通常，在构建神经网络时，您需要为您的优化过程选择两个超参数(称为批量大小和历元数)。`batch_size`参数决定了在优化算法的每次迭代中要包含的数据示例的数量。`batch_size=None`相当于梯度下降的标准版本，在每次迭代中使用整个数据集。`epochs`参数决定了优化算法在停止之前遍历整个训练数据集的次数。

例如，假设我们有一个大小为`n=400`的数据集，我们选择`batch_size=5`和`epochs=20`。在这种情况下，优化器将在整个数据集上进行一次`400/5 = 80`迭代。因为它应该遍历整个数据集`20`次，所以总共会有`80 * 20`次迭代。

注意

在 Keras 中构建模型时，需要选择训练模型时要使用的优化器类型。在 Keras 中，除了 SGD 和 Adam 之外，还有一些其他选项。你可以在这里阅读更多关于 Keras 优化器的可能选项:[https://packt.live/2SnvPp7](https://packt.live/2SnvPp7)。

本章的所有活动和练习都将记录在 Jupyter 笔记本上。请从[https://packt.live/39pOUMT](https://packt.live/39pOUMT)下载这本书的 GitHub 资源库，以及所有准备好的模板。

## 练习 3.01:使用 Keras 实现神经网络

在本练习中，您将学习使用 Keras 实现神经网络的逐步过程。我们的模拟数据集代表了森林中树木的各种测量值，如高度、树枝数量、底部树干的周长等。我们的目标是根据给定的测量值将记录分类为落叶树或针叶树。首先，执行下面的代码块来加载由代表两个树种的两个类组成的`10000`记录的模拟数据集，其中每个数据示例都有`10`特征值:

```
import numpy as np
import pandas as pd
X = pd.read_csv('../data/tree_class_feats.csv')
y = pd.read_csv('../data/tree_class_target.csv')
# Print the sizes of the dataset
print("Number of Examples in the Dataset = ", X.shape[0])
print("Number of Features for each example = ", X.shape[1]) 
print("Possible Output Classes = ", np.unique(y))
```

**预期产量**:

```
Number of Examples in the Dataset = 10000
Number of Features for each example = 10
Possible Output Classes = [0 1]
```

由于这个数据集中的每个数据示例只能属于两个类中的一个，所以这是一个二元分类问题。二元分类问题非常重要，在现实生活场景中非常常见。例如，让我们假设该数据集中的示例代表森林中`10000`棵树的测量结果。我们的目标是使用这个数据集建立一个模型来预测每棵树的物种是落叶还是针叶树种。树木的`10`特征可以包括预测因子，例如高度、分枝数量和底部树干的周长。输出类别`0`意味着该树是针叶树种，而输出类别`1`意味着该树是落叶树种。

现在，让我们来看一下构建和训练 Keras 模型以执行分类的步骤:

1.  在`numpy`和`tensorflow`中设置一个种子，并将您的模型定义为 Keras 顺序模型。`Sequential`事实上，模型是一层层的堆叠。定义模型后，我们可以根据需要添加任意多个层:

    ```
    from keras.models import Sequential
    from tensorflow import random
    np.random.seed(42)
    random.set_seed(42)
    model = Sequential()
    ```

2.  添加一个尺寸为`10`的隐藏层，激活函数类型为`tanh`(记住输入尺寸等于`10`)。Keras 中有不同类型的图层。现在，我们将只使用最简单的层，称为`Dense`层。密集层相当于我们到目前为止在所有例子中看到的`fully connected layers`:

    ```
    from keras.layers import Dense, Activation
    model.add(Dense(10, activation='tanh', input_dim=10))
    ```

3.  给你的模型添加另一个隐藏层，这次尺寸为`5`，激活函数类型为`tanh`。请注意，输入尺寸参数仅提供给第一层，因为下一层的输入尺寸是已知的:

    ```
    model.add(Dense(5, activation='tanh'))
    ```

4.  使用`sigmoid`激活功能添加输出层。请注意，输出层中的单元数等于输出尺寸:

    ```
    model.add(Dense(1, activation='sigmoid'))
    ```

5.  Ensure that the loss function is binary cross-entropy and that the optimizer is `SGD` for training the model using the `compile()` method and print out a summary of the model to see its architecture:

    ```
    model.compile(optimizer='sgd', loss='binary_crossentropy')
    model.summary()
    ```

    下图显示了上述代码的输出:

    ![Figure 3.9: A summary of the model that was created
    ](image/B15777_03_09.jpg)

    图 3.9:所创建模型的概要

6.  Train your model for `100` epochs and set a `batch_size` equal to 5 and a `validation_split` equal to `0.2`, and then set `shuffle` equal to `false` using the `fit()` method. Remember that you need to pass the input data, `X`, and its corresponding outputs, `y`, to the `fit()` method to train the model. Also, keep in mind that training a network may take a long time, depending on the size of the dataset, the size of the network, the number of epochs, and the number of CPUs or GPUs available. Save the results to a variable named `history`:

    ```
    history = model.fit(X, y, epochs=100, batch_size=5, verbose=1, validation_split=0.2, shuffle=False)
    ```

    `verbose`参数可以取以下三个值中的任意一个:`0`、`1`或`2`。选择`verbose=0`，在训练过程中不会打印任何信息。`verbose=1`将在每次迭代时打印一个完整的进度条，而`verbose=2`将只打印纪元编号:

    ![Figure 3.10: The loss details of the last 5 epochs out of 400
    ](image/B15777_03_10.jpg)

    图 3.10:400 个时段中最后 5 个时段的损失详情

7.  Print the accuracy and loss of the model on the training and validation data as a function of the epoch:

    ```
    import matplotlib.pyplot as plt
    %matplotlib inline
    # Plot training & validation accuracy values
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    plt.show()
    # Plot training & validation loss values
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    plt.show()
    ```

    下图显示了上述代码的输出:

    ![Figure 3.11: The model’s accuracy and loss as a function of an epoch during the training process
    ](image/B15777_03_11.jpg)

    图 3.11:在训练过程中，模型的准确性和损失作为历元的函数

8.  Use your trained model to predict the output class for the first 10 input data examples (`X.iloc[0:10,:]`):

    ```
    y_predicted = model.predict(X.iloc[0:10,:])
    ```

    您可以使用以下代码块打印预测的类:

    ```
     # print the predicted classes
    print("Predicted probability for each of the examples belonging to class 1: "),
    print(y_predicted)
    print("Predicted class label for each of the examples: "), 
    print(np.round(y_predicted))
    ```

    **预期产量**:

    ```
    Predicted probability for each of the examples belonging to class 1:
    [[0.00354007]
     [0.8302744 ]
     [0.00316998]
     [0.95335543]
     [0.99479216]
     [0.00334176]
     [0.43222323]
     [0.00391936]
     [0.00332899]
     [0.99759173]
    Predicted class label for each of the examples:
    [[0.]
     [1.]
     [0.]
     [1.]
     [1.]
     [0.]
     [0.]
     [0.]
     [0.]
     [1.]]
    ```

    这里，我们使用训练好的模型来预测数据集中前 10 个树种的产量。如您所见，模型预测第二、第四、第五和第十棵树被预测为第一类的物种，即落叶类。

请注意，您可以通过在网络中添加更多隐藏层来扩展这些步骤。事实上，在添加输出层之前，您可以向模型中添加任意数量的层。但是，输入尺寸参数仅提供给第一层，因为后续层的输入尺寸是已知的。现在，您已经学习了如何在 Keras 中实现神经网络，您可以通过在以下活动中实现可以执行分类的神经网络来进一步练习。

## 活动 3.01:建立一个单层神经网络来执行二元分类

在本活动中，我们将使用 Keras 顺序模型来构建一个二元分类器。所提供的模拟数据集代表了飞机螺旋桨生产的测试结果。我们的目标变量将是人工检查螺旋桨的结果，指定为“通过”(表示为值 1)或“失败”(表示为值 0)。

我们的目标是将测试结果分为“通过”或“失败”两类，以匹配人工检查。我们将使用具有不同架构的模型，并观察不同模型性能的可视化。这将帮助您更好地理解从一个处理单元到一个处理单元层是如何改变模型的灵活性和性能的。

假设该数据集包含两个特征，代表检查超过`3000`个螺旋桨的飞机螺旋桨的两个不同测试的测试结果(这两个特征被归一化为平均值为零)。输出是螺旋桨通过测试的可能性，1 代表通过，0 代表失败。该公司希望减少对耗时、易出错的飞机螺旋桨人工检查的依赖，并将资源转移到开发自动化测试上，以更快地评估螺旋桨。因此，我们的目标是建立一个模型，在给出两次测试结果的情况下，预测飞机螺旋桨是否会通过人工检查。在本活动中，您将首先构建一个逻辑回归模型，然后构建一个包含三个单元的单层神经网络，最后构建一个包含六个单元的单层神经网络，以执行分类。按照以下步骤完成本活动:

1.  导入所需的包:

    ```
    # import required packages from Keras
    from keras.models import Sequential 
    from keras.layers import Dense, Activation 
    import numpy as np
    import pandas as pd
    from tensorflow import random
    from sklearn.model_selection import train_test_split
    # import required packages for plotting
    import matplotlib.pyplot as plt 
    import matplotlib
    %matplotlib inline 
    import matplotlib.patches as mpatches
    # import the function for plotting decision boundary
    from utils import plot_decision_boundary
    ```

2.  为随机数生成器设置一个种子，以便结果可重复:

    ```
    # define a seed for random number generator so the result will be reproducible
    seed = 1
    ```

3.  使用`pandas`库中的`read_csv`函数加载数据集。使用`feats.shape`、`target.shape`和`feats.shape[0]` :

    ```
    feats = pd.read_csv('outlier_feats.csv')
    target = pd.read_csv('outlier_target.csv')
    print("X size = ", feats.shape)
    print("Y size = ", target.shape)
    print("Number of examples = ", feats.shape[0])
    ```

    打印`X`和`Y`的尺寸以及训练数据集中的样本数
4.  使用以下代码绘制数据集:

    ```
    plt.scatter(feats[:,0], feats[:,1], s=40, c=Y, cmap=plt.cm.Spectral)
    ```

5.  在 Keras 中将逻辑回归模型实现为序列模型。记住，二元分类的激活函数需要是 sigmoid。
6.  用`optimizer='sgd'`、`loss='binary_crossentropy'`、`batch_size = 5`、`epochs = 100`、`shuffle=False`训练模型。使用`verbose=1`和`validation_split=0.2`观察每次迭代中的损失值。
7.  使用以下代码绘制训练模型的决策边界:

    ```
    plot_decision_boundary(lambda x: model.predict(x), X_train, y_train)
    ```

8.  实现一个单层神经网络，在隐藏层有三个节点，在`200`个时期有`ReLU activation function`个节点。重要的是要记住，输出层的激活函数仍然需要是 sigmoid，因为这是一个二元分类问题。为输出层选择`ReLU`或没有激活功能将不会产生可被解释为类别标签的输出。用`verbose=1`训练模型，观察每次迭代中的损失。训练完模型后，绘制决策边界，并评估测试数据集的损失和准确性。
9.  对`size 6`和`400`历元的隐藏层重复*步骤 8* ，比较最终损失值和决策边界图。
10.  Repeat *steps 8* and *9* using the `tanh` activation function for the hidden layer and compare the results with the models with `relu` activation. Which activation function do you think is a better choice for this problem?

    注意

    这项活动的解决方案可在第 323 页找到。

在本练习中，您了解了如何在一个层中堆叠多个处理单元来创建比单个处理单元更强大的模型。这就是为什么神经网络是如此强大的模型的基本原因。您还观察到，增加层中单元的数量会增加模型的灵活性，这意味着可以更精确地估计非线性分离决策边界。

但是，具有更多处理单元的模型需要更长的时间来学习模式，需要更多的历元来训练，并且可能会过度拟合训练数据。因此，神经网络是计算成本很高的模型。您还观察到，与使用`ReLU activation function`相比，使用 tanh 激活功能会导致训练过程变慢。

在本节中，我们创建了各种模型，并根据我们的数据对它们进行了训练。我们观察到，通过对训练数据进行评估，一些模型比其他模型表现得更好。在下一节中，我们将了解一些可用于评估模型的替代方法，这些方法提供了一个无偏的评估。

# 模型评估

在本节中，我们将继续学习多层或深度神经网络，同时学习评估模型性能的技术。正如您可能已经意识到的那样，在构建深度神经网络时，需要选择许多超参数。

应用深度学习的一些挑战包括如何为隐藏层的数量、每个隐藏层中的单元数量、每个层使用的激活函数的类型以及用于训练网络的优化器和损失函数的类型找到正确的值。做出这些决定时，需要进行模型评估。通过执行模型评估，您可以判断特定的深层体系结构或一组特定的超参数在特定数据集上的效果是好是坏，从而决定是否更改它们。

此外，您将了解到`overfitting`和`underfitting`。这是在构建和训练深度神经网络时可能出现的两个非常重要的问题。当涉及到为特定问题找到正确的深度神经网络并尽可能提高其性能时，理解过拟合和欠拟合的概念以及它们是否在实践中发生是至关重要的。

## 使用 Keras 评估已训练的模型

在之前的活动中，我们通过预测输入的每个可能值的输出来绘制模型的决策边界。这种模型性能的可视化是可能的，因为我们处理的是二维输入数据。输入空间中的特征或测量值的数量几乎总是多于两个，因此通过 2D 绘图的可视化不是一个选项。确定模型在特定数据集上表现如何的一种方法是，在预测许多示例的输出时，计算总损失。这可以通过使用 Keras 中的`evaluate()`方法来完成，该方法接收一组输入(`X`)及其相应的输出(`y`)，并计算和返回模型在输入上的总损耗`X`。

例如，让我们考虑构建一个神经网络的情况，该神经网络分别具有大小为`8`和`4`的两个隐藏层，以便执行二元或两类分类。可用的数据点及其对应的类别标签存储在`X`、`y`数组中。我们可以如下建立和训练所提到的模型:

```
model = Sequential()
model.add(Dense(8, activation='tanh', input_dim=2))
model.add(Dense(4, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='sgd', loss='binary_crossentropy')
model.fit(X, y, epochs=epochs, batch_size=batch_size)
```

现在，我们不再使用`model.predict()`来预测给定输入集的输出，而是通过编写以下代码来计算整个数据集的损失，从而评估模型的整体性能:

```
model.evaluate(X, y, batch_size=None, verbose=0)
```

如果在为模型定义`compile()`方法时包含其他度量，比如准确性，那么`evaluate()`方法将在被调用时返回这些度量和损失。例如，如果我们将指标添加到`compile()`参数中，如以下代码所示，那么调用`evaluate()`方法将返回整个数据集上已训练模型的总体损失和总体准确性:

```
model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])
model.evaluate(X, y, batch_size=None, verbose=0)
```

注意

你可以在这里查看 Keras 中`metrics`参数的所有可能选项:[https://packt.live/37ks7AG](https://packt.live/37ks7AG)。

在下一节中，我们将了解如何将数据集分为训练数据集和测试数据集。就像我们在*第 1 章*、*中使用 Keras* 介绍机器学习一样，对单独的评估数据进行训练可以提供对模型性能的无偏见评估。

## 将数据分为训练集和测试集

通常，在用于训练模型的相同数据集上评估模型是一种方法错误。由于模型已被定型以减少此数据集上的错误，因此对其执行评估将导致对模型性能的有偏估计。换句话说，用于训练的数据集的错误率总是低估了新的看不见的例子的错误率。

另一方面，当建立机器学习模型时，目标不是仅在训练数据上实现良好的性能，而是在模型在训练期间未见过的未来示例上实现良好的性能。这就是为什么我们对使用尚未用于训练模型的数据集来评估模型的性能感兴趣。

实现这一点的一种方法是将可用数据集分成两组:训练集和测试集。训练集用于训练模型，而测试集用于性能评估。更准确地说，训练集的作用是为模型提供足够的示例，使其能够学习数据中的关系和模式，而测试集的作用是为我们提供对新的未知示例的模型性能的无偏估计。机器学习中常见的做法是对训练测试集进行`70%-30%`或`80%-20%`分裂。相对较小的数据集通常就是这种情况。

当处理具有数百万个示例的数据集，其中目标是训练大型深度神经网络时，可以使用`98%-2%`或`99%-1%`比率来进行训练-测试分割。下图显示了将数据集划分为定型集和测试集的过程。注意在`training`组和`test`组之间没有重叠:

![Figure 3.12: Illustration of splitting a dataset into training and test sets
](image/B15777_03_12.jpg)

图 3.12:将数据集分成训练集和测试集的图示

您可以使用 scikit-learn 的`train_test_split`函数轻松地对数据集执行分割。例如，以下代码将对数据集执行`70%-30%`训练测试分割:

```
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None)
```

`test_size`参数表示数据集要保留在测试集中的比例，所以应该在`0`和`1`之间。通过给`random_state`参数分配一个`int`，您可以选择用于在训练集和测试集之间生成随机分割的种子。

在将数据分成训练集和测试集之后，我们可以通过只将训练集作为参数提供给`fit()`来修改上一节的代码:

```
model = Sequential()
model.add(Dense(8, activation='tanh', input_dim=2))
model.add(Dense(4, activation='tanh'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='sgd', loss='binary_crossentropy')
model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)
```

现在，我们可以分别计算训练集和测试集的模型错误率:

```
model.evaluate(X_train, y_train, batch_size=None, verbose=0)
model.evaluate(X_test, y_test, batch_size=None, verbose=0)
```

进行拆分的另一种方式是在 Keras 中包含`fit()`方法的`validation_split`参数。例如，只需将代码中的`model.fit(X, y)`行从前面的部分更改为`model.fit(X, y, validation_split=0.3)`，模型就会将最后 30%的数据示例保留在一个单独的测试集中。它将只在其他 70%的样本上训练模型，并且它将在每个时期结束时在训练集和测试集上评估模型。这样做，就有可能观察到训练错误率的变化，以及随着训练的进行测试错误率的变化。

我们希望对我们的模型进行公正的评估的原因是，我们可以看到哪里有改进的空间。由于神经网络有如此多的参数要学习，并且可以学习复杂的函数，因此它们通常会过度适应训练数据，并学习训练数据中的噪声，这可能会阻止模型在新的、看不见的数据上表现良好。下一节将详细探讨这些概念。

## 欠拟合和过拟合

在本节中，您将了解在构建需要适应数据集的机器学习模型时可能会面临的两个问题。这些问题被称为过度拟合和欠拟合，类似于模型的偏差和方差的概念。

一般而言，如果模型不够灵活，无法学习数据集中的关系和模式，则训练误差会很高。我们可以把这样的模型称为高偏差模型。另一方面，如果模型对于给定的数据集过于灵活，它将学习训练数据中的噪声，以及数据中的关系和模式。与训练误差相比，这样的系统将导致测试误差的大幅度增加。我们之前提到过，测试误差总是略高于训练误差。

然而，测试误差和训练误差之间的较大差距表明系统具有较高的方差。在数据分析中，这两种情况(`high bias`和`high variance`)都不可取。事实上，我们的目标是同时找到偏差和方差最小的模型。

例如，让我们考虑一个数据集，该数据集表示两种蝴蝶出现的归一化位置，如下图所示。目标是找到一个模型，当给定它们出现的位置时，可以将这两种蝴蝶分开。显然，这两个阶级之间的分界线不是线性的。

因此，如果我们选择一个简单的模型，如逻辑回归(具有一个大小为 1 的隐藏层的神经网络)来对该数据集执行分类，我们将得到两个类之间的线性分隔线/决策边界，这不能捕获数据集中的真实模式:

![Figure 3.13: Two-dimensional data points of two different classes
](image/B15777_03_13.jpg)

图 3.13:两个不同类别的二维数据点

下图说明了这种模型实现的决策边界。通过评估该模型，将观察到训练错误率高，并且测试错误率略高于训练错误。具有高训练误差率表示模型具有高偏差，而训练误差和测试误差之间的微小差异表示低方差模型。这是一个明显的不适合的例子；该模型未能拟合这两个类别之间的真正分界线:

![Figure 3.14: Underfitting
](image/B15777_03_14.jpg)

图 3.14:欠拟合

如果我们通过向神经网络添加更多层并增加每层中的单元数量来增加神经网络的灵活性，我们可以训练更好的模型，并成功地捕捉决策边界中的非线性。这样的模型可以在下面的图中看到。这是一个训练错误率低，测试错误率低的模型(同样，测试错误率略高于训练错误率)。具有低的训练错误率以及测试错误率和训练错误率之间的微小差异表示模型具有低偏差和低方差。低偏差和低方差的模型代表了给定数据集的正确拟合程度:

![Figure 3.15: Correct fit
](image/B15777_03_15.jpg)

图 3.15:正确的配合

但是如果我们进一步增加神经网络的灵活性，会发生什么呢？通过给模型增加太多的灵活性，它将不仅学习训练数据中的模式和关系，而且学习其中的噪声。换句话说，该模型将适合每个单独的训练示例，而不是仅适合其中的总体趋势和关系。下图显示了这样一个系统。评估该模型将显示非常低的训练错误率和高的测试错误率(训练错误率和测试错误率之间有很大的差异)。这是一个低偏差、高方差的模型，这种情况称为过拟合:

![Figure 3.16: Overfitting
](image/B15777_03_16.jpg)

图 3.16:过度拟合

评估训练集和测试集上的模型并比较它们的错误率，可以提供关于当前模型是否适合给定数据集的有价值的信息。此外，在当前模型没有正确拟合数据集的情况下，可以确定它是过度拟合还是欠拟合数据，并相应地更改模型以找到正确的模型。例如，如果模型欠拟合，您可以使网络变大。另一方面，如果模型过度拟合，您可以通过缩小网络或向其提供更多训练数据来减少过度拟合。在实践中，有许多方法可以用来防止欠拟合或过拟合，其中之一我们将在下一节探讨。

## 提前停止

有时，模型的灵活性适合数据集，但过度拟合或欠拟合仍会发生。这是因为我们为模型训练的迭代次数太多或太少。当使用像`gradient descent`这样的迭代优化器时，优化器试图在每次迭代中更好地拟合训练数据。因此，如果我们在学习了数据中的模式后不断更新参数，它将开始适合单个数据示例。

通过观察每次迭代中的训练和测试错误率，可以确定网络何时开始过度适应训练数据，并在这种情况发生之前停止训练过程。与`underfitting`和`overfitting`相关的区域已经在下面的图中标出。用于训练模型的正确迭代次数可以从测试误差率具有最低值的区域中确定。我们在图上将该区域标记为右拟合，可以看出，在该区域中，`training error rate`和`test error rate`都很低:

![Figure 3.17: Plot of training error rate and test error rate while training a model
](image/B15777_03_17.jpg)

图 3.17:训练模型时训练错误率和测试错误率的曲线图

使用 Keras 进行训练时，您可以轻松存储每个时期的训练损失和测试损失值。为此，您需要在为模型定义`fit()`方法时提供测试集作为`validation_data`参数，并将其存储在`history`字典中:

```
history = model.fit(X_train, y_train, validation_data=(X_test, y_test))
```

您可以稍后绘制存储在`history`中的值，以找到正确的迭代次数来训练您的模型:

```
import matplotlib.pyplot as plt 
import matplotlib
# plot training loss
plt.plot(history.history['loss'])
# plot test loss
plt.plot(history.history['val_loss'])
```

一般来说，由于深度神经网络是高度灵活的模型，过拟合发生的几率非常高。有一整套技术，称为正则化技术，已经被开发来减少一般机器学习模型中的过拟合，特别是深度神经网络中的过拟合。你将在第五章、*提高模型精度*中了解更多这些技术。在下一个活动中，我们将把我们的理解付诸实践，并试图找到训练的最佳时期数，以防止过度拟合。

## 活动 3.02:利用神经网络进行高级纤维化诊断

在本活动中，您将使用真实数据集，根据年龄、性别和身体质量指数等指标来预测患者是否患有晚期纤维化。该数据集由接受丙肝治疗剂量的 1385 名患者的信息组成。对于每个患者，`28`个不同的属性可用，以及一个只能取两个值的类别标签:`1`，表示晚期纤维化；和`0`，表示无晚期纤维化迹象。这是一个输入维数等于`28`的二元/两类分类问题。

在本活动中，您将实现不同的深度神经网络架构来执行此分类。绘制训练错误率和测试错误率的趋势，并确定最终分类器需要训练多少个时期:

注意

本次活动中使用的数据集可以在这里找到:[https://packt.live/39pOUMT](https://packt.live/39pOUMT)。

![Figure 3.18: Schematic view of the binary classifier for a diabetes diagnosis
](image/B15777_03_18.jpg)

图 3.18:用于糖尿病诊断的二元分类器的示意图

按照以下步骤完成本活动:

1.  导入所有必要的依赖项。从 GitHub:

    ```
    X = pd.read_csv('../data/HCV_feats.csv')
    y = pd.read_csv('../data/HCV_target.csv')
    ```

    的`Chapter03`文件夹的`data`子文件夹中加载数据集
2.  打印数据集中示例的数量、可用要素的数量以及类标签的可能值。
3.  使用来自`sklearn.preprocessing`的`StandardScalar`函数缩放数据，并将数据集分成具有`80:20`比率的训练集和测试集。然后，打印拆分后每组中示例的数量。
4.  使用一个大小为 3 的隐藏层和一个`tanh`激活函数实现一个浅层神经网络来执行分类。使用以下超参数值编译模型:

    ```
    optimizer = 'sgd', loss = 'binary_crossentropy', metrics = ['accuracy']
    ```

5.  用以下超参数拟合模型，并存储训练过程中的训练错误率和测试错误率的值:`batch_size = 20`、`epochs = 100`、`validation_split=0.1`和`shuffle=False`。
6.  绘制每个训练时期的训练错误率和测试错误率。使用该图确定网络在哪个时期开始过度拟合数据集。此外，打印在训练集和测试集上达到的最佳准确度的值，以及在测试数据集上评估的损失和准确度。
7.  Repeat *steps 4* and *5* for a deep neural network with two hidden layers (the first layer of size 4 and the second layer of size 3) and a `'tanh'` activation function for both layers in order to perform the classification.

    注意

    这项活动的解决方案可在第 334 页找到。

请注意，与`test`组相比，两个模型都能够在`training`或`validation`组上获得更好的`accuracy`，并且当`training error rate`被训练了相当多的时期后，它会持续下降。然而，`validation error rate`在训练期间下降到某个值，之后开始上升，这表示`overfitting`到`training`数据。最大验证准确度对应于图上验证损失最低的点，并且真实地代表了模型稍后在独立示例上的表现。

从结果可以看出，与两层模型相比，具有一个隐藏层的模型能够达到较低的有效性和`test error rate`。由此，我们可以得出结论，这个模型是这个特殊问题的最佳匹配。具有一个隐藏层的模型显示了大量的偏差，这由训练和验证误差之间的大差距来指示，并且两者仍在减少，这指示该模型可以被训练更多的时期。最后，从图中可以确定，我们应该在验证错误率开始增加的区域周围停止训练，以防止模型从`overfitting`到达数据点。

# 总结

在本章中，您扩展了深度学习的知识，从理解常见的表示和术语，到通过练习和活动在实践中实现它们。您了解了神经网络中的`forward propagation`如何工作，以及它如何用于预测输出，损失函数如何作为模型性能的度量，以及反向传播如何用于计算损失函数相对于模型参数的导数。

您还了解了梯度下降，它使用由`backpropagation`计算的梯度来逐渐更新模型参数。除了基本理论和概念，您还使用 Keras 实现和训练了浅层和深层神经网络，并利用它们对给定输入的输出进行预测。

为了适当地评估您的模型，您将数据集拆分为训练集和测试集，作为改进网络评估的替代方法，并了解了根据训练示例评估模型可能会产生误导的原因。这有助于加深您对训练模型时可能发生的过度拟合和欠拟合的理解。最后，您利用训练错误率和测试错误率来检测网络中的过拟合和欠拟合，并实现早期停止以减少网络中的过拟合。

在下一章中，您将了解带有`scikit-learn`的 Keras 包装器，以及如何通过使用交叉验证等重采样方法来进一步改进模型评估。通过这样做，您将学习如何为深度神经网络找到最佳的超参数集。