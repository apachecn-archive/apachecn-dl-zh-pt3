<title>B15777_02_Final_SP_ePub</title> <link href="css/epub.css" rel="stylesheet" type="text/css">

# 2.机器学习与深度学习

概观

在本章中，我们将开始使用 Keras 库创建人工神经网络(ann)。在利用这个库进行建模之前，我们将先介绍一下构成人工神经网络的数学知识——了解线性变换以及如何在 Python 中应用它们。你将牢固掌握构成人工神经网络的数学知识。在本章结束时，我们将通过用 Keras 建立一个逻辑回归模型来应用这些知识。

# 简介

在前一章中，我们讨论了机器学习的一些应用，甚至用 scikit-learn Python 包建立了模型。前一章介绍了如何预处理真实世界的数据集，以便它们可以用于建模。为此，我们将所有变量转换为数字数据类型，并将`categorical`变量转换为`dummy`变量。我们使用`logistic regression`算法从`online shoppers purchasing intention`数据集中根据用户的购买意愿对网站用户进行分类。我们通过向数据集添加`regularization`来提高我们的模型构建技能，从而改善我们的模型的性能。

在本章中，我们将继续学习如何构建机器学习模型，并扩展我们的知识，以便我们可以使用 Keras 包构建一个`Artificial Neural Network` ( `ANN`)。(请记住，`ANNs`代表了一大类机器学习算法，之所以如此，是因为它们的架构类似于人脑中的神经元。)

`Keras`是一个专门为构建神经网络而设计的机器学习库。虽然 scikit-learn 的功能涵盖了更广泛的机器学习算法领域，但`scikit-learn`对于神经网络的功能却微乎其微。

`ANNs`可以用于其他算法可以执行的相同的机器学习任务，比如`logistic regression`用于`classification`任务，`linear regression`用于`regression`问题，`k-means`用于`clustering`。每当我们开始任何机器学习问题时，为了确定它是哪种类型的任务(`regression`、`classification`或`clustering`，我们需要问以下问题:

*   **什么结果对我或我的企业最重要？**例如，如果您正在预测股票市场指数的价值，您可以预测价格是高于还是低于之前的时间点(这将是一个`classification`任务)，或者您可以预测价值本身(这将是一个`regression`问题)。每个都可能导致不同的后续行动或交易策略。
*   下面的图显示了一个`candlestick chart`。它描述了金融数据中的价格变动，并描述了股票价格。颜色代表股票价格在每个时期的值是上升(绿色)还是下降(红色)，每个蜡烛图显示数据的开盘价、收盘价、最高价和最低价，这些都是股票价格的重要信息。
*   对这些数据建模的一个目标是预测第二天会发生什么。一个`classification`任务可能预测股票价格的正或负变化，因为只有两个可能的值，这将是一个二元分类任务。另一个选择是预测第二天股票的价值。由于预测值是一个`continuous`变量，这将是一个`regression`任务:

![Figure 2.1: A candlestick chart indicating the movement of a stock index over the span of a month
](image/B15777_02_01.jpg)

图 2.1:显示一个月内股票指数走势的蜡烛图

*   我们有适当标记的数据来训练一个模型吗？对于有监督的学习任务，我们必须至少有一些带标签的数据，以便训练一个模型。例如，如果我们想要构建一个模型来将图像分类为狗图像和猫图像，我们将需要训练数据、图像本身以及指示它们是狗图像还是猫图像的数据标签。人工神经网络通常需要大量数据。对于图像分类来说，这可以对数百万幅图像开发出精确、鲁棒的模型。这可能是决定哪种算法适合给定任务的决定性因素。

人工神经网络是一种可以用来解决任务的机器学习算法。它们在某些方面有优势，在其他方面有缺点，在选择这种类型的算法之前应该考虑这些利弊。深度学习网络与单层神经网络的区别在于其深度——网络中隐藏层的总数。

因此，深度学习实际上只是依赖于多层人工神经网络的机器学习的一个特定子群。我们经常会遇到深度学习的结果，无论是在图像分类模型中，比如帮助在你的脸书照片中标记朋友的朋友识别模型，还是帮助在 Spotify 上推荐你下一首喜欢的歌曲的推荐算法。由于各种原因，深度学习模型正变得比传统的机器学习模型更加普遍，包括深度学习模型擅长的非结构化数据的规模不断增长，以及计算成本更低。

对于特定的任务，选择使用人工神经网络还是传统的机器学习算法(如线性回归和决策树)是一个经验问题，也是对算法本身内部工作方式的理解。因此，使用传统机器学习算法或人工神经网络的好处将在下一节中提到。

## 人工神经网络优于传统机器学习算法的优势

*   **最佳表现**:对于任何监督学习任务，最佳模型都是经过大量数据训练的人工神经网络。例如，在分类任务中，如对来自`ImageNet challenge`的图像进行分类(将图像分类到`1000 classes`的大规模视觉识别挑战)，人工神经网络可以获得比人类更高的准确性。
*   **利用数据**进行有效扩展:传统的机器学习算法，如`logistic regression`和`decision trees`，在性能上停滞不前，而人工神经网络架构能够学习更高级的特征，即输入特征的非线性组合，这对于分类或回归任务可能很重要。这使得人工神经网络在提供大量数据时表现得更好，尤其是那些具有深层结构的人工神经网络。例如，在 ImageNet 挑战赛中表现出色的人工神经网络会被提供`14 million images`进行训练。下图显示了深度学习算法和传统机器学习算法的性能随数据量的变化:

![Figure 2.2: Performance scaling with the amount of data for both deep learning algorithms and traditional machine learning algorithms
](image/B15777_02_02.jpg)

图 2.2:深度学习算法和传统机器学习算法的性能随数据量的变化

*   **不需要特征工程**:人工神经网络能够识别哪些特征在建模中是重要的，因此它们能够直接从原始数据中建模。例如，在将狗和猫图像二进制分类到它们各自的类别中时，不需要定义诸如动物的颜色大小或重量之类的特征。图像本身足以让人工神经网络成功地确定分类。在传统的机器学习算法中，这些特征必须在迭代过程中设计，这是手动的，并且可能是耗时的。
*   **适应性强且可转移的**:从人工神经网络中学习到的权重和特征可以应用到类似的任务中。在计算机视觉任务中，预训练的分类模型可以用作为其他分类任务建立模型的起点。例如，VGG-16 是`ImageNet`用来对`1000 random objects`进行分类的`16-layer deep learning model`。在模型中学习的权重可以被转移以在明显更少的时间内对其他对象进行分类。

然而，使用传统的机器学习算法比人工神经网络有一些优势，如下一节所述。

## 传统机器学习算法相对于人工神经网络的优势

*   **当可用的训练数据很少时相对较好的性能**:为了获得高性能，人工神经网络需要大量的数据，网络越深，需要的数据就越多。随着层数的增加，需要学习的参数数量也在增加。这导致有更多的时间根据训练数据进行训练，以达到最佳参数值。例如，`VGG-16`超过了`138 million parameters`，需要`14 million hand-labeled images`训练学习所有参数。
*   **性价比**:无论是财务上还是计算上，深度网络都需要大量的计算能力和时间来训练。这需要大量的资源，但并非所有人都能获得。此外，这些模型的有效调优非常耗时，并且需要熟悉模型内部工作原理的领域专家来实现最佳性能。
*   **容易解读**:很多传统的机器学习模型都很容易解读。因此，识别哪个特征在模型中具有最强的预测能力是很简单的。当与希望理解和解释模型结果的非技术团队成员一起工作时，这非常有用。人工神经网络更多地被认为是一种`black box`，因为尽管它们成功地对图像和其他任务进行了分类，但对预测是如何做出的背后的理解是不直观的，并且隐藏在层层计算中。因此，解释结果需要更多的努力。

## 分层数据表示法

人工神经网络性能如此之好的一个原因是，大量的层允许网络学习许多不同层次的数据表示。下图说明了这一点，其中显示了用于识别人脸的人工神经网络的表示。在模型的较低层，学习简单的特征，例如边缘和梯度，这可以通过查看在初始层中学习的特征来看出。随着模型的进展，低级特征的组合被激活以形成面部部分，并且在模型的后面的层中，学习通用的面部。这就是所谓的要素等级，它展示了这种分层表示对于模型构建和解释的强大功能。

深度神经网络的现实世界应用的许多输入示例涉及图像、视频和自然语言文本。深度神经网络学习的特征层次结构允许它们发现未标记的非结构化数据(如图像、视频和自然语言文本)中的潜在结构，这使它们对于处理真实世界的数据(通常是原始和未处理的)非常有用。

下图显示了深度学习模型的学习表示的示例-较低的特征，如`edges`和`gradients`一起激活以形成通用的面部形状，这可以在较深的层中看到:

![Figure 2.3: Learned representation at various parts of a deep learning model
](image/B15777_02_03.jpg)

图 2.3:深度学习模型各部分的学习表征

由于深度神经网络变得更加容易获得，各种公司已经开始开发它们的应用。以下是一些使用人工神经网络的公司的例子:

*   **Yelp** : Yelp 使用深度神经网络来更有效地处理、分类和标记他们的图像。由于照片是 Yelp 评论的一个重要方面，该公司一直强调对它们进行分类和归类。这可以通过深度神经网络更有效地实现。
*   这家基于云的公司能够使用基于深度神经网络的模型对图像和视频进行分类。
*   **Enlitic** :这家公司使用深度神经网络来分析医学图像数据，如 x 光或核磁共振成像。在这种应用中使用这种网络提高了诊断准确性，并减少了诊断时间和成本。

既然我们理解了使用人工神经网络的潜在应用，我们就可以理解它们如何工作背后的数学原理。虽然它们看起来令人生畏且复杂，但它们可以被分解成一系列线性和非线性转换，这些转换本身很容易理解。通过顺序组合一系列线性和非线性变换来创建 ANN。下一节将讨论构成人工神经网络数学的线性变换的基本组成部分和运算。

# 线性变换

在这一节中，我们将介绍线性变换。线性变换是人工神经网络建模的基础。实际上，人工神经网络建模的所有过程都可以看作是一系列的线性变换。线性变换的工作组件是标量、向量、矩阵和张量。对这些部件进行`addition`、`transposition`、`multiplication`等操作。

## 标量、向量、矩阵和张量

`Scalars`、`vectors`、`matrices`和`tensors`是任何深度学习模型的实际组件。对如何利用这些组件以及可以对它们执行的操作有一个基本的了解，是理解人工神经网络如何工作的关键。`Scalars`、`vectors`和`matrices`是被称为`tensor`的一般实体的例子，因此术语`tensors`可以贯穿本章使用，但可以指任何组件。`Scalars`、`vectors`、`matrices`是指`tensors`的具体尺寸。`tensor`的等级是决定`tensor`跨越的维度数量的属性。下面列出了每一项的定义:

*   **标量**:它们是单个数，是 0 阶`tensors`的一个例子。例如，任何给定点的温度都是一个`scalar`字段。
*   **Vector**:Vector 是单个数的一维数组，是一阶`tensors`的一个例子。给定物体的`velocity`是一个`vector`场的例子，因为它在`two (x,y)`或`three (x,y,z)`维度上有速度。
*   **Matrix** : `Matrices`是跨越两个维度的矩形数组，由单个数字组成。他们是二阶`tensors`的例子。使用`matrices`的一个例子是存储一个给定对象的`velocity`。`matrix`的一维包括给定方向上的速度，而另一维`matrix`包括每个给定的时间点。
*   **张量** : `Tensors`是封装`scalars`、`vectors`和`matrices`的一般实体。一般来说，该名称保留给`3`级或更高级别的`tensors`。使用`tensors`的一个例子是存储许多对象的`velocity`。`matrix`的一个维度包括给定方向上的速度，另一个`matrix`维度针对每个给定时间点给出，第三维度描述各种对象。

下图显示了一个`scalar`、`vector`、`matrix`和`three-dimensional tensor`的一些例子:

![Figure 2.4: A visual representation of scalars, vectors, matrices, and tensors
](image/B15777_02_04.jpg)

图 2.4:标量、向量、矩阵和张量的可视化表示

## 张量加法

`Tensors`可以加在一起创建新的`tensors`。我们将在本章中使用矩阵的例子，但是这个概念可以扩展到任何秩的`tensors`。`Matrices`在一定条件下可以添加到`scalars`、`vectors`和其他`matrices`中。

如果两个矩阵具有相同的形状，它们可以相加(或相减)。对于这种矩阵-矩阵相加，结果矩阵由输入矩阵的元素相加来确定。因此，合成矩阵将具有与两个输入矩阵相同的形状。我们可以将矩阵 C = [cij]定义为矩阵和 **C = A + B** ，其中 cij = aij + bij 并且 **C** 中的每个元素都是 **A** 和 **B** 中相同元素的和。矩阵加法是可交换的，也就是说 **A** 和 **B** 的顺序并不重要——**A+B = B+A**。矩阵加法也是关联的，这意味着即使加法的顺序不同，或者即使运算应用了多次，也能获得相同的结果: **A + (B + C) = (A + B) + C** 。

相同的矩阵加法原理适用于`scalars`、`vectors`和`tensors`。这方面的一个例子如下:

![Figure 2.5: An example of matrix-matrix addition
](image/B15777_02_05.jpg)

图 2.5:矩阵-矩阵加法的一个例子

`Scalars`也可以加到`matrices`上。这里，`matrix`的每个元素都被单独添加到`scalar`中，如下图所示:

![Figure 2.6: An example of matrix-scalar addition
](image/B15777_02_06.jpg)

图 2.6:矩阵标量加法的一个例子

如果两个矩阵之间的列数彼此匹配，则可以将向量添加到矩阵中。这就是所谓的广播。

## 练习 2.01:用向量、矩阵和张量进行各种运算

注意

对于本章中的练习和活动，您需要在系统上安装 Python 3.7、Jupyter 和 NumPy。所有的练习和活动将主要在 Jupyter 笔记本上进行。建议为不同的作业准备一个单独的笔记本，除非有人建议不要这样做。使用下面的链接从本书的 GitHub 资源库下载它们:[https://packt.live/2vpc9rO](https://packt.live/2vpc9rO)。

在本练习中，我们将演示如何在 Python 中创建和使用`vectors`、`matrices`和`tensors`。我们假设您对 sca lars 有所了解。这都可以通过 NumPy 库使用`array`和`matrix`函数来实现。使用 NumPy `array`函数可以创建任何等级的张量。按照以下步骤执行本练习:

1.  Open the Jupyter Notebook to implement this exercise. Import all the necessary dependencies. Create a `one-dimensional array`, or a `vector`, as follows:

    ```
    import numpy as np
    vec1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    vec1
    ```

    上述代码产生以下输出:

    ```
    array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    ```

2.  Create a `two-dimensional array`, or `matrix`, with the `array` function:

    ```
    mat1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
    mat1
    ```

    上述代码产生以下输出:

    ```
    array([[ 1, 2, 3],
           [ 4, 5, 6],
           [ 7, 8, 9],
           [10, 11, 12]])
    ```

3.  使用`matrix`函数创建矩阵，它将显示类似的输出:

    ```
    mat2 = np.matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
    ```

4.  Create a `three-dimensional array`, or `tensor`, using the `array` function:

    ```
    ten1 = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])
    ten1
    ```

    上述代码产生以下输出:

    ```
    array([[[ 1, 2, 3],
            [ 4, 5, 6],
           [[ 7, 8, 9],
            [10, 11, 12]]])
    ```

5.  Determining the `shape` of a given `vector`, `matrix`, or `tensor` is important since certain operations, such as `addition` and `multiplication`, can only be applied to components of certain shapes. The shape of an n-dimensional array can be determined using the `shape` method. Write the following code to determine the `shape` of `vec1`:

    ```
    vec1.shape
    ```

    上述代码产生以下输出:

    ```
    (10, )
    ```

6.  Write the following code to determine the `shape` of `mat1`:

    ```
    mat1.shape
    ```

    上述代码产生以下输出:

    ```
    (4, 3)
    ```

7.  Write the following code to determine the `shape` of `ten1`:

    ```
    ten1.shape
    ```

    上述代码产生以下输出:

    ```
    (2, 2, 3)
    ```

8.  Create a `matrix` with `four rows` and `three columns` with whichever numbers you like. Print the resulting matrix to verify its `shape`:

    ```
    mat1 = np.matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
    mat1
    ```

    上述代码产生以下输出:

    ```
    matrix([[ 1, 2, 3],
            [ 4, 5, 6],
            [ 7, 8, 9],
            [10, 11, 12]])
    ```

9.  Create another matrix with `four rows` and `three columns` with whichever numbers you like. Print the resulting matrix to verify its `shape`:

    ```
    mat2 = np.matrix([[2, 1, 4], [4, 1, 7], [4, 2, 9], [5, 21, 1]])
    mat2
    ```

    上述代码产生以下输出:

    ```
    matrix([[ 2, 1, 4],
            [ 4, 1, 7],
            [ 4, 2, 9],
            [ 5, 21, 1]])
    ```

10.  Add `matrix 1` and `matrix 2`:

    ```
    mat3 = mat1 + mat2
    mat3
    ```

    上述代码产生以下输出:

    ```
    matrix([[ 3, 3, 7],
            [ 8, 6, 13],
            [ 11, 10, 18],
            [ 15, 32, 13]])
    ```

11.  Add `scalars` to the `arrays` with the following code:

    ```
    mat1 + 4
    ```

    上述代码产生以下输出:

    ```
    matrix([[ 5, 6, 7],
            [ 8, 9, 10],
            [ 11, 12, 13],
            [ 14, 15, 16]])
    ```

在这个练习中，我们学习了如何用`vectors`、`matrices`和`tensors`进行各种操作。我们还学习了如何确定`matrix`的`shape`。

## 整形

只要元素总数保持不变，任何大小的`tensor`都可以被整形。例如，一个`(4x3) matrix`可以被改造成一个`(6x2) matrix`，因为它们都有总共`12`个元素。在`reshaping`过程中也可以改变`rank`或`number of dimensions`。比如一个`(4x3) matrix`可以被重塑成一个`(3x2x2) tensor`。这里的`rank`从`2`变成了`3`。`(4x3) matrix`也可以改造成`(12x1) vector`，其中`rank`由`2`变为`1`。下图说明了张量整形——左边是一个带`shape (4x1x3)`的张量，可以被整形为一个`shape (4x3)`的张量。

这里，元素的数量(`12`)保持不变，尽管张量的`shape`和`rank`发生了变化:

![Figure 2.7: Visual representation of reshaping a (4x1x3) tensor into a (4x3) tensor
](image/B15777_02_07.jpg)

图 2.7:将(4x1x3)张量整形为(4x3)张量的可视化表示

## 矩阵转置

矩阵的`transpose`是一个在矩阵对角线上翻转矩阵的算子。当这种情况发生时，行变成列，反之亦然。转置操作通常表示为矩阵上的一个`T`上标。任何秩的张量也可以转置:

![Figure 2.8: A visual representation of matrix transposition
](image/B15777_02_08.jpg)

图 2.8:矩阵转置的可视化表示

下图显示了矩阵`A`和`B`的矩阵转置属性:

![Figure 2.9: Matrix transposition properties where A and B are matrices
](image/B15777_02_09.jpg)

图 2.9:矩阵转置属性，其中 A 和 B 是矩阵

如果矩阵的转置等价于原矩阵，则称方阵(即行数和列数相等的矩阵)是对称的。

## 练习 2.02:矩阵整形和转置

在这个练习中，我们将演示如何重塑和转置矩阵。这将变得很重要，因为如果某些张量维数匹配，某些运算只能应用于分量。例如，只有当两个张量的内部维数匹配时，才能应用张量乘法。对张量进行整形或转置是修改张量维数的一种方法，以确保可以应用某些运算。按照以下步骤完成本练习:

1.  Open a Jupyter notebook from the start menu to implement this exercise. Create a `two-dimensional array` with `four rows` and `three columns`, as follows:

    ```
    import numpy as np
    mat1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
    mat1
    ```

    我们可以通过观察矩阵的形状来确定它的形状:

    ```
    mat1.shape
    ```

2.  Reshape the array so that it has `three rows` and `four columns` instead, as follows:

    ```
    mat2 = np.reshape(mat1, [3,4])
    mat2
    ```

    上述代码产生以下输出:

    ```
    array([[ 1, 2, 3, 4],
            [ 5, 6, 7, 8],
            [ 9, 10, 11, 12]])
    ```

3.  Confirm this by printing the `shape` of the array:

    ```
    mat2.shape
    ```

    上述代码产生以下输出:

    ```
    (3, 4)
    ```

4.  Reshape the matrix into a `three-dimensional array`, as follows:

    ```
    mat3 = np.reshape(mat1, [3,2,2])
    mat3
    ```

    上述代码产生以下输出:

    ```
    array([[ 1, 2],
            [ 3, 4]],
           [[ 5, 6],
            [ 7, 8]],
           [[ 9, 10],
            [ 11, 12]]]) 

    ```

5.  Print the `shape` of the array to confirm its dimensions:

    ```
    mat3.shape
    ```

    上述代码产生以下输出:

    ```
    (3, 2, 2)
    ```

6.  Reshape the matrix into a `one-dimensional array`, as follows:

    ```
    mat4 = np.reshape(mat1, [12])
    mat4
    ```

    上述代码产生以下输出:

    ```
    array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])
    ```

7.  Confirm this by printing the `shape` of the array:

    ```
    mat4.shape
    ```

    上述代码产生以下输出:

    ```
    (12, )
    ```

8.  Taking the transpose of an array will flip it across its diagonal. For a one-dimensional array, a row-vector will be converted into a column vector and vice versa. For a two-dimensional array or matrix, each row becomes a column and vice versa. Call the transpose of an array using the `T` method:

    ```
    mat = np.matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
    mat.T
    ```

    下图显示了上述代码的输出:

    ![Figure 2.10: Visual demonstration of the transpose function
    ](image/B15777_02_10.jpg)

    图 2.10:转置功能的可视化演示

9.  Check the `shape` of the matrix and its transpose to verify that the dimensions have changed:

    ```
    mat.shape
    ```

    上述代码产生以下输出:

    ```
    (4, 3)
    ```

10.  Check the `shape` of the transposed matrix:

    ```
    mat.T.shape
    ```

    上述代码产生以下输出:

    ```
    (3, 4)
    ```

11.  Verify the matrix elements do not match when a matrix is reshaped, and a matrix is transposed:

    ```
    np.reshape(mat1, [3,4]) == mat1.T
    ```

    上述代码产生以下输出:

    ```
    array([[ True, False, False, False],
           [False, False, False, False],
           [False, False, False, True]], dtype = bool)
    ```

    在这里，我们可以看到只有第一个和最后一个元素匹配。

在这一节中，我们介绍了线性代数的一些基本组成部分，包括标量、向量、矩阵和张量。我们还讲述了线性代数组件的一些基本操作，如加法、换位和整形。通过这样做，我们学习了如何通过使用`NumPy`库中的函数来执行这些操作，从而将这些概念付诸实践。在下一节中，我们将通过介绍与人工神经网络相关的最重要的变换之一——矩阵乘法，来扩展我们对线性变换的理解。

## 矩阵乘法

矩阵乘法是神经网络运算的基础。加法规则简单直观，矩阵和张量的乘法规则更复杂。矩阵乘法不仅仅是简单的逐元素乘法。取而代之的是，执行更复杂的过程，包括一个矩阵的整行和另一个矩阵的整列。在这一节中，我们将解释乘法如何适用于二维张量或矩阵；但是，更高阶的张量也可以相乘。

给定一个矩阵 A = [aij]m x n，另一个矩阵 B = [bij]n x p，两个矩阵的乘积为 C = AB = [Cij]m x p，每个元素 Cij 按元素定义为![formula 1](image/B15777_02_10a.png)。注意，所得矩阵的形状与矩阵乘积的外部尺寸或第一矩阵的行数和第二矩阵的列数相同。为了使乘法有效，矩阵乘积的内部维数必须匹配，或者第一个矩阵的列数和第二个矩阵的列数必须匹配。矩阵乘法的内外维概念如下图所示:

![Figure 2.11: A visual representation of the inner and outer dimensions in matrix multiplication
](image/B15777_02_11.jpg)

图 2.11:矩阵乘法中内部和外部维度的可视化表示

与矩阵加法不同，矩阵乘法是不可交换的，这意味着乘积中矩阵的顺序很重要:

![Figure 2.12: Matrix multiplication is non-commutative
](image/B15777_02_12.jpg)

图 2.12:矩阵乘法是不可交换的

例如，假设我们有以下两个矩阵:

![Figure 2.13: Two matrices, A and B
](image/B15777_02_13.jpg)

图 2.13:两个矩阵，A 和 B

构造乘积的一种方法是首先有矩阵 **A** ，乘以 **B** :

![Figure 2.14: Visual representation of matrix A multiplied by B
](image/B15777_02_14.jpg)

图 2.14:矩阵 A 乘以 B 的可视化表示

这产生了一个`2x2`矩阵。构建产品的另一种方法是先有 **B** ，再乘以 **A** :

![Figure 2.15: Visual representation of matrix B multiplied by A
](image/B15777_02_15.jpg)

图 2.15:矩阵 B 乘以 A 的可视化表示

在这里，我们可以看到从产品 **BA** 形成的矩阵是一个`3x3`矩阵，并且与从产品 **AB** 形成的矩阵非常不同。

标量矩阵乘法要简单得多，它是矩阵中每个元素乘以标量的乘积，因此λA = [λaij]m x n，其中λ是标量， **A** 是矩阵。

在下面的练习中，我们将通过利用`NumPy`库在 Python 中执行矩阵乘法来将我们的理解付诸实践。

## 练习 2.03:矩阵乘法

在这个练习中，我们将演示如何将矩阵相乘。按照以下步骤完成本练习:

1.  Open a Jupyter notebook from the start menu to implement this exercise.

    为了演示矩阵乘法的基本原理，从两个形状相同的矩阵开始:

    ```
    import numpy as np
    mat1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
    mat2 = np.array([[2, 1, 4], [4, 1, 7], [4, 2, 9], [5, 21, 1]])
    ```

2.  Since both matrices have the same shape and they are not square, they cannot be multiplied as is, otherwise, the inner dimensions of the product won't match. One way we could resolve this is to take the transpose of one of the matrices; then, we would be able to perform the multiplication. Take the transpose of the second matrix, which would mean that a `(4x3) matrix` is multiplied by a `(3x4) matrix`. The result would be a `(4x4) matrix`. Perform the multiplication using the `dot` method:

    ```
    mat1.dot(mat2.T)
    ```

    上述代码产生以下输出:

    ```
    array([[ 16, 27, 35, 50],
           [ 37, 63, 80, 131],
           [ 58, 99, 125, 212],
           [ 79, 135, 170, 293]])
    ```

3.  Take the transpose of the first matrix, which would mean that a `(3x4) matrix` is multiplied by a `(4x3) matrix`. The result would be a `(3x3) matrix`:

    ```
    mat1.T.dot(mat2)
    ```

    上述代码产生以下输出:

    ```
    array([[ 96, 229, 105],
           [ 111, 254, 126],
           [ 126, 279, 147]])
    ```

4.  Reshape one of the arrays to make sure the inner dimension of the matrix multiplication matches. For example, we can reshape the first array to make it a `(3x4) matrix` instead of transposing. Note that the result is not the same as it is when transposing:

    ```
    np.reshape(mat1, [3,4]).dot(mat2)
    ```

    上述代码产生以下输出:

    ```
    array([[ 42, 93, 49],
           [ 102, 193, 133],
           [ 162, 293, 217]])
    ```

在这个练习中，我们学习了如何将两个矩阵相乘。同样的概念可以适用于所有秩的张量，而不仅仅是二阶张量。如果内部维数匹配，不同秩的张量甚至可以相乘。下一个练习演示如何将三维张量相乘。

## 练习 2.04:张量乘法

在这个练习中，我们将把矩阵乘法的知识应用于高阶张量。按照以下步骤完成本练习:

1.  Open a Jupyter notebook from the start menu to implement this exercise. Begin by creating a three-dimensional tensor using the NumPy library and the `array` function. Import all the necessary dependencies:

    ```
    import numpy as np
    mat1 = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])
    mat1
    ```

    上述代码产生以下输出:

    ```
    array([[[ 1, 2, 3],
            [ 4, 5, 6],
           [[ 1, 2, 3],
            [ 4, 5, 6]]])
    ```

2.  Confirm the shape using the `shape` method:

    ```
    mat1.shape
    ```

    这个张量的形状是(2x2x3)。

3.  Create a new `three-dimensional tensor` that we will be able to multiply the tensor by. Take the transpose of the original matrix:

    ```
    mat2 = mat1.T
    mat2
    ```

    上述代码产生以下输出:

    ```
    array([[[ 1, 1],
            [ 4, 4]],
           [[ 2, 2],
            [ 5, 5]],
           [[ 3, 3],
            [ 6, 6]]])
    ```

4.  Confirm the shape using the `shape` method:

    ```
    mat2.shape
    ```

    这个张量的形状是(3x2x2)。

5.  Take the `dot` product of the `two matrices`, as follows:

    ```
    mat3 = mat2.dot(mat1)
    mat3
    ```

    上述代码产生以下输出:

    ```
    array([[[[ 5, 7, 9],
             [ 5, 7, 9]],
            [[ 20, 28, 36],
             [ 20, 28, 36]]],
           [[[ 10, 14, 18],
             [ 10, 14, 18]],
            [[ 25, 35, 45],
             [ 25, 35, 45]]],
           [[[ 15, 21, 27],
             [ 15, 21, 27]],
            [[ 30, 42, 54],
             [ 30, 42, 54]]]])
    ```

6.  Look at the `shape` of this resultant tensor:

    ```
    mat3.shape
    ```

    上述代码产生以下输出:

    ```
    (3, 2, 2, 3)
    ```

    现在，我们有一个四维张量。

在本练习中，我们学习了如何使用 Python 中的 NumPy 库执行矩阵乘法。虽然我们用 Keras 创建 ann 时不必直接执行矩阵乘法，但理解底层数学仍然是有用的。

# Keras 简介

构建人工神经网络包括创建节点层。每个节点可以被认为是在训练过程中学习的权重张量。一旦人工神经网络与数据拟合，就可以通过将输入数据逐层乘以权重矩阵来进行预测，并在需要时应用任何其他线性变换，如激活函数，直到到达最终输出层。每个权重张量的大小由输入节点的形状和输出节点的形状的大小决定。例如，在单层 ANN 中，我们的单个隐藏层的大小可以认为如下:

![Figure 2.16: Solving the dimensions of the hidden layer of a single-layer ANN
](image/B15777_02_16.jpg)

图 2.16:求解单层人工神经网络隐藏层的尺寸

如果要素的输入矩阵有`n`行(或观察值)和`m`列(或要素),并且我们希望我们的预测目标有`n`行(每个观察值一行)和一列(预测值),我们可以通过使矩阵乘法有效所需的内容来确定隐藏层的大小。下面是单层人工神经网络的示意图:

![Figure 2.17: Representation of a single-layer ANN
](image/B15777_02_17.jpg)

图 2.17:单层人工神经网络的表示

这里，我们可以确定权重矩阵的大小(`mx1`)以确保矩阵乘法有效。

如果我们在一个人工神经网络中有一个以上的隐藏层，那么我们对这些权重矩阵的大小有更多的自由。其实可能性是无穷无尽的，取决于有多少层，每层我们想要多少个节点。然而，在实践中，某些架构设计比其他的更好，正如我们将在本书中学到的。

一般来说，Keras 从构建神经网络中抽象出许多线性代数，以便用户可以专注于设计架构。对于大多数网络，在 Keras 中创建网络只需要输入大小、输出大小和每个隐藏层中的节点数。

Keras 中最简单的模型结构是`Sequential`模型，可以从`keras.models`导入。`Sequential`类的模型描述了一个由线性层叠层组成的人工神经网络。一个`Sequential`模型可以被实例化如下:

```
from keras.models import Sequential
model = Sequential()
```

可以将层添加到该模型实例中，以创建模型的结构。

注意

在初始化您的模型之前，使用 NumPy 的随机库中的`seed`函数和 TensorFlow 的随机库中的`set_seed`函数设置一个种子是有帮助的。

## 图层类型

层的概念是 Keras 核心 API 的一部分。层可以被认为是节点的组合，并且在每个节点处，发生一组计算。在 Keras 中，层的所有节点都可以通过简单地初始化层本身来初始化。下图显示了广义层节点的单独操作。在每个节点，输入数据使用矩阵乘法乘以一组权重，如我们在本章前面所学。应用权重和输入之间的乘积之和，可能包括也可能不包括偏差，如下图中输入节点等于`1`所示。进一步的函数可以应用于该矩阵乘法的输出，例如激活函数:

![Figure 2.18: A depiction of a layer node
](image/B15777_02_18.jpg)

图 2.18:层节点的描述

Keras 中的一些常见图层类型如下:

*   密集:这是一个完全连接的层，其中该层的所有节点都直接连接到所有输入和所有输出。用于对表格数据进行分类或回归任务的人工神经网络在体系结构中通常有很大比例的层具有这种类型。
*   卷积:这种层类型创建一个卷积核，它与输入层卷积以产生输出张量。这种卷积可以在一维或多维中发生。用于图像分类的人工神经网络通常在其结构中具有一个或多个卷积层。
*   池化:这种类型的层用于降低输入层的维度。常见的池类型包括最大池(将给定窗口的最大值传递到输出)或平均池(传递窗口的平均值)。这些层通常与卷积层结合使用，其目的是降低后续层的维数，从而在信息损失很小的情况下学习更少的训练参数。
*   递归:递归层从序列中学习模式，因此每个输出都依赖于上一步的结果。对自然语言或时间序列数据等序列数据进行建模的人工神经网络通常具有一种或多种循环层类型。

Keras 中还有其他图层类型；然而，在使用 Keras 构建模型时，这些是最常见的类型。

让我们通过实例化一个`Sequential`类的模型并向模型添加一个`Dense`层来演示如何向模型添加层。连续的层可以按照我们希望的计算顺序添加到模型中，并且可以从`keras.layers`导入。需要指定单元或节点的数量。该值还将决定图层结果的形状。一个`Dense`层可以通过以下方式添加到一个`Sequential`模型中:

```
from keras.layers import Dense
from keras.models import Sequential
input_shape = 20
units = 1
model.add(Dense(units, input_dim=input_shape))
```

注意

在第一层之后，不需要指定输入尺寸，因为它是由前一层确定的。

## 激活功能

激活函数通常应用于节点的输出，以限制或限定其值。每个节点的值是无限的，可以是从负到正无穷大的任意值。在已经计算了权重和损失的值的神经网络中，这些可能是麻烦的，并且可能趋向于无穷大并产生不可用的结果。在这方面，激活函数可以通过限制值来提供帮助。通常，这些激活函数将该值推到两个极限。激活函数对于决定节点是否应该被“触发”也很有用。常见的激活功能如下:

*   **步骤**功能:超过某个阈值，该值为非零；否则为零。
*   **线性**函数:![formula 2](image/B15777_02_18a.png)，是输入值的标量乘法。
*   **Sigmoid** 函数:![formula 3](image/B15777_02_18b.png)，比如平滑渐变的平滑阶跃函数。该激活函数对于分类是有用的，因为值从 0 到 1 被绑定。
*   **Tanh** 函数:![formula 4](image/B15777_02_18c.png)，它是在`x=0`周围具有更陡梯度的 sigmoid 的缩放版本。
*   **ReLU** 函数:![formula 5](image/B15777_02_18d.png)，否则为 0。

现在，我们已经看到了一些主要的组件，我们可以开始看看如何从这些组件中创建有用的神经网络。事实上，我们可以用本章所学的所有概念创建一个逻辑回归模型。逻辑回归模型的运行方式是，将输入和一组学习到的权重的乘积求和，然后将输出传递给逻辑函数。这可以通过具有 sigmoid 激活函数的单层神经网络来实现。

激活功能可以像层添加到模型中一样添加到模型中。激活函数将应用于模型中上一步的输出。可将`tanh`激活功能添加到`Sequential`模型，如下所示:

```
from keras.layers import Dense, Activation
from keras.models import Sequential
input_shape = 20
units = 1
model.add(Dense(units, input_dim=input_shape))
model.add(Activation('tanh'))
```

注意

激活函数也可以通过在定义层时将它们作为参数添加到模型中。

## 模型拟合

一旦创建了模型的架构，就必须对模型进行编译。编译过程配置所有学习参数，包括要使用的优化器、要最小化的损失函数以及要在模型训练的各个阶段计算的可选度量，例如精度。使用`compile`方法编译模型，如下所示:

```
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
```

模型编译完成后，就可以适合训练数据了。这是通过使用`fit`方法的实例化模型来实现的。使用`fit`方法时有用的参数如下:

*   **X** :要拟合数据的训练特征数据的数组。
*   **y** :训练目标数据的数组。
*   **时期**:运行模型的时期数。一个历元是对整个训练数据集的迭代。
*   **batch_size** :每次梯度更新使用的训练数据样本数。
*   **validation_split** :每个历元后评估的用于验证的训练数据的比例。
*   **混洗**:表示是否在每个历元之前混洗训练数据。

`fit`方法可通过以下方式用于模型:

```
history = model.fit(x=X_train, y=y_train['y'], epochs=10, batch_size=32, validation_split=0.2, shuffle=False)
```

保存调用模型的`fit`方法的输出是有益的，因为它包含关于模型在整个训练中的性能的信息，包括在每个时期后评估的损失。如果定义了验证分割，则在验证分割的每个时期后评估损失。同样，如果在训练中定义了任何指标，它们也会在每个时期后进行计算。绘制这样的损失和评估度量以确定作为时期的函数的模型性能是有用的。作为历元的函数的模型损失可以被可视化如下:

```
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(history.history['loss'])
plt.show()
```

Keras 模型可以通过利用模型实例的`evaluate`方法来评估。此方法返回损失和传递给模型用于训练的任何指标。评估样本外测试数据集时，可以按如下方式调用该方法:

```
test_loss = model.evaluate(X_test, y_test['y'])
```

这些模型拟合步骤代表了使用 Keras 包构建、训练和评估模型时需要遵循的基本步骤。从这里开始，有无限多的方法来构建和评估一个模型，这取决于你想要完成的任务。在下面的活动中，我们将创建一个 ANN 来执行我们在第 1 章、*中使用 Keras 完成的任务。*事实上，我们将使用人工神经网络重新创建逻辑回归算法。因此，我们预计这两种型号的性能相似。

## 活动 2.01:使用 Keras 创建逻辑回归模型

在本活动中，我们将使用 Keras 库创建一个基本模型。我们将使用 Keras 执行我们在*第 1 章*、*机器学习介绍中执行的相同分类任务。我们将使用相同的在线购物购买意向数据集，并尝试预测相同的变量。*

在上一章中，我们使用了一个逻辑回归模型来预测当给定了关于在线会话的行为和网页属性的各种属性时，用户是否会从网站购买产品。在本活动中，我们将介绍 Keras 库，不过我们将继续利用我们之前介绍的库，例如用于轻松加载数据的`pandas`，以及用于任何数据预处理和模型评估指标的`sklearn`。

完成此活动的步骤如下:

1.  加载上一章中的要素和目标数据集。
2.  将训练数据和目标数据拆分为训练数据集和测试数据集。该模型将适合训练数据集，并且测试数据集将用于评估该模型。
3.  从`keras.models`库中实例化一个`Sequential`类的模型。
4.  从`keras.layers`包中添加一层`Dense`类到模型实例中。结点的数量应等于要素数据集中要素的数量。
5.  向模型添加 sigmoid 激活函数。
6.  通过指定要使用的优化器、要评估的损失度量以及每个时期后要评估的任何其他度量来编译模型实例。
7.  使模型符合定型数据，指定要运行的时期数和要使用的验证拆分。
8.  针对将在训练和验证数据集上评估的历元绘制损失和其他评估指标。
9.  评估测试数据集的损失和其他评估指标。

实现这些步骤后，您应该得到以下预期输出:

```
2466/2466 [==============================] - 0s 15us/step
The loss on the test set is 0.3632 and the accuracy is 86.902%
```

注意

这项活动的解决方案可在第 318 页找到。

在本次活动中，我们学习了在 Keras 中创建人工神经网络的一些基本概念，包括各种图层类型和激活函数。我们使用这些组件创建了一个简单的逻辑回归模型，该模型使用了一个软件包，该软件包为我们提供了与我们在*第 1 章*、【Keras 机器学习简介中使用的逻辑回归模型相似的结果。我们学习了如何使用 Keras 库构建模型，使用真实数据集训练模型，并在测试数据集上评估模型的性能，以提供对模型性能的公正评估。

# 总结

在这一章中，我们讨论了与机器学习相关的各种类型的线性代数组件和操作。这些组件包括标量、向量、矩阵和张量。应用于这些张量的运算包括加法、转置和乘法，所有这些都是理解人工神经网络基础数学的基础。

我们还学习了 Keras 包的一些基础知识，包括在每个节点发生的数学运算。我们复制了前一章中的模型，其中我们建立了一个逻辑回归模型来预测来自在线购物购买意向数据集的相同目标。然而，在本章中，我们使用 Keras 库来创建模型，使用 ANN 而不是 scikit-learn 逻辑回归模型。我们使用人工神经网络达到了相似的精确度。

本书接下来的章节将使用我们在本章中学到的相同概念；然而，我们将继续用 Keras 包构建人工神经网络。我们将通过创建具有多个隐藏层的模型，将我们的人工神经网络扩展到不止一个层。通过在我们的神经网络中添加多个隐藏层，我们将把“深度”放入“深度学习”中。我们还将解决欠拟合和过拟合的问题，因为它们与人工神经网络的训练模型有关。