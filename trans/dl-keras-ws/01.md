

# 一、Keras 机器学习简介

概观

本章介绍用 Python 进行机器学习。我们将使用现实生活中的数据集来演示机器学习的基础知识，包括为机器学习模型预处理数据，以及使用带有 scikit-learn 的逻辑回归模型构建分类模型。然后，我们将通过将正则化合并到我们的模型中并使用模型评估指标评估它们的性能来提高我们的模型构建技能。本章结束时，您将能够使用 Python 中的 scikit-learn 库自信地创建模型来解决分类任务，并有效地评估这些模型的性能。

# 简介

机器学习是利用机器来模拟人类任务，并让机器随着时间的推移提高其任务性能的科学。通过将真实世界事件的观察数据输入机器，它们可以开发出优化目标函数的模式和关系，例如二元分类任务的准确性或回归任务中的误差。

一般来说，机器学习的用处在于机器能够在大型数据集中学习高度复杂和非线性的关系，并多次复制该学习的结果。机器学习算法的一个分支在学习与大型、通常非结构化的数据集(如图像、音频和文本数据)相关的高度复杂和非线性关系方面显示出很大的前景，这就是人工神经网络(ann)。然而，人工神经网络的编程、训练和评估可能会很复杂，这对于该领域的初学者来说可能是令人生畏的。Keras 是一个 Python 库，提供了构建、训练和评估人工神经网络的简单介绍，对学习机器学习的人非常有用。

例如，把一组狗或猫的图片分类成它们各自的类型。对于一个人来说，这很简单，而且精确度可能非常高。然而，对每张图片进行分类可能需要一秒钟左右的时间，而扩大任务规模只能通过增加人类的数量来实现，这可能是不可行的。虽然对于机器来说，要达到与人类相同的精度水平可能很困难，但肯定不是不可能的，机器可以每秒钟对许多图像进行分类，并且可以通过提高单台机器的处理能力或提高算法的效率来轻松实现缩放:

![Figure 1.1: The classification of images as either dog or cat is a simple task for humans, but quite difficult for machines
](image/B15777_01_01.jpg)

图 1.1:对人类来说，将图像分类为狗或猫是一项简单的任务，但对机器来说却相当困难

虽然对狗和猫进行分类的琐碎任务对我们人类来说可能很简单，但用于创建对狗和猫进行分类的机器学习模型的相同原则可以应用于人类可能难以完成的其他分类任务。这方面的一个例子是在磁共振图像(MRIs)中识别肿瘤。对于人类来说，这项任务需要具有多年经验的医学专业人员，而机器可能只需要一组标记图像。下图显示了大脑的 MRI 图像，其中一些包括肿瘤:

![Figure 1.2: A non-trivial classification task for humans – MRIs of brains, some of which include the presence of tumors
](image/B15777_01_02.jpg)

图 1.2:人类的一项重要分类任务——大脑磁共振成像，其中一些包括肿瘤的存在

# 数据表示

我们构建模型，以便我们可以了解我们正在训练的数据以及数据集特征之间的关系。当我们遇到新的观察时，这种学习可以通知我们。然而，我们必须认识到，我们在现实世界中交互的观察结果和训练机器学习模型所需的数据格式非常不同。处理文本数据就是一个很好的例子。当我们阅读文本时，我们能够理解每个单词，并应用每个单词与周围单词相关的上下文——这不是一项简单的任务。然而，机器无法解释这些上下文信息。除非经过特殊编码，否则他们不知道如何将文本转换成可以输入数字的东西。因此，我们必须适当地表示数据，通常是通过转换非数字数据类型——例如，将文本、日期和分类变量转换为数字变量。

## 数据表

输入到机器学习问题中的许多数据都是二维的，可以用行或列来表示。图像是三维甚至四维数据集的一个很好的例子。每个图像的形状将是二维的(一个高度和一个宽度)，图像的数量加在一起将增加第三维，颜色通道(红色、绿色和蓝色)将增加第四维:

![Figure 1.3: A color image and its representation as red, green, and blue images
](image/B15777_01_03.jpg)

图 1.3:彩色图像及其红、绿、蓝图像表示

不要

记录购物网站各种用户在线会话活动的数据集可以在这里找到:[https://packt.live/39rdA7S](https://packt.live/39rdA7S)。

下图显示了从 UCI 存储库中获取的数据集中的几行，该存储库记录了购物网站的各种用户的在线会话活动。数据集的列表示会话活动的各种属性和页面的一般属性，而行表示各种会话，都对应于不同的用户。名为`Revenue`的列表示用户是否通过从网站购买产品来结束会话。

分析数据集的一个目的可能是尝试并使用给定的信息来预测给定用户是否会从网站购买任何产品。然后，我们可以通过将我们的预测与名为`Revenue`的列进行比较来检查我们是否正确。这样做的长期好处是，我们可以使用我们的模型来识别会话或网页的重要属性，这些属性可以预测购买意图:

![Figure 1.4: An image showing the first 20 instances of the online shoppers purchasing intention dataset
](image/B15777_01_04.jpg)

图 1.4:显示在线购物者购买意向数据集的前 20 个实例的图像

## 加载数据

数据可以有不同的形式，并且可以在许多地方获得。初学者的数据集通常以平面格式给出，这意味着它们是二维的，有行和列。其他常见的数据形式可能包括图像、`JSON`对象和文本文档。每种数据格式都必须以特定的方式加载。例如，可以使用`NumPy`库将数字数据加载到内存中，这是一个在 Python 中处理矩阵的高效库。

然而，我们不能使用`NumPy`库将营销数据`.csv`文件加载到内存中，因为数据集包含字符串值。对于我们的数据集，我们将使用`pandas`库，因为它能够轻松处理各种数据类型，比如字符串、整数、浮点和二进制值。事实上，`pandas`依赖于`NumPy`对数值数据类型进行操作。`pandas`还能够使用 SQL 查询读取 JSON、Excel 文档和数据库，这使得该库在用 Python 加载和操作数据的实践者中很常见。

下面是一个如何使用`NumPy`库加载 CSV 文件的例子。在有标题的情况下，我们使用`skiprows`参数，标题通常包含列名:

```
import numpy as np
data = np.loadtxt(filename, delimiter=",", skiprows=1)
```

下面是一个使用`pandas`库加载数据的例子:

```
import pandas as pd
data = pd.read_csv(filename, delimiter=",")
```

在这里，我们正在加载一个`.CSV`文件。默认分隔符是逗号，因此将其作为参数传递是不必要的，但这样做很有用。pandas 库还可以处理非数字数据类型，这使得该库更加灵活:

```
import pandas as pd
data = pd.read_json(filename)
```

`pandas`库将使 JSON 变平并返回一个`DataFrame`。

该库甚至可以连接到数据库，查询可以直接输入到函数中，返回的表将作为`pandas` DataFrame 加载:

```
import pandas as pd
data = pd.read_sql(con, "SELECT * FROM table")
```

我们必须向函数传递一个数据库连接，这样才能工作。根据数据库的风格，有无数种方法可以实现这一点。

深度学习中常见的其他形式的数据，如图像和文本，也可以加载进来，并将在本课程的稍后部分进行讨论。

注释

你可以在这里找到所有关于熊猫的文件:[https://packt.live/3bwi26O](https://packt.live/3bwi26O)。NumPy 的文档可以在这里找到:【https://packt.live/39qAs7j[。](https://packt.live/39qAs7j)

## 练习 1.01:从 UCI 机器学习知识库中加载数据集

在本练习中，我们将从 UCI 机器学习知识库中加载`online shoppers purchasing intention`数据集。本练习的目标是加载 CSV 数据，并确定要预测的目标变量和用于对目标变量建模的特征变量。最后，我们将分离特性和目标列，并将其保存到`.CSV`文件中，以便在后续的活动和练习中使用。

注意

对于本章中的所有练习和活动，您需要在系统上安装 Python 3.7、Jupyter 和 pandas。这些都是在 Jupyter 笔记本上开发的。对于不同的作业，建议保留一个单独的笔记本。你可以从这本书的 GitHub 资源库下载所有的笔记本，可以在这里找到:[https://packt.live/2OL5E9t](https://packt.live/2OL5E9t)。

该数据集与在线商店的客户的在线行为和活动相关，并且指示用户是否从该网站购买了任何产品。按照以下步骤完成本练习:

1.  从`start`菜单中打开一个 Jupyter 笔记本来完成这个练习。
2.  向下从[https://packt.live/39rdA7S](https://packt.live/39rdA7S)加载数据集。
3.  To verify that the data looks as follows, we will look at the first `10` rows of the data file using the `head` function:

    ```
    !head data/online_shoppers_intention.csv
    ```

    上述代码的输出如下:

    ![Figure 1.5: The first 10 rows of the dataset
    ](image/B15777_01_05.jpg)

    图 1.5:数据集的前 10 行

4.  使用 pandas 库的`read_csv`功能将数据加载到内存中。导入`pandas`库，读入`data`文件:

    ```
    import pandas as pd
    data = pd.read_csv('../data/online_shoppers_intention.csv')
    ```

5.  To verify that we have loaded the data into the memory correctly, we can print the first few rows. Then, print out the top `20` values of the variable:

    ```
    data.head(20)
    ```

    打印输出应该如下所示:

    ![Figure 1.6: The first 20 rows and first 8 columns of the pandas DataFrame
    ](image/B15777_01_06.jpg)

    图 1.6:熊猫数据帧的前 20 行和前 8 列

6.  We can also print the `shape` of the `DataFrame`:

    ```
    data.shape
    ```

    打印输出应该如下所示，显示数据帧有`12330`行和`18`列:

    ```
    (12330, 18)
    ```

    我们已经成功地将数据加载到内存中，所以现在我们可以操作和清理数据，以便可以使用这些数据来训练模型。请记住，机器学习模型需要将数据表示为数字数据类型，以便进行训练。从数据集的前几行我们可以看到，有些列是字符串类型，所以在本章的后面我们必须将它们转换成数字数据类型。

7.  We can see that there is a given output variable for the dataset, known as `Revenue`, which indicates whether or not the user purchased a product from the website. This seems like an appropriate target to predict, since the design of the website and the choice of the products featured may be based upon the user's behavior and whether they resulted in purchasing a particular product. Create `feature` and `target` datasets as follows, providing the `axis=1` argument:

    ```
    feats = data.drop('Revenue', axis=1)
    target = data['Revenue']
    ```

    注意

    `axis=1`参数告诉函数删除列而不是行。

8.  To verify that the shapes of the datasets are as expected, print out the number of `rows` and `columns` of each:

    ```
    print(f'Features table has {feats.shape[0]} rows and {feats.shape[1]} columns')
    print(f'Target table has {target.shape[0]} rows')
    ```

    上述代码产生以下输出:

    ```
    Features table has 12330 rows and 17 columns
    Target table has 12330 rows
    ```

    在这里我们可以看到两个重要的事情，我们应该在继续之前验证:首先，`feature`数据帧和`target`数据帧的行数是相同的。在这里，我们可以看到两者都有`12330`行。第二，特征数据帧的列数应该比总数据帧少一列，而目标数据帧正好有一列。

    关于第二点，我们必须验证目标不包含在特征数据集中；否则，模型将很快发现这是最小化总误差所需的唯一列，一直到零。目标列不一定是一列，但是对于二进制分类，就像在我们的例子中一样，它将是一列。记住，这些机器学习模型试图最小化一些成本函数，其中`target`变量将是该成本函数的一部分，通常是预测值和`target`变量之间的差异函数。

9.  Finally, save the DataFrames as CSV files so that we can use them later:

    ```
    feats.to_csv('../data/OSI_feats.csv', index=False)
    target.to_csv('../data/OSI_target.csv', header='Revenue', index=False)
    ```

    注意

    `header='Revenue'`参数用于提供列名。我们将这样做以减少以后的混乱。使用`index=False`参数是为了不保存索引列。

在本节中，我们已经成功演示了如何使用`pandas`库将数据加载到 Python 中。对于大多数表格数据，这将构成将数据加载到内存中的基础。图像和大型文档都是机器学习应用程序的其他常见数据形式，必须使用其他方法加载，所有这些都将在本书的后面讨论。

# 数据预处理

为了使模型适合数据，数据必须以数字格式表示，因为所有机器学习算法中使用的数学只对数字矩阵有效(不能对图像执行线性代数)。这将是本节的一个目标:学习如何将所有的特征编码成数字表示。例如，在二进制文本中，包含两个可能值之一的值可以表示为 0 或 1。下图中可以看到这样一个例子。因为只有两个可能的值，所以值`0`被假定为`cat`，值`1`被假定为`dog`。我们还可以重命名该列进行解释:

![Figure 1.7: A numerical encoding of binary text values
](image/B15777_01_07.jpg)

图 1.7:二进制文本值的数字编码

另一个目标是以数字格式恰当地表示数据——恰当地，我们的意思是我们希望通过数字的分布对相关信息进行数字编码。例如，对一年中的月份进行编码的一种方法是使用一年中的月份数。例如，`January`将被编码为`1`，因为它是第一个月，`December`将被编码为`12`。这里有一个实际例子:

![Figure 1.8: A numerical encoding of months
](image/B15777_01_08.jpg)

图 1.8:月份的数字编码

没有将信息适当地编码成数字特征会导致机器学习模型学习不直观的表示，以及证明对人类解释无用的`feature`数据和`target`变量之间的关系。

对你打算使用的机器学习算法的理解也将有助于将特征适当地编码成数字表示。例如，用于分类任务的算法，如人工神经网络(ann)和逻辑回归，容易受到特征之间尺度的较大变化的影响，这可能会妨碍它们的模型拟合能力。

以一个回归问题为例，该问题试图将房屋属性(如平方英尺面积和卧室数量)与房价进行拟合。面积的界限可能是从`0`到`5000`的任何地方，而卧室的数量可能只从`0`到`6`变化，因此变量的规模之间有很大的差异。

应对要素之间比例差异较大的有效方法是对数据进行归一化。对数据进行标准化将适当地缩放数据，使其具有相似的大小。这确保了可以正确地比较任何模型系数或权重。决策树之类的算法不受数据缩放的影响，因此对于使用基于树的算法的模型，可以省略这一步。

在本节中，我们演示了一些不同的信息数字编码方法。有无数的替代技术可以在其他地方探索。在这里，我们将展示一些简单而流行的方法，可以用来处理常见的数据格式。

## 练习 1.02:清理数据

重要的是，我们要适当地清理数据，以便它可以用于训练模型。这通常包括将非数值数据类型转换为数值数据类型。这将是本练习的重点-将要素数据集中的所有列转换为数字列。要完成练习，请执行以下步骤:

1.  首先，我们将`feature`数据集加载到内存:

    ```
    %matplotlib inline
    import pandas as pd
    data = pd.read_csv('../data/OSI_feats.csv')
    ```

2.  Again, look at the first `20` rows to check out the data:

    ```
    data.head(20)
    ```

    以下屏幕截图显示了上述代码的输出:

    ![Figure 1.9: First 20 rows and 8 columns of the pandas feature DataFrame
    ](image/B15777_01_09.jpg)

    图 1.9:熊猫特征数据框的前 20 行和 8 列

    在这里，我们可以看到有许多列需要转换成数字格式。我们可能不需要修改的数字列是名为**行政**、**行政 _ 持续时间**、**信息**、**信息 _ 持续时间**、**产品相关**、**产品相关 _ 持续时间**、**反弹率**、**出口**、**页面值**、**特殊日**、**操作的列**

    还有一个二进制列，它有两个可能的值。这是名为**周末**的专栏。

    最后，还有字符串类型的分类列，但是该列可以选择的选项(`>2`)是有限的。这些是名为**月**和**访问者类型**的列。

3.  For the numerical columns, use the `describe` function to get a quick indication of the bounds of the numerical columns:

    ```
    data.describe()
    ```

    以下屏幕截图显示了上述代码的输出:

    ![Figure 1.10: Output of the describe function in the feature DataFrame
    ](image/B15777_01_10.jpg)

    图 1.10:特征数据帧中描述功能的输出

4.  Convert the binary column, **Weekend**, into a numerical column. To do so, we will examine the possible values by printing the count of each value and plotting the result, and then convert one of the values into `1` and the other into `0`. If appropriate, rename the column for interpretability.

    对于上下文，查看每个值的分布是有帮助的。我们可以使用`value_counts`函数来实现。我们可以在`Weekend`栏中尝试一下:

    ```
    data['Weekend'].value_counts()
    ```

    我们还可以通过调用结果数据帧的`plot`方法并传递`kind='bar'`参数来绘制值计数，从而以条形图的形式查看这些值:

    ```
    data['Weekend'].value_counts().plot(kind='bar')
    ```

    注意

    `kind='bar'`参数将把数据绘制成条形图。默认是一个`line graph`。在 Jupyter 笔记本中绘图时，为了在笔记本中绘图，可能需要运行以下命令:`%matplotlib inline`。

    下图显示了上述代码的输出:

    ![Figure 1.11: A plot of the distribution of values of the default column
    ](image/B15777_01_11.jpg)

    图 1.11:默认列的值分布图

5.  Here, we can see that this distribution is skewed toward `false` values. This column represents whether the visit to the website occurred on a weekend, corresponding to a `true` value, or a weekday, corresponding to a `false` value. Since there are more weekdays than weekends, this skewed distribution makes sense. Convert the column into a numerical value by converting the `True` values into `1` and the `False` values into `0`. We can also change the name of the column from its default to `is_weekend`. This makes it a bit more obvious what the column means:

    ```
    data['is_weekend'] = data['Weekend'].apply(lambda row: 1 if row == True else 0)
    ```

    注意

    `apply`函数遍历列中的每个元素，并应用作为参数提供的函数。必须提供一个函数作为参数。这里提供了一个`lambda`函数。

6.  Take a look at the original and converted columns side by side. Take a sample of the last few rows to see examples of both values being manipulated so that they're numerical data types:

    ```
    data[['Weekend','is_weekend']].tail()
    ```

    注意

    除了函数返回数据帧底部的`n`值而不是顶部的`n`值外，`tail`函数与`head`函数相同。

    下图显示了上述代码的输出:

    ![Figure 1.12: The original and manipulated column
    ](image/B15777_01_12.jpg)

    图 1.12:原始的和被操作的列

    在这里，我们可以看到`True`被转换为`1`,`False`被转换为`0`。

7.  Next, we have to deal with categorical columns. We will approach the conversion of categorical columns into numerical values slightly differently than with binary text columns, but the concept will be the same. Convert each categorical column into a set of dummy columns. With dummy columns, each categorical column will be converted into `n` columns, where `n` is the number of unique values in the category. The columns will be `0` or `1`, depending on the value of the categorical column.

    这是通过`get_dummies`功能实现的。如果我们需要任何帮助来理解这个函数，我们可以使用`help`函数或任何函数:

    ```
    help(pd.get_dummies)
    ```

    下图显示了上述代码的输出:

    ![Figure 1.13: The output of the help command being applied to the pd.get_dummies function
    ](image/B15777_01_13.jpg)

    图 1.13:应用于 pd.get_dummies 函数的 help 命令的输出

8.  Let's demonstrate how to manipulate categorical columns with the `age` column. Again, it is helpful to see the distribution of values, so look at the value counts and plot them:

    ```
    data['VisitorType'].value_counts()
    data['VisitorType'].value_counts().plot(kind='bar')
    ```

    下图显示了上述代码的输出:

    ![Figure 1.14: A plot of the distribution of values of the age column
    ](image/B15777_01_14.jpg)

    图 1.14:年龄列值的分布图

9.  Call the `get_dummies` function on the `VisitorType` column and take a look at the rows alongside the original:

    ```
    colname = 'VisitorType'
    visitor_type_dummies = pd.get_dummies(data[colname], prefix=colname)
    pd.concat([data[colname], visitor_type_dummies], axis=1).tail(n=10)
    ```

    下图显示了上述代码的输出:

    ![Figure 1.15: Dummy columns from the VisitorType column
    ](image/B15777_01_15.jpg)

    图 1.15:visitor type 列中的虚拟列

    在这里，我们可以看到，在每一行中，可以有一个值`1`，它位于与`VisitorType`列中的值相对应的列中。

    事实上，在使用虚拟列时，会有一些冗余信息。因为我们知道有三个值，如果虚拟列中的两个值对于特定的行是`0`，那么剩余的列必须等于`1`。重要的是消除特征中的任何冗余和相关性，因为很难确定哪个特征对于最小化总误差是最重要的。

10.  To remove the interdependency, drop the `VisitorType_Other` column because it occurs with the lowest frequency:

    ```
    visitor_type_dummies.drop('VisitorType_Other', axis=1, inplace=True)
    visitor_type_dummies.head()
    ```

    注意

    在`drop`函数中，`inplace`参数将就地应用该函数，因此不必声明新变量。

    查看前几行，我们可以看到原始`VisitorType`列的虚拟列的剩余部分:

    ![Figure 1.16: Final dummy columns from the VisitorType column
    ](image/B15777_01_16.jpg)

    图 1.16:来自 VisitorType 列的最终虚拟列

11.  最后，通过将两个数据帧按列连接并删除原始列，将这些虚拟列添加到原始特征数据:

    ```
    data = pd.concat([data, visitor_type_dummies], axis=1)
    data.drop('VisitorType', axis=1, inplace=True) 
    ```

12.  对剩余的分类列`Month`重复完全相同的步骤。首先，检查列值的分布，这是一个可选步骤。其次，创建虚拟列。第三，删除其中一列以消除冗余。第四，将虚拟列连接到一个要素数据集中。最后，如果原始列仍在数据集中，则删除它。
13.  Now, we should have our entire dataset as numerical columns. Check the types of each column to verify this:

    ```
    data.dtypes
    ```

    下图显示了上述代码的输出:

    ![Figure 1.17: The datatypes of the processed feature dataset
    ](image/B15777_01_17.jpg)

    图 1.17:已处理要素数据集的数据类型

14.  既然我们已经验证了数据类型，我们就有了一个可以用来训练模型的数据集，所以让我们把它留到以后:

    ```
    data.to_csv('../data/OSI_feats_e2.csv', index=False)
    ```

15.  Let's do the same for the `target` variable. First, load the data in, convert the column into a numerical datatype, and save the column as a CSV file:

    ```
    target = pd.read_csv('../data/OSI_target.csv')
    target.head(n=10)
    ```

    下图显示了上述代码的输出:

    ![Figure 1.18: First 10 rows of the target dataset
    ](image/B15777_01_18.jpg)

    图 1.18:目标数据集的前 10 行

    在这里，我们可以看到这是一个`Boolean`数据类型，并且有两个唯一的值。

16.  Convert this into a binary numerical column, much like we did with the binary columns in the feature dataset:

    ```
    target['Revenue'] = target['Revenue'].apply(lambda row: 1 if row==True else 0)
    target.head(n=10)
    ```

    下图显示了上述代码的输出:

    ![Figure 1.19: First 10 rows of the target dataset when converted into integers
    ](image/B15777_01_19.jpg)

    图 1.19:转换成整数时目标数据集的前 10 行

17.  最后，将目标数据集保存到一个 CSV 文件:

    ```
    target.to_csv('../data/OSI_target_e2.csv', index=False)
    ```

在本练习中，我们学习了如何适当地清理数据，以便可以用它来训练模型。我们将非数值数据类型转换为数值数据类型；也就是说，我们将要素数据集中的所有列都转换为数字列。最后，我们将目标数据集保存为 CSV 文件，以便我们可以在下面的练习和活动中使用它。

## 数据的适当表示

在我们的在线购物者购买意向数据集中，我们有一些列被定义为数字变量，但仔细观察后，它们实际上是被赋予数字标签的分类变量。这些列是`OperatingSystems`、`Browser`、`TrafficType`和`Region`。目前，我们将它们视为数字变量，尽管它们是分类的，如果我们希望我们建立的模型学习特征和目标之间的关系，则应该将它们编码到特征中。

我们这样做是因为我们可能在特性中编码了一些误导性的关系。例如，如果`OperatingSystems`字段的值等于`2`，这是否意味着它的值是具有值`1`的值的两倍？可能不会，因为它指的是操作系统。为此，我们将把字段转换成分类变量。这同样适用于`Browser`、`TrafficType`和`Region`列。

## 练习 1.03:数据的适当表示

在本练习中，我们将把`OperatingSystems`、`Browser`、`TrafficType`和`Region`列转换成分类类型，以准确反映信息。为此，我们将以类似于我们在*练习 1.02* 、*清理数据*中所做的方式从列中创建虚拟变量。为此，请执行以下步骤:

1.  Open a Jupyter notebook.
2.  将数据集加载到内存。我们可以使用与*练习 1.02* 、*输出相同的特征数据集，清理数据*，它包含`OperatingSystems`、`Browser`、`TrafficType`和`Region`列的原始数字版本:

    ```
    import pandas as pd
    data = pd.read_csv('../data/OSI_feats_e2.csv')
    ```

3.  Look at the distribution of values in the `OperatingSystems` column:

    ```
    data['OperatingSystems'].value_counts()
    ```

    下图显示了上述代码的输出:

    ![Figure 1.20: The distribution of values in the OperatingSystems column
    ](image/B15777_01_20.jpg)

    图 1.20:operating systems 列中值的分布

4.  从`OperatingSystem`列创建虚拟变量:

    ```
    colname = 'OperatingSystems'
    operation_system_dummies = pd.get_dummies(data[colname], prefix=colname)
    ```

5.  删除代表出现频率最低的值的虚拟变量，并重新加入原始数据:

    ```
    operation_system_dummies.drop(colname+'_5', axis=1, inplace=True)
    data = pd.concat([data, operation_system_dummies], axis=1)
    ```

6.  Repeat this for the `Browser` column:

    ```
    data['Browser'].value_counts()
    ```

    下图显示了上述代码的输出:

    ![Figure 1.21: The distribution of values in the Browser column
    ](image/B15777_01_21.jpg)

    图 1.21:浏览器列中值的分布

7.  创建虚拟变量，删除出现频率最低的虚拟变量，并与原始数据重新连接:

    ```
    colname = 'Browser'
    browser_dummies = pd.get_dummies(data[colname], prefix=colname)
    browser_dummies.drop(colname+'_9', axis=1, inplace=True)
    data = pd.concat([data, browser_dummies], axis=1)
    ```

8.  对`TrafficType`和`Region`列重复此操作:

    ```
    colname = 'TrafficType'
    data[colname].value_counts()
    traffic_dummies = pd.get_dummies(data[colname], prefix=colname)
    # value 17 occurs with lowest frequency
    traffic_dummies.drop(colname+'_17', axis=1, inplace=True)
    data = pd.concat([data, traffic_dummies], axis=1)
    colname = 'Region'
    data[colname].value_counts()
    region_dummies = pd.get_dummies(data[colname], prefix=colname)
    # value 5 occurs with lowest frequency
    region_dummies.drop(colname+'_5', axis=1, inplace=True)
    data = pd.concat([data, region_dummies], axis=1)
    ```

9.  Check the column types to verify they are all numerical:

    ```
    data.dtypes
    ```

    下图显示了上述代码的输出:

    ![Figure 1.22: The datatypes of the processed feature dataset
    ](image/B15777_01_22.jpg)

    图 1.22:已处理要素数据集的数据类型

10.  最后，将数据集保存到 CSV 文件中以备后用:

    ```
    data.to_csv('../data/OSI_feats_e3.csv', index=False)
    ```

现在，我们可以准确地测试浏览器类型、操作系统、流量类型或地区是否会影响目标变量。这个练习演示了如何恰当地表示用于机器学习算法的数据。我们已经介绍了一些可以用来将数据转换成数字数据类型的技术，这些技术涵盖了使用表格数据时可能遇到的许多情况。

# 模型创建的生命周期

在这一部分，我们将涵盖创建高性能机器学习模型的生命周期，从工程特性到拟合模型到训练数据，以及使用各种度量标准评估我们的模型。下图演示了构建机器学习模型的迭代过程。特征被设计成表示特征和目标之间的潜在相关性，模型被拟合，然后模型被评估。

根据如何根据模型的评估指标对模型进行评分，对特性进行进一步的设计，并且该过程继续进行。实现来创建模型的许多步骤在所有机器学习库之间是高度可转移的。我们将从 scikit-learn 开始，它的优势是被广泛使用，因此，在互联网上可以找到大量的文档、教程和学习材料:

![Figure 1.23: The life cycle of model creation
](image/B15777_01_23.jpg)

图 1.23:模型创建的生命周期

## 机器学习库

虽然这本书是对 Keras 深度学习的介绍，但正如我们前面提到的，我们将从利用 scikit-learn 开始。这将帮助我们建立使用 Python 编程语言构建机器学习模型的基础。

与 scikit-learn 类似，Keras 通过一个易于使用的 API，使得用 Python 编程语言创建模型变得很容易。然而，Keras 的目标是创建和训练神经网络，而不是一般的机器学习模型。ann 代表了一大类机器学习算法，之所以称之为 ann，是因为它们的架构类似于人脑中的神经元。Keras 库内置了许多通用函数，如`optimizers`、`activation functions`和`layer properties`，这样用户就像在 scikit-learn 中一样，不必从头开始编写这些算法。

# scikit-learn

Scikit-learn 最初是由 David Cournapeau 在 2007 年创建的，作为一种用 Python 编程语言轻松创建机器学习模型的方法。从一开始，这个库就非常受欢迎，因为它易于使用，在机器学习社区中被广泛采用，并且使用灵活。scikit-learn 通常是实践者使用 Python 实现的第一个机器学习包，因为有大量算法可用于`classification`、`regression`和`clustering`任务，以及获得结果的速度。

例如，如果您希望快速训练一个简单的回归模型，scikit-learn 的`LinearRegression`类是一个很好的选择，而如果需要一个能够学习非线性关系的更复杂的算法，scikit-learn 的`GradientBoostingRegressor`或任何一个`support vector machine`算法都是很好的选择。同样，对于分类或聚类任务，scikit-learn 提供了多种算法可供选择。

以下是使用 scikit-learn 进行机器学习的一些优点和缺点。

scikit-learn 的优势如下:

*   **成熟** : Scikit-learn 已经在社区内得到了很好的确立，并被所有技能水平的社区成员所使用。该软件包包括大多数用于分类、回归和聚类任务的常见机器学习算法。
*   **用户友好的** : Scikit-learn 具有一个易于使用的 API，允许初学者有效地进行原型开发，而不必深入理解或编写每个特定模式的代码。
*   **开源**:有一个活跃的开源社区致力于改进这个库，添加文档，并发布定期更新，这确保了这个包是稳定的和最新的。

scikit-learn 的缺点如下:

**缺乏神经网络支持**:使用人工神经网络算法的估计器很少。

注意

您可以在这里找到 scikit-learn 库的所有文档:[https://packt.live/2HcbkFj](https://packt.live/2HcbkFj)。

scikit-learn 中的估计器通常可以分为监督学习和非监督学习技术。当出现一个`target`变量时，监督学习发生。在给定其他变量的情况下，`target`变量是您试图预测的数据集的一个变量。`Supervised learning`要求已知目标变量，并训练模型以正确预测该变量。使用`logistic regression`是监督学习技术的一个很好的例子。

在`unsupervised learning`中，训练数据中没有给出目标变量，但是模型旨在分配一个目标变量。无监督学习技术的一个例子是 k 均值聚类。该算法根据数据与相邻数据点的接近程度，将数据划分为给定数量的聚类。被赋值的`target`变量可以是聚类数或聚类中心。

在实践中利用聚类示例的示例可能如下所示。假设你是一个夹克制造商，你的目标是开发各种尺寸的夹克。您无法为每位顾客量身打造一件夹克，因此，在确定夹克尺寸时，您可以选择从顾客群体中抽取与合身程度相关的各种参数样本，如身高和体重。然后，您可以使用 scikit-learn 的`k-means clustering`算法将人群分组，分组编号与您希望生产的夹克尺寸数量相匹配。从聚类算法创建的聚类中心成为夹克尺寸所基于的参数。下图显示了这一点:

![Figure 1.24: An unsupervised learning example of grouping customer parameters into clusters
](image/B15777_01_24.jpg)

图 1.24:将客户参数分组为聚类的无监督学习示例

甚至还有在机器学习模型的训练中使用未标记数据的`semi-supervised learning`技术。如果只有少量的标记数据和大量的未标记数据，可以使用这种技术。在实践中，与无监督学习相比，半监督学习在模型性能方面产生了显著的改进。

scikit-learn 库非常适合初学者，因为构建机器学习管道的一般概念可以很容易地学会。数据预处理(机器学习模型中使用的数据准备)、超参数调整(选择适当模型参数的过程)、模型评估(模型性能的定量评估)等概念都包含在库中。即使有经验的用户也发现该库易于使用，以便在使用更专业的机器学习库之前快速原型化模型。

事实上，我们讨论的各种机器学习技术，如监督和非监督学习，可以通过使用不同架构的神经网络应用于 Keras，所有这些都将在本书中讨论。

# Keras

Keras 旨在成为一种高级神经网络 API，构建在 TensorFlow、CNTK 和 Theano 等框架之上。对于初学者来说，使用 Keras 作为深度学习的入门，一个很大的好处就是非常用户友好；优化器和层等高级功能已经内置到库中，不必从头开始编写。这就是为什么 Keras 不仅受到初学者的欢迎，也受到经验丰富的专家的欢迎。此外，该库允许神经网络的快速原型化，支持各种各样的网络架构，并且可以在 CPU 和 GPU 上运行。

注意

你可以在这里找到 Keras 的库和所有文档:[https://packt.live/31Lqwms](https://packt.live/31Lqwms)。

Keras 用于创建和训练神经网络，在其他机器学习算法方面没有提供太多，包括监督算法如支持向量机和非监督算法如`k-means clustering`。不过，Keras 确实提供了一个设计良好的 API，可用于创建和训练神经网络，这消除了准确应用线性代数和多元微积分所需的大量工作。

Keras 库中可用的特定模块，如`neural layers`、`cost functions`、`optimizers`、`initialization schemes`、`activation functions`和`regularization schemes`，将在本书中详细解释。所有这些模块都具有相关的功能，可用于优化为特定任务训练神经网络的性能。

### Keras 的优势

以下是将 Keras 用于机器学习的一些主要优势:

*   用户友好的:与 scikit-learn 非常相似，Keras 具有易于使用的 API，允许用户专注于模型构建，而不是算法的细节。
*   模块化:API 由完全可配置的模块组成，这些模块可以被插在一起，无缝地工作。
*   **可扩展**:向库中添加新模块相对简单。这允许用户利用库中许多健壮的模块，同时为他们提供创建自己的模块的灵活性。
*   **开源** : Keras 是一个开源库，由于许多合作者的共同努力，Keras 不断改进并向其代码库添加模块，为所有人创建了一个健壮的库。
*   **与 Python 协同工作** : Keras 模型直接在 Python 中声明，而不是在单独的配置文件中声明，这使得 Keras 可以利用与 Python 协同工作的优势，比如易于调试和可扩展性。

### Keras 的缺点

以下是将 Keras 用于机器学习的一些主要缺点:

*   **高级定制**:虽然创建简单的定制损失函数或神经层等简单的表层定制很容易，但要改变底层架构的工作方式却很困难。
*   缺乏例子:初学者通常依靠例子来启动他们的学习。Keras 文档中可能缺少高级示例，这可能会阻碍初学者的学习。

Keras 为那些熟悉 Python 编程语言和机器学习的人提供了轻松创建神经网络架构的能力。由于神经网络相当复杂，我们将使用 scikit-learn 介绍许多机器学习概念，然后再将其应用于 Keras 库。

## 不止是建筑模型

虽然 scikit-learn 和 Keras 等机器学习库是为了帮助建立和训练预测模型而创建的，但它们的实用性远远不止于此。构建模型的一个常见用例是可以利用它们对新数据进行预测。一旦一个模型被训练，新的观察可以被输入到模型中以生成预测。模型甚至可以用作中间步骤。例如，神经网络模型可用作`feature extractors`，对图像中的对象进行分类，然后可将其输入后续模型，如下图所示:

![Figure 1.25: Classifying objects using deep learning
](image/B15777_01_25.jpg)

图 1.25:使用深度学习对对象进行分类

模型的另一个常见用例是，它们可用于通过学习数据的表示来汇总数据集。这种模型被称为自编码器，是一种神经网络架构，可用于学习给定数据集的这种表示。因此，数据集因此可以以最小的信息损失在降低的维度中表示:

![Figure 1.26: An example of using deep learning for text summarization
](image/B15777_01_26.jpg)

图 1.26:使用深度学习进行文本摘要的示例

# 模特培训

在本节中，我们将开始使我们的模型适应我们已经创建的数据集。在本章中，我们将回顾创建一个机器学习模型所需的最少步骤，该模型可以在使用任何机器学习库(包括 scikit-learn 和 Keras)构建模型时应用。

## 分类器和回归模型

这本书是关于深度学习的应用。绝大多数的深度学习任务都是监督学习，其中有一个给定的目标，我们想要拟合一个模型，这样我们就可以了解特征和目标之间的关系。

监督学习的一个例子是识别图片是否包含`dog`或`cat`。我们想要确定`input`(像素值矩阵)和`target`变量之间的关系，即图像是属于`dog`还是`cat`:

![Figure 1.27: A simple supervised learning task to classify images as dogs and cats
](image/B15777_01_27.jpg)

图 1.27:一个简单的监督学习任务，将图像分类为狗和猫

当然，我们可能需要训练数据集中更多的图像来对新图像进行稳健分类，但在这样的数据集上训练的模型能够识别区分猫和狗的各种关系，然后可以用于预测新数据的标签。

**监督学习模型**通常用于分类或回归任务。

## 分类任务

分类任务的目标是将数据模型与可用于标记`unlabeled data`的离散类别相匹配。例如，这些类型的模型可用于将图像分类为狗或猫。但并不止于二元分类；多标签分类也是可能的。这可能是一个`classification`任务的另一个例子是预测图像中狗的存在。正预测将指示图像中存在狗，而负预测将指示不存在狗。请注意，这可以很容易地转换成一个`regression`任务，即通过预测图像中狗的数量来估计连续变量，而不是分类任务估计的离散变量。

大多数分类任务输出每个唯一类的概率。该预测被确定为概率最高的类，如下图所示:

![Figure 1.28: An illustration of a classification model labeling an image
](image/B15777_01_28.jpg)

图 1.28:标记图像的分类模型的图示

一些最常见的分类算法如下:

*   **Logistic 回归**:该算法类似于线性回归，学习特征系数，取特征系数和特征的乘积之和进行预测。
*   **决策树**:该算法遵循树状结构。决策是在每个节点上做出的，分支代表节点上的可能选项，以预测的结果结束。
*   **ann**:ann 复制了生物神经网络的结构和性能，以执行模式识别任务。人工神经网络由相互连接的神经元组成，这些神经元按照设定的架构布局，相互传递信息，直到获得结果。

## 回归任务

`classification`任务的目的是用离散变量标记数据集，而`regression`任务的目的是用连续变量提供输入数据并输出数值。例如，如果您有一个股票市场价格的数据集，分类任务可以预测是买入、卖出还是持有，而回归任务将预测股票市场价格。

线性回归是一种简单但非常流行的回归算法。它仅由一个独立特征(`x`)组成，其与从属特征(`y`)的关系是线性的。由于它的简单性，它经常被忽视，尽管它对于简单的数据问题表现得非常好。

一些最常见的回归算法如下:

*   **线性回归**:该算法学习特征系数，通过取特征系数和特征的乘积之和进行预测。
*   **支持向量机**:这种算法使用内核将输入数据映射到多维特征空间，以理解特征和目标之间的关系。
*   **ann**:ann 复制了生物神经网络的结构和性能，以执行模式识别任务。人工神经网络由相互连接的神经元组成，这些神经元按照设定的架构布局，相互传递信息，直到获得结果。

## 训练数据集和测试数据集

每当我们创建机器学习模型时，我们都会将数据分成`training`和`test`数据集。训练数据是用于训练模型的一组数据。通常，它在整个数据集中占很大比例，大约在`80%`。测试数据集是从一开始就存在的数据集的样本，用于提供模型的无偏评估。测试数据集应该尽可能准确地表示真实世界的数据。报告的任何模型评估指标都应该应用于测试数据集，除非明确声明这些指标已经在训练数据集上进行了评估。这是因为模型通常会在接受训练的数据上表现得更好。

此外，模型可能会过度适应训练数据集，这意味着它们在训练数据集上表现良好，但在`test`数据集上表现不佳。如果模型在`training`数据集上的表现非常好，但在`test`数据集上表现不佳，则称该模型与数据过度拟合。相反，模型可能不符合数据。在这种情况下，模型将无法学习`features`和`target`之间的关系，这将导致在`training`和`test`数据集上进行评估时性能不佳。

我们的目标是在两者之间取得平衡，不要过度依赖`training`数据集，而是允许模型学习`features`和`target`之间的关系，以便模型能够很好地推广到新数据。下图说明了这一概念:

![Figure 1.29: An example of underfitting and overfitting a dataset
](image/B15777_01_29.jpg)

图 1.29:数据集欠拟合和过拟合的例子

有许多方法可以通过`sampling`方法分割数据集。将数据集拆分为训练集的一种方法是简单地随机采样数据，直到获得所需数量的数据点。这通常是 scikit-learn `train_test_spilt`函数等函数中的默认方法。

另一种方法是分层抽样。在分层抽样中，每个子群体都是独立抽样的。每个亚群由目标变量决定。这在像`binary classification`这样的例子中可能是有利的，在这些例子中，目标变量高度偏向一个值或另一个值，并且随机采样可能不会在`training`和`test`数据集中提供两个值的数据点。还有验证数据集，我们将在本章后面讨论。

## 模型评估指标

能够有效地评估我们的模型是很重要的，不仅仅是根据模型的性能，而且是在我们试图解决的问题的背景下。例如，假设我们构建了一个`classification`任务，根据历史股票市场价格来预测是买入、卖出还是持有股票。如果我们的模型只预测每次购买，这将不是一个有用的结果，因为我们可能没有无限的资源来购买股票。不太准确但也包括一些卖出预测可能会更好。

`classification`任务的常用评估指标包括准确度、精确度、召回率和 f1 分数。`Accuracy`定义为正确预测数除以预测总数。`Accuracy`非常容易理解和理解，适合平衡的班级。然而，当类别高度倾斜时，准确性可能会产生误导:

![Figure 1.30: Formula to calculate accuracy
](image/B15777_01_30.jpg)

图 1.30:计算精确度的公式

`Precision`是另一个有用的指标。它被定义为真阳性结果的数量除以模型预测的阳性结果(真和假)的总数:

![Figure 1.31: Formula to calculate precision
](image/B15777_01_31.jpg)

图 1.31:计算精度的公式

`Recall`被定义为正确阳性结果的数量除以来自地面实况的所有阳性结果:

![Figure 1.32: Formula to calculate recall
](image/B15777_01_32.jpg)

图 1.32:计算召回率的公式

`precision`和`recall`的得分都在`zero`和`one`之间，但其中一个得分高可能意味着另一个得分低。例如，一个模型可能具有高精度但是低召回率，这表明该模型非常准确但是遗漏了大量的正面实例。拥有一个结合了查全率和查准率的指标是很有用的。输入`F1 score`，它决定了你的模型有多精确和稳健:

![Figure 1.33: Formula to calculate the F1 score
](image/B15777_01_33.jpg)

图 1.33:计算 F1 分数的公式

在评估模型时，查看一系列不同的评估指标是很有帮助的。它们将帮助确定最合适的模型，并评估模型对预测的错误分类。

例如，以一个帮助医生预测病人是否患有罕见疾病的模型为例。通过预测每个实例的负面结果，该模型可以提供高度准确的评估，但这不会对医生或患者有很大帮助。相反，检查`precision`或`recall`可能会提供更多信息。

高精度模型非常挑剔，可能会确保所有标记为阳性的预测确实是阳性的。高召回模型可能召回许多`true`阳性实例，代价是招致许多假阳性。

当您希望确保标记为`true`的预测有很高的可能性为真时，需要一个`high precision model`。在我们的例子中，如果治疗罕见疾病的费用或治疗并发症的风险很高，这可能是所希望的。如果你想确保你的模型尽可能多地回忆起积极的一面，一个`high recall model`是需要的。在我们的例子中，如果这种罕见的疾病具有高度传染性，并且我们希望确保该疾病的所有病例都得到治疗，就可能出现这种情况。

## 练习 1.04:创建一个简单的模型

在本练习中，我们将从`scikit-learn`包中创建一个简单的`logistic regression model`。然后，我们将创建一些模型评估指标，并根据这些模型评估指标测试预测。

我们应该始终将训练任何机器学习模型作为一种迭代方法，从一个简单的模型开始，并使用模型评估指标来评估模型的性能。在这个模型中，我们的目标是将在线购物者购买意向数据集中的用户分为会在会话期间购买和不会购买的用户。按照以下步骤完成本练习:

1.  载入数据:

    ```
    import pandas as pd
    feats = pd.read_csv('../data/OSI_feats_e3.csv')
    target = pd.read_csv('../data/OSI_target_e2.csv')
    ```

2.  Begin by creating a `test` and `training` dataset. Train the data using the `training` dataset and evaluate the performance of the model on the `test` dataset.

    我们将使用`test_size = 0.2`，这意味着数据的`20%`将被保留用于测试，我们将为`random_state`参数设置一个数字:

    ```
    from sklearn.model_selection import train_test_split
    test_size = 0.2
    random_state = 42
    X_train, X_test, y_train, y_test = train_test_split(feats, target, test_size=test_size, random_state=random_state)
    ```

3.  Print out the `shape` of each DataFrame to verify that the dimensions are correct:

    ```
    print(f'Shape of X_train: {X_train.shape}')
    print(f'Shape of y_train: {y_train.shape}')
    print(f'Shape of X_test: {X_test.shape}')
    print(f'Shape of y_test: {y_test.shape}')
    ```

    上述代码产生以下输出:

    ```
    Shape of X_train: (9864, 68)
    Shape of y_train: (9864, 1)
    Shape of X_test: (2466, 68)
    Shape of y_test: (2466, 1)
    ```

    这些尺寸看起来是正确的；每个`target`数据集有一个单独的列，训练特征和`target`数据帧有相同的行数，这同样适用于`test`特征和`target`数据帧，测试数据帧是整个数据集的`20%`。

4.  Next, instantiate the model:

    ```
    from sklearn.linear_model import LogisticRegression
    model = LogisticRegression(random_state=42)
    ```

    虽然我们可以向 scikit-learn 的逻辑回归模型添加许多参数(例如正则化参数的类型和值、求解器的类型以及模型的最大迭代次数)，但我们将只通过`random_state`。

5.  然后，`fit`模型对数据进行训练:

    ```
    model.fit(X_train, y_train['Revenue'])
    ```

6.  To test the performance of the model, compare the predictions of the model with the true values:

    ```
    y_pred = model.predict(X_test)
    ```

    我们可以使用许多类型的模型评估指标。让我们从`accuracy`开始，它被定义为预测值等于真实值的比例:

    ```
    from sklearn import metrics
    accuracy = metrics.accuracy_score(y_pred=y_pred, y_true=y_test)
    print(f'Accuracy of the model is {accuracy*100:.4f}%')
    ```

    上述代码产生以下输出:

    ```
    Accuracy of the model is 87.0641%
    ```

7.  Other common evaluation metrics for classification models include `precision`, `recall`, and `fscore`. Use the scikit-learn `precison_recall_fscore_support` function, which can calculate all three:

    ```
    precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_pred=y_pred, y_true=y_test, average='binary')
    print(f'Precision: {precision:.4f}\nRecall: {recall:.4f}\nfscore: {fscore:.4f}')
    ```

    注意

    Python 中使用下划线有很多原因。它可以用来调用解释器中最后一个表达式的值，但是在本例中，我们用它来忽略函数输出的特定值。

    下图显示了上述代码的输出:

    ```
    Precision: 0.7347
    Recall: 0.3504
    fscore: 0.4745
    ```

    由于这些指标是在`0`和`1`之间评分的，因此`recall`和`fscore`不像`accuracy`那样令人印象深刻，尽管将所有这些指标放在一起看可以帮助我们找到我们的模型做得好的地方，以及它们可以通过检查模型在哪些观察中得到不正确的预测来改进的地方。

8.  Look at the coefficients that the model outputs to observe which features have a greater impact on the overall result of the prediction:

    ```
    coef_list = [f'{feature}: {coef}' for coef, feature in sorted(zip(model.coef_[0], X_train.columns.values.tolist()))]
    for item in coef_list:
        print(item)
    ```

    下图显示了上述代码的输出:

    ![Figure 1.34: The sorted important features of the model with their respective coefficients
    ](image/B15777_01_34.jpg)

图 1.34:模型中排序后的重要特征及其各自的系数

这个练习教会了我们如何创建和训练一个预测模型，在给定`feature`个变量的情况下预测`target`个变量。我们将`feature`和`target`数据集分成`training`和`test`数据集。然后，我们在`training`数据集上训练我们的模型，并在`test`数据集上评估我们的模型。最后，我们观察了这个模型的训练系数。

# 模型调整

在这一节中，我们将进一步深入评估模型的性能，并检查我们可以使用的技术，以便使用`regularization`将模型推广到新数据。提供模型性能的背景是极其重要的。我们的目标是确定我们的模型与普通或明显的方法相比是否表现良好。我们通过创建一个基线模型来做到这一点，我们训练的机器学习模型将与该基线模型进行比较。需要强调的是，所有模型评估指标都是通过`test`数据集进行评估和报告的，因为这将让我们了解模型在新数据上的表现。

## 基线模型

基线模型应该是一个简单且易于理解的过程，并且该模型的性能应该是我们构建的任何模型的最低可接受性能。对于分类模型，一个有用且简单的基线模型是计算模型结果值。例如，如果有`60%` `false`个值，我们的基线模型将预测每个值为假，这将给我们一个`60%`的`accuracy`。对于`regression models`，可以使用`mean`或`median`作为基线。

## 练习 1.05:确定基线模型

在本练习中，我们将把模型性能放入上下文中。我们从模型中获得的准确性似乎不错，但我们需要一些东西来与之比较。由于机器学习模型的性能是相对的，因此开发一个健壮的基线来比较模型是很重要的。我们再次使用在线购物者购买意向数据集，我们的`target`变量是每个用户是否会在他们的会话中购买产品。按照以下步骤完成本练习:

1.  导入`pandas`库并载入`target`数据集:

    ```
    import pandas as pd
    target = pd.read_csv('../data/OSI_target_e2.csv')
    ```

2.  Next, calculate the relative proportion of each value of the `target` variables:

    ```
    target['Revenue'].value_counts()/target.shape[0]*100
    ```

    下图显示了上述代码的输出:

    ![Figure 1.35: Relative proportion of each value
    ](image/B15777_01_35.jpg)

    图 1.35:每个值的相对比例

3.  Here, we can see that `0` is represented `84.525547%` of the time—that is, there is no purchase by the user, and this is our `baseline` accuracy. Now, for the other model evaluation metrics:

    ```
    from sklearn import metrics
    y_baseline = pd.Series(data=[0]*target.shape[0])
    precision, recall, fscore, _ = metrics.precision_recall_fscore_support(y_pred=y_baseline, y_true=target['Revenue'], average='macro')
    ```

    这里，我们已经设置了预测`0`的基线模型，并重复了该值，使其与`test`数据集中的行数相同。

    注意

    `precision_recall_fscore_support`函数中的平均参数必须设置为`macro`，因为当它设置为`binary`时，就像以前一样，该函数正在寻找`true`值，而我们的`baseline`模型只包含`false`值。

4.  Print the final output for precision, recall, and fscore:

    ```
    print(f'Precision: {precision:.4f}\nRecall:{recall:.4f}\nfscore: {fscore:.4f}')
    ```

    上述代码产生以下输出:

    ```
    Precision: 0.9226
    Recall: 0.5000
    Fscore: 0.4581
    ```

现在，我们有了一个基线模型，可以与之前的模型以及任何后续模型进行比较。通过这样做，我们可以看出，虽然我们以前的模型的准确性似乎很高，但它并不比这个`baseline`模型好多少。

## 正规化

在本章的早些时候，我们学习了`overfitting`和它的样子。`overfitting`的标志是当一个模型在训练数据上被训练，表现非常好，但在`test`数据上表现很糟糕。出现这种情况的一个原因可能是，该模型可能过于依赖某些特征，这些特征在训练数据集中产生良好的性能，但不能很好地推广到数据或测试数据集的新观察结果。

可以用来避免这种情况的一种技术叫做`regularization`。正则化将系数值限制为零，这不利于复杂的模型。有许多不同类型的正则化技术。例如在`linear`和`logistic`回归中，`ridge`和`lasso`正则化是最常见的。在基于树的模型中，限制树的最大深度起到正则化的作用。

有两种不同类型的正则化，即`L1`和`L2`。这一项或者是权重的`L2`范数(平方和),或者是权重的`L1`范数(绝对值和)。由于`l1`正则化参数充当特征选择器，它能够将特征的系数降低到零。我们可以使用该模型的输出来观察哪些特性对性能没有太大贡献，如果需要的话，可以完全删除它们。`l2`正则化参数不会将特征的系数降为零，所以我们会观察到它们都有非零值。

以下代码显示了如何使用这些正则化技术实例化模型:

```
model_l1 = LogisticRegressionCV(Cs=Cs, penalty='l1', cv=10, solver='liblinear', random_state=42)
model_l2 = LogisticRegressionCV(Cs=Cs, penalty='l2', cv=10, random_state=42)
```

以下代码显示了如何拟合模型:

```
model_l1.fit(X_train, y_train['Revenue'])
model_l2.fit(X_train, y_train['Revenue'])
```

lasso 和 ridge 正则化中的相同概念可以应用于 ann。然而，惩罚发生在权重矩阵而不是系数上。辍学是用于防止人工神经网络过度拟合的另一种正规化形式。Dropout 在每次迭代中随机选择节点，并删除它们及其连接，如下图所示:

![Figure 1.36: Dropout regularization in ANNs
](image/B15777_01_36.jpg)

图 1.36:人工神经网络中的辍学调整

## 交叉验证

交叉验证通常与正则化结合使用，以帮助调整超参数。以岭和套索回归中的`penalization`参数为例，或者使用人工神经网络的剔除技术在每次迭代中剔除的节点比例。您将如何确定使用哪个参数？一种方法是为正则化参数的每个值运行模型，并在测试集上评估它们；然而，使用测试集经常会在模型中引入偏差。

交叉验证的一个流行的例子叫做 k-fold 交叉验证。这项技术使我们能够在看不见的数据上测试我们的模型，同时保留我们将在最后用来测试的测试集。使用这种方法，数据被分成`k`个子集。在每一次`k`迭代中，子集的`k-1`被用作训练数据，剩余的子集被用作验证集。重复`k`次，直到所有 k 个子集都被用作验证集。

通过使用这种技术，可以显著降低偏差，因为大部分数据都用于拟合。由于大部分数据也用于验证，因此差异也减少了。典型地，有在`5`和`10`之间的折叠，并且该技术甚至可以被分层，这在存在大的类别不平衡时是有用的。

下面的例子显示了数据的`5-fold cross-validation`和`20%`作为测试集。剩下的`80%`被分成 5 折。这些折叠中的四个构成了训练数据，剩余的折叠是验证数据。这总共重复五次，直到每个折叠都被用于一次验证:

![Figure 1.37: A figure demonstrating how 5-fold cross-validation works
](image/B15777_01_37.jpg)

图 1.37:展示了五重交叉验证是如何工作的

## 活动 1.01:向模型添加正则化

在本活动中，我们将利用 scikit-learn 软件包中的相同逻辑回归模型。然而，这一次，我们将在模型中添加正则化，并搜索最佳正则化参数，这一过程通常称为超参数调整。训练模型后，我们将测试预测，并将模型评估指标与基线模型和未经正则化的模型产生的指标进行比较。

我们将采取的步骤如下:

1.  从`'../data/OSI_feats_e3.csv'`和`'../data/OSI_target_e2.csv'`加载在线购物者购买意向数据集的特征和目标数据集。
2.  为每个`feature`和`target`数据集创建`training`和`test`数据集。`training`数据集将用于训练，模型将使用`test`数据集进行评估。
3.  实例化 scikit-learn 的`linear_model`包的`LogisticRegressionCV`类的模型实例。
4.  根据`training`数据拟合模型。
5.  使用训练好的模型对`test`数据集进行预测。
6.  使用评估指标，通过比较模型与`true`值的得分来评估模型。

实现这些步骤后，您应该得到以下预期输出:

```
l1
Precision: 0.7300
Recall: 0.4078
fscore: 0.5233
l2
Precision: 0.7350
Recall: 0.4106
fscore: 0.5269
```

注意

这项活动的解决方案可在第 312 页找到。

这个活动教会了我们如何使用`conjunction`中的`regularization`和`cross-validation`来恰当地给一个模型打分。我们已经学会了如何使用正则化和交叉验证来使模型适合数据。正则化是一项重要的技术，用于确保模型不会过度拟合训练数据。经过正则化训练的模型将在新数据上表现更好，这通常是机器学习模型的目标-在给定输入数据的新观察值时预测目标。选择最佳正则化参数可能需要迭代多个不同的选择。

`Cross-validation`是一种用于确定哪组正则化参数最适合数据的技术。交叉验证将在不同的数据切片上用不同的正则化参数值训练多个模型。该技术确保选择最佳的正则化参数集，而不增加偏差和最小化方差。

# 总结

在本章中，我们讨论了如何准备数据和构建机器学习模型。我们通过利用 Python 和库(如 pandas 和 scikit-learn)实现了这一点。我们还使用 scikit-learn 中的算法来构建我们的机器学习模型。

然后，我们学习了如何将数据加载到 Python 中，以及如何操作数据，以便可以根据数据训练机器学习模型。这涉及到将所有列转换成数字数据类型。我们还使用 scikit-learn 算法创建了一个基本的逻辑回归分类模型。我们将数据集分为训练数据集和测试数据集，并使模型适合训练数据集。我们使用模型评估指标，即准确度、精确度、召回率和 fscore，在测试数据集上评估了模型的性能。

最后，我们通过创建两个具有不同正则化类型的模型来迭代这个基本模型。我们利用交叉验证来确定用于正则化参数的最佳参数。

在下一章中，我们将使用这些相同的概念通过 Keras 库来创建模型。我们将使用相同的数据集，并尝试为相同的分类任务预测相同的目标值。通过这样做，我们将学习如何使用`regularization`、`cross-validation`和`model evaluation metrics`来拟合我们的神经网络数据。