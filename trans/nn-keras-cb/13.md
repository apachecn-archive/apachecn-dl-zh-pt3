<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 序列对序列学习

在前面的章节中，我们学习了 RNN 应用程序，其中有多个输入(每个时间步长一个)和一个输出。然而，还有一些应用，其中有多个输入，也有多个时间步长，例如机器翻译，其中源句子中有多个输入单词，目标句子中有多个输出单词。给定多个输入和多个输出，这就变成了一个基于多输出 RNN 的应用程序——本质上是一个序列到序列的学习任务。这就要求我们建立不同于我们到目前为止所建立的模型架构，我们将在本章中学习。在本章中，我们将了解以下内容:

*   从网络返回序列
*   双向 LSTM 如何帮助命名实体提取
*   提取意图和实体来构建聊天机器人
*   编码器解码器网络架构的功能
*   使用编码器/解码器架构将英语句子翻译成法语
*   使用注意机制改进翻译

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 介绍

在前面的章节中，我们了解到 LSTM，甚至是 RNN，会返回上一时间步的结果(上一时间步的隐藏状态值会传递到下一层)。想象一个场景，其中输出的大小为五维，五维是五个输出(不是五个类的 softmax 值)。为了进一步解释这个想法，假设我们预测的不仅仅是下一个日期的股票价格，而是未来五天的股票价格。或者，对于给定的输入序列组合，我们不仅要预测下一个单词，还要预测下五个单词的序列。

这种情况要求在构建网络时采用不同的方法。在下一节中，我们将研究构建网络的多种场景，以提取不同时间步长的输出。

**场景 1** :命名实体提取

在命名实体抽取中，我们试图为出现在句子中的每个单词分配一个标签——无论它是否与人或地点相关。因此，这就变成了一个输入单词和它的输出类之间的一一对应的问题。虽然它是输入和输出之间的一对一映射，但是在某些情况下，周围的单词在决定所考虑的输入是否是命名实体时起作用。例如，单词 *new* 本身可能不是一个命名实体。但是，如果 *new* 伴随着 *york* ，那么我们知道它是一个命名实体。因此，输入时间步长在确定一个词是否是命名实体时起作用，这是一个问题，即使在大多数情况下，输入和输出之间可能存在一对一的映射。

此外，这是一个序列返回问题，因为我们根据单词的输入序列来指定命名实体的输出序列。鉴于此，这是一个输入之间存在一对一关系的问题，并且周围时间步长中的输入在确定输出时也起着关键作用。到目前为止，我们所学的传统 LSTM 是可行的，只要我们确保时间步长的两个方向上的单词都会影响输出。因此，双向 LSTM 在解决这类问题时就派上了用场。

双向 LSTM 的体系结构如下所示:

![](Images/94365b7c-707e-40a3-91cf-6d910224b71e.png)

请注意，在上图中，我们修改了传统的 LSTM，使输入也以相反的方向相互连接，从而确保信息从两个方向流动。我们将在后面的章节中了解更多关于双向 LSTM 的工作原理以及如何应用它。

**场景 2** :文本摘要

文本摘要任务需要与我们之前讨论的不同的架构，因为我们通常只有在阅读完整个输入句子(在这种情况下是输入文本/评论)之后才能从文本中生成摘要。

这要求将所有输入编码成一个向量，然后基于输入的编码向量生成输出。此外，假设对于文本中的给定单词序列有多个输出(多个单词)，这就变成了多输出生成问题，因此，这是可以利用 RNN 的多输入多输出能力的另一种情况。

让我们看看我们如何潜在地构建模型以得出解决方案:

![](Images/8b95da24-105a-44a0-95b4-f7013121e0f0.png)

注意，在前面的架构中，我们将所有输入文本编码到在输入序列的结束字处产生的向量中，并且该编码的向量作为输入被传递到解码器序列。关于如何建立这个网络的更多信息将在本章的后面部分提供。

**场景三**:机器翻译

在前面的场景中，我们将输入编码到一个向量中，希望这个向量也包含单词顺序。但是，如果我们通过网络明确地提供一种机制，在这种机制中，网络能够根据我们正在解码的单词的位置，为位于给定位置的输入单词分配不同的权重，会怎么样呢？例如，如果源单词和目标单词的对齐方式相似，即两种语言的词序相似，则源语言开头出现的单词对目标语言的最后一个单词的影响非常小，但对决定目标语言的第一个单词的影响非常大。

注意机制看起来如下:

![](Images/436d46d9-15db-42f7-9905-f830e190e66a.png)

注意，注意力向量(在中间)受到输入编码向量和输出值隐藏状态的影响。关于如何利用注意力机制的更多信息将在？

凭着对不同编码器/解码器架构的原因的直觉，让我们深入了解更多关于在 Keras 中生成输出序列的知识。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 从网络返回输出序列

正如我们在上一节中所讨论的，有多种方法可以构建网络来生成输出序列。在本节中，我们将了解编码器解码器生成输出的方式，以及玩具数据集上输入到输出网络的一对一映射，以便我们对其工作原理有一个深刻的理解。

让我们定义一个输入序列和一个相应的输出序列，如下所示(代码文件在 GitHub 中以`Return_state_and_sequences_working_details.ipynb`的形式提供):

```
input_data = np.array([[1,2],[3,4]])
output_data = np.array([[3,4],[5,6]])
```

我们可以看到，在一个输入中有两个时间步长，输入有一个相应的输出。

如果我们要用传统的方法解决这个问题，我们可以用下面的代码来定义模型架构。请注意，在后面的场景中，我们使用的是函数式 API，我们将提取多个输出，并检查中间层:

```
# define model
inputs1 = Input(shape=(2,1))
lstm1 = LSTM(1, activation = 'tanh', return_sequences=False,recurrent_initializer='Zeros',recurrent_activation='sigmoid')(inputs1)
out= Dense(2, activation='linear')(lstm1)
model = Model(inputs=inputs1, outputs=out)
model.summary()
```

![](Images/41ed74e0-5c52-4375-8fb3-a94922771671.png)

请注意，在前面的场景中，向 LSTM 提供了以下形式的数据(`batch_size`，时间步长，每个时间步长的特征)。假设 LSTM 不返回输出序列，则 LSTM 的输出是隐藏层的一个值(因为 LSTM 中的单位数是 1)。

假设输出是二维的，我们将添加一个密集层，它接受隐藏层的输出并从中提取`2`值。

让我们继续拟合模型，如下所示:

```
model.compile(optimizer='adam',loss='mean_squared_error')
model.fit(input_data.reshape(2,2,1), output_data,epochs=1000)
print(model.predict(input_data[0].reshape(1,2,1)))
# [[2.079641 1.8290598]]
```

现在我们已经有了输出，让我们继续验证上一章的结果(注意，这和我们上一章的代码完全一样——在[第 11 章](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml)、*构建循环神经网络*的*用 Python 从零开始构建 LSTM*部分提供了解释):

```
input_t0 = 1
cell_state0 = 0
forget0 = input_t0*model.get_weights()[0][0][1] + model.get_weights()[2][1]
forget1 = 1/(1+np.exp(-(forget0)))
cell_state1 = forget1 * cell_state0
input_t0_1 = input_t0*model.get_weights()[0][0][0] + model.get_weights()[2][0]
input_t0_2 = 1/(1+np.exp(-(input_t0_1)))
input_t0_cell1 = input_t0*model.get_weights()[0][0][2] + model.get_weights()[2][2]
input_t0_cell2 = np.tanh(input_t0_cell1)
input_t0_cell3 = input_t0_cell2*input_t0_2
input_t0_cell4 = input_t0_cell3 + cell_state1
output_t0_1 = input_t0*model.get_weights()[0][0][3] + model.get_weights()[2][3]
output_t0_2 = 1/(1+np.exp(-output_t0_1))
hidden_layer_1 = np.tanh(input_t0_cell4)*output_t0_2
input_t1 = 2
cell_state1 = input_t0_cell4
forget21 = hidden_layer_1*model.get_weights()[1][0][1] + model.get_weights()[2][1] + input_t1*model.get_weights()[0][0][1]
forget_22 = 1/(1+np.exp(-(forget21)))
cell_state2 = cell_state1 * forget_22
input_t1_1 = input_t1*model.get_weights()[0][0][0] + model.get_weights()[2][0] + hidden_layer_1*model.get_weights()[1][0][0]
input_t1_2 = 1/(1+np.exp(-(input_t1_1)))
input_t1_cell1 = input_t1*model.get_weights()[0][0][2] + model.get_weights()[2][2]+ hidden_layer_1*model.get_weights()[1][0][2]
input_t1_cell2 = np.tanh(input_t1_cell1)
input_t1_cell3 = input_t1_cell2*input_t1_2
input_t1_cell4 = input_t1_cell3 + cell_state2
output_t1_1 = input_t1*model.get_weights()[0][0][3] + model.get_weights()[2][3]+ hidden_layer_1*model.get_weights()[1][0][3]
output_t1_2 = 1/(1+np.exp(-output_t1_1))
hidden_layer_2 = np.tanh(input_t1_cell4)*output_t1_2
final_output = hidden_layer_2 * model.get_weights()[3][0] + model.get_weights()[4]
```

`final_output`的输出如下:

```
[[2.079 1.829]]
```

您应该注意到，前面生成的`final_output`与我们在`model.predict`输出中看到的完全相同。

以这种方式生成输出的一个缺点是，在时间*步骤 1* 的输出绝对不依赖于时间*步骤 2* 的情况下，我们很难让模型想出一种方法来分离时间*步骤 2* 值对时间*步骤 1* 的影响，因为我们从时间*步骤 2* 获取隐藏层输出(这是时间*步骤 1* 和时间的输入值的组合

我们可以通过从每个时间步长提取隐藏层值，然后将其传递给密集层来解决这个问题。

**返回每个时间步的隐藏层值序列**

在以下代码中，我们将了解如何在每个时间步长返回隐藏层值序列:

```
# define model
inputs1 = Input(shape=(2,1))
lstm1 = LSTM(1, activation = 'tanh', return_sequences=False,recurrent_initializer='Zeros',recurrent_activation='sigmoid')(inputs1)
out= Dense(1, activation='linear')(lstm1)
model = Model(inputs=inputs1, outputs=out)
model.summary()
```

请注意，我们在代码中做了如下两处更改:

*   将参数`return_sequences`的值更改为`True`
*   密集层，给出一个值`1`作为输出:

![](Images/a0f4c974-43dd-4375-8968-774c5e1e97d0.png)

注意，因为我们已经在每个时间步长提取了隐藏层值的输出(其中隐藏层有一个单位)，所以 LSTM 的输出形状是(批量大小，时间步长，1)。

此外，由于每个时间步长都有一个密集图层将 LSTM 输出连接到最终输出，因此输出形状保持不变。

让我们继续拟合模型，如下所示:

```
model.compile(optimizer='adam',loss='mean_squared_error')
model.fit(input_data.reshape(2,2,1), output_data.reshape(2,2,1),epochs=1000)
```

预测值如下:

```
print(model.predict(input_data[0].reshape(1,2,1)))
```

前面的执行将给出以下输出:

```
[[[1.7584195] [2.2500749]]]
```

与上一节类似，我们将通过权重向前传递输入来验证我们的结果，然后匹配我们的预测值。

我们将提取第一时间步的输出，如下所示:

```
input_t0 = 1
cell_state0 = 0
forget0 = input_t0*model.get_weights()[0][0][1] + model.get_weights()[2][1]
forget1 = 1/(1+np.exp(-(forget0)))
cell_state1 = forget1 * cell_state0
input_t0_1 = input_t0*model.get_weights()[0][0][0] + model.get_weights()[2][0]
input_t0_2 = 1/(1+np.exp(-(input_t0_1)))
input_t0_cell1 = input_t0*model.get_weights()[0][0][2] + model.get_weights()[2][2]
input_t0_cell2 = np.tanh(input_t0_cell1)
input_t0_cell3 = input_t0_cell2*input_t0_2
input_t0_cell4 = input_t0_cell3 + cell_state1
output_t0_1 = input_t0*model.get_weights()[0][0][3] + model.get_weights()[2][3]
output_t0_2 = 1/(1+np.exp(-output_t0_1))
hidden_layer_1 = np.tanh(input_t0_cell4)*output_t0_2
final_output_1 = hidden_layer_1 * model.get_weights()[3][0] + model.get_weights()[4]
final_output_1
*# 1.7584*
```

您应该注意到`final_output_1`值与第一个时间步长的预测值相匹配。同样，让我们继续验证第二个时间步的预测:

```
input_t1 = 2
cell_state1 = input_t0_cell4
forget21 = hidden_layer_1*model.get_weights()[1][0][1] + model.get_weights()[2][1] + input_t1*model.get_weights()[0][0][1]
forget_22 = 1/(1+np.exp(-(forget21)))
cell_state2 = cell_state1 * forget_22
input_t1_1 = input_t1*model.get_weights()[0][0][0] + model.get_weights()[2][0] + hidden_layer_1*model.get_weights()[1][0][0]
input_t1_2 = 1/(1+np.exp(-(input_t1_1)))
input_t1_cell1 = input_t1*model.get_weights()[0][0][2] + model.get_weights()[2][2]+ hidden_layer_1*model.get_weights()[1][0][2]
input_t1_cell2 = np.tanh(input_t1_cell1)
input_t1_cell3 = input_t1_cell2*input_t1_2
input_t1_cell4 = input_t1_cell3 + cell_state2
output_t1_1 = input_t1*model.get_weights()[0][0][3] + model.get_weights()[2][3]+ hidden_layer_1*model.get_weights()[1][0][3]
output_t1_2 = 1/(1+np.exp(-output_t1_1))
hidden_layer_2 = np.tanh(input_t1_cell4)*output_t1_2
final_output_2 = hidden_layer_2 * model.get_weights()[3][0] + model.get_weights()[4]
final_output_2
*# 2.250*
```

您应该注意到，这将返回与第二个时间步长的`model.predict`值完全相同的值。

现在我们已经了解了网络中的参数`return_sequences`，让我们继续学习另一个名为`return_state`的参数。我们知道一个网络的两个输出是隐层值(当`return_sequences`为`False`时也是 LSTM 在最后一个时间步长的输出，当`return_sequences`为`True`时也是 LSTM 在每个时间步长的输出)和细胞状态值。

`return_state`有助于提取网络的小区状态值。

当输入文本被编码成向量时，提取单元状态是有用的，我们不仅将编码的向量，而且将输入编码器的最终单元状态传递给解码器网络(在*用于机器翻译的编码器解码器架构*部分中有更多相关信息)。

在下面的部分，我们来了解一下`return_state`是如何工作的。请注意，我们只需理解单元状态是如何在每个时间步生成的，因为在实践中，我们将使用该步骤的输出(隐藏层值和单元状态值)作为解码器的输入:

```

inputs1 = Input(shape=(2,1))
lstm1,state_h,state_c = LSTM(1, activation = 'tanh', return_sequences=True, return_state = True, recurrent_initializer='Zeros',recurrent_activation='sigmoid')(inputs1)
model = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])

```

在前面的代码中，我们也将参数`return_state`设置为`True`。现在请注意 LSTM 的输出:

*   `lstm1`:每个时间步的隐藏层(因为`return_sequences`在前面的场景中是`True`)
*   `state_h`:最后一个时间步的隐藏层值
*   `state_c`:最后一个时间步的单元状态值

让我们继续预测值，如下所示:

```
print(model.predict(input_data[0].reshape(1,2,1)))
```

我们将获得以下值:

```
[array([[[-0.256911 ], [-0.6683883]]], dtype=float32), array([[-0.6683883]], dtype=float32), array([[-0.96862674]], dtype=float32)]
```

您应该看到有三个输出数组，正如我们之前讨论的:隐藏层值序列、最终隐藏层值和单元格状态值。

让我们验证之前得出的数字:

```
input_t0 = 1
cell_state0 = 0
forget0 = input_t0*model.get_weights()[0][0][1] + model.get_weights()[2][1]
forget1 = 1/(1+np.exp(-(forget0)))
cell_state1 = forget1 * cell_state0
input_t0_1 = input_t0*model.get_weights()[0][0][0] + model.get_weights()[2][0]
input_t0_2 = 1/(1+np.exp(-(input_t0_1)))
input_t0_cell1 = input_t0*model.get_weights()[0][0][2] + model.get_weights()[2][2]
input_t0_cell2 = np.tanh(input_t0_cell1)
input_t0_cell3 = input_t0_cell2*input_t0_2
input_t0_cell4 = input_t0_cell3 + cell_state1
output_t0_1 = input_t0*model.get_weights()[0][0][3] + model.get_weights()[2][3]
output_t0_2 = 1/(1+np.exp(-output_t0_1))
hidden_layer_1 = np.tanh(input_t0_cell4)*output_t0_2
print(hidden_layer_1)
```

前面计算的`hidden_layer_1`的值是`-0.2569`，这是我们通过`model.predict`方法得到的:

```
input_t1 = 2
cell_state1 = input_t0_cell4
forget21 = hidden_layer_1*model.get_weights()[1][0][1] + model.get_weights()[2][1] + input_t1*model.get_weights()[0][0][1]
forget_22 = 1/(1+np.exp(-(forget21)))
cell_state2 = cell_state1 * forget_22
input_t1_1 = input_t1*model.get_weights()[0][0][0] + model.get_weights()[2][0] + hidden_layer_1*model.get_weights()[1][0][0]
input_t1_2 = 1/(1+np.exp(-(input_t1_1)))
input_t1_cell1 = input_t1*model.get_weights()[0][0][2] + model.get_weights()[2][2]+ hidden_layer_1*model.get_weights()[1][0][2]
input_t1_cell2 = np.tanh(input_t1_cell1)
input_t1_cell3 = input_t1_cell2*input_t1_2
input_t1_cell4 = input_t1_cell3 + cell_state2
output_t1_1 = input_t1*model.get_weights()[0][0][3] + model.get_weights()[2][3]+ hidden_layer_1*model.get_weights()[1][0][3]
output_t1_2 = 1/(1+np.exp(-output_t1_1))
hidden_layer_2 = np.tanh(input_t1_cell4)*output_t1_2
print(hidden_layer_2, input_t1_cell4)
```

`hidden_layer_2`和`input_t1_cell4`的值分别为`-0.6683`和`-0.9686`。

您会注意到输出与我们在 predict 函数中看到的完全相同。

在双向网络的情况下，当我们从两侧计算隐藏层值时，我们会合并隐藏层值，编码如下:

```
inputs1 = Input(shape=(2,1))
lstm1,state_fh,state_fc,state_bh,state_bc = Bidirectional(LSTM(1, activation = 'tanh', return_sequences=True, return_state = True, recurrent_initializer='Zeros',recurrent_activation='sigmoid'))(inputs1)
model = Model(inputs=inputs1, outputs=[lstm1, state_fh,state_fc,state_bh,state_bc])
model.summary()
```

请注意，在双向 LSTM 中，最终隐藏状态有两个输出，一个是从左到右考虑输入时间步长，另一个是从右到左考虑输入时间步长。以类似的方式，我们有两个可能的单元状态值。

通常，我们会将得到的隐藏状态连接到一个向量中，并将单元格状态连接到另一个连接的向量中。

为简洁起见，我们在本书中不验证双向 LSTM 的输出。然而，你可以在本章附带的 Jupyter 笔记本中检查验证。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 构建聊天机器人

在聊天机器人自动执行一些更常见的查询的情况下，聊天机器人很有帮助。这在实际场景中非常有用，尤其是在您必须从数据库中查找结果或查询 API 以获得查询结果的情况下。

有鉴于此，有两种方法可以设计聊天机器人，如下所示:

*   将非结构化用户查询转换成结构化格式:
    *   基于转换后的结构从数据库查询
*   基于输入文本生成响应

对于这个练习，我们将采用第一种方法，因为它更有可能给出可以在呈现给用户之前进一步调整的预测。此外，在我们完成机器翻译和文本摘要案例研究后，我们还将理解为什么我们可能不想基于输入文本生成响应。

将用户查询转换为结构化格式包括以下两个步骤:

1.  为查询中的每个单词分配实体
2.  理解查询的意图

命名实体识别是跨行业的多种应用之一。例如，用户想去哪里旅行？用户考虑购买哪种产品？诸如此类。在这些例子中，命名实体识别可能是从现有城市名或产品名的字典中进行简单的查找。然而，考虑一个场景，其中用户说*我想从波士顿旅行到西雅图*。在这种情况下，虽然机器知道*波斯顿*和*西雅图*都是城市名，但我们无法分辨哪个是从城市出发的*，哪个是从*到*城市。*

虽然我们可以添加一些试探法，比如在从*到城市*之前有一个从*到*的名字，另一个是从城市到*的名字，但是它是不可伸缩的，因为我们在多个这样的例子中重复这个过程。在这种情况下，神经网络就派上了用场，因为它不太依赖我们来手动调整特性。我们将让机器来处理特征工程部分，以给我们输出。*

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 做好准备

有了前面的直觉，让我们继续定义我们的方法，为一个包含与航空公司相关的用户查询的数据集解决这个问题。

**目标**:从查询中提取各种实体以及查询的意图。

**接近**:

*   我们将找到一个数据集，它具有查询标签和查询中每个单词所属的实体:
    *   如果我们没有带标签的数据集，我们将手动注释查询中的实体，以获得合理数量的示例，这样我们就可以训练我们的模型
*   考虑到周围的单词会对给定单词的分类产生影响，让我们使用基于 RNN 的技术来解决这个问题
*   此外，考虑到周围的单词可能在给定单词的左边或右边，我们将使用双向 RNN 来解决这个问题
*   预处理输入数据集，以便将其输入到 RNN 的多个时间步长中
*   一次热编码输出数据集，以便我们可以优化模型
*   构建返回查询中每个单词所属实体的模型
*   类似地，构建另一个提取查询意图的模型

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 怎么做...

让我们对之前定义的方法进行编码，如下所示(代码文件在 GitHub 中以`Intent_and_entity_extraction.ipynb`的形式提供):

1.  导入数据集，如以下代码所示:

```
!wget https://www.dropbox.com/s/qpw1wnmho8v0gi4/atis.zip
!unzip atis.zip
```

加载训练数据集:

```
import numpy as np 
import pandas as pd
import pickle
DATA_DIR="/content"
def load_ds(fname='atis.train.pkl'):
     with open(fname, 'rb') as stream:
     ds,dicts = pickle.load(stream)
     print('Done loading: ', fname)
     print(' samples: {:4d}'.format(len(ds['query'])))
     print(' vocab_size: {:4d}'.format(len(dicts['token_ids'])))
     print(' slot count: {:4d}'.format(len(dicts['slot_ids'])))
     print(' intent count: {:4d}'.format(len(dicts['intent_ids'])))
     return ds,dicts
```

```
import os
train_ds, dicts = load_ds(os.path.join(DATA_DIR,'atis.train.pkl'))
test_ds, dicts = load_ds(os.path.join(DATA_DIR,'atis.test.pkl'))
```

上述代码给出了以下输出:

![](Images/05d0278c-db84-4685-817d-f16a1f3dd349.png)

请注意，附加数据集中的样本是用户查询，slot 是单词所属的实体，intent 是查询的总体意图。

2.  将 id 应用于查询、位置和意图中的每个词:

```
t2i, s2i, in2i = map(dicts.get, ['token_ids', 'slot_ids','intent_ids'])
i2t, i2s, i2in = map(lambda d: {d[k]:k for k in d.keys()}, [t2i,s2i,in2i])
query, slots, intent = map(train_ds.get, ['query', 'slot_labels', 'intent_labels'])
```

令牌(词汇表中的单词)、槽(单词的实体)和意图的 id 示例如下:

![](Images/176a3d2e-003c-4b7a-b8ec-fb5a2a0cea62.png)

最后，查询、插槽和意向被转换为 ID 值，如下所示(其中我们报告第一个查询、意向和插槽的输出):

![](Images/340d4444-f66e-435e-af01-a205b4b24886.png)

对应于查询中的单词的查询、意图和实体的示例如下:

```
for j in range(len(query[i])):
        print('{:>33} {:>40}'.format(i2t[query[i][j]],
                                     i2s[slots[i][j]]))
```

![](Images/c5a633a2-8dd1-41bc-bf74-c6e032c8af78.png)

Query 是前面屏幕截图顶部的语句。槽代表每个单词所属的对象类型。注意，`O`代表一个对象，其他所有的实体名称都是自描述的。此外，总共有 23 种可能的意图类别在总体上描述了查询。

在下面的代码中，我们将总数据转换为列表的列表，其中每个列表对应于数据集中的一个查询:

```
i2t2 = []
i2s2 = []
c_intent=[]
for i in range(4978):
     a_query = []
     b_slot = []
     c_intent.append(i2in[intent[i][0]])
     for j in range(len(query[i])):
         a_query.append(i2t[query[i][j]])
         b_slot.append(i2s[slots[i][j]])
     i2t2.append(a_query)
     i2s2.append(b_slot)
i2t2 = np.array(i2t2)
i2s2 = np.array(i2s2)
i2in2 = np.array(c_intent)
```

令牌、意图和查询的示例如下:

![](Images/ac4fb687-0852-40b5-98b4-b9716ee3dff6.png)

3.  创建索引输入和输出:

```
final_sentences = []
final_targets = []
final_docs = []
for i in range(len(i2t2)):
  tokens = ''
  entities = ''
  intent = ''
  for j in range(len(i2t2[i])):
    tokens= tokens + i2t2[i][j] + ' '
    entities = entities + i2s2[i][j] +' '
  intent = i2in2[i]
  final_sentences.append(tokens)
  final_targets.append(entities)
  final_docs.append(intent)
```

前面的代码给出了最终查询和目标的列表，如下所示:

![](Images/e318c431-d86c-4258-8a57-42afef0e49c2.png)

现在，我们将每个**输入的**句子转换成相应的组成单词的 id 列表:

```
from collections import Counter
counts = Counter()
for i,sentence in enumerate(final_sentences):
     counts.update(sentence.split())
sentence_words = sorted(counts, key=counts.get, reverse=True)
chars = sentence_words
nb_chars = len(chars)
sentence_word_to_int = {word: i for i, word in enumerate(sentence_words, 1)}
sentence_int_to_word = {i: word for i, word in enumerate(sentence_words, 1)}
mapped_reviews = []
for review in final_sentences:
     mapped_reviews.append([sentence_word_to_int[word] for word in review.split()])
```

在下面的代码中，我们将每个**输出**字转换成它的组成字 id:

```
from collections import Counter
counts = Counter()
for i,sentence in enumerate(final_targets):
    counts.update(sentence.split())
target_words = sorted(counts, key=counts.get, reverse=True)
chars = target_words
nb_chars = len(target_words)
```

```
target_word_to_int = {word: i for i, word in enumerate(target_words, 1)}
target_int_to_word = {i: word for i, word in enumerate(target_words, 1)}
mapped_targets = []
for review in final_targets:
    mapped_targets.append([target_word_to_int[word] for word in review.split()])
```

4.  填充输入并对输出进行一次热编码:

```
from keras.preprocessing.sequence import pad_sequences
y = pad_sequences(maxlen=124, sequences=mapped_targets, padding="post", value=0)
from keras.utils import to_categorical
y2 = [to_categorical(i, num_classes=124) for i in y]
y3 = np.array(y2)
```

在下面的代码中，我们在填充输入之前决定查询的最大长度:

```
length_sent = []
for i in range(len(mapped_reviews)):
     a = mapped_reviews[i]
     b = len(a)
     length_sent.append(b)
np.max(length_sent)
```

在前面的代码中，我们在填充输入之前决定了查询的最大长度—恰好是`48`。

在下面的代码中，我们用最大长度`50`填充输入和输出，因为没有长度超过`48`个单词的输入查询(即`max(length_sent)`):

```
from keras.preprocessing.sequence import pad_sequences
X = pad_sequences(maxlen=50, sequences=mapped_reviews, padding="post", value=0)
Y = pad_sequences(maxlen=50, sequences=mapped_targets, padding="post", value=0)
```

在下面的代码中，我们将输出转换为一个热编码版本:

```
from keras.utils import to_categorical
y2 = [to_categorical(i, num_classes=124) for i in Y]
y2 = np.array(y2)
```

我们总共有`124`个类，因为总共有`123`个唯一类，单词 index 以`1`开头。

5.  构建、训练和测试数据集以及模型:

```
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y2, test_size=0.30,random_state=10)
```

在前面的代码中，我们将数据集分为训练数据集和测试数据集:

```
input = Input(shape=(50,))
model = Embedding(input_dim=891, output_dim=32, input_length=50)(input)
model = Dropout(0.1)(model)
model = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)
out = (Dense(124, activation="softmax"))(model)
```

```
model = Model(input, out)
model.summary()
```

该模型的摘要如下:

![](Images/083bf99b-1860-45af-a90a-5537a1108d2a.png)

请注意，在前面的代码中，我们有一个双向 LSTM，因此，隐藏层有 200 个单位(LSTM 层有 100 个单位)。

6.  编译并拟合模型，如下所示:

```
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
model.fit(X_train,y_train, batch_size=32, epochs=5, validation_data = (X_test,y_test), verbose=1)
```

前面的代码生成了一个模型，该模型在识别查询中给定单词的正确实体方面的准确率为 95%:

![](Images/69930f68-e78b-4661-9ec4-0ed72bf4c79e.png)

从前面的输出中，我们可以看到，我们为每个单词分配正确实体的准确率> 95%。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 意图提取

既然我们已经构建了一个模型，它在预测查询中的实体方面具有很好的准确性，那么让我们继续寻找查询的意图。

我们将重用在之前的模型中初始化的大部分变量:

1.  将每个查询的意图转换为 ID:

```
from collections import Counter
counts = Counter()
for i,sentence in enumerate(final_docs):
     counts.update(sentence.split())
intent_words = sorted(counts, key=counts.get, reverse=True)
chars = intent_words
nb_chars = len(intent_words)
intent_word_to_int = {word: i for i, word in enumerate(intent_words, 1)}
intent_int_to_word = {i: word for i, word in enumerate(intent_words, 1)}
mapped_docs = []
for review in final_docs:
     mapped_docs.append([intent_word_to_int[word] for word in review.split()])
```

2.  提取意向的一次性编码版本:

```
from keras.utils import to_categorical
doc2 = [to_categorical(i[0], num_classes=23) for i in mapped_docs]
doc3 = np.array(doc2)
```

3.  构建模型，如以下代码所示:

```
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,doc3, test_size=0.30,random_state=10)
```

```
input = Input(shape=(50,))
model2 = Embedding(input_dim=891, output_dim=32, input_length=50)(input)
model2 = Dropout(0.1)(model2)
model2 = Bidirectional(LSTM(units=100))(model2)
out = (Dense(23, activation="softmax"))(model2)
model2 = Model(input, out)
```

```
model2.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])
```

```
model2.fit(X_train,y_train, batch_size=32, epochs=5, validation_data = (X_test,y_test), verbose=1)
```

前面的代码生成的模型在识别验证数据集中查询的正确意图方面具有 90%的准确性:

![](Images/7653b79e-d64e-4f58-a465-ed0c04bfb805.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 把所有的放在一起

在上一节中，我们构建了两个模型，其中第一个模型预测查询中的实体，第二个模型提取查询的意图。

在本节中，我们将定义一个接受查询并将其转换为结构化格式的函数:

1.  预处理新的输入文本，以便可以将其传递给模型:

```
def preprocessing(text):
     text2 = text.split()
     a=[]
     for i in range(len(text2)):
         a.append(sentence_word_to_int[text2[i]])
     return a
```

2.  预处理输入文本，将其转换为单词 id 列表:

```
text = "BOS i would fly from boston to dallas EOS"
```

```
indexed_text = preprocessing(text)
padded_text=np.zeros(50)
padded_text[:len(indexed_text)]=indexed_text
padded_text=padded_text.reshape(1,50)
```

上述操作会产生如下经过处理的输入文本:

![](Images/f247e339-2e03-4054-96d4-1e0073daf1c6.png)

现在，我们将预测前面列表的意图:

```
pred_index_intent = np.argmax(model2.predict(c),axis=1)
entity_int_to_word[pred_index_intent[0]]
```

上述代码导致查询的目的是关于航班，如下所示:

![](Images/6b2208a9-fac9-4df0-88bf-75c54c8f2dff.png)

3.  提取与查询中的单词相关的实体:

```
pred_entities = np.argmax(model.predict(padded_text),axis=2)

for i in range(len(pred_entities[0])):
      if pred_entities[0][i]>1:
            print('word: ',text.split()[i], 'entity: ',target_int_to_word[pred_entities[0][i]])

```

![](Images/aaae4594-3625-403a-8025-1476f985fb32.png)

从前面的代码中，我们可以看到模型已经正确地将单词分类到正确的实体中。

既然我们已经确定了实体和意图，我们可以有一个预定义的 SQL 查询(或 API ),它的参数由提取的实体填充，每个意图可能有一个不同的 API/SQL 查询来为用户提取信息。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 机器翻译

到目前为止，我们已经看到了输入和输出一对一映射的场景。在这一节中，我们将探讨如何构建架构，将所有输入数据映射到一个向量，然后将其解码为输出向量。

在本案例研究中，我们将把英文输入文本翻译成法文文本。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 做好准备

我们将定义用于执行机器翻译的架构如下:

*   取一个带标签的数据集，其中有可用的输入句子和相应的法语翻译
*   标记和提取在英语和法语文本中频繁出现的单词:
    *   为了识别频繁出现的单词，我们将计算每个单词的频率
    *   构成所有单词的总累积频率的前 80%的单词被认为是频繁单词
*   对于不在常用单词中的所有单词，用未知(`unk`)符号替换它们
*   给每个单词分配一个 ID
*   构建一个编码器 LSTM 来获取输入文本的向量
*   将编码向量通过密集层，以便我们提取每个时间步解码文本的概率
*   拟合模型以最小化输出损耗

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 怎么做...

可能有多个模型架构可以帮助翻译输入文本。我们将在接下来的章节中讨论其中的一些(代码文件在 GitHub 中以`Machine_translation.ipynb`的形式提供)。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 预处理数据

为了将输入和输出数据传递给我们的模型，我们必须对数据集进行如下预处理:

1.  导入相关的包和数据集:

```
import pandas as pd
import numpy as np
import string
from string import digits
import matplotlib.pyplot as plt
%matplotlib inline
import re
from sklearn.model_selection import train_test_split
from keras.models import Model
from keras.layers import Input, LSTM, Dense
import numpy as np
```

```
$ wget https://www.dropbox.com/s/2vag8w6yov9c1qz/english%20to%20french.txt
```

```
lines= pd.read_table('english to french.txt', names=['eng', 'fr'])
```

2.  假设数据集中有超过 140，000 个句子，让我们只考虑前 50，000 个句子翻译对来构建模型:

```
lines = lines[0:50000]
```

3.  将输入和输出文本转换为小写，并删除标点符号:

```
lines.eng=lines.eng.apply(lambda x: x.lower())
lines.fr=lines.fr.apply(lambda x: x.lower())
exclude = set(string.punctuation)
lines.eng=lines.eng.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))
lines.fr=lines.fr.apply(lambda x: ''.join(ch for ch in x if ch not in exclude))
```

4.  向输出句子(法语句子)添加开始和结束标记。我们添加这些，以便开始和结束标记在编码器解码器架构中有所帮助。这将有所帮助的原因将在*机器翻译的编码器解码器架构*部分提供:

```
lines.fr = lines.fr.apply(lambda x : 'start '+ x + ' end')
```

数据示例如下所示:

![](Images/b5773770-4057-4fec-a4af-4d033094e420.png)

5.  识别常用词。如果某个词在所有词中的出现频率占总出现频率的 80%,则我们将该词定义为频繁出现的词:

```
# fit a tokenizer
from keras.preprocessing.text import Tokenizer
import json
from collections import OrderedDict
def create_tokenizer(lines):
     tokenizer = Tokenizer()
     tokenizer.fit_on_texts(lines)
     return tokenizer
```

```
eng_tokenizer = create_tokenizer(lines.eng)
output_dict = json.loads(json.dumps(eng_tokenizer.word_counts))
df =pd.DataFrame([output_dict.keys(), output_dict.values()]).T
df.columns = ['word','count']
df = df.sort_values(by='count',ascending = False)
df['cum_count']=df['count'].cumsum()
df['cum_perc'] = df['cum_count']/df['cum_count'].max()
final_eng_words = df[df['cum_perc']<0.8]['word'].values
```

前面的代码提取了累计占输入英语单词总数 80%的英语单词数:

```
fr_tokenizer = create_tokenizer(lines.fr)
output_dict = json.loads(json.dumps(fr_tokenizer.word_counts))
df =pd.DataFrame([output_dict.keys(), output_dict.values()]).T
df.columns = ['word','count']
df = df.sort_values(by='count',ascending = False)
df['cum_count']=df['count'].cumsum()
df['cum_perc'] = df['cum_count']/df['cum_count'].max()
final_fr_words = df[df['cum_perc']<0.8]['word'].values
```

前面的代码提取法语单词的数量，这些单词在输出中累计占法语单词总数的 80%。

6.  过滤掉不常用的词。如果一个词不在常用词中，我们将用一个未知词来代替它:

```
def filter_eng_words(x):
     t = []
     x = x.split()
     for i in range(len(x)):
         if x[i] in final_eng_words:
             t.append(x[i])
         else:
             t.append('unk')
     x3 = ''
     for i in range(len(t)):
         x3 = x3+t[i]+' '
     return x3
```

前面的代码以一个句子作为输入，提取唯一的单词，如果一个单词不存在于更频繁出现的英语单词(`final_eng_words`)中，那么它将被替换为`unk`:

```
def filter_fr_words(x):
     t = []
     x = x.split()
     for i in range(len(x)):
         if x[i] in final_fr_words:
             t.append(x[i])
         else:
             t.append('unk')
     x3 = ''
     for i in range(len(t)):
         x3 = x3+t[i]+' '
     return x3
```

前面的代码以一个句子作为输入，提取唯一的单词，如果一个单词不存在于更频繁出现的法语单词(`final_fr_words`)中，那么它将被替换为`unk`。

例如，在一个包含常用词和非常用词的随机句子中，输出如下:

```
filter_eng_words('he is extremely good')
```

![](Images/7e4f6d3b-1d82-4d60-bf15-dffc91a1ba42.png)

```
lines['fr']=lines['fr'].apply(filter_fr_words)
lines['eng']=lines['eng'].apply(filter_eng_words)
```

在前面的代码中，我们根据之前定义的函数替换了所有的英语和法语句子。

7.  为英语(输入)和法语(输出)句子中的每个单词分配 ID:
    1.  将所有唯一单词的列表存储在数据中(英语和法语句子):

```
all_eng_words=set()
for eng in lines.eng:
     for word in eng.split():
         if word not in all_eng_words:
             all_eng_words.add(word)

all_french_words=set()
for fr in lines.fr:
     for word in fr.split():
         if word not in all_french_words:
             all_french_words.add(word)
```

```
input_words = sorted(list(all_eng_words))
target_words = sorted(list(all_french_words))
num_encoder_tokens = len(all_eng_words)
num_decoder_tokens = len(all_french_words)
```

```
input_token_index = dict( [(word, i+1) for i, word in enumerate(input_words)])
target_token_index = dict( [(word, i+1) for i, word in enumerate(target_words)])
```

8.  提取输入和目标句子的最大长度，以便所有句子都具有相同的长度:

```
length_list=[]
for l in lines.fr:
     length_list.append(len(l.split(' ')))
fr_max_length = np.max(length_list)
```

```
length_list=[]
for l in lines.eng:
     length_list.append(len(l.split(' ')))
eng_max_length = np.max(length_list)
```

既然我们已经预处理了数据集，那么让我们在数据集上尝试多种架构来比较它们的性能。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 传统的多对多架构

在这个架构中，我们将把每个输入词嵌入到一个 128 维的向量中，得到一个 shape ( `batch_size, 128, 17`)的输出向量。我们希望这样做，因为在这个版本中，我们希望测试输入数据有 17 个时间步长，输出数据集也有 17 个时间步长的情况。

我们将通过 LSTM 将每个输入时间步长连接到输出时间步长，然后在预测的基础上执行 softmax:

1.  创建输入和输出数据集。注意我们有`decoder_input_data`和`decoder_target_data`。现在，让我们创建`decoder_input_data`作为对应于目标句子单词的单词 ID。`decoder_target_data`是`start`标记之后所有单词的目标数据的一位热编码版本:

```
encoder_input_data = np.zeros((len(lines.eng), fr_max_length),dtype='float32')
decoder_input_data = np.zeros((len(lines.fr), fr_max_length),dtype='float32')
decoder_target_data = np.zeros((len(lines.fr), fr_max_length, num_decoder_tokens+1),dtype='float32')
```

请注意，我们在`num_decodder_tokens`中添加了一个`+1`，因为在我们在上一节的*步骤 7b* 中创建的字典中没有对应于索引`0`的单词:

```
for i, (input_text, target_text) in enumerate(zip(lines.eng, lines.fr)):
     for t, word in enumerate(input_text.split()):
         encoder_input_data[i, t] = input_token_index[word]
     for t, word in enumerate(target_text.split()):
 # decoder_target_data is ahead of decoder_input_data by one timestep
         decoder_input_data[i, t] = target_token_index[word]
         if t>0: 
 # decoder_target_data will be ahead by one timestep
 # and will not include the start character.
             decoder_target_data[i, t - 1, target_token_index[word]] = 1.
         if t== len(target_text.split())-1:
             decoder_target_data[i, t:, 89] = 1
```

在前面的代码中，我们遍历输入文本和目标文本，将英语或法语句子替换为英语和法语单词 id。

此外，我们在解码器中对目标数据进行一次热编码，以便我们可以将其传递给模型。此外，假设现在所有的句子都具有相同的长度，在`for`循环中超过句子长度后，我们将在第 89 个索引处用 1 替换目标数据的值(因为`89`属于结束索引):

```
for i in range(decoder_input_data.shape[0]):
     for j in range(decoder_input_data.shape[1]):
         if(decoder_input_data[i][j]==0):
             decoder_input_data[i][j] = 89
```

在前面的代码中，我们将解码器输入数据中的零值替换为 89(因为 89 是结束标记，而零在我们创建的单词索引中没有任何与之相关联的单词)。

请注意，我们创建的三个数据集的形状如下:

```
print(decoder_input_data.shape,encoder_input_data.shape,decoder_target_data.shape)
```

以下是上述代码的输出:

```
(50000, 17) (50000, 17) (50000, 17, 359)
```

2.  构建并拟合模型，如下所示:

```
model = Sequential()
model.add(Embedding(len(input_words)+1, 128, input_length=fr_max_length, mask_zero=True))
model.add((Bidirectional(LSTM(256, return_sequences = True))))
model.add((LSTM(256, return_sequences=True)))
model.add((Dense(len(target_token_index)+1, activation='softmax')))
```

![](Images/5b1c05e4-823c-4fae-8955-1d164f10ebdd.png)

```
model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])
```

```
model.fit(encoder_input_data, decoder_target_data,
 batch_size=32, epochs=5, validation_split=0.05)
```

![](Images/58c5e0ac-b3aa-4bf6-a6cb-0800b766d9dd.png)

请注意，来自模型的精度数字可能会产生误导，因为它在精度度量中也计算了`end`标记。

3.  计算正确翻译的单词数:

```
count = 0
correct_count = 0
pred = model2.predict(encoder_input_data[47500:])
for i in range(2500):
  t = np.argmax(pred[i], axis=1)
  act = np.argmax(decoder_target_data[47500],axis=1)
  correct_count += np.sum((act==t) & (act!=89))
  count += np.sum(act!=89)
correct_count/count
# 0.19
```

在前面的代码中，我们对测试数据进行了预测(这是总数据集的最后 5%，因为验证拆分是 5%)。

从前面的代码中，我们可以看到大约 19%的单词被正确翻译。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 多对多隐藏的架构

以前架构的一个缺点是，我们必须人为地将输入的时间步长增加到 17，即使我们知道输入最多有 8 个时间步长。

在这个架构中，让我们继续构建一个在输入的最后一个时间步提取隐藏状态值的模型。此外，它复制隐藏状态值 17 次(因为输出中有 17 个时间步长)。它通过密集层传递复制的隐藏时间步长，最终提取输出中可能的类。让我们将逻辑编码如下:

1.  重新创建输入和输出数据集，使我们在输入中有 8 个时间步长，在输出中有 17 个时间步长。这与之前的迭代不同，因为之前的迭代输入有 17 个时间步长，而当前版本只有 8 个:

```
encoder_input_data = np.zeros(
    (len(lines.eng), eng_max_length),
    dtype='float32')
decoder_input_data = np.zeros(
    (len(lines.fr), fr_max_length),
    dtype='float32')
decoder_target_data = np.zeros(
    (len(lines.fr), fr_max_length, num_decoder_tokens+1),
    dtype='float32')

for i, (input_text, target_text) in enumerate(zip(lines.eng, lines.fr)):
    for t, word in enumerate(input_text.split()):
        encoder_input_data[i, t] = input_token_index[word]
    for t, word in enumerate(target_text.split()):
        # decoder_target_data is ahead of decoder_input_data by one timestep
        decoder_input_data[i, t] = target_token_index[word]
        if t>0: 
            # decoder_target_data will be ahead by one timestep
            # and will not include the start character.
          decoder_target_data[i, t - 1, target_token_index[word]] = 1.
          if t== len(target_text.split())-1:
            decoder_target_data[i, t:, 89] = 1

for i in range(decoder_input_data.shape[0]):
  for j in range(decoder_input_data.shape[1]):
    if(decoder_input_data[i][j]==0):
      decoder_input_data[i][j] = 89 
```

2.  建立模型。注意，`RepeatVector`层将双向层的输出复制了 17 次:

```
model2 = Sequential()
model2.add(Embedding(len(input_words)+1, 128, input_length=eng_max_length, mask_zero=True))
model2.add((Bidirectional(LSTM(256))))
model2.add(RepeatVector(fr_max_length))
model2.add((LSTM(256, return_sequences=True)))
model2.add((Dense(len(target_token_index)+1, activation='softmax')))
```

模型总结如下:

![](Images/0049d9b6-60e5-4351-a51d-7281e266f1c8.png)

3.  编译并拟合模型:

```
model2.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])
model2.fit(encoder_input_data, decoder_target_data,
 batch_size=128,epochs=5,validation_split=0.05)
```

![](Images/4d726ed2-1d9e-488d-8982-4608468389d6.png)

4.  计算正确翻译的单词总数的百分比:

```
count = 0
correct_count = 0
pred = model2.predict(encoder_input_data[47500:])
for i in range(2500):
  t = np.argmax(pred[i], axis=1)
  act = np.argmax(decoder_target_data[47500],axis=1)
  correct_count += np.sum((act==t) & (act!=89))
  count += np.sum(act!=89)
correct_count/count
```

前面的结果是 19%的准确性，与前面的迭代相比几乎相同。

上述情况是意料之中的，因为当所有输入时间步长的信息仅存储在最后一个隐藏层值中时，我们往往会丢失大量信息。

此外，我们没有利用包含大量关于在哪个时间步中需要忘记什么的信息的单元状态。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 用于机器翻译的编码器解码器架构

对于我们在上一节中定义的架构，有两个潜在的逻辑增强:

1.  在生成翻译时，利用单元状态中存在的信息
2.  利用先前翻译的单词作为预测下一个单词的输入

第二种手法叫**老师逼**。本质上，通过在生成当前时间步长的同时给出前一个时间步长的实际值作为输入，我们可以更快地调整网络，并且实际上更精确。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 做好准备

我们将采用以下策略来使用编码器/解码器架构构建一个机器翻译系统:

*   在准备输入和输出数据集时，我们有两个解码器数据集:
    *   与`encoder_input_data`组合的`decoder_input_data`是输入，`decoder_target_data`是输出
    *   `decoder_input_data`以`start`字开始
*   当我们在解码器中预测第一个单词时，我们使用输入单词集，将它们转换为向量，然后通过以`start`为输入的解码器模型。预期输出是输出中`start`之后的第一个字
*   我们以类似的方式进行，其中输出中的实际第一个单词是输入，而预测第二个单词
*   我们将基于这个策略计算模型的准确性

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 怎么做...

现在，让我们继续在前面部分已经准备好的输入和输出数据集上构建模型(前面部分的多对多隐藏体系结构的*步骤 1* 保持不变)。该代码文件在 GitHub 中以`Machine_translation.ipynb`的名称提供。

1.  构建模型，如下所示:

```
# We shall convert each word into a 128 sized vector
embedding_size = 128
```

```
encoder_inputs = Input(shape=(None,))
en_x= Embedding(num_encoder_tokens+1, embedding_size)(encoder_inputs)
encoder = LSTM(256, return_state=True)
encoder_outputs, state_h, state_c = encoder(en_x)
# We discard `encoder_outputs` and only keep the states.
encoder_states = [state_h, state_c]
```

请注意，我们使用的是函数式 API，因为我们正在提取编码器网络的中间层，并将传递多个数据集作为输入(编码器输入数据和解码器输入数据)。

```
decoder_inputs = Input(shape=(None,))
dex= Embedding(num_decoder_tokens+1, embedding_size)
final_dex= dex(decoder_inputs)
decoder_lstm = LSTM(256, return_sequences=True, return_state=True)
decoder_outputs, _, _ = decoder_lstm(final_dex, initial_state=encoder_states)
decoder_outputs = Dense(2000,activation='tanh')(decoder_outputs)
decoder_dense = Dense(num_decoder_tokens+1, activation='softmax')
decoder_outputs = decoder_dense(decoder_outputs)
```

2.  构建模型，如下所示:

```
model3 = Model([encoder_inputs, decoder_inputs], decoder_outputs)
model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])
```

![](Images/7d7eed63-bb60-43d2-a998-0b25454011cf.png)

3.  拟合模型，如以下代码所示:

```
history3 = model3.fit([encoder_input_data, decoder_input_data], decoder_target_data,
 batch_size=32,epochs=5,validation_split=0.05)
```

![](Images/920127b1-562b-4a64-b5eb-b5100a0d54fd.jpg)

4.  计算准确转录的单词的百分比:

```
act = np.argmax(decoder_target_data, axis=2)
```

```
count = 0
correct_count = 0
pred = model3.predict([encoder_input_data[47500:],decoder_input_data[47500:]])
for i in range(2500):
     t = np.argmax(pred[i], axis=1)
     correct_count += np.sum((act[47500+i]==t) & (act[47500+i]!=0))
     count += np.sum(decoder_input_data[47500+i]!=0)
correct_count/count
```

请注意，在这个场景中，我们已经正确翻译了 44%的单词。

但是，请注意，在计算测试数据集的准确性时，我们不应该使用`decoder_input_data`，因为我们在实时场景中无法访问它。

这要求我们使用前一时间步中的预测字作为当前时间步的解码器输入字，如下所示。

我们将把`decoder_input_data`重新初始化为`decoder_input_data_pred`:

```
decoder_input_data_pred = np.zeros((len(lines.fr),fr_max_length),dtype='float32')
final_pred = []
for i in range(2500):
word = 284
     for j in range(17):
         decoder_input_data_pred[(47500+i), j] = word
         pred =         model3.predict([encoder_input_data[(47500+i)].reshape(1,8),decoder_input_data_pred[47500+i].reshape(1,17)])
         t = np.argmax(pred[0][j])
         word = t
         if word==89:
             break
     final_pred.append(list(decoder_input_data_pred[47500+i]))
```

注意，在前面的代码中，单词索引 284 对应于开始单词。我们将起始单词作为解码器输入中的第一个单词传递，并在下一个时间步长中预测概率最高的单词。

一旦预测到第二个字，我们就更新`decoder_input_word_pred`，预测第三个字，继续下去，直到遇到停用字。

既然我们已经修改了我们预测的翻译单词，让我们来计算我们翻译的准确性:

```
final_pred2 = np.array(final_pred)
```

```
count = 0
correct_count = 0
for i in range(2500):
     correct_count += np.sum((decoder_input_data[47500+i]==final_pred2[i]) & (decoder_input_data[47500+i]!=89))
     count += np.sum(decoder_input_data[47500+i]!=89)
correct_count/count
```

通过这种方法，上述方法可以正确翻译 46%的单词。

虽然与以前的方法相比，翻译的准确性有了相当大的提高，但我们仍然没有直觉地认为，在源语言中位于开头的单词很可能位于开头，即使在目标语言中也是如此，也就是说，没有考虑单词对齐。在下一节中，我们将研究如何解决单词对齐的问题。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 关注机器翻译的编码器解码器架构

在上一节中，我们了解到我们可以通过启用教师强制技术来提高翻译的准确性，其中目标的前一时间步中的实际单词被用作模型的输入。

在本节中，我们将进一步扩展这一思想，并根据编码器和解码器向量在每个时间步长的相似程度，为输入编码器分配权重。这样，根据解码器的时间步长，我们使得某些单词在编码器的隐藏向量中具有更高的权重。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 怎么做...

有了这个，让我们看看如何建立编码器解码器架构，以及注意机制。该代码文件在 GitHub 中以`Machine_translation.ipynb`的名称提供。

1.  构建编码器，如以下代码所示:

```
encoder_inputs = Input(shape=(eng_max_length,))
en_x= Embedding(num_encoder_tokens+1, embedding_size)(encoder_inputs)
en_x = Dropout(0.1)(en_x)
encoder = LSTM(256, return_sequences=True, unroll=True)(en_x)
encoder_last = encoder[:,-1,:]
```

2.  构建解码器，如下所示:

```
decoder_inputs = Input(shape=(fr_max_length,))
dex= Embedding(num_decoder_tokens+1, embedding_size)
decoder= dex(decoder_inputs)
decoder = Dropout(0.1)(decoder)
decoder = LSTM(256, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])
```

请注意，在前面的代码中，我们尚未最终确定解码器架构。我们只提取了解码器中的隐藏层值。

3.  建立关注机制。注意机制将基于编码器隐藏向量和解码器隐藏向量在每个时间步长的相似程度。基于这种相似性(执行 softmax 以给出在所有可能的输入时间步长上总计为 1 的权重值)，我们将权重分配给编码器向量，如下所示。

使编码器解码器向量通过激活和密集层，以便我们在获取向量之间的点积(相似性的度量—余弦相似性)之前实现进一步的非线性:

```
t = Dense(5000, activation='tanh')(decoder)
t2 = Dense(5000, activation='tanh')(encoder)
attention = dot([t, t2], axes=[2, 2])
```

确定要给予输入时间步长的权重:

```
attention = Dense(eng_max_length, activation='tanh')(attention)
attention = Activation('softmax')(attention)
```

计算加权编码器向量，如下所示:

```
context = dot([attention, encoder], axes = [2,1])
```

4.  组合解码器和加权编码器向量:

```
decoder_combined_context = concatenate([context, decoder])
```

5.  将解码器和加权编码向量的组合连接到输出层:

```
output_dict_size = num_decoder_tokens+1
decoder_combined_context=Dense(2000, activation='tanh')(decoder_combined_context)
output=(Dense(output_dict_size, activation="softmax"))(decoder_combined_context)
```

6.  编译并拟合模型，如以下代码所示:

```
model4 = Model(inputs=[encoder_inputs, decoder_inputs], outputs=[output])
model4.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])
```

建筑的一个情节如下:

![](Images/05ba35a2-0d0a-4d38-aad8-640520b67267.png)

```
model4.fit([encoder_input_data, decoder_input_data], decoder_target_data,
 batch_size=32,epochs=5,validation_split=0.05)
```

![](Images/ca8e9bfd-b064-4ba3-8414-c9b80eda03ec.png)

一旦您拟合了模型，您将会注意到这个模型中的验证损失比之前的迭代稍微好一点。

7.  按照与上一节类似的方式计算翻译的准确性:

```
decoder_input_data_pred=np.zeros((len(lines.fr), fr_max_length), dtype='float32')
```

```
final_pred_att = []
for i in range(2500):
     word = 284
     for j in range(17):
         decoder_input_data_pred[(47500+i), j] = word
         pred =         model4.predict([encoder_input_data[(47500+i)].reshape(1,8),decoder_input_data_pred[47500+i].reshape(1,17)])
         t = np.argmax(pred[0][j])
         word = t
         if word==89:
             break
     final_pred_att.append(list(decoder_input_data_pred[47500+i]))
```

```
final_pred2_att = np.array(final_pred_att)
count = 0
correct_count = 0
for i in range(2500):
     correct_count += np.sum((decoder_input_data[47500+i]==final_pred2_att[i]) & (decoder_input_data[47500+i]!=89))
     count += np.sum(decoder_input_data[47500+i]!=89)
correct_count/count
```

前面的代码导致 52%的单词被正确翻译，这比前一次迭代有所改进。

8.  现在我们已经构建了一个具有合理准确性的翻译系统，让我们检查测试数据集中的一些翻译(测试数据集是总数据集的最后 5%，因为我们指定`validation_split`为 5%)，如下所示:

```
k = -1500
t = model4.predict([encoder_input_data[k].reshape(1,encoder_input_data.shape[1]),decoder_input_data[k].reshape(1,decoder_input_data.shape[1])]).reshape(decoder_input_data.shape[1], num_decoder_tokens+1)
```

根据单词提取预测的翻译:

```
t2 = np.argmax(t,axis=1)
for i in range(len(t2)):
     if int(t2[i])!=0:
         print(list(target_token_index.keys())[int(t2[i]-1)])
```

将英语句子转换为法语后，上述代码的输出如下:

```
je unk manger pas manger end end
```

提取单词的实际翻译:

```
t2 = decoder_input_data[k]
for i in range(len(t2)):
     if int(t2[i])!=89:
         print(list(target_token_index.keys())[int(t2[i]-1)])
```

上述代码的输出如下:

```
 je unk ne pas manger ça end
```

我们看到预测的翻译与原始翻译相当接近。以类似的方式，让我们在验证数据集上探索更多的翻译:

| **原文翻译** | **预测翻译** |
| *曾经如此忙碌的 unk 结局* | *已经忙得不可开交了* |
| *我只做这个 unk 告诉我结局* | *我来让这个 unk 终结我* |
| *jai unk 做 unk 结尾* | *是我们的家，我们的家，我们的家，我们的家，我们的家，我们的家，我们的家，我们的家，我们的家* |

从上表中，我们可以看到有一个不错的翻译，但是有几个潜在的改进领域:

*   考虑单词相似性:
    *   像 *je* 和 *j'ai* 这样的词是相当相似的，所以它们不应该被严重处罚，即使这会降低准确性
*   减少`unk`字数:
    *   我们减少了`unk`单词的数量，以降低数据集的维度
    *   当我们收集更大的语料库并在具有工业规模配置的机器上工作时，我们可以潜在地在高维数据上工作