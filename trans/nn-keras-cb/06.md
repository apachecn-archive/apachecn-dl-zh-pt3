        

# 检测和定位图像中的对象

在关于构建深度卷积神经网络和迁移学习的章节中，我们学习了如何使用深度 CNN 和利用迁移学习来检测图像所属的类别。

虽然对象分类有效，但在现实世界中，我们还会遇到必须在图像中定位对象的情况。

例如，在自动驾驶汽车的情况下，我们不仅需要检测汽车视野中的行人，还需要检测行人离汽车有多远，以便采取适当的行动。

在这一章中，我们将讨论在图像中检测物体的各种技术。我们将在本章中讨论的案例研究如下:

*   创建包围盒的训练数据集
*   使用选择性搜索在图像内生成区域提议
*   计算两幅图像的并集上的交集
*   使用基于区域提议的 CNN 检测目标
*   执行非最大抑制
*   使用基于锚盒的算法检测人

        

# 介绍

随着自动驾驶汽车、面部检测、智能视频监控和人数统计解决方案的兴起，快速准确的物体检测系统需求很大。这些系统不仅包括图像中的对象识别和分类，还可以通过在它们周围画出适当的方框来定位它们中的每一个。这使得物体检测比其传统的计算机视觉前身图像分类更难。

为了了解对象检测的输出是什么样的，我们来看一下下图:

![](Images/310b9e9a-7372-4f0d-ba7d-00ce7a848ffd.png)

到目前为止，在前面的章节中，我们已经了解了分类。

在这一章中，我们将学习在图片中的物体周围有一个紧密的边界框，这就是定位任务。

此外，我们还将学习如何检测图片中的多个对象，这是对象检测任务。

        

# 为边界框创建数据集

我们已经知道，对象检测给出的输出是图像中感兴趣的对象周围的边界框。为了构建一个算法来检测图像中对象周围的边界框，我们必须创建输入-输出映射，其中输入是图像，输出是给定图像中对象周围的边界框。

注意，当我们检测边界框时，我们检测的是图像周围边界框左上角的像素位置，以及边界框相应的宽度和高度。

为了训练提供边界框的模型，我们需要图像，以及图像中所有对象的相应边界框坐标。

在本节中，我们将重点介绍创建训练数据集的方法之一，其中图像将作为输入给出，相应的边界框存储在 XML 文件中。

我们将使用`labelImg`包来注释边界框和相应的类。

        

# 怎么做...

图像中对象周围的边界框可以如下准备:

        

# Windows 操作系统

1.  从这里的链接下载`labelImg`的可执行文件:[https://github . com/Tzu talin/labelImg/files/2638199/windows _ v 1 . 8 . 1 . zip](https://github.com/tzutalin/labelImg/files/2638199/windows_v1.8.1.zip)。
2.  提取并打开`labelImg.exe` GUI，如下图所示:

![](Images/93cd65bd-cb08-4193-a83f-b4c592fa6e51.png)

3.  在`data`文件夹的`predefined_classes.txt`文件中指定图像中所有可能的标签。我们需要确保所有的类都列在单独的一行中，如下所示:

![](Images/77c07038-7b8a-4a11-b2a9-1f14354f96eb.jpg)

4.  通过在 GUI 中单击 Open 来打开图像，并通过单击 Create RectBox 来注释图像，这将弹出将被选择的类，如下所示:

![](Images/9f98e69a-7ac4-42c9-85e3-d0adecbbe359.png)

5.  单击保存并保存 XML 文件。
6.  检查 XML 文件。绘制矩形边界框后，XML 文件的快照如下所示:

![](Images/74ad33e9-c30c-4f97-a457-25d976da745d.png)

从前面的截图中，您应该注意到`bndbox`包含了与图像中感兴趣的物体相对应的 *x* 和 *y* 坐标的最小值和最大值的坐标。此外，我们还应该能够提取与图像中的对象相对应的类别。

        

# 人的本质

在 Ubuntu 中，可以通过键入以下命令来执行与前面相同的步骤:

```
$sudo apt-get install pyqt5-dev-tools
$sudo pip3 install -r requirements/requirements-linux-python3.txt
$make qt5py3
$python3 labelImg.py
```

剧本`labelImg.py`可以在 GitHub 链接这里找到:[https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg)。

一旦我们执行了前面的代码，我们应该能够执行与我们在*窗口*部分看到的相同的分析。

        

# 马科斯

在 macOS 中，可以通过键入以下命令来执行上述相同的步骤:

```
$brew install qt  # will install qt-5.x.x
$brew install libxml2
$make qt5py3
$python3 labelImg.py
```

剧本`labelImg.py`可以在 GitHub 链接这里找到:[https://github.com/tzutalin/labelImg](https://github.com/tzutalin/labelImg)。

一旦我们执行了前面的脚本，我们应该能够执行与我们在*窗口*部分看到的相同的分析。

        

# 使用选择性搜索在图像内生成区域提议

为了理解什么是区域提议，让我们把这个术语分解成它的组成部分——区域和提议。

**区域**是整个图像的一部分，其中该部分中的像素具有非常相似的值。

**区域提议**是整个图像的较小部分，其中该部分属于特定对象的可能性较高。

区域提议是有用的，因为我们从图像中生成候选者，其中物体位于这些区域之一的机会是高的。这在对象定位任务中很方便，我们需要在对象周围有一个边界框，类似于上一节中的图片。

        

# 做好准备

在这一节中，我们将研究一种在人物图像中生成边界框的方法。

选择性搜索是一种用于目标检测的区域建议算法。它被设计成具有很高召回率的快速程序。它是基于颜色、纹理、大小和形状兼容性计算相似区域的分级分组。

可以使用名为`selectivesearch`的 Python 包生成区域建议，如下所示。

选择性搜索通过使用由 Felzenszwalb 和 Huttenlocher 提出的基于图形的分割方法，基于像素的强度对图像进行过度分割(生成数千个区域提议)开始。

选择性搜索算法将这些过分段作为初始输入，并执行以下步骤:

1.  将对应于分割部分的所有边界框添加到区域建议列表中
2.  基于相似性对相邻线段进行分组
3.  转到第一步

在每次迭代中，形成更大的分段，并将其添加到区域提议列表中。因此，我们采用自下而上的方法，从较小的细分市场到较大的细分市场创建区域提案。

选择性搜索使用基于颜色、纹理、大小和形状兼容性的四种相似性度量来提出区域提议。

区域提议有助于识别图像中可能感兴趣的对象。因此，我们可以潜在地将定位练习转换成分类练习，其中我们将根据每个区域是否包含感兴趣的对象来对其进行分类。

        

# 怎么做...

在本节中，我们将演示提取区域建议，如下所示(代码文件在 GitHub 中以`Selective_search.ipynb`的形式提供):

1.  按照以下步骤安装`selectivesearch`:

```
$pip install selectivesearch
```

2.  导入相关的包，如下面的代码所示:

```
import matplotlib.pyplot as plt
%matplotlib inline
import selectivesearch
import cv2
```

3.  加载图像，如下所示:

```
img = cv2.imread('/content/Hemanvi.jpeg')
```

4.  提取区域建议:

```
img_lbl, regions = selectivesearch.selective_search(img, scale=100, min_size=2000)
```

参数`min_size`提供了一个约束，即区域提议的大小至少应为 2000 个像素，并且参数 scale 有效地设置了观察的尺度，因为较大的尺度导致对较大组件的偏好。

5.  检查得到的区域数量，并将它们存储在列表中:

```
print(len(regions))
candidates = set()
for r in regions:
     if r['rect'] in candidates:
         continue
 # excluding regions smaller than 2000 pixels
     if r['size'] < 2000:
         continue
     x, y, w, h = r['rect']
 candidates.add(r['rect'])
```

在前面的步骤中，我们已经将所有大小(面积)超过 2000 像素的区域存储到一组候选区域中。

6.  绘制候选人的结果图像:

```
import matplotlib.patches as mpatches
fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))
ax.imshow(img)
for x, y, w, h in candidates:
    rect = mpatches.Rectangle(
        (x, y), w, h, fill=False, edgecolor='red', linewidth=1)
    ax.add_patch(rect)
plt.axis('off')
plt.show()
```

![](Images/eb1ac059-09d8-4f87-bb46-23877f68f432.png)

从前面的屏幕截图中，我们看到从图像中提取了多个区域。

        

# 计算两幅图像的并集上的交集

为了理解所提出的区域有多精确，我们使用了一个名为 Union ( **IoU** )上的**交集的度量。IoU 可以如下图所示:**

![](Images/c167b0ff-23eb-4dd7-8e62-f3c265fe7b82.png)

请注意，在上图中，蓝框(下一个)是地面实况，红框(上矩形)是区域提议。

区域建议的并集上的交集被计算为建议和基础事实的交集与区域建议和基础事实的并集上的比值。

        

# 怎么做...

IoU 的计算如下(代码文件在 GitHub 中以`Selective_search.ipynb`的形式提供):

1.  定义 IoU 提取功能，如以下代码所示:

```
from copy import deepcopy
import numpy as np
def extract_iou(candidate, current_y,img_shape):
     boxA = deepcopy(candidate)
     boxB = deepcopy(current_y)

     img1 = np.zeros(img_shape)
     img1[boxA[1]:boxA[3],boxA[0]:boxA[2]]=1

     img2 = np.zeros(img_shape)
     img2[int(boxB[1]):int(boxB[3]),int(boxB[0]):int(boxB[2])]=1

     iou = np.sum(img1*img2)/(np.sum(img1)+np.sum(img2)- np.sum(img1*img2))
     return iou
```

在前面的函数中，我们将候选对象、实际对象区域和图像形状作为输入。

此外，我们为候选图像和实际对象位置图像初始化了相同形状的两个零值数组。

我们已经用一个图像覆盖了候选图像和实际对象位置图像，无论图像和对象位于何处。

最后，我们计算候选图像与实际目标位置图像的交集。

2.  导入感兴趣的图像:

```
img = cv2.imread('/content/Hemanvi.jpeg')
```

3.  绘制图像并验证感兴趣对象的实际位置:

```
plt.imshow(img)
plt.grid('off')
```

![](Images/77532f5a-d1fd-4883-a1f0-92c9aa15685c.png)

请注意，感兴趣的区域是从左下方大约 50 个像素延伸到图像的大约 290 个像素。此外，在 *y* 轴上，它也从大约第 50 个像素开始，直到图像结束。

因此，对象的实际位置是(50，50，290，500)，其格式为(`xmin`、`ymin`、`xmax`、`ymax`)。

4.  提取区域建议:

```
img_lbl, regions = selectivesearch.selective_search(img, scale=100, min_size=2000)
```

从`selectivesearch`方法中提取的区域的格式为(`xmin`、`ymin`、`width`、`height`)。因此，在提取区域的 IoU 之前，我们应该确保图像的候选位置和实际位置具有相同的格式，即(`xmin`、`ymin`、`xmax`、`ymax`)

5.  将 IoU 提取功能应用于目标图像。请注意，该函数将实际对象的位置和候选图像形状作为输入:

```
regions =list(candidates)
actual_bb = [50,50,290,500]
iou = []
for i in range(len(regions)):
     candidate = list(regions[i])
     candidate[2] += candidate[0]
     iou.append(extract_iou(candidate, actual_bb, img.shape))
```

6.  识别与感兴趣的实际对象(地面真实边界框)具有最高重叠的区域:

```
np.argmax(iou)
```

该特定图像的先前输出是坐标为 0，0，299，515 的第十个候选图像。

7.  让我们打印实际的边界框和候选边界框。为此，我们需要将(`xmin`、`ymin`、`xmax`、`ymax`)格式的输出转换为(`xmin`、`ymin`、`width`、`height`):

```
max_region = list(regions[np.argmax(iou)])
max_region[2] -= max_region[0]
max_region[3] -= max_region[1]

actual_bb[2] -= actual_bb[0]
actual_bb[3] -= actual_bb[1]
```

让我们附加实际的和具有最高 IoU 的边界框:

```
maxcandidate_actual = [max_region,actual_bb]
```

现在，我们将遍历前面的列表，并为图像中对象的实际位置分配一个更大的线宽，以便我们能够区分候选对象和实际对象的位置:

```
import matplotlib.patches as mpatches
fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))
ax.imshow(img)
for i,(x, y, w, h) in enumerate(maxcandidate_actual):
 if(i==0):
 rect = mpatches.Rectangle(
 (x, y), w, h, fill=False, edgecolor='blue', linewidth=2)
 ax.add_patch(rect)
 else:
 rect = mpatches.Rectangle(
 (x, y), w, h, fill=False, edgecolor='red', linewidth=5)
 ax.add_patch(rect)
plt.axis('off')
plt.show()
```

![](Images/46f2426e-a451-4087-bce0-45d0c1b1d303.png)

这样，我们就可以用图像中某个对象的实际位置来识别每个候选对象的 IoU。此外，我们还可以根据图像中对象的实际位置来确定 IoU 最高的候选对象。

        

# 使用基于区域提议的 CNN 检测对象

在上一节中，我们已经了解了如何从图像中生成区域建议。在本节中，我们将利用区域建议来实现图像中的对象检测和定位。

        

# 做好准备

我们将采用以下策略来执行基于区域提议的对象检测和定位:

1.  对于当前的练习，我们将基于只包含一个对象的图像来构建模型
2.  我们将提取图像中的各种区域提议(候选)
3.  我们将计算候选对象与实际对象位置的接近程度:
    *   本质上，我们计算候选对象与对象实际位置的交集
4.  如果并集上的交集大于某个阈值，则认为候选对象包含感兴趣的对象，否则不包含感兴趣的对象:
    *   这将为每个候选对象创建标签，其中候选对象的图像作为输入，超过联合阈值的交集作为输出
5.  我们将调整每个候选图像的大小，并通过 VGG16 模型(我们在上一章已经学过了)来提取候选图像的特征
6.  此外，我们将通过比较候选对象的位置和对象的实际位置来创建包围盒校正的训练数据
7.  构建一个分类模型，将候选对象的特征映射到该区域是否包含对象的输出
8.  对于包含图像的区域(根据模型)，构建一个回归模型，将候选对象的输入特征映射到提取对象的精确边界框所需的校正

9.  在生成的边界框顶部执行非最大抑制:
    *   非最大抑制确保重叠很多的候选减少到 1，其中只留下包含对象的概率最高的候选
10.  通过执行非最大值抑制，我们将能够复制我们为包含多个对象的图像构建的模型

前面的示意图如下所示:

![](Images/cffb313a-6080-4b13-a921-c383a7830bb1.png)

        

# 怎么做...

在本节中，我们将编写上一节中讨论过的算法(代码文件和相应的推荐数据集链接在 GitHub 中作为`Region_proposal_based_object_detection.ipynb`与推荐数据集一起提供):

1.  下载包含一组图像、图像中包含的对象以及图像中对象的相应边界框的数据集。GitHub 中提供了数据集和相应的代码文件。

样本图像和相应的边界框坐标以及图像中的对象类别如下所示:

![](Images/98a29fdc-3138-4c23-b471-afd9c7b39b9b.png)

对象类和边界框坐标可在 XML 文件中获得(如何获得 XML 文件的细节可在 GitHub 的代码文件中获得),并可从 XML 文件中提取，如下所示:

如果`xml["annotation"]["object"]`是一个列表，则表示同一幅图像中存在多个对象。

`xml["annotation"]["object"]["bndbox"]`提取图像中对象的边界框，该边界框可作为图像中对象的“xmin”、“ymin”、“xmax”和“ymax”坐标。

`xml["annotation"]["object"]["name"]`提取图像中存在的物体类别。

2.  按如下方式导入相关包:

```
import matplotlib.pyplot as plt
%matplotlib inline
import tensorflow as tf, selectivesearch
import json, scipy, os, numpy as np,argparse,time, sys, gc, cv2, xmltodict
from copy import deepcopy
```

3.  定义 IoU 提取函数，如以下代码所示:

```
def extract_iou2(candidate, current_y,img_shape):
     boxA = deepcopy(candidate)
     boxB = deepcopy(current_y)
     boxA[2] += boxA[0]
     boxA[3] += boxA[1]
     iou_img1 = np.zeros(img_shape)
     iou_img1[boxA[1]:boxA[3],boxA[0]:boxA[2]]=1
     iou_img2 = np.zeros(img_shape)
     iou_img2[int(boxB[1]):int(boxB[3]),int(boxB[0]):int(boxB[2])]=1
     iou = np.sum(iou_img1*iou_img2)/(np.sum(iou_img1)+np.sum(iou_img2)-np.sum(iou_img1*iou_img2))
     return iou
```

4.  定义候选提取函数，如下所示:

```
def extract_candidates(img):
     img_lbl, regions = selectivesearch.selective_search(img, scale=100, min_size=100)
     img_area = img.shape[0]*img.shape[1]
     candidates = []
     for r in regions:
         if r['rect'] in candidates:
             continue
         if r['size'] < (0.05*img_area):
             continue
         x, y, w, h = r['rect']
         candidates.append(list(r['rect']))
     return candidates
```

注意，在前面的函数中，我们排除了占据图像面积不到 5%的所有候选对象。

5.  导入预训练的 VGG16 模型，如下所示:

```
from keras.applications import vgg16
from keras.utils.vis_utils import plot_model
vgg16_model = vgg16.VGG16(include_top=False, weights='imagenet')
```

6.  为只包含一个对象的图像创建输入和输出映射。初始化将在我们浏览图像时填充的多个列表:

```
training_data_size = N = 1000

final_cls = []
final_delta = []
iou_list = []
imgs = []
```

我们将遍历这些图像，并且只处理那些包含单个对象的图像:

```
for ix, xml in enumerate(XMLs[:N]):
    print('Extracted data from {} xmls...'.format(ix), end='\r')
    xml_file = annotations + xml
    fname = xml.split('.')[0]
    with open(xml_file, "rb") as f: # notice the "rb" mode
        xml = xmltodict.parse(f, xml_attribs=True)
        l = []        
        if isinstance(xml["annotation"]["object"], list):
            #'let us ignore cases with multiple objects...'
            continue
```

在前面的代码中，我们提取图像的`xml`属性，并检查图像是否包含多个对象(如果`xml["annotation"]["object"]`的输出是一个列表，那么图像包含多个对象)。

规范化对象位置坐标，以便我们可以在规范化的边界框上工作。这样做是为了即使图像形状因进一步处理而改变，归一化的边界框也不会改变。例如，如果对象的`xmin`在 *x* 轴的 20%处，在 *y* 轴的 50%处，即使图像被整形，情况也是一样的(但是，如果我们处理像素值，`xmin`值将会改变):

```
        bndbox = xml['annotation']['object']['bndbox']
        for key in bndbox:
              bndbox[key] = float(bndbox[key])
        x1, x2, y1, y2 = [bndbox[key] for key in ['xmin', 'xmax', 'ymin', 'ymax']]

        img_size = xml['annotation']['size']
        for key in img_size:
              img_size[key] = float(img_size[key])
        w, h = img_size['width'], img_size['height']

        #'converting pixel values from bndbox to fractions...'
        x1 /= w; x2 /= w; y1 /= h; y2 /= h
        label = xml['annotation']['object']['name']

        y = [x1, y1, x2-x1, y2-y1, label]  # top-left x & y, width and height
```

在前面的代码中，我们已经规范化了边界框坐标。

提取候选图像:

```
         filename = jpegs+fname+'.jpg' # Path to jpg files here
         img = cv2.resize(cv2.imread(filename), (224,224)) # since VGG's input shape is 224x224
         candidates = extract_candidates(img)
```

在前面的代码中，我们使用了`extract_candidates`函数来提取调整过大小的图像的区域建议。

遍历候选对象以计算每个候选对象与图像中对象的实际边界框的交集以及实际边界框和候选对象之间的相应增量:

```
         for jx, candidate in enumerate(candidates):
                current_y2 = [int(i*224) for i in [x1,y1,x2,y2]] # [int(x1*224), int(y1*224), int(x2*224), int(y2*224)]
                iou = extract_iou2(candidate, current_y2, (224, 224))
                candidate_region_coordinates = c_x1, c_y1, c_w, c_h = np.array(candidate)/224

                dx = c_x1 - x1 
                dy = c_y1 - y1 
                dw = c_w - (x2-x1)
                dh = c_h - (y2-y1)

                final_delta.append([dx,dy,dw,dh]) 
```

计算每个区域提案的 VGG16 特性，并根据区域提案的 IoU 和实际边界框分配目标:

```
               if(iou>0.3): 
                    final_cls.append(label)
               else:
                    final_cls.append('background')

            #"We'll predict our candidate crop using VGG"
               l = int(c_x1 * 224)
               r = int((c_x1 + c_w) * 224)
               t = int(c_y1 * 224)
               b = int((c_y1 + c_h) * 224)

               img2 = img[t:b,l:r,:3]
               img3 = cv2.resize(img2,(224,224))/255
               img4 = vgg16_model.predict(img3.reshape(1,224,224,3))
               imgs.append(img4)
```

7.  创建输入和输出数组:

```
targets = pd.DataFrame(final_cls, columns=['label'])
labels = pd.get_dummies(targets['label']).columns
y_train = pd.get_dummies(targets['label']).values.astype(float)
```

我们利用`get_dummies`方法，因为类是分类文本值:

```
x_train = np.array(imgs)
x_train = x_train.reshape(x_train.shape[0],x_train.shape[2],x_train.shape[3],x_train.shape[4])
```

8.  构建和编译模型:

```
model = Sequential()
model.add(Flatten(input_shape=((7,7,512))))
model.add(Dense(512, activation='relu'))
model.add(Dense(all_classes.shape[1],activation='softmax'))

model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])
```

9.  拟合模型，如下所示:

```
model.fit(xtrain3/x_train.max(),y_train,validation_split = 0.1, epochs=5, batch_size=32, verbose=1)
```

上述结果在测试数据集上的分类准确率为 97%。

我们将输入数组除以`x_train.max()`，因为输入数组中的最大值约为 11。通常，输入值介于 0 和 1 之间是一个好主意，并且假设对缩放输入的 VGG16 预测的最大值约为 11，我们将输入数组除以`x_train.max()`，等于约 11。

10.  根据数据集对该类进行预测(确保我们不考虑用于训练的图像)。
11.  选择一个未用于测试的图像:

```
import matplotlib.patches as mpatches
ix = np.random.randint(N, len(XMLs))
filename = jpegs + XMLs[ix].replace('xml', 'jpg')
```

12.  构建一个函数，该函数执行图像预处理以提取候选对象，对调整大小的候选对象执行模型预测，过滤掉预测的背景类区域，最后绘制包含背景类以外的类的概率最高的区域(候选对象):

```
def test_predictions(filename):
     img = cv2.resize(cv2.imread(filename), (224,224))
     candidates = extract_candidates(img)
```

在前面的代码中，我们正在调整输入图像的大小，并从中提取候选图像:

```
    _, ax = plt.subplots(1, 2)
    ax[0].imshow(img)
    ax[0].grid('off')
    ax[0].set_title(filename.split('/')[-1])
    pred = []
    pred_class = []
```

在上述代码中，正在绘制图像，并正在初始化将在后续步骤中填充的预测概率和预测类别列表:

```
    for ix, candidate in enumerate(candidates):
        l, t, w, h = np.array(candidate).astype(int)
        img2 = img[t:t+h,l:l+w,:3]
        img3 = cv2.resize(img2,(224,224))/255
        img4 = vgg16_model.predict(img3.reshape(1,224,224,3))
        final_pred = model.predict(img4/x_train.max())
        pred.append(np.max(final_pred))
        pred_class.append(np.argmax(final_pred))
```

在前面的代码中，我们循环遍历候选对象，调整它们的大小，并将它们传递给 VGG16 模型。此外，我们通过我们的模型传递 VGG16 输出，这提供了图像属于不同类别的概率:

```
    pred = np.array(pred)
    pred_class = np.array(pred_class)
    pred2 = pred[pred_class!=1]
    pred_class2 = pred_class[pred_class!=1]
    candidates2 = np.array(candidates)[pred_class!=1]
    x, y, w, h = candidates2[np.argmax(pred2)]
```

在前面的代码中，我们提取了最有可能包含非背景对象的候选对象(一个对象的预测类别对应于背景):

```
   ax[1].set_title(labels[pred_class2[np.argmax(pred2)]])
   ax[1].imshow(img)
   ax[1].grid('off')
   rect = mpatches.Rectangle((x, y), w, h, fill=False, edgecolor='red', linewidth=1)
   ax[1].add_patch(rect)
```

在前面的代码中，我们沿着边界框的矩形块绘制图像。

13.  调用用新图像定义的函数:

```
filename = '...' #Path to new image
test_predictions(filename)
```

![](Images/3dbde263-8f5e-4c2e-8c25-2cd98af80476.png)

请注意，该模型准确地计算了图像中对象的类别。此外，包含人的概率最高的边界框(候选)需要一点修正。

在下一步中，我们将进一步修正边界框。

14.  建立并编译一个模型，该模型将图像的 VGG16 特征作为输入，并预测包围盒校正:

```
model2 = Sequential()
model2.add(Flatten(input_shape=((7,7,512))))
model2.add(Dense(512, activation='relu'))
model2.add(Dense(4,activation='linear'))

model2.compile(loss='mean_absolute_error',optimizer='adam')
```

15.  构建模型以预测边界框修正。然而，我们需要确保我们只为那些可能包含图像的区域预测边界框校正:

```
for i in range(1000):
     samp=random.sample(range(len(x_train)),500)
     x_train2=[x_train[i] for i in samp if pred_class[i]!=1]
     x_train2 = np.array(x_train2)
     final_delta2 = [final_delta[i] for i in samp if pred_class[i]!=1]
     model2.fit(x_train2/x_train.max(), np.array(final_delta2), validation_split = 0.1, epochs=1, batch_size=32, verbose=0)
```

在前面的代码中，我们遍历输入数组数据集，并创建一个新的数据集，该数据集仅用于那些可能包含非背景的区域。

此外，我们将重复前面的步骤 1000 次，以微调模型。

16.  构建一个以图像路径作为输入并预测图像类别的函数，同时校正边界框:

```
'TESTING'
import matplotlib.patches as mpatches
def test_predictions2(filename):
     img = cv2.resize(cv2.imread(filename), (224,224))
     candidates = extract_candidates(img)
     _, ax = plt.subplots(1, 2)
     ax[0].imshow(img)
     ax[0].grid('off')
     ax[0].set_title(filename.split('/')[-1])
     pred = []
     pred_class = []
     del_new = []
     for ix, candidate in enumerate(candidates):
        l, t, w, h = np.array(candidate).astype(int)
        img2 = img[t:t+h,l:l+w,:3]
        img3 = cv2.resize(img2,(224,224))/255
        img4 = vgg16_model.predict(img3.reshape(1,224,224,3)) 
        final_pred = model.predict(img4/x_train.max())
        delta_new = model2.predict(img4/x_train.max())[0] 
        pred.append(np.max(final_pred))
        pred_class.append(np.argmax(final_pred))
        del_new.append(delta_new) 
     pred = np.array(pred)
     pred_class = np.array(pred_class)
     non_bgs = (pred_class!=1)
     pred = pred[non_bgs]
     pred_class = pred_class[non_bgs] 
     del_new = np.array(del_new)
     del_new = del_new[non_bgs]
     del_pred = del_new*224
     candidates = C = np.array(candidates)[non_bgs]
     C = np.clip(C, 0, 224)
     C[:,2] += C[:,0]
     C[:,3] += C[:,1]
     bbs_pred = candidates - del_pred
     bbs_pred = np.clip(bbs_pred, 0, 224) 
     bbs_pred[:,2] -= bbs_pred[:,0]
     bbs_pred[:,3] -= bbs_pred[:,1]
     final_bbs_pred = bbs_pred[np.argmax(pred)]
     x, y, w, h = final_bbs_pred
     ax[1].imshow(img)
     ax[1].grid('off')
     rect = mpatches.Rectangle((x, y), w, h, fill=False, edgecolor='red', linewidth=1)
     ax[1].add_patch(rect)
     ax[1].set_title(labels[pred_class[np.argmax(pred)]])
```

17.  提取仅包含一个对象的测试图像(因为我们已经在包含单个对象的图像上构建了模型):

```
single_object_images = []
for ix, xml in enumerate(XMLs[N:]):
     xml_file = annotations + xml
     fname = xml.split('.')[0]
     with open(xml_file, "rb") as f: # notice the "rb" mode
         xml = xmltodict.parse(f, xml_attribs=True)
         l = []
         if isinstance(xml["annotation"]["object"], list):
             continue
         single_object_images.append(xml["annotation"]['filename'])
         if(ix>100):
             break
```

在前面的代码中，我们遍历图像注释并识别包含单个对象的图像。

18.  在单个对象图像上预测:

```
test_predictions2(filename)
```

![](Images/3ae32885-779f-4d76-997b-a69f51db9f4e.png)

注意，第二个模型能够校正边界框以适合人；然而，边界框仍然需要多一点修正。当在更多的数据点上训练时，这有可能实现。

        

# 执行非最大抑制

到目前为止，在前面的部分中，我们仅考虑了不具有背景的候选者，并且进一步考虑了具有感兴趣对象的最高概率的候选者。然而，这在图像中存在多个对象的情况下是失败的。

在本节中，我们将讨论候选区域建议的入围方式，以便我们能够在图像中提取尽可能多的对象。

        

# 做好准备

我们采取的执行 NMS 的策略如下:

*   从图像中提取区域建议
*   重塑区域建议并预测图像中包含的对象
*   如果目标是非背景，我们将保留候选对象
*   对于所有的非背景类候选对象，我们将根据它们包含对象的概率对它们进行排序
*   第一个候选人(按任何类别概率递减的后等级排序)将根据 IoU 与所有其余候选人进行比较
*   如果任何其他区域与第一个候选区域有相当大的重叠，则应将其丢弃
*   在剩下的候选者中，我们将再次考虑包含一个对象的概率最高的候选者
*   我们将重复第一个候选项(在与上一步中的第一个候选项有有限重叠的过滤列表中)与其余候选项的比较
*   这一过程一直持续到没有候选项可供比较为止
*   我们将为在前面的步骤之后剩余的候选对象绘制候选对象，作为最终的边界框

        

# 怎么做...

非最大抑制在 Python 中编码如下。我们将从上一个菜谱中的步骤 14 继续(代码文件和相应的推荐数据集链接在 GitHub 中以`Region_proposal_based_object_detectionn.ipynb`的形式提供)。

1.  从图像中提取所有区域，在这些区域中，包含非背景类的对象的置信度很高:

```
filename = jpegs + single_object_images[ix]
img = cv2.imread(filename)
img = cv2.resize(img,(224,224))
img_area = img.shape[0]*img.shape[1]
candidates = extract_candidates(img)
plt.imshow(img)
plt.grid('off')
```

我们正在考虑的图像如下:

![](Images/ad993779-4983-499c-841a-61bc9b3c2692.png)

2.  预处理候选者——将它们通过 VGG16 模型，然后预测每个区域提议的类别，以及区域的边界框:

```
pred = []
pred_class = []
del_new = []

for ix, candidate in enumerate(candidates):
    l, t, w, h = np.array(candidate).astype(int)
    img2 = img[t:t+h,l:l+w,:3]
    img3 = cv2.resize(img2,(224,224))/255
    img4 = vgg16_model.predict(img3.reshape(1,224,224,3)) 
    final_pred = model.predict(img4/x_train.max())
    delta_new = model2.predict(img4/x_train.max())[0]
    pred.append(np.max(final_pred))
    pred_class.append(np.argmax(final_pred))
    del_new.append(delta_new)
pred = np.array(pred)
pred_class = np.array(pred_class)
```

3.  提取非背景类预测及其相应的包围盒校正:

```
non_bgs = ((pred_class!=1))
pred = pred[non_bgs]
pred_class = pred_class[non_bgs]

del_new = np.array(del_new)
del_new = del_new[non_bgs]
del_pred = del_new*224
```

在前面的步骤中，我们已经过滤了非背景区域的所有概率、类别和边界框校正(在我们的数据准备过程中，预测类别`1`属于背景类别)。

4.  使用边界框校正值校正候选字:

```
candidates = C = np.array(candidates)[non_bgs]
C = np.clip(C, 0, 224)
C[:,2] += C[:,0]
C[:,3] += C[:,1]

bbs_pred = candidates - del_pred
bbs_pred = np.clip(bbs_pred, 0, 224)
```

此外，我们还确保了`xmax`和`ymax`坐标不能大于`224`。

此外，我们需要确保边界框的宽度和高度不能为负:

```
bbs_pred[:,2] -= bbs_pred[:,0]
bbs_pred[:,3] -= bbs_pred[:,1]
bbs_pred = np.clip(bbs_pred, 0, 224)

bbs_pred2 = bbs_pred[(bbs_pred[:,2]>0) & (bbs_pred[:,3]>0)]
pred = pred[(bbs_pred[:,2]>0) & (bbs_pred[:,3]>0)]
pred_class = pred_class[(bbs_pred[:,2]>0) & (bbs_pred[:,3]>0)]
```

5.  沿着边界框绘制图像:

```
import matplotlib.patches as mpatches
fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))
ax.imshow(img)
for ix, (x, y, w, h) in enumerate(bbs_pred2):
    rect = mpatches.Rectangle(
        (x, y), w, h, fill=False, edgecolor='red', linewidth=1)
    ax.add_patch(rect)

plt.axis('off')
plt.show()
```

![](Images/509c6bb7-d6d8-4e13-b9d7-7c10a5da93ed.png)

6.  在边界框顶部执行非最大抑制。为此，我们将在以下步骤中定义一个函数，该函数通过获取两个边界框可能具有的最小交集(阈值)、边界框坐标以及与每个边界框相关联的概率得分来执行 NMS:
    1.  计算每个边界框的`x`、`y`、`w`和`h`值、它们相应的面积以及它们的概率顺序:

```
def nms_boxes(threshold, boxes, scores):
     x = boxes[:, 0]
     y = boxes[:, 1]
     w = boxes[:, 2]
     h = boxes[:, 3]
     areas = w * h
     order = scores.argsort()[::-1]
```

```
     keep = []
     while order.size > 0:
         i = order[0]
         keep.append(i)
         xx1 = np.maximum(x[i], x[order[1:]])
         yy1 = np.maximum(y[i], y[order[1:]])
         xx2 = np.minimum(x[i] + w[i], x[order[1:]] + w[order[1:]])
         yy2 = np.minimum(y[i] + h[i], y[order[1:]] + h[order[1:]])
         w1 = np.maximum(0.0, xx2 - xx1 + 1)
         h1 = np.maximum(0.0, yy2 - yy1 + 1)
         inter = w1 * h1
         iou = inter / (areas[i] + areas[order[1:]] - inter)
```

```
        inds = np.where(ovr <= threshold)[0]
        order = order[inds + 1]
```

在前面的步骤中，我们确保我们有下一组候选项(除了第一个候选项)，这些候选项将在相同的步骤中循环(注意函数开始处的`while`循环)。

```
    keep = np.array(keep)
    return keep
```

7.  执行前面的功能:

```
keep_box_ixs = nms_boxes(0.3, bbs_pred2, pred)
```

8.  绘制上一步留下的边界框:

```
import matplotlib.patches as mpatches
fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))
ax.imshow(img)
for ix, (x, y, w, h) in enumerate(bbs_pred2):
     if ix not in keep_box_ixs:
         continue
     rect = mpatches.Rectangle((x, y), w, h, fill=False, edgecolor='red', linewidth=1)
     ax.add_patch(rect)
     centerx = x + w/2
     centery = y + h - 10
     plt.text(centerx, centery,labels[pred_class[ix]]+" "+str(round(pred[ix],2)),fontsize = 20,color='red')
plt.axis('off')
plt.show()
```

![](Images/415d8874-7c1a-4f5a-af2e-6e66958d87e6.png)

从前面的屏幕截图中，我们看到我们删除了所有其他作为区域建议生成的边界框。

        

# 使用基于锚盒的算法检测人

基于区域提议的 CNN 的一个缺点是它不能实现实时对象识别，因为选择性搜索需要相当长的时间来提议区域。这导致基于区域提议的对象检测算法在像自动驾驶汽车这样实时检测非常重要的情况下没有用。

为了实现实时检测，我们将从头开始构建一个受**你只看一次** ( **YOLO** )算法启发的模型，该模型查看包含图像中的人的图像，并在图像中的人周围绘制一个边界框。

        

# 做好准备

为了理解 YOLO 是如何克服在生成区域建议时消耗大量时间的缺点的，让我们将 YOLO 项分解成它的组成项——我们将从神经网络的单次正向传递中做出所有的预测(图像类别以及边界框)。与我们用基于区域建议的 CNN 所做的相比，在 CNN 中，选择性搜索算法给我们区域建议，然后我们在其上建立分类算法。

为了弄清楚 YOLO 的工作细节，让我们通过一个玩具的例子。假设输入图像如下所示，其中图像被划分为 3 x 3 的网格:

![](Images/d767d1cb-cba0-4129-af5e-a1ce782a7733.png)

我们的神经网络模型的输出大小应为 3×3×5，其中前 3×3 对应于我们在图像中拥有的网格数量，五个通道的第一个输出对应于包含对象的网格的概率，其他四个组成部分是与图像中的网格相对应的 x，y，w，h 坐标的*增量。*

我们使用的另一个杠杆是锚箱。本质上，我们已经知道在我们拥有的图像集合中有一些特定的形状。例如，一辆汽车的宽度大于高度，而一个站着的人通常比宽度高。

因此，我们将把图像集中的所有高度和宽度值聚集成五个集群，这将产生五个锚定框的高度和宽度，我们将使用它们来识别图像中对象周围的边界框。

如果有五个锚盒在一个图像上工作，那么输出应该是 3×3×5×5，其中 5×5 对应于五个锚盒中的每一个的五个成分(对象的一个概率和沿着 *x* 、 *y* 、 *w* 和 *h* 的四个增量)。

从前面的描述中，我们可以看到，3×3×5×5 的输出可以通过神经网络模型的单次正向传递来生成。

在下面的部分中，伪代码理解了生成大小锚的方法:

*   提取数据集中所有图像的宽度和高度
*   使用五个聚类运行 k-means 聚类，以识别图像中存在的宽度和高度聚类
*   五个聚类中心对应于五个锚盒的宽度和高度来构建模型

此外，在以下部分，我们将了解 YOLO 算法是如何工作的:

*   将图像分成固定数量的网格单元
*   对应于图像边界框的地面真实中心的网格应该是负责预测边界框的网格
*   锚箱的中心应与网格的中心相同
*   创建训练数据集:
    *   对于包含物体中心的网格，因变量为 1，需要计算每个锚盒的 *x* 、 *y* 、 *w* 和 *h* 的增量
    *   对于不包含物体中心的网格，因变量为零，并且 *x* 、 *y* 、 *w* 和 *h* 的增量无关紧要
*   在第一个模型中，我们将预测包含图像中心的定位框和网格单元的组合
*   在第二个模型中，我们预测锚框的边界框修正

        

# 怎么做...

我们将构建执行人员检测的代码(代码文件和相应的推荐数据集链接在 GitHub 中作为`Anchor_box_based_person_detection.ipynb`与推荐数据集一起提供):

1.  下载包含一组图像、图像中包含的对象以及图像中对象的相应边界框的数据集。GitHub 中提供了推荐的数据集和相应的代码文件。

样本图像及其相应的边界框位置输出看起来类似于我们在“使用基于区域提议的 CNN 方法的*对象检测”的步骤 1 中看到的图像。*

2.  导入相关的包，如下所示:

```
import matplotlib.pyplot as plt
%matplotlib inline
import tensorflow as tf, selectivesearch
import json, scipy, os, numpy as np,argparse,time, sys, gc, cv2, xmltodict
from copy import deepcopy
```

3.  定义 IoU 提取函数，如以下代码所示:

```
def extract_iou2(candidate, current_y,img_shape):
     boxA = deepcopy(candidate)
     boxB = deepcopy(current_y)
     boxA[2] += boxA[0]
     boxA[3] += boxA[1]
     iou_img1 = np.zeros(img_shape)
     iou_img1[boxA[1]:boxA[3],boxA[0]:boxA[2]]=1
     iou_img2 = np.zeros(img_shape)
     iou_img2[int(boxB[1]):int(boxB[3]),int(boxB[0]):int(boxB[2])]=1
     iou = np.sum(iou_img1*iou_img2)/(np.sum(iou_img1)+np.sum(iou_img2)-np.sum(iou_img1*iou_img2))
     return iou
```

4.  将定位框的宽度和高度定义为图像总宽度和高度的百分比:

```
y_train = []

for i in mylist[:10000]:
     xml_file = xml_filepath +i
     arg1=i.split('.')[0]
     with open(xml_file, "rb") as f: # notice the "rb" mode
         d = xmltodict.parse(f, xml_attribs=True)
         l=[]
         if type(d["annotation"]["object"]) == type(l):
             discard=1
         else:
             x1=((float(d['annotation']['object']
             ['bndbox']['xmin'])))/(float(d['annotation']['size']['width']))
             x2=((float(d['annotation']['object']
             ['bndbox']['xmax'])))/(float(d['annotation']['size']['width']))
             y1=((float(d['annotation']['object']
             ['bndbox']['ymin'])))/(float(d['annotation']['size']['height']))
             y2=((float(d['annotation']['object']
             ['bndbox']['ymax'])))/(float(d['annotation']['size']['height']))
             cls=d['annotation']['object']['name']
             if(cls == 'person'):
                 y_train.append([x2-x1, y2-y1])
```

在前面的代码中，我们遍历了所有只包含一个对象的图像，然后计算边界框的宽度和高度(如果图像中包含一个人)。

```
y_train = np.array(y_train)
from sklearn.cluster import KMeans
km = KMeans(n_clusters=5)
km.fit(y_train)
km.cluster_centers_
```

上述结果产生聚类中心，如下所示:

```
anchors = [[[0.84638352, 0.90412013],        
            [0.28036872, 0.58073186],        
            [0.45700897, 0.87035502],        
            [0.15685545, 0.29256264],        
            [0.59814951, 0.64789503]]]

```

5.  创建训练数据集:
    1.  初始化空列表，以便在进一步处理时可以向它们追加数据:

```
k=-1
pre_xtrain = []
y_train = []
cls = []
xtrain=[]
final_cls = []
dx = []
dy = []
dw= []
dh = []
final_delta = []
av = 0
x_train = []
img_paths = []
label_coords = []
y_delta = []
anc = []
```

```
for i in mylist[:10000]:
     xml_file = xml_filepath +i
     arg1=i.split('.')[0]
     discard=0
     with open(xml_file, "rb") as f: # notice the "rb" mode
         d = xmltodict.parse(f, xml_attribs=True)
         l=[]
         if type(d["annotation"]["object"]) == type(l):
             discard=1
         else:
             coords={arg1:[]}
             pre_xtrain.append(arg1)
             m=pre_xtrain[(k+1)]
             k = k+1
             if(discard==0):
                 x1=((float(d['annotation']['object']['bndbox']['xmin'])))/(float(d['annotation']['size']['width']))
                 x2=((float(d['annotation']['object']['bndbox']['xmax'])))/(float(d['annotation']['size']['width']))
                 y1=((float(d['annotation']['object']['bndbox']['ymin'])))/(float(d['annotation']['size']['height']))
                 y2=((float(d['annotation']['object']['bndbox']['ymax'])))/(float(d['annotation']['size']['height']))
                 cls=d['annotation']['object']['name']
                 if(cls == 'person'):
                     coords[arg1].append(x1)
                     coords[arg1].append(y1)
                     coords[arg1].append(x2)
                     coords[arg1].append(y2)
                     coords[arg1].append(cls)
```

前面的代码追加了对象的位置(针对图像的宽度和高度进行后规范化)。

```
                     filename = base_dir+m+'.jpg'
                     # reference to jpg files here
                     img = filename
                     img_size=224
                     img = cv2.imread(filename)
                     img2 = cv2.resize(img,(img_size,img_size))
                     img2 = img2/255
```

```
                    current_y = [int(x1*224), int(y1*224), int(x2*224), int(y2*224)]
                    current_y2 = [float(d['annotation']['object']['bndbox']['xmin']), float(d['annotation']['object']['bndbox']['ymin']),
 float(d['annotation']['object']['bndbox']['xmax'])-float(d['annotation']['object']['bndbox']['xmin']),
 float(d['annotation']['object']['bndbox']['ymax'])-float(d['annotation']['object']['bndbox']['ymin'])]

                    label_center = [(current_y[0]+current_y[2])/2,(current_y[1]+current_y[3])/2] 
                    label = current_y
```

```
            vgg_predict =vgg16_model.predict(img2.reshape(1,img_size,img_size,3))
            x_train.append(vgg_predict)
```

至此，我们已经创建了输入要素。

```
            target_class = np.zeros((num_grids,num_grids,5))
            target_delta = np.zeros((num_grids,num_grids,20))
```

在前面的步骤中，我们已经为目标类和边界框校正初始化了零数组:

```
def positive_grid_cell(label,img_width = 224, img_height = 224): 
     label_center = [(label[0]+label[2])/(2),(label[1]+label[3])/(2)] 
     a = int(label_center[0]/(img_width/num_grids)) 
     b = int(label_center[1]/(img_height/num_grids)) 
     return a, b
```

在前面的步骤中，我们已经定义了一个包含对象中心的函数:

```
            a,b = positive_grid_cell(label)
```

前面的代码帮助我们将类别`1`分配给包含对象中心的网格，并且每隔一个网格将有一个零标签。

此外，让我们定义一个函数来查找与感兴趣对象的形状最接近的锚点:

```
def find_closest_anchor(label,img_width, img_height):
     label_width = (label[2]-label[0])/img_width
     label_height = (label[3]-label[1])/img_height 
     label_width_height_array = np.array([label_width, label_height]) 
     distance = np.sum(np.square(np.array(anchors) - label_width_height_array), axis=1) 
     closest_anchor = anchors[np.argmin(distance)] 
     return closest_anchor
```

前面的代码将图像中感兴趣对象的宽度和高度与所有可能的锚点进行比较，并确定与图像中实际对象的宽度和高度最接近的锚点。

最后，我们还将定义一个计算锚点边界框修正的函数，如下所示:

```
def closest_anchor_corrections(a, b, anchor, label, img_width, img_height): 
     label_center = [(label[0]+label[2])/(2),(label[1]+label[3])/(2)] 
     anchor_center = [a*img_width/num_grids , b*img_height/num_grids ] 
     dx = (label_center[0] - anchor_center[0])/img_width 
     dy = (label_center[1] - anchor_center[1])/img_height
     dw = ((label[2] - label[0])/img_width) / (anchor[0])
     dh = ((label[3] - label[1])/img_height) / (anchor[1]) 
     return dx, dy, dw, dh 
```

我们现在已经准备好创建目标数据了:

```
for a2 in range(num_grids):
     for b2 in range(num_grids):
         for m in range(len(anchors)):
             dx, dy, dw, dh = closest_anchor_corrections(a2, b2, anchors[m], label, 224, 224)
             target_class[a2,b2,m] = 0
             target_delta[a2,b2,((4*m)):((4*m)+4)] = [dx, dy, dw, dh]
             anc.append(anchors[m])
             if((anchors[m] == find_closest_anchor(label,224, 224)) & (a2 == a) & (b2 == b)):
                 target_class[a2,b2,m] = 1
```

在前面的代码中，当考虑的锚点是与图像中的对象形状匹配的最接近的锚点时，我们分配了一个目标类`1`。

我们还将边界框校正存储在另一个列表中:

```
            y_train.append(target_class.flatten())
            y_delta.append(target_delta)
```

6.  建立一个模型来识别最有可能包含对象的网格单元和锚点:

```
from keras.optimizers import Adam
optimizer = Adam(lr=0.001)
from keras.layers import BatchNormalization
from keras import regularizers
model = Sequential()
model.add(BatchNormalization(input_shape=(7,7,512)))
model.add(Conv2D(1024, (3,3), activation='relu',padding='valid'))
model.add(BatchNormalization())
model.add(Conv2D(5, (1,1), activation='relu',padding='same'))
model.add(Flatten())
model.add(Dense(125, activation='sigmoid'))
model.summary()
```

7.  创建用于分类的输入和输出数组:

```
y_train = np.array(y_train)
x_train = np.array(x_train)
x_train = x_train.reshape(x_train.shape[0],7,7,512)
```

8.  编译并拟合用于分类的模型:

```
model.compile(loss='binary_crossentropy', optimizer=optimizer)

model.fit(x_train/np.max(x_train), y_train, epochs=5, batch_size = 32, validation_split = 0.1, verbose = 1)
```

9.  根据前面的模型，我们可以确定最有可能有一个人的网格单元和锚定框组合。在这一步中，我们将建立一个数据集，其中我们为最有可能包含对象的预测校正边界框:

```
delta_x = []
delta_y = []
for i in range(len(x_train)):
     delta_x.append(x_train[i])
     delta = y_delta[i].flatten()
     coord = np.argmax(model.predict(x_train[i].reshape(1,7,7,512)/12))
     delta_y.append(delta[(coord*4):((coord*4)+4)])
```

在前面的步骤中，我们已经为最可能包含对象的预测准备了输入(原始图像的 VGG16 特征)和输出边界框校正。

注意，我们将`coord`乘以因子 4，对于每个网格单元和锚定框组合，对于 *x* 、 *y* 、 *w* 和 *h* 有四个可能的校正值。

10.  建立一个模型，预测在 *x* 、 *y* 、 *w* 和 *h* 坐标中的修正:
    1.  创建输入和输出数组，并归一化四个边界框校正值，以便所有四个值具有相似的范围:

```
delta_x = np.array(delta_x)
delta_y = np.array(delta_y)

max_y = np.max(delta_y, axis=0)
delta_y2 = deltay/max_y
```

```
model2 = Sequential()
model2.add(BatchNormalization(input_shape=(7,7,512)))
model2.add(Conv2D(1024, (3,3), activation='relu',padding='valid'))
model2.add(BatchNormalization())
model2.add(Conv2D(5, (1,1), activation='relu',padding='same'))
model2.add(Flatten())
model2.add(Dense(2, activation='linear'))
```

11.  编译并拟合模型:

```
model2.compile(loss = 'mean_absolute_error', optimizer = optimizer)
model2.fit(delta_x/np.max(x_train), delta_y2, epochs = 10, batch_size = 32, verbose = 1, validation_split = 0.1)
```

12.  预测新图像上的边界框:
    1.  提取最可能包含对象的位置:

```
img = cv2.imread('/content/Hemanvi.jpg')
img = cv2.resize(img,(224,224))
img = img/255
img2 = vgg16_model.predict(img.reshape(1,224,224,3))
arg = np.argmax(model.predict(img2/np.max(x_train)))
```

在前面的步骤中，我们选取了一个包含人物的图像，并调整了它的大小，以便可以进一步处理它来提取 VGG16 特征。最后，我们正在识别最有可能包含一个人的位置的锚。

前面预测了最有可能包含感兴趣的对象的网格单元和锚定框组合，其完成方式如下:

```
count = 0
for a in range(num_grids):
     for b in range(num_grids):
         for c in range(len(anchors)):
             if(count == arg):
                 a2 = a
                 b2 = b
                 c2 = c 
             count+=1
```

在前面的代码中，`a2`和`b2`将是最有可能包含对象的网格单元( *x* 轴和 *y* 轴的组合)，而`c2`是可能与对象形状相同的锚框。

```
pred = model2.predict(img2/np.max(delta_x))[0]
```

```
pred1 = pred*max_y
```

```
xmin = pred1[0]*224+a2*224/num_grids - (anchors[c2][0]*pred1[2] * 224)/2
ymin = pred1[1]*224+b2*224/num_grids - (anchors[c2][1]*pred1[3] * 224)/2

w = anchors[c2][0]*pred1[2] * 224
h = anchors[c2][1]*pred1[3] * 224
```

```
import matplotlib.patches as mpatches
cand = [xmin, ymin, w, h]
cand = np.clip(cand, 1, 223)
fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(6, 6))
ax.imshow(img)
rect = mpatches.Rectangle(
(cand[0], cand[1]), cand[2], cand[3], fill=False, edgecolor='red', linewidth=1)
ax.add_patch(rect)
plt.grid('off')
plt.show()
```

![](Images/e075ef65-23d0-48bd-b284-2b3b3e9ff3b0.png)

这种方法的一个缺点是，当我们检测与图像尺寸相比非常小的物体时会变得困难。

        

# 还有更多...

让我们考虑一个场景，其中要检测的对象尺寸很小。如果我们通过预先训练的网络传递该图像，则该对象将在更早的层中被检测到，因为在最后几层中，该图像将通过多个汇集层，导致要检测的对象缩小到非常小的空间。

类似地，如果要检测的对象尺寸较大，则该对象将在预训练网络的最后几层中被检测到。

单镜头检测器使用预先训练的网络，其中网络的不同层致力于检测不同类型的图像:

![](Images/363ff5ac-cf10-4007-8109-73940627fee0.png)

资料来源:https://arxiv.org/pdf/1512.02325.pdf

在上图中，您应该注意到来自不同图层的要素通过密集图层，最后连接在一起，以便可以构建和微调模型。

另外，也可以基于此处提供的教程实现 YOLO:[https://pjreddie.com/darknet/yolo/](https://pjreddie.com/darknet/yolo/)。