<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 多对一架构 RNN 的应用

在上一章中，我们了解了 RNN 和 LSTM 的运作方式。我们还学习了情感分类，这是一个经典的多对一应用程序，因为输入中的许多单词对应一个输出(积极或消极的情感)。

在这一章中，我们将通过以下方法加深对多对一架构 RNN 的理解:

*   生成文本
*   电影推荐
*   使用嵌入的主题建模
*   预测股票价格的价值

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 生成文本

在我们在第十一章[、*构建循环神经网络*中执行的情感分类食谱中，我们试图预测一个离散事件(情感分类)。这属于多对一架构。在这个菜谱中，我们将学习如何实现多对多架构，其中输出将是给定的 10 个单词序列的下一个可能的 50 个单词。](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 做好准备

我们将采用以下策略来生成文本:

1.  导入古腾堡项目的*爱丽丝漫游奇境记*数据集，可以从 https://www.gutenberg.org/files/11/11-0.txt[下载](https://www.gutenberg.org/files/11/11-0.txt)。

2.  对文本数据进行预处理，使每个单词都采用相同的大小写，并删除标点符号。
3.  为每个唯一的单词分配一个 ID，然后将数据集转换为单词 ID 序列。
4.  遍历整个数据集，一次 10 个单词。将这 10 个单词视为输入，将随后的第 11 个单词视为输出。
5.  通过在输入单词 id 的顶部执行嵌入，然后将嵌入连接到 LSTM(通过隐藏层连接到输出层),来构建和训练模型。输出图层中的值是输出的一次性编码版本。
6.  通过随机选择一个单词的位置来预测下一个单词，并在选择随机单词的位置之前考虑历史单词。
7.  将输入单词的窗口从我们之前选择的种子单词的位置移动一个，第十个时间步长单词将是我们在上一步中预测的单词。
8.  继续这个过程，继续生成文本。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 怎么做...

典型的 RNN 需求，我们将查看一个给定的 10 个单词的序列来预测下一个可能的单词。在这个练习中，我们将使用 Alice 数据集来生成单词，如下所示(代码文件在 GitHub 中以`RNN_text_generation.ipynb`的形式提供):

1.  导入相关的包和数据集:

```
from keras.models import Sequential
from keras.layers import Dense,Activation
from keras.layers.recurrent import SimpleRNN
from keras.layers import LSTM
import numpy as np
fin=open('alice.txt',encoding='utf-8-sig')
lines=[]
for line in fin:
  line = line.strip().lower()
  if(len(line)==0):
    continue
  lines.append(line)
fin.close()
text = " ".join(lines)
```

输入文本的示例如下所示:

![](Images/e3436727-b2bd-4f9c-af35-991347215f99.png)

2.  规范化文本以删除标点符号并将其转换为小写:

```
import re
text = text.lower()
text = re.sub('[^0-9a-zA-Z]+',' ',text)
```

3.  将唯一的单词分配给索引，以便在构造训练和测试数据集时可以引用它们:

```
from collections import Counter
counts = Counter()
counts.update(text.split())
words = sorted(counts, key=counts.get, reverse=True)
nb_words = len(text.split())
word2index = {word: i for i, word in enumerate(words)}
index2word = {i: word for i, word in enumerate(words)}
```

4.  构建导致输出单词的输入单词集。注意，我们正在考虑一系列的`10`单词，并试图预测第 *11 个* ^(*第*) 个单词:

```
SEQLEN = 10
STEP = 1
input_words = []
label_words = []
text2=text.split()
for i in range(0,nb_words-SEQLEN,STEP):
     x=text2[i:(i+SEQLEN)]
     y=text2[i+SEQLEN]
     input_words.append(x)
     label_words.append(y)
```

`input_words`和`label_words`列表的示例如下:

![](Images/a1db4f29-47c6-403b-a342-3442e1107f62.png)

注意`input_words`是列表的列表，而`output_words`不是列表。

5.  构建输入和输出数据集的向量:

```
total_words = len(set(words))
X = np.zeros((len(input_words), SEQLEN, total_words), dtype=np.bool)
y = np.zeros((len(input_words), total_words), dtype=np.bool)
```

我们在前面的步骤中创建了空数组，这些数组将在下面的代码中填充:

```
# Create encoded vectors for the input and output values
for i, input_word in enumerate(input_words):
     for j, word in enumerate(input_word):
         X[i, j, word2index[word]] = 1
     y[i,word2index[label_words[i]]]=1
```

在前面的代码中，第一个`for`循环用于遍历输入单词序列中的所有单词(输入中的`10`单词)，第二个`for`循环用于遍历所选输入单词序列中的单个单词。另外，假设输出是一个列表，我们不需要使用第二个`for`循环来更新它(因为没有 id 序列)。`X`和`y`的输出形状如下:

![](Images/d6fc2f39-53df-42c7-b878-1a7b062b084c.png)

6.  定义模型的架构:

```
HIDDEN_SIZE = 128
BATCH_SIZE = 32
NUM_ITERATIONS = 100
NUM_EPOCHS_PER_ITERATION = 1
NUM_PREDS_PER_EPOCH = 100

model = Sequential()
model.add(LSTM(HIDDEN_SIZE,return_sequences=False,input_shape=(SEQLEN,total_words)))
model.add(Dense(total_words, activation='softmax'))
model.compile(optimizer='adam', loss='categorical_crossentropy')
model.summary()
```

该模型的摘要如下:

![](Images/1369b6ce-e893-4f4c-8c0c-156ad528671e.png)

7.  符合模型。看看输出在越来越多的时期内是如何变化的。生成一组随机的`10`单词序列，并尝试预测下一个可能的单词。我们可以观察到，随着时代的增加，我们的预测变得越来越好:

```
for iteration in range(50):
     print("=" * 50)
     print("Iteration #: %d" % (iteration))
     model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION, validation_split = 0.1)
     test_idx = np.random.randint(int(len(input_words)*0.1)) * (-1)
     test_words = input_words[test_idx]
     print("Generating from seed: %s" % (test_words))
     for i in range(NUM_PREDS_PER_EPOCH): 
         Xtest = np.zeros((1, SEQLEN, total_words))
         for i, ch in enumerate(test_words):
             Xtest[0, i, word2index[ch]] = 1
         pred = model.predict(Xtest, verbose=0)[0]
         ypred = index2word[np.argmax(pred)]
         print(ypred,end=' ')
         test_words = test_words[1:] + [ypred]
```

在前面的代码中，我们在一个时期的输入和输出数组上拟合我们的模型。此外，我们选择一个随机的种子词(`test_idx`——它是一个随机数，位于输入数组的最后 10%(因为`validation_split`是`0.1`)并且在一个随机的位置收集输入词。我们将 IDs 的输入序列转换成一个热编码的版本(从而获得一个形状为 1 x 10 x `total_words`的数组)。

最后，我们对刚刚创建的数组进行预测，并获得概率最高的单词。让我们看看第一个时期的输出，并与第 *25 ^第时期的输出进行对比:*

![](Images/241f2377-e03e-4480-9486-3f2db105aaf5.png)

注意，在第一个时期，输出总是`the`。然而，在 50 个时期结束时，它变得更加合理，如下所示:

![](Images/e2a8dab4-322a-446e-b552-0c8844540497.png)

`Generating from seed`线是预测的集合。

注意，虽然训练损失随着时期的增加而减少，但是验证损失在 50 个时期结束时变得更糟。这将随着我们对更多文本的训练和/或对模型的进一步微调而得到改善。

此外，该模型可以通过使用双向 LSTM 进一步改进，我们将在*序列到序列学习*章节中讨论。具有双向 LSTM 的输出如下:

![](Images/76fac46c-8de1-4a31-a8b5-190b4ba55fa9.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 电影推荐

推荐系统在用户的发现过程中扮演着重要的角色。想象一下一个包含数千种不同产品的电子商务目录。此外，还存在产品的变体。在这种情况下，让用户了解产品或事件(如果某些产品正在打折)成为增加销售的关键。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 做好准备

在这个菜谱中，我们将学习如何为用户给电影的评级数据库建立一个推荐系统。这个练习的目的是最大化电影对用户的相关性。在定义目标时，我们还应该考虑推荐的电影可能仍然相关，但用户可能不会立即观看。同时也要保证所有的推荐都不是关于同一个流派的。这尤其适用于在零售环境中给出建议的情况，在这种情况下，我们不希望在我们提供的所有建议中推荐同一产品的变体。

让我们正式确定我们目标和约束:

*   **目标**:最大化推荐与用户的相关性
*   **约束**:增加推荐的多样性，最多向用户提供 12 个推荐

相关性的定义可以因用例而异，并且通常由业务原则指导。在这个食谱中，让我们狭义地定义相关性；也就是说，如果用户购买了给定用户的前 12 个推荐项目中的任何产品，这就是成功的。

现在，让我们继续定义我们将采用的构建模型的步骤:

1.  导入数据。
2.  推荐一部用户评价很高的电影——因此，让我们基于用户在历史中喜欢的电影来训练我们的模型。用户不喜欢某些电影的洞察力将有助于进一步改进我们的推荐。然而，现在让我们保持简单。
3.  只保留看过五部以上电影的用户。
4.  将 id 分配给唯一的用户和电影。

5.  考虑到用户的偏好可能会随着时间而改变，我们需要考虑用户的历史，其中历史中的不同事件具有不同的权重。鉴于现在是一个时间序列分析问题，我们将利用 RNN 来解决这个问题。
6.  对数据进行预处理，以便将其传递给 LSTM:
    *   输入将是用户观看的五部历史电影
    *   输出是用户观看的第六部电影
7.  构建一个执行以下操作的模型:
    1.  为输入电影创建嵌入
    2.  通过 LSTM 层传递嵌入
    3.  通过密集层传递 LSTM 输出
    4.  在最终图层中应用 softmax，以创建一个电影推荐列表

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 怎么做...

现在我们已经完成了要执行的各个步骤的策略，让我们对它进行编码(代码文件在 GitHub 中以`Chapter_12_Recommender_systems.ipynb`的形式提供):

1.  导入数据。我们将处理一个数据集，该数据集包含用户列表、用户为不同电影提供的评级，以及用户提供评级时的相应时间戳:

```
import numpy as np
import pandas as pd
ratings = pd.read_csv('..') # Path to the file containing required fields
```

数据集的示例如下所示:

![](Images/033e8f33-fbdb-4a5c-b80a-626371a6ca12.png)

2.  过滤掉用户不喜欢电影的数据点或用户没有足够历史记录的用户。在下面的代码中，我们排除了用户评价较低的电影:

```
ratings = ratings[ratings['rating']>3]
ratings = ratings.sort_values(by='timestamp')
ratings.reset_index(inplace=True)
ratings = ratings.drop(['index'],axis=1)
```

在下面的代码中，我们只保留那些在历史记录中提供了超过`5`个评级(一个大于`3`的评级值)的用户:

```
user_movie_count =ratings.groupby('User').agg({'Movies':'nunique'}).reset_index()
user_movie_count.columns = ['User','Movie_count']
ratings2 = ratings.merge(user_movie_count,on='User',how='inner')
movie_count = ratings2[ratings2['Movie_count']>5]
movie_count = movie_count.sort_values('timestamp')
movie_count.reset_index(inplace=True)
movie_count = movie_count.drop(['index'],axis=1)
```

3.  为唯一的`users`和`Movies`分配 id，以便我们随后使用它们:

```
ratings = movie_count
users = ratings.User.unique()
articles = ratings.Movies.unique()
userid2idx = {o:i for i,o in enumerate(users)}
articlesid2idx = {o:i for i,o in enumerate(articles)}
idx2userid = {i:o for i,o in enumerate(users)}
idx2articlesid = {i:o for i,o in enumerate(articles)}

ratings['Movies2'] = ratings.Movies.apply(lambda x: articlesid2idx[x])
ratings['User2'] = ratings.User.apply(lambda x: userid2idx[x])
```

4.  预处理数据，以便输入是最近五部电影，输出是观看的第六部电影:

```
user_list = movie_count['User2'].unique()
historical5_watched = []
movie_to_predict = []
for i in range(len(user_list)):
     total_user_movies = movie_count[movie_count['User2']==user_list[i]].copy()
     total_user_movies.reset_index(inplace=True)
     total_user_movies = total_user_movies.drop(['index'],axis=1)
     for j in range(total_user_movies.shape[0]-6):
         historical5_watched.append(total_user_movies.loc[j:(j+4),'Movies2'].tolist())                                                                          movie_to_predict.append(total_user_movies.loc[(j+5),'Movies2'].tolist())
```

5.  预处理`historical5_watched`和`movie_to_predict`变量，以便它们可以被传递给模型，然后创建训练和测试数据集:

```
movie_to_predict2 = to_categorical(y, num_classes = max(y)+1)
trainX = np.array(historical5_watched[:40000])
testX = np.array(historical5_watched[40000:])
trainY = np.array(movie_to_predict2[:40000])
testY = np.array(movie_to_predict2[40000:])
```

6.  构建模型:

```
src_vocab = ratings['Movies2'].nunique()
n_units = 32
src_timesteps = 5
tar_vocab = len(set(y))

from keras.models import Sequential, Model
from keras.layers import Embedding
from keras.layers import LSTM, RepeatVector, TimeDistributed, Dense, Bidirectional

model = Sequential()
model.add(Embedding(src_vocab, n_units, input_length=src_timesteps))
model.add((LSTM(100)))
model.add(Dense(1000,activation='relu'))
model.add(Dense(max(y)+1,activation='softmax'))
model.summary()
```

请注意，在最后一层中，我们将 1 添加到可能的激活中，因为没有 ID 为 0 的电影，如果我们只将值设置为`max(y)`，那么最后一部电影将会被忽略。

该模型的摘要如下:

![](Images/82bd4fe6-5261-4651-b339-d7ee590096d0.png)

7.  符合模型:

```
model.fit(trainX, trainY, epochs=5, batch_size=32, validation_data=(testX, testY), verbose = 1)
```

8.  对测试数据进行预测:

```
pred = model.predict(testX)
```

9.  了解历史五部电影之后的下一部电影的数据点(用户)数量，这是最值得推荐的`12`:

```
count = 0
for i in range(testX.shape[0]):
    rank = np.argmax(np.argsort(pred[i])[::-1]==np.argmax(testY[i]))
    if rank<12:
        count+=1
count/testX.shape[0]
# 0.104
```

我们应该注意到，在 10.4%的情况下，我们将用户推荐观看的电影作为下一部电影。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 考虑用户历史

在发送我们在之前的迭代中错过的前 12 个推荐时，需要考虑的一个因素是*如果用户已经看过一部电影，他们不太可能再次观看同一部电影*(注意，这个假设在零售环境中不成立，因为那里有大量的再订购)。

让我们继续应用这个逻辑做出我们的 12 大预测。

首先，我们将存储用户在观看我们试图预测的电影之前观看的所有(不仅仅是最近五部)电影:

```
historically_watched = []
for i in range(len(user_list)):
     total_user_movies = movie_count[movie_count['User2']==user_list[i]].copy()
     total_user_movies.reset_index(inplace=True)
     total_user_movies = total_user_movies.drop(['index'],axis=1)
     for j in range(total_user_movies.shape[0]-6):
         historically_watched.append(total_user_movies.loc[0:(j+4),'Movies2'].tolist())
```

在前面的代码中，我们过滤了用户观看的所有电影。

如果用户已经看过一部电影，我们会将该用户-电影组合的概率改写为零值:

```
for j in range(pred.shape[0]):
  for i in range(pred.shape[1]):
    pred[j][i]= np.where(i in historically_watched[j], 0 , pred[j][i])
```

在下面的代码中，我们计算了测试数据中用户观看了前 12 部推荐电影的总场景的百分比:

```
count = 0
for i in range(testX.shape[0]):
  rank = np.argmax(np.argsort(pred[i])[::-1]==np.argmax(testY[i]))
  if rank<12:
    count+=1
count/testX.shape[0]
#12.6
```

前面的结果是，现在推荐对总用户的 12.6%有效，比前一次迭代的 10.4%相关。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 主题建模，使用嵌入

在上一个菜谱中，我们学习了如何为用户可能观看的电影生成预测。先前生成预测的方法的一个限制是，如果我们不在电影预测的基础上执行进一步的处理，电影推荐的种类将会受到限制。

各种建议很重要；如果没有多样性，用户只会发现特定类型的产品。

在这份食谱中，我们将根据电影的相似性对它们进行分组，并找出电影的共同主题。此外，我们还将研究如何增加可以提供给用户的各种推荐。话虽如此，这种策略很可能在电影推荐的特定情况下效果不佳，因为与零售/电子商务环境相比，其多样性要低得多，而在零售/电子商务环境中，产品的类别和替代品的数量要比电影多得多。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 做好准备

我们将根据相似性对电影进行分组的策略如下:

1.  从我们在电影推荐食谱中构建的模型中提取每部电影的嵌入值
    *   我们还可以使用 gensim 为每部电影创建嵌入
    *   用户观看的所有电影可以被认为是一个句子中的单词
    *   创建构成一个句子的单词 id 列表
    *   通过 gensim 的`Word2Vec`方法传递列表列表以提取单词向量(电影 ID 向量)
2.  将电影的嵌入值(向量)通过 k-means 聚类过程来提取一定数量的聚类
3.  确定集群的最佳数量
4.  确定在每个聚类中购买产品的高概率(在历史上未购买的产品中),并根据其概率对产品重新排序
5.  推荐排名前 *n* 的产品

在这个过程中，变量之一是要形成的集群的数量。聚类的数量越多，每个聚类中的产品就越少，同时，一个聚类中的每个产品之间的相似性就越大。本质上，在组中的点的数量和相同聚类内的数据点的相似性之间存在权衡。

我们可以通过计算所有的点相对于它们的聚类中心的平方距离的总和来得出一个组内点的相似性的度量。超过其惯性度量不会显著降低的聚类数是最优的聚类数。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 怎么做...

既然我们已经形成了在我们的推荐中获取各种产品的策略，让我们编写代码(我们将从*电影推荐*食谱的第 3 步继续)。该代码文件在 GitHub 中以`Chapter_12_Recommender_systems.ipynb`的名称提供。

1.  使用`Word2Vec`提取每部电影的嵌入值。
    1.  创建所有用户观看的各种电影的列表:

```
user_list = movie_count['User2'].unique()
user_movies = []
for i in range(len(user_list)):
     total_user_movies = movie_count[movie_count['User2']==user_list[i]].copy()
     total_user_movies.reset_index(inplace=True)
     total_user_movies = total_user_movies.drop(['index'],axis=1)
     total_user_movies['Movies3'] = total_user_movies['Movies2'].astype(str)
     user_movies.append(total_user_movies['Movies3'].tolist())
```

在前面的代码中，我们过滤了用户观看的所有电影，并创建了所有用户观看的电影列表。

2.  提取每部电影的单词向量:

```
from gensim.models import Word2Vec
w2v_model = Word2Vec(user_movies,size=100,window=5,min_count=5, iter = 500)
```

4.  提取电影的`TSNE`值，以获得我们在上一步中提取的电影词嵌入的可视化表示:

```
from sklearn.manifold import TSNE
tsne_model = TSNE(n_components=2, verbose=1, random_state=0)
tsne_img_label = tsne_model.fit_transform(w2v_model.wv.syn0)
tsne_df = pd.DataFrame(tsne_img_label, columns=['x', 'y'])
tsne_df['image_label'] = list(w2v_model.wv.vocab.keys())

from ggplot import *
chart = ggplot(tsne_df, aes(x='x', y='y'))+geom_point(size=70,alpha=0.5)
chart
```

二维空间中嵌入的可视化如下:

![](Images/cb69b499-e7ba-41a8-b2ea-dd2b11c96e87.png)

从前面的输出中，我们可以看到有一些电影被组合在一起(厚的区域)。

5.  将电影 ID 和电影索引值存储在数据帧中:

```
idx2movie = pd.DataFrame([idx2moviesid.keys(), idx2moviesid.values()]).T
idx2movie.columns = ['image_label','movieId']
```

6.  合并`tsne_df`和`idx2movie`数据集，以便我们在单个数据帧中拥有所有值:

```
tsne_df['image_label'] = tsne_df['image_label'].astype(int)
tsne_df2 = pd.merge(tsne_df, idx2movie, on='image_label', how='right')
```

7.  导入`movies`数据集:

```
movies = pd.read_csv('...') # Path to movies dataset
```

8.  将`TSNE`数据集与电影数据合并，并删除不需要的列:

```
tsne_df3 = pd.merge(tsne_df2, movies, left_on='movieId', right_on = 0, how='inner')
tsne_df4 = tsne_df3.drop([2,3,4],axis=1)
tsne_df4.rename(columns={1:'movie_name'}, inplace=True)
```

9.  排除具有安南值的行(我们对某些电影具有空值，因为某些电影出现的频率较低，导致`Word2Vec`没有给出很少出现的单词的单词向量(由于`min_count`参数):

```
tsne_df5 = tsne_df4.loc[~np.isnan(tsne_df4['x']),]
```

10.  通过了解惯性变化(所有点距其各自聚类中心的平方距离的总和)来确定最佳聚类数:

```
X = tsne_df5.loc[:,['x','y']]
inertia = []
for i in range(10):
      km = KMeans((i+1)*10)
      km.fit(X)
      inertia.append(km.inertia_)

import matplotlib.pyplot as plt
%matplotlib inline
plt.plot((np.arange(10)+1)*10,inertia)
plt.title('Variation of inertia over different number of clusters')
plt.xlabel('Number of clusters')
plt.ylabel('Inertia')
```

不同簇数的惯性变化如下:

![](Images/d667c7e1-453a-402c-a79b-1decc6dfa5bb.png)

从前面的曲线中，我们可以看到惯性的减少没有簇通过的数目`40`高。因此，我们将使用`40`作为我们的数据集中电影的最佳聚类数。

11.  如果电影在同一个群集中有意义，通过手动检查属于同一个群集中的一些电影来验证群集结果:

```
km = KMeans(40)
km.fit(X)
tsne_df5['clusterlabel'] = km.labelstsne_df5[tsne_df5['cluster_label']==0].head()
```

一旦执行代码，您会注意到位于`cluster_label` : `0`的电影主要是爱情和喜剧电影。

12.  移除用户已经观看过的电影:

```
for j in range(pred.shape[0]):
     for i in range(pred.shape[1]):
         pred[j][i]= np.where(i in historically_watched[j], 0 , pred[j][i])
```

13.  对于每个用户，映射电影的概率和电影所属的聚类数，以便我们提取在给定用户的聚类中具有最高概率的电影。然后从不同类别的热门电影中推荐前 12 部电影:

```
movie_cluster_id = tsne_df5[['image_label','cluster_label']]
count = 0
for j in range(pred.shape[0]):
      t = movie_cluster_id.copy()
      t['pred']=pred[j,list(movie_cluster_id['image_label'])]
      t2= t.sort_values(by='pred',ascending=False).groupby('cluster_label').first().reset_index()
      t3 = t2.sort_values(by='pred',ascending=False).reset_index()
      final_top_preds = t3.loc[:11]['image_label'].values
      if (np.argmax(testY[j]) in final_top_preds):
            count+=1

```

上述结果导致 13.6%的用户观看了向他们推荐的电影。

虽然前面的结果仅比 12.6%的结果稍好，但在没有任何推荐变化的情况下，当我们不仅考虑用户的下一次购买，而且考虑用户的所有未来购买时，有多种推荐更可能更好。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 还有更多...

虽然我们已经研究了为用户生成预测以及增加提供给用户的各种预测，但是我们可以通过考虑以下因素来进一步改进结果:

*   合并关于用户不喜欢的电影的信息
*   结合用户的人口统计信息
*   包含与电影相关的细节，例如发行年份和演员

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 预测股票价格的价值

专家们通过各种各样的技术分析来提出买卖股票的建议。大多数技术分析依赖于历史模式，假设只要我们对某些事件进行标准化，历史就会重复。

鉴于我们到目前为止所做的也是关于通过考虑历史来做出决策，让我们继续应用我们到目前为止所学的技能来预测股票价格。

然而，在股票价格预测等应用程序中依赖算法分析来做出买入或卖出决策时，要格外小心。其他配方和这个配方的最大区别在于，虽然在其他配方中做出的决定是可逆的(例如:如果生成的文本看起来不合适，你可以撤销它)或要花钱(一个糟糕的推荐意味着客户不会再次购买该产品)，但在股价预测中做出的决定是不可逆的。钱一旦丢了，就再也回不来了。

考虑到这一点，让我们继续应用我们到目前为止学到的技术来预测股票价格。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 做好准备

为了预测股票的价格，让我们运用两种策略:

*   仅根据最近五天的股价预测股价
*   根据最近五天的股票价格和感兴趣公司的最新消息预测股票价格

对于第一个分析，我们可以用与我们为 LSTM 准备数据集非常相似的方式来准备数据集；第二次分析需要不同的数据集准备方式，因为它涉及数字和文本数据。

我们将为上述两种方法处理数据的方式如下:

*   **仅最近五天的股票价格**:
    1.  将数据集从最早的日期到最新的日期排序
    2.  将第一个`5`股票价格作为输入，第六个股票价格作为输出
    3.  滑动它，以便在下一个数据点中，第二到第六个数据点是输入，第七个数据点是输出，依此类推，直到我们到达最后一个数据点:
        1.  五个数据点中的每一个都是 LSTM 中五个时间步长的输入
        2.  第六个数据点是输出
    4.  假设我们预测的是一个连续的数字，这次的损失函数将是*均方误差*值
*   **最近五天的股票价格加上新闻标题，关于公司的数据**:对于这个分析，有两种类型的数据预处理。虽然过去五天的股票价格的数据预处理保持不变，但新闻标题的数据预处理步骤是在此分析中要执行的附加步骤。让我们看看如何将这两者融入我们的模型:
    1.  假设这是两种数据类型，让我们有两种不同的模型:
        *   一个模型采用五天的历史股价数据。
        *   另一个模型通过增加或减少输出来修改过去五天的股票价格模型的输出。
        *   第二个模型是新闻标题的结果，数据集。假设是正面的标题可能会提高股票价格，负面的标题会降低股票价值。
    2.  为了简化问题，假设只有股票价值预测日之前的最新头条新闻会对预测日的股票价值结果产生影响
    3.  假设我们有两个不同的模型，使用函数式 API，这样我们就可以结合两个因素的影响

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 怎么做...

我们将把解决这个问题的方法分成三个部分(代码文件在 GitHub 中以`Chapter_12_stock_price_prediction.ipynb`的形式提供):

*   仅根据最近五天的股价预测股价
    *   随机训练-测试分裂的陷阱
*   给较新的股票价格值分配较高的权重
*   将最近五天的股票价格与新闻标题的文本数据结合起来

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 仅最近五天的股票价格

在这个食谱中，我们将只根据最近 5 个数据点来预测股票价格。在下一个菜谱中，我们将根据新闻和历史数据预测股票价格:

1.  导入相关的包和数据集:

```
import pandas as pd
data2 = pd.read_csv('/content/stock_data.csv')
```

2.  准备数据集，其中输入是最近五天的股价值，输出是第六天的股价值:

```
x= []
y = []
for i in range(data2.shape[0]-5):
     x.append(data2.loc[i:(i+4)]['Close'].values)
     y.append(data2.loc[i+5]['Close'])

import numpy as np
x = np.array(x)
y = np.array(y)
```

3.  重塑数据集，使其具有`batch_size`、`time_steps`、`features_per_time_step`形式:

```
x = x.reshape(x.shape[0],x.shape[1],1)
```

4.  创建训练和测试数据集:

```
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30,random_state=10)
```

5.  构建模型:

```
model = Sequential()
model.add(Dense(100, input_shape = (5,1), activation = 'relu'))
model.add((LSTM(100)))
model.add(Dense(1000,activation='relu'))
model.add(Dense(1,activation='linear'))
model.summary()
```

该模型的摘要如下:

![](Images/5800a442-ce90-4d0c-a004-b02a43688d92.png)

6.  编译模型，以便我们定义`loss`函数并调整学习率值:

```
from keras.optimizers import Adam
adam = Adam(lr=0.0001)
model.compile(optimizer=adam, loss='mean_squared_error')
```

7.  符合模型:

```
model.fit(X_train, y_train, epochs=400, batch_size=64, validation_data=(X_test, y_test), verbose = 1)
```

前面的结果是测试数据集上的均方误差值为 641 美元(每次预测的平均值约为 25 美元)。预测股价与实际股价的对比图如下:

```
pred = model.predict(X_test)

import matplotlib.pyplot as plt
%matplotlib inline
plt.figure(figsize=(20,10))
plt.plot(y_test,'r')
plt.plot(pred,'--')

plt.title('Variation of actual and predicted stock price')
plt.ylabel('Stock price')
```

预测价格和实际价格的变化如下:

![](Images/a2feff03-73e4-4623-93c1-b925100125da.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 陷阱

既然我们已经有了相当准确的预测，事实上，也是很好的预测，让我们深入了解为什么会有这么好的预测。

在我们的训练数据集中，既有很久以前的数据点，也有最近的数据点。这是一种漏损形式，因为在建立模型时，我们没有未来的股票价格。由于我们构建数据的方式，我们可以将 12 月 20 日的数据放在我们的训练数据集中，而将 12 月 19 日的数据放在测试数据集中。

让我们用由相应日期划分的训练和测试数据集来重建我们的模型:

```
X_train = x[:2100,:,:]
y_train = y[:2100]
X_test = x[2100:,:,:]
y_test = y[2100:]
```

我们在新测试数据集的*仅最近 5 天的股票价格*部分构建的模型的输出如下(测试数据集损失约 57，000):

![](Images/117796e9-8284-4993-b262-5909a8774f44.png)

请注意，与上一次迭代相比，现在得到的实际与预测股价图要差得多。但是，本节中生成的图表是比在*仅过去 5 天的股票价格*节中获得的图表更真实的情景数据。

现在我们已经获得了前面的图表，让我们通过检查股票价格数据随时间的变化图来理解该图表看起来是这样的原因，如下所示:

```
plt.plot(data2['Close'])
```

股价随时间变化的曲线图如下:

![](Images/74f903fe-d2e4-415a-b085-4a4c73cc32da.png)

请注意，股票价格在开始时增长缓慢，中间加速，最后减速。

该模型效果不佳，原因如下:

*   对历史上更早的预测和更近的预测的误差给予同等的重视
*   我们没有考虑减速的趋势

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 为不同的时间段分配不同的权重

我们了解到，我们将为最近的时间段分配较高的权重，为历史时间段分配较低的权重。

我们可以提出如下培训`weights`:

```
weights = np.arange(X_train.shape[0]).reshape((X_train.shape[0]),1)/2100
```

前面的代码将权重`0`分配给最近的历史数据点，将权重`1`分配给最近的数据点。所有中间数据点将具有在`0`和`1`之间的权重值。

现在我们已经定义了`weights`，让我们定义我们的自定义损失函数，它在计算平方误差损失时应用先前初始化的损失:

```
import numpy as np
from keras.layers import Dense, Input
from keras import Model
import keras.backend as K
from functools import partial

def custom_loss(y_true, y_pred, weights):
     return K.square(K.abs(y_true - y_pred) * weights)
cl = partial(custom_loss, weights=weights_tensor)
```

现在我们已经初始化了`weights`并定义了自定义损失函数，让我们使用函数式 API 向模型提供输入层和权重值(我们使用函数式 API，因为我们在训练模型时传递多个输入):

```
input_layer = Input(shape=(5,1))
weights_tensor = Input(shape=(1,))

i1 = Dense(100, activation='relu')(input_layer)
i2 = LSTM(100)(i1)
i3 = Dense(1000, activation='relu')(i2)
out = Dense(1, activation='linear')(i3)
model = Model([input_layer, weights_tensor], out)
```

现在我们已经定义了模型，该模型具有与*仅最近 5 天的股票价格*部分相同的参数，但是有一个额外的输入，即权重张量。让我们编译我们的模型:

```
from keras.optimizers import Adam
adam = Adam(lr=0.0001)
model = Model([input_layer, weights_tensor], out)
model.compile(adam, cl)
```

现在我们已经编译了模型，让我们来拟合它:

```
model.fit(x=[X_train, weights], y=y_train, epochs=300,batch_size = 32, validation_data = ([X_test, test_weights], y_test))
```

该模型在测试数据集上返回了 40，000 的平方误差损失，而不是来自*陷阱*部分的 57，000 的损失。让我们在测试数据集上绘制预测股票价格与实际股票价格的对比图:

![](Images/42c55eae-c592-4663-8ec3-eccb1898150c.png)

我们现在注意到，在最近的历史中，预测的和实际的股票价格之间存在相关性(图表的最右边部分)，而图表中间的峰值没有在预测中考虑。

在下一个食谱中，让我们看看新闻标题是否可以在中间加入尖峰。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 过去五天的股票价格加上新闻数据

在下面的代码中，我们将包含感兴趣的公司生成的标题的文本数据(从卫报网站提供的开源 API 中获取)以及最近五天的股票价格数据。然后，我们将耦合自定义损失函数，该函数将事件的新近性考虑在内:

1.  从这里导入卫报网站的头条新闻数据:[https://open-platform.theguardian.com/](https://open-platform.theguardian.com/)(注意，您必须申请自己的访问密钥才能从网站下载数据集)。下载标题和标题出现的相应日期，然后对日期进行预处理，以便将其转换为日期格式:

```
from bs4 import BeautifulSoup
from bs4 import BeautifulSoup
import urllib, json

dates = []
titles = []
for i in range(100):
 try:
        url = 'https://content.guardianapis.com/search?from-date=2010-01-01&section=business&page-          size=200&order-by=newest&page='+str(i+1)+'&q=amazon&api-key=0d7'
        response = urllib.request.urlopen(url)
        encoding = response.info().get_content_charset('utf8')
        data = json.loads(response.read().decode(encoding))
        print(i)
        for j in range(len(data['response']['results'])):
              dates.append(data['response']['results'][j]['webPublicationDate'])
              titles.append(data['response']['results'][j]['webTitle']) 
 except:
       break

import pandas as pd
data = pd.DataFrame(dates, titles)
data = data.reset_index()
data.columns = ['title','date']
data['date']=data['date'].str[:10]
data['date']=pd.to_datetime(data['date'], format = '%Y-%m-%d')
data = data.sort_values(by='date')
data_final = data.groupby('date').first().reset_index()
```

2.  通过`Date`连接历史价格数据集和文章标题数据集:

```
data2['Date'] = pd.to_datetime(data2['Date'],format='%Y-%m-%d')
data3 = pd.merge(data2,data_final, left_on = 'Date', right_on = 'date', how='left')
```

3.  预处理文本数据以删除停用词和标点符号，然后对文本输入进行编码，就像我们在[第 11 章](7a47ef1f-4c64-4f36-8672-6e589e513b16.xhtml)、*构建循环神经网络*的情感分类练习中所做的那样:

```
import nltk
import re
nltk.download('stopwords')
stop = nltk.corpus.stopwords.words('english')
def preprocess(text):
     text = str(text)
     text=text.lower()
     text=re.sub('[^0-9a-zA-Z]+',' ',text)
     words = text.split()
     words2=[w for w in words if (w not in stop)]
     words3=' '.join(words2)
     return(words3)
data3['title'] = data3['title'].apply(preprocess)
data3['title']=np.where(data3['title'].isnull(),'-','-'+data3['title'])
docs = data3['title'].values
from collections import Counter
counts = Counter()
for i,review in enumerate(docs):
     counts.update(review.split())
words = sorted(counts, key=counts.get, reverse=True)
vocab_size=len(words)
word_to_int = {word: i for i, word in enumerate(words, 1)}
encoded_docs = []
for doc in docs:
     encoded_docs.append([word_to_int[word] for word in doc.split()])
max_length = 20
from keras.preprocessing.sequence import pad_sequences
padded_docs = pad_sequences(encoded_docs, maxlen=max_length,padding='pre')
```

4.  将最近五天的股票价格和最近的标题(在股票价格预测日期之前)作为输入。让我们预处理数据以获得输入和输出值，然后准备训练和测试数据集:

在下面的代码中，x1 对应于历史股票价格，x2 对应于股票预测日期的文章标题:

```
x1 = []
x2 = []
y = []
for i in range(data3.shape[0]-5):
     x1.append(data3.loc[i:(i+4)]['Close'].values)
     x2.append(padded_docs[i+5])
     y.append(data3.loc[i+5]['Close'])

x1 = np.array(x1)
x2 = np.array(x2)
y = np.array(y)
x1 = x1.reshape(x1.shape[0],x1.shape[1],1)
X1_train = x1[:2100,:,:]
X2_train = x2[:2100,:]
y_train = y[:2100]

X1_test = x1[2100:,:,:]
X2_test = x2[2100:,:]
y_test = y[2100:]
```

5.  假设我们传递多个变量作为输入(历史股票价格、编码文本数据和权重值)，我们将使用 functional API 来构建模型:

```
input1 = Input(shape=(20,))
model = Embedding(input_dim=vocab_size+1, output_dim=32, input_length=20)(input1)
model = (LSTM(units=100))(model)
model = (Dense(1, activation='tanh'))(model)

input2 = Input(shape=(5,1))
model2 = Dense(100, activation='relu')(input2)
model2 = LSTM(units=100)(model2)
model2 = (Dense(1000, activation="relu"))(model2)
model2 = (Dense(1, activation="linear"))(model2)

from keras.layers import multiply
conc = multiply([model, model2])
conc2 = (Dense(1000, activation="relu"))(conc)
out = (Dense(1, activation="linear"))(conc2)
```

请注意，我们将股票价格模型和文本数据模型的输出值相乘，因为文本数据应该根据历史股票价格模型的输出进行调整:

```
model = Model([input1, input2, weights_tensor], out)
```

上述模型的架构如下:

![](Images/95795ad5-7d62-4cf3-a38d-2cc879bb6826.png)

6.  定义损失函数并编译模型:

```
def custom_loss(y_true, y_pred, weights):
 return K.square(K.abs(y_true - y_pred) * weights)
cl = partial(custom_loss, weights=weights_tensor)

model = Model([input1, input2, weights_tensor], out)
model.compile(adam, cl)
```

7.  符合模型:

```
model.fit(x=[X2_train, X1_train, weights], y=y_train, epochs=300,batch_size = 32, validation_data = ([X2_test, X1_test, test_weights], y_test))
```

8.  在测试数据集中绘制股票价格的实际值与预测值:

```
pred = model.predict([X2_test, X1_test, test_weights])

import matplotlib.pyplot as plt
%matplotlib inline
plt.figure(figsize=(20,10))
plt.plot(y_test,'r',label='actual')
plt.plot(pred,'--', label = 'predicted')
plt.title('Variation of actual and predicted stock price')
plt.ylabel('Stock price')
plt.legend()
```

实际和预测股票价格的变化如下:

![](Images/a804018e-6956-4350-9d12-2428053d76ac.png)

注意，在该迭代中，中间部分与非文本数据版本相比具有稍好的斜率，并且与前一次迭代的 40，000 值相比，在 35，000 处具有稍低的平方误差。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 还有更多...

正如本食谱开头所提到的，预测股票价格时要非常小心，因为有各种各样的因素会影响股票价格的变动，在进行预测时需要将所有这些因素都考虑在内。

此外，您还应该注意到，虽然实际值和预测值看起来相关，但与实际值行相比，预测值行有一点延迟。这种延迟会在很大程度上改变最优策略，从买入决策变成卖出决策。因此，股票价格从前一个日期开始显著变化的行应该有更大的权重——这进一步使我们的损失函数变得复杂。

在进行预测时，我们还可以潜在地纳入更多的信息来源，如额外的新闻标题和季节性(例如:某些股票通常在假期表现良好)以及其他宏观经济因素。

最后，我们可以缩放数据集，使神经网络的输入不是一个巨大的数字。