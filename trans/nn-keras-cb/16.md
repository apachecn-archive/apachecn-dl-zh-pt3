        

# 十六、强化学习

在前面的章节中，我们学习了将输入映射到目标——其中提供了输入和输出值。在本章中，我们将学习强化学习，强化学习提供了我们想要达到的目标和我们操作的环境，但没有任何输入或输出映射。强化学习的工作方式是，我们通过在开始时采取随机行动，并逐渐从生成的输入数据(状态中的行动)和输出值(采取特定行动获得的奖励)中学习，生成输入值(主体所处的状态)和相应的输出值(主体在状态中采取特定行动获得的奖励)。

在本章中，我们将介绍以下内容:

*   在有非负奖励的模拟博弈中采取的最优行动
*   模拟游戏中某一状态下采取的最佳行动
*   学习在玩冰封湖时获得最大回报
*   深度 Q-学习平衡推车杆
*   深度 Q-学习玩太空入侵者游戏

        

# 在有非负奖励的模拟博弈中采取的最优行动

在本节中，我们将了解在模拟游戏中采取正确行动的方式。注意，这个练习将主要帮助你掌握强化学习是如何工作的。

        

# 做好准备

让我们定义一下我们在这个模拟环境中操作的环境。

你有三个盒子，两个玩家在上面玩游戏。玩家 1 用 1 标记一个盒子，玩家 2 用 2 标记一个盒子。能够连续标记两个盒子的玩家获胜。

该游戏的空白棋盘如下所示:

![](Images/a6def8c7-0658-4359-b45a-8cae86613db7.png)

对于我们刚刚定义的问题，只有 1 号玩家有机会赢得游戏。一号玩家获胜的可能情况如下:

![](Images/129d43fc-ab45-4bc1-acc2-63b404e4cd47.png)

从问题设置来看，1 号玩家获胜的直观方式是 1 号玩家选择中间的盒子。这样，不管玩家 2 选择了哪个盒子，玩家 1 都会在下一步棋中获胜。

虽然玩家 1 的第一步对我们来说很直观，但在下一节中，我们将了解代理如何自动计算出最佳的第一步。

我们将采取以下策略来解决这个问题:

*   我们初始化一个空板
*   一号玩家随机选择一个盒子
*   玩家 2 从剩余的 2 个盒子中随机选择一个盒子
*   根据玩家 1 剩下的盒子，我们更新玩家 1 的奖励:
    *   如果玩家 1 能够在两个连续的方块中放置 1，他就是赢家，并将获得 1 的奖励
    *   否则，玩家 1 将获得 0 的奖励
*   重复前面的练习 100 次，游戏开始，我们为给定的移动顺序存储奖励
*   现在，我们将继续计算所采取的各种第一步行动的平均回报
*   在第一步中选择的盒子，在 100 次迭代中有最高的平均回报，是参与人 1 的最佳第一步

        

# 怎么做...

上面定义的策略编码如下(代码文件在 GitHub 中以`Finding_optimal_policy.ipynb`的形式提供):

1.  定义游戏环境和玩游戏的功能:

```
def play_game():
     empty_board = [0,0,0]
     move = []
     for step in range(3):
         index_to_choose = [i for i,j in enumerate(empty_board) if j==0]
         samp = random.sample(range(len(index_to_choose)), 1)[0] 
         if(step%2==0):
             empty_board[index_to_choose[samp]]=1
             move.append(index_to_choose[samp])
         else:
             empty_board[index_to_choose[samp]]=2 
     return(reward(empty_board), move[0])
```

在前面的代码中，我们用零值初始化一个空棋盘，并玩一个名为`samp`的随机移动。玩家 1 先行动，然后轮到玩家 2，接着是玩家 1。我们用这种方式填满空白的板子。

2.  定义一个函数来计算游戏结束时的奖励:

```
def reward(empty_board):
     reward = 0
     if((empty_board[0]==1 & empty_board[1]==1) | (empty_board[1]==1 & empty_board[2]==1)):
         reward = 1
     else:
         reward = 0
     return reward
```

3.  玩游戏`100`次:

```
rew = []
step = []
for i in range(100):
     r, move = play_game()
     rew.append(r)
     step.append(move) 
```

4.  计算选择某个第一步的奖励:

```
sub_list = [i for i,j in enumerate(step) if j==1]
final_reward = 0
count = 0
for i in sub_list:
     final_reward += rew[i]
     count+=1
final_reward/count
```

当你对第一步的多个选项重复前面的代码时，你会注意到占据第二个方块时的平均奖励最高。

        

# 模拟游戏中某一状态下采取的最佳行动

在前面的场景中，我们考虑了一个简单的例子，当目标实现时会有奖励。在这种情况下，我们也会因为负面奖励而使游戏变得复杂。然而，目标仍然是一样的:在给定的问题设置中最大化奖励，其中环境既有积极的奖励也有消极的奖励。

        

# 做好准备

我们工作的环境如下:

![](Images/77011aa8-e3e7-41cc-9bf4-452707112100.png)

我们从带有 **S** 的单元格开始，我们的目标是到达奖励为 **+1** 的单元格。为了最大化获得奖励的机会，我们将使用贝尔曼方程，该方程计算前面网格中每个单元格的值，如下所示:

*当前单元格的值=从当前单元格移动到下一单元格的奖励+折扣因子*下一单元格的值*

另外，在当前问题中，移动到奖励为 **+1** 的单元格以外的任何单元格的奖励为 *0* 。

折扣因子可以认为是从一个单元移动到另一个单元所消耗的能量。因此，与当前问题设置中的其他单元相比，远离奖励单元的单元将具有较低的值。

一旦我们计算了每个单元的值，我们就移动到代理可以移动到的所有单元中具有最高值的单元。

我们将采用以下策略来计算每个单元格的值:

*   初始化一个空板。
*   定义代理在单元中可能采取的操作。
*   为代理在当前单元格中采取的操作定义代理将处于的状态。
*   计算当前状态的值，这取决于移动到下一个状态的奖励，以及下一个状态的值。
*   基于先前的计算更新当前状态的单元值。
*   此外，存储在当前状态下采取的行动，以转移到下一个状态。
*   注意，在初始迭代中，远离最终目标的单元的值保持为零，而靠近最终状态的单元的值上升。
*   当我们多次重复前面的步骤时，我们将能够更新单元值，从而能够决定代理要遵循的最佳路线。

        

# 怎么做...

在这一节中，我们将对上一节中的策略进行编码(代码文件在 GitHub 中以`Finding_optimal_policy.ipynb`的形式提供):

1.  初始化空白板:

```
empty_board = [[0,0,0]
             ,[0,0,1]]
```

2.  定义在不同状态下可以采取的动作——其中`D`代表向下，`R`代表向右，`L`代表向左，`U`代表向上:

```
state_actions = {(0,0):('D','R')
                 ,(0,1):('D','R','L')
                 ,(0,2):('D','L')
                 ,(1,0):('U','R')
                 ,(1,1):('L','U','R') 
                 }
```

3.  根据当前状态和在当前状态下采取的行动，定义提取下一个状态的函数:

```
def get_next_state(curr_state, action):
     i,j = curr_state
     if action=='D':
         i = i+1
     elif action=='U':
         i = i-1
     elif action=='R':
         j = j+1
     elif action=='L':
         j = j-1
     else:
         print('unk')
     return((i,j))
```

4.  初始化附加了状态、动作和奖励的列表:

```
curr_state = (0,0)
state_action_reward = []
state = []
state_action = []
```

5.  在一个单元(状态)中采取随机行动的一集(一集是游戏的一个实例)中最多执行 100 个行动，并根据移动到下一个状态的奖励以及下一个状态的值计算当前状态的值。

重复上述练习进行`100`次迭代(剧集/游戏),并计算每个单元格的值:

```
for m in range(100):
     curr_state = (0,0)
     for k in range(100):
         reward = 0
         action = state_actions[curr_state][random.sample(range(len(state_actions[curr_state])),1)[0]]
         next_state = get_next_state(curr_state, action)
```

在前面的代码中，我们在一个状态中采取随机操作，然后为在当前状态中采取的操作计算下一个状态:

```
        state.append(curr_state)
        empty_board[curr_state[0]][curr_state[1]] = reward + empty_board[next_state[0]][next_state[1]]*0.9 
        empty_board[curr_state[0]][curr_state[1]])
```

在前面的代码中，我们正在更新状态的值:

```
        curr_state = next_state
        state_action.append(action)

        if(next_state==(1,2)):
             reward+=1
             break
```

上述结果会在所有单元格中产生以下最终单元格状态值:

![](Images/695f027e-2888-4b08-b8ca-dba64348fc4e.png)

基于前面的输出，代理可以在游戏开始时采取向右动作或向下动作(代理从左上角开始)。但是，如果代理在第一步中采取了向下动作，最好在下一步中采取*r*right 动作，因为与当前单元状态之上的状态相比，右边的状态的单元状态值更高。

        

# 还有更多...

想象环境(细胞和它们相应的奖励)看起来如下:

![](Images/a7b18107-739a-4819-b47b-2a8998b62a92.png)

可在不同州采取的行动如下:

```
state_actions = {(0,0):('D','R')
                 ,(0,1):('D','R')
                 ,(1,0):('R')
                 ,(1,1):('R') 
                 }
```

多次迭代游戏后，各个单元格中的单元格状态值如下:

![](Images/96dfeacf-52d9-47f3-928c-275ff5b32948.png)

从前面的结果中，我们可以看到代理从左上角向下采取操作比向右移动更好，因为起始单元下面的单元状态的单元状态值更高。

        

# 学习在玩冰封湖时获得最大回报

到目前为止，在前面的章节中，我们已经在给定的状态下采取了随机的行动。此外，我们还定义了环境，并通过代码计算下一个状态、动作和移动的回报。在这一节中，我们将利用 OpenAI 的健身房包在冰冻的湖泊环境中导航。

        

# 做好准备

冰湖环境看起来如下:

![](Images/2f4064cc-944d-4d94-ab94-0a5c8b963098.png)

代理从 **S** 状态开始，目标是通过尽可能避开 **H** 状态到达 **G** 状态。

在前面的环境中，代理有 16 种可能的状态。此外，代理可以采取四种可能的动作(向上、向下、向右或向左)。

我们将定义一个 q 表，其中有 16 行对应于 16 种状态，4 列对应于在每种状态下可以采取的四个动作。

在上一节中，我们了解到:

*在一个状态下采取的行动的价值=奖励+折扣因子*在下一个状态下采取的最佳可能行动的价值*

我们将修改前面的公式如下:

*在一个状态中采取的行动的价值=在一个状态中采取的行动的价值+ 1*(奖励+折扣因子*在下一个状态中采取的最佳可能行动的价值-在一个状态中采取的行动的价值)*

最后，我们将把 1 替换为学习率，这样一个状态中一个动作的值更新就不会发生剧烈的变化。这类似于神经网络中具有学习率的效果。

*在一个状态中采取的行动的价值=在一个状态中采取的行动的价值+学习率*(奖励+折扣因子*在下一个状态中采取的最佳可能行动的价值-在一个状态中采取的行动的价值)*

根据前面的内容，我们现在可以更新 q 表，以便我们可以确定在不同状态下可以采取的最佳行动。

我们将采用以下策略来解决本案例研究:

*   在 OpenAI 的健身房注册环境
*   初始化形状为 16×4 的零数组 q 表
*   在给定状态下选择行动时，采用探索对开发的方法:
    *   到目前为止，我们只是探讨了在给定状态下随机选择一个动作时可能的整体动作。
    *   在这一节中，我们将探索最初的迭代，因为我们不确定在游戏的最初几集采取的最佳行动。
    *   然而，随着我们对这个游戏了解的越来越多，我们在采取随机行动(随着剧集数量的增加，频率降低)的同时，利用了我们在可能采取的行动方面所学到的知识。
*   在某一集里:
    *   根据我们是尝试、探索还是利用来选择行动
    *   确定新的状态和奖励，并通过采取上一步中选择的行动来检查游戏是否结束
    *   初始化学习率参数和折扣因子
    *   通过使用前面讨论的公式，更新在 q 表的状态中采取前面的动作的值
    *   重复前面的步骤，直到游戏结束
*   此外，对 1，000 个不同的游戏重复上述步骤
*   检查 q 表以确定在给定状态下采取的最佳行动
*   根据 q 表绘制代理在某个状态下采取行动的路径

        

# 怎么做...

在本节中，我们将对我们之前讨论的策略进行编码(代码文件在 GitHub 中以`Frozen_Lake_with_Q_Learning.ipynb`的形式提供):

1.  导入相关包:

```
import gym
from gym import envs
from gym.envs.registration import register
```

Gym 是一个开发和比较强化学习算法的工具包。它支持教导代理从走路到玩游戏(如乒乓球和弹球)的一切。

更多关于健身房的信息，请访问:[https://gym.openai.com/](https://gym.openai.com/)。

2.  注册环境:

```
register(
 id = 'FrozenLakeNotSlippery-v1',
 entry_point = 'gym.envs.toy_text:FrozenLakeEnv',
 kwargs = {'map_name': '4x4', 'is_slippery':False},
 max_episode_steps = 100,
 reward_threshold = 0.8196)
```

3.  创造环境:

```
env = gym.make('FrozenLakeNotSlippery-v1')
```

4.  检查创建的环境:

```
env.render()
```

![](Images/eb411224-67a2-44b0-921e-610d1061e23a.png)

前面的步骤渲染(打印)了环境:

```
env.observation_space
```

前面的代码提供了环境中状态操作对的数量。在我们的例子中，假设它是一个 4 x 4 的网格，我们总共有 16 个状态。因此，我们总共有 16 个观察值。

```
env.action_space.n
```

前面的代码定义了在环境中的某个状态下可以采取的操作数量:

```
env.action_space.sample()
```

上述代码从一组可能的操作中抽取一个操作:

```
env.step(action)
```

前面的代码执行操作并生成新的状态和操作的奖励，标记游戏是否完成，并为该步骤提供附加信息:

```
env.reset()
```

前面的代码重置环境，以便代理返回到启动状态。

5.  初始化 q 表:

```
import numpy as np
qtable = np.zeros((16,4))
```

我们将其初始化为(16，4)的形状，因为有 16 个状态，每个状态下有 4 个可能的动作。

6.  运行玩游戏的多次迭代:

初始化超参数:

```
total_episodes=15000
learning_rate=0.8
max_steps=99
gamma=0.95
epsilon=1
max_epsilon=1
min_epsilon=0.01
decay_rate=0.005
```

播放多集游戏:

```
rewards=[]
for episode in range(total_episodes):
    state=env.reset()
    step=0
    done=False
    total_rewards=0
```

在下面的代码中，我们定义了要采取的动作。如果`eps`(0 到 1 之间产生的随机数)小于 0.5，我们探索；否则，我们利用(考虑 q 表中的最佳行动)

```
    for step in range(max_steps):
        exp_exp_tradeoff=random.uniform(0,1)        
        ## Exploitation:
        if exp_exp_tradeoff>epsilon:
            action=np.argmax(qtable[state,:])
        else:
            ## Exploration
            action=env.action_space.sample()
```

在下面的代码中，我们获取新的状态和奖励，并通过采取给定步骤中的动作来标记游戏是否完成:

```
        new_state, reward, done, _ = env.step(action)
```

在下面的代码中，我们基于在一个状态中采取的动作来更新 q 表。此外，我们还使用在当前状态下采取行动后获得的新状态来更新状态:

```
        qtable[state,action]=qtable[state,action]+learning_rate*(reward+gamma*np.max(qtable[new_state,:])-qtable[state,action])
        total_rewards+=reward
        state=new_state
```

在下面的代码中，随着游戏的结束，我们进入游戏的新一集。然而，我们确保随机系数(`eps`)得到更新，该系数用于决定我们是去勘探还是去开采。

```
        if(done):
             break
        epsilon=min_epsilon+(max_epsilon-min_epsilon)*np.exp(decay_rate*episode)
        rewards.append(total_rewards)
```

7.  一旦我们建立了 q 表，我们现在部署代理按照 q 表建议的最优动作进行机动:

```
env.reset()

for episode in range(1):
    state=env.reset()
    step=0
    done=False
    print("-----------------------")
    print("Episode",episode)
    for step in range(max_steps):
        env.render()
        action=np.argmax(qtable[state,:])
        print(action)
        new_state,reward,done,info=env.step(action)

        if done:
            #env.render()
            print("Number of Steps",step+1)
            break
        state=new_state
```

前面给出了代理为达到最终目标而必须经过的最佳路径。

        

# 深度 Q-学习平衡推车杆

在前面的章节中，我们学习了基于 q 表值采取行动。然而，达到最佳值是耗时的，因为代理将不得不玩多次来达到最佳 q 表。

在本节中，我们将学习如何使用神经网络，以便我们可以比使用 Q-learning 更快地获得最佳值。

        

# 做好准备

在本练习中，我们将注册 cart-pole 环境，其中可能的操作是向右或向左移动，以便平衡杆子。此外，小车位置、小车速度、极角和尖端的极速度是我们所拥有的关于状态的信息。

这个游戏的规则可以在这里找到:[https://gym.openai.com/envs/CartPole-v1/](https://gym.openai.com/envs/CartPole-v1/)。

一根杆子通过一个非驱动关节连接到一辆小车上，小车沿着一条无摩擦的轨道移动。通过对推车施加+1 或-1 的力来控制该系统。钟摆开始直立，目标是防止它翻倒。柱子保持直立的每个时间步长提供+1 的奖励。当柱子偏离垂直方向超过 15 度，或者手推车偏离中心超过 2.4 个单位时，该集结束。

为了平衡车杆，我们将采用与上一节相同的策略。然而，深度 q 学习的不同之处在于，我们将使用神经网络来帮助我们预测代理需要采取的最佳行动。

我们训练神经网络的方式如下:

*   我们将存储有关状态值、采取的行动和获得的奖励的信息:
    *   如果游戏没有结束，奖励为 1，否则为 0。
*   最初，模型基于随机初始化的权重进行预测，其中模型的输出层有两个节点，对应于两个可能动作的新状态值。
*   新状态值将基于使新状态值最大化的动作
*   如果游戏没有结束，我们用奖励的总和以及新状态的最大状态值和折扣因子的乘积来更新当前状态的值。
*   现在，我们将从之前获得的更新后的当前状态值中覆盖操作的值:
    *   如果当前步骤中采取的动作是错误的(即游戏结束)，则当前状态中动作的值将为 0。
    *   否则，当前步骤中的目标值是正数。
    *   通过这种方式，我们让模型计算出要采取的正确行动。
    *   此外，我们可以认为这是一种在奖励为零的情况下指明行为是错误的方式。然而，考虑到当奖励为 1 时我们不确定它是否是正确的行为，我们将只为我们采取的行为更新它，并保持新状态的值(如果我们采取另一个行为)不变。
*   我们将状态值附加到输入数组中，并将在当前状态下采取一个或另一个动作的值作为输出数组。
*   我们拟合了使前面数据点的均方误差最小化的模型。
*   最后，我们会在越来越多的剧集中减少探索。

        

# 怎么做...

我们将我们之前讨论的策略编码如下(代码文件在 GitHub 中以`Deep_Q_learning_to_balance_a_cart_pole.ipynb`的形式提供):

1.  创建环境并将动作大小和状态大小存储在变量中:

```
import gym 
env = gym.make('CartPole-v0') 
state_size = env.observation_space.shape[0] 
action_size = env.action_space.n
```

推车杆环境如下所示:

![](Images/2a3a72ea-30b2-4b46-b545-908bc873ef5a.png)

2.  导入相关包:

```
import numpy as np
import random
from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from collections import deque
```

3.  定义模型:

```
model=Sequential()
model.add(Dense(24,input_dim=state_size,activation='relu'))
model.add(Dense(24,activation='relu'))
model.add(Dense(2,activation='linear'))
model.compile(loss='mse',optimizer=Adam(lr=0.01))
```

4.  定义需要追加的列表:

```
memory = deque(maxlen=2000)
gamma = 0.95 # discount rate
epsilon = 1.0 # exploration rate
epsilon_min = 0.01
epsilon_decay = 0.995
done = False
batch_size=32
```

5.  定义一个重放游戏的函数:

```
def replay(model, batch_size,epsilon):
    epsilon_min = 0.01
    epsilon_decay = 0.995
    minibatch = random.sample(memory, batch_size)
    for state, action, reward, next_state, done in minibatch:
        target = reward
        if not done:
            target = (reward + gamma *np.amax(model.predict(next_state)[0]))
        new_action_value = model.predict(state)
        new_action_value[0][action] = target
        model.fit(state,new_action_value, epochs=1, verbose=0)
    if epsilon > epsilon_min:
        epsilon *= epsilon_decay
    return model,epsilon
```

在前面的代码中，我们定义了一个函数，该函数采用神经网络模型、批量大小和 epsilon(表示我们是探索还是利用的参数)。我们正在获取一个`batch_size`大小的随机样本。请注意，您将在下一步学习记忆结构(包括状态、动作、奖励和`next_state`)。如果游戏没有完成，我们会更新采取行动的奖励；否则，目标将为 0(因为游戏结束时奖励将为 0)。

此外，该模型预测采取某个行动的价值(因为该模型在输出中有 2 个节点，其中每个节点提供采取一个行动而不是另一个行动的输出)。该函数返回更新的模型和勘探/开采系数(ε)。

6.  在多集上玩游戏，并附加代理获得的分数。此外，确保代理采取的行动由基于ε值的模型指示:

```
episodes=200
maxsteps=200
score_list = []
for e in range(episodes):
    state = env.reset()
    state = np.reshape(state, [1, state_size])
```

在前面的代码中，我们总共播放了 200 集，并在这一集的开始重置了环境。此外，我们正在重塑状态，以便可以将其传递给神经网络模型:

```
    for step in range(maxsteps):
        if np.random.rand()<=epsilon:
            action=env.action_space.sample()
        else:
            action = np.argmax(model.predict(state)[0])
```

在前面的步骤中，我们基于探索参数(ε)采取行动，其中我们在某些情况下采取随机行动(`env.actionspace.sample()`)，并在其他情况下利用模型的预测:

```
        next_state, reward, done, _ = env.step(action)
        reward = reward if not done else -10
        next_state = np.reshape(next_state, [1, state_size])
        memory.append((state, action, reward, next_state, done))
```

在前面的步骤中，我们正在执行一个动作，并提取下一个状态、奖励和关于游戏是否结束的信息。此外，如果游戏结束，我们会用-10 覆盖奖励值(这意味着代理人走了一步错误的棋)。此外，我们提取下一个状态并将其添加到内存中。这样，我们就为要训练的模型创建了一个数据集，其中模型采用当前状态和奖励来计算两个可能操作之一的奖励:

```
        state = next_state
        if done:
          print("episode: {}/{}, score: {}, exp prob: {:.2}".format(e, episodes, step, epsilon))
          score_list.append(step)
          break
        if len(memory) > batch_size:
          model,epsilon=replay(model, batch_size,epsilon)
```

在前面的代码中，如果游戏结束了，我们追加分数(游戏过程中走的步数)；否则，我们更新模型。此外，只有当内存中的数据点数量达到预定义的批量时，我们才会更新模型。

绘制递增时期的分数如下所示:

![](Images/c8b05e8e-9ac2-4b2c-94cc-ee1ca482baad.png)

        

# 深度 Q-学习玩太空入侵者游戏

在上一节中，我们使用深度 Q 学习来玩推车杆游戏。在本节中，我们将利用深度 Q 学习来玩太空入侵者，这是一个比 Cart-Pole 更复杂的环境。

《太空入侵者》游戏的截图如下:

![](Images/61dfcc9b-3271-482d-aef3-0eaebddf9a93.png)

资料来源:https://gym.openai.com/envs/SpaceInvaders-v0/

这项练习的目的是最大限度地提高单次游戏的得分。

        

# 做好准备

我们将采用以下策略来构建一个能够最大化得分的代理:

*   初始化*太空入侵者-Atari2600* 游戏的环境。
*   预处理图像帧:
    *   移除不一定影响动作预测的像素
        *   例如，低于玩家位置的像素
    *   归一化输入图像。
    *   在将图像传递给神经网络模型之前调整其大小
*   根据健身房环境的要求堆叠框架
*   让代理在多个情节中玩游戏:
    *   在最初的情节中，我们会有高探索，随着情节的增加而衰减。
    *   在一个状态下需要采取的动作取决于探索系数的值。
    *   将游戏状态和在某个状态下采取的行动的相应奖励存储在内存中。
    *   根据前几集获得的奖励更新模型。

        

# 怎么做...

我们前面讨论的策略编码如下:

1.  下载包含太空入侵者游戏的 ROM 并安装`retro`包:

```
$ wget http://www.atarimania.com/roms/Roms.rar && unrar x Roms.rar && unzip Roms/ROMS.zip
$ pip3 install gym-retro
$ python3 -m retro.import ROMS/
```

2.  创建环境并提取观察空间:

```
env=retro.make(game='SpaceInvaders-Atari2600')
env.observation_space
# Box(210,160,3)
```

3.  构建一个预处理帧的函数(图片/太空入侵者游戏截图):

```
def preprocess_frame(frame):
     # Greyscale frame 
     gray = rgb2gray(frame)
     # Crop the screen (remove the part below the player)
     # [Up: Down, Left: right]
     cropped_frame = gray[8:-12,4:-12]
     # Normalize Pixel Values
     normalized_frame = cropped_frame/255.0
     # Resize
     preprocessed_frame = transform.resize(normalized_frame, [110,84])
     return preprocessed_frame 
```

4.  构建一个在给定状态下堆叠帧的函数:

```
stack_size = 4 # We stack 4 frames
# Initialize deque with zero-images one array for each image
stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4)
def stack_frames(stacked_frames, state, is_new_episode):
     # Preprocess frame
     frame = preprocess_frame(state) 
     if is_new_episode:
         # Clear our stacked_frames
         stacked_frames = deque([np.zeros((110,84), dtype=np.int) for i in range(stack_size)], maxlen=4) 
         # Because we're in a new episode, copy the same frame 4x
         stacked_frames.append(frame)
         stacked_frames.append(frame)
         stacked_frames.append(frame)
         stacked_frames.append(frame) 
         # Stack the frames
         stacked_state = np.stack(stacked_frames, axis=2) 
     else:
         # Append frame to deque, automatically removes the oldest frame
         stacked_frames.append(frame)
         # Build the stacked state (first dimension specifies different frames)
         stacked_state = np.stack(stacked_frames, axis=2) 
     return stacked_state, stacked_frames
```

5.  初始化模型超参数:

```
### MODEL HYPERPARAMETERS
state_size = [110, 84, 4] # Our input is a stack of 4 frames hence 110x84x4 (Width, height, channels) 
action_size = env.action_space.n # 8 possible actions
learning_rate = 0.00025 # Alpha (aka learning rate)
```

```
### TRAINING HYPERPARAMETERS
total_episodes = 50 # Total episodes for training
max_steps = 50000 # Max possible steps in an episode
batch_size = 32 # Batch size
```

```
# Exploration parameters for epsilon greedy strategy
explore_start = 1.0 # exploration probability at start
explore_stop = 0.01 # minimum exploration probability 
decay_rate = 0.00001 # exponential decay rate for exploration prob
```

```
# Q learning hyperparameters
gamma = 0.9 # Discounting rate
```

```
### MEMORY HYPERPARAMETERS
pretrain_length = batch_size # Number of experiences stored in the Memory when initialized for the first time
memory_size = 1000000 # Number of experiences the Memory can keep
```

```
### PREPROCESSING HYPERPARAMETERS
stack_size = 4 # Number of frames stacked
```

```
### MODIFY THIS TO FALSE IF YOU JUST WANT TO SEE THE TRAINED AGENT
training = False
```

```
## TURN THIS TO TRUE IF YOU WANT TO RENDER THE ENVIRONMENT
episode_render = False
```

6.  构建一个从总内存中采样数据的函数:

```
memory = deque(maxlen=100000)
```

```
def sample(memory, batch_size):
     buffer_size = len(memory)
     index = np.random.choice(np.arange(buffer_size),
     size = batch_size,
     replace = False) 
     return [memory[i] for i in index]
```

7.  构建一个返回代理需要采取的操作的函数:

```
def predict_action(model,explore_start, explore_stop, decay_rate, decay_step, state, actions):
     exp_exp_tradeoff = np.random.rand()
     explore_probability = explore_stop + (explore_start - explore_stop) * np.exp(-decay_rate * decay_step)
     if (explore_probability > exp_exp_tradeoff):
         choice = random.randint(1,len(possible_actions))-1
         action = possible_actions[choice]
     else:
         Qs = model.predict(state.reshape((1, *state.shape)))
         choice = np.argmax(Qs)
         action = possible_actions[choice]
     return action, explore_probability
```

8.  构建一个微调模型的函数:

```
def replay(agent,batch_size,memory):
     minibatch = sample(memory,batch_size)
     for state, action, reward, next_state, done in minibatch:
     target = reward
     if not done:
         target = reward + gamma*np.max(agent.predict(next_state.reshape((1,*next_state.shape)))[0])
     target_f = agent.predict(state.reshape((1,*state.shape)))
     target_f[0][action] = target
     agent.fit(state.reshape((1,*state.shape)), target_f, epochs=1, verbose=0)
 return agent
```

9.  定义神经网络模型:

```
def DQNetwork():
     model=Sequential()
     model.add(Convolution2D(32,input_shape=(110,84,4),kernel_size=8, strides=4, padding='valid',activation='elu'))
     model.add(Convolution2D(64, kernel_size=4, strides=2, padding='valid',activation='elu'))
     model.add(Convolution2D(128, kernel_size=3, strides=2, padding='valid',activation='elu'))
     model.add(Flatten())
     model.add(Dense(units=512))
     model.add(Dense(units=3,activation='softmax'))
     model.compile(optimizer=Adam(0.01),loss='mse')
     return model
```

模型总结如下:

![](Images/4a623396-afa7-491b-96da-6853a83906e6.jpg)

10.  循环播放多集，并在更新模型的同时继续玩游戏:

```
agent = DQNetwork()
agent.summary()
rewards_list=[]
Episodes=200
# Iterate the game
for episode in range(Episodes):
     # reset state in the beginning of each game
     step = 0
     decay_step = 0
     episode_rewards = []
     state = env.reset()
     state, stacked_frames = stack_frames(stacked_frames, state, True)
     while step < max_steps:
         step += 1
         decay_step +=1
         # Predict the action to take and take it
         action, explore_probability = predict_action(agent,explore_start, explore_stop, decay_rate, decay_step, state, possible_actions)
 #Perform the action and get the next_state, reward, and done information
         next_state, reward, done, _ = env.step(action)
         # Add the reward to total reward
         episode_rewards.append(reward)
     if done:
 # The episode ends so no next state
         next_state = np.zeros((110,84), dtype=np.int)
         next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)
 # Set step = max_steps to end the episode
         step = max_steps
 # Get the total reward of the episode
         total_reward = np.sum(episode_rewards)
         print('Episode:{}/{} Score:{} Explore Prob:{}'.format(episode,Episodes,total_reward,explore_probability))
         rewards_list.append((episode, total_reward))
 # Store transition <st,at,rt+1,st+1> in memory D
         memory.append((state, action, reward, next_state, done))
     else:
 # Stack the frame of the next_state
         next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)
 # Add experience to memory
         memory.append((state, action, reward, next_state, done))
 # st+1 is now our current state
         state = next_state
     env.render() 
 # train the agent with the experience of the episode
 agent=replay(agent,batch_size,memory)
```

11.  绘制在增加的剧集中获得的奖励:

```
score=[]
episode=[]
for e,r in rewards_list:
     episode.append(e)
     score.append(r)
import matplotlib.pyplot as plt
plt.plot(episode,score)
```

![](Images/3f3345e3-c0b6-49bc-9009-b2f000a06908.png)

由此可见，模特在某些剧集中已经学会了 800 分以上。