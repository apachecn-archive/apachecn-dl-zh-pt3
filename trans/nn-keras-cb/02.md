        

# 构建深度前馈神经网络

在本章中，我们将介绍以下配方:

*   训练一个普通的神经网络
*   缩放输入数据集
*   当大多数输入大于零时培训的影响
*   批量大小对模型准确性的影响
*   构建深度神经网络，提高网络精度
*   改变学习速率以提高网络精度
*   改变损耗优化器以提高网络精度
*   了解过度拟合的情况
*   使用批量标准化加速训练过程

在前一章中，我们看了神经网络功能的基础。我们还了解到，有各种超参数会影响神经网络的准确性。在本章中，我们将详细介绍神经网络中各种超参数的功能。

本章的所有代码可从 https://github . com/Kishore-ayyadevara/Neural-Networks-with-Keras-Cookbook/blob/master/Neural _ network _ hyper _ parameters . ipynb 获得

        

# 训练一个普通的神经网络

为了理解如何训练一个普通的神经网络，我们将经历预测 MNIST 数据集中一个数字的标签的任务，该数据集中是一个流行的数字图像(每个图像一个数字)和图像中包含的数字的相应标签的数据集。

        

# 做好准备

训练神经网络按以下步骤完成:

1.  导入相关的包和数据集
2.  预处理目标(将它们转换为独热编码向量)，以便我们可以在它们之上执行优化:
    *   我们将最小化分类交叉熵损失
3.  创建训练和测试数据集:
    *   我们有训练数据集，因此我们可以基于它创建一个模型
    *   模型看不到测试数据集:
        *   因此，测试数据集的准确性是模型生产化时模型处理数据的能力的指标，因为模型看不到生产场景中的数据(可能在构建模型后几天/几周出现)
4.  初始化模型
5.  定义模型架构:
    *   指定隐藏层中的单位数
    *   指定要在隐藏层中执行的激活功能
    *   指定隐藏层的数量
    *   指定我们想要最小化的损失函数
    *   提供将损失函数最小化的优化器
6.  符合模型:
    *   提及批量大小以更新重量
    *   提及纪元的总数
7.  测试模型:
    *   提及验证数据，否则，提及验证分割，它会将总数据的最后 x%视为测试数据
    *   根据测试数据集计算准确度和损失值
8.  检查损失值和精度值在越来越多的时期内发生变化的方式是否有什么有趣的地方

使用这个策略，让我们在下一节中继续在 Keras 中构建一个神经网络模型。

        

# 怎么做...

1.  导入相关包和数据集，并可视化输入数据集:

```
from keras.datasets import mnist
import numpy
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.utils import np_utils
(X_train, y_train), (X_test, y_test) = mnist.load_data()
```

在前面的代码中，我们导入了相关的 Keras 文件，还导入了 MNIST 数据集(作为 Keras 中的内置数据集提供)。

2.  MNIST 数据集包含数字图像，其中图像的形状为 28 x 28。让我们绘制一些图像，看看它们在代码中会是什么样子:

```
import matplotlib.pyplot as plt
%matplotlib inline
plt.subplot(221)
plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))
plt.grid('off')
plt.subplot(222)
plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))
plt.grid('off')
plt.subplot(223)
plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))
plt.grid('off')
plt.subplot(224)
plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))
plt.grid('off')
plt.show()
```

下面的屏幕截图显示了前面代码块的输出:

![](Images/97a5b714-f5d0-493b-a367-5565bf21d112.png)

3.  展平 28 x 28 的图像，以便输入的都是 784 像素值。此外，对输出进行一键编码。这一步是数据集准备过程中的关键:

```
# flatten 28*28 images to a 784 vector for each image
num_pixels = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')
```

在上一步中，我们使用整形方法对输入数据集进行整形，该方法将给定形状的数组转换为不同的形状。在这个具体的例子中，我们将一个具有`X_train.shape[0]`个数据点(图像)的数组(其中每个图像中有`X_train.shape[1]`行和`X_train.shape[2]`列)转换成一个具有`X_train.shape[0]`个数据点(图像)和每个图像的`X_train.shape[1] * X_train.shape[2]`值的数组。类似地，我们在测试数据集上执行相同的练习:

```
# one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
```

让我们试着理解一下一键编码是如何工作的。如果唯一的可能标签是 *{0，1，2，3}* ，它们将被一键编码，如下所示:

| **标签** | **0** | **1** | **2** | **3** |
| **0** | one | Zero | Zero | Zero |
| **1** | Zero | one | Zero | Zero |
| **2** | Zero | Zero | one | Zero |
| **3** | Zero | Zero | Zero | one |

本质上，每个标签将在数据集中占据一个唯一的列，如果标签存在，列值将为 1，其他列值将为零。

在 Keras 中，标签上的独热编码方法是使用`to_categorical`方法执行的，该方法计算出目标数据中唯一标签的数量，然后将它们转换为独热编码向量。

4.  建立一个隐含层为 1，000 个单元的神经网络:

```
model = Sequential()
model.add(Dense(1000, input_dim=784, activation='relu'))
model.add(Dense(10,  activation='softmax'))
```

在前面的步骤中，我们提到输入有 784 个值，它们与隐藏层中的 1，000 个值相连接。此外，我们还规定，将在输入和连接输入和隐藏层的权重的矩阵相乘之后在隐藏层中执行的激活是 ReLu 激活。

最后，隐藏层连接到一个有 10 个值的输出(因为在由`to_categorical`方法创建的向量中有 10 列)，我们在输出的顶部执行 softmax，这样我们就可以获得图像属于某个类的概率。

5.  前面的模型体系结构可以如下所示:

```
model.summary()

```

模型总结如下:

![](Images/02ffb258-590f-499a-a9b7-6d7f2bd3b7ee.png)

在前面的架构中，第一层中的参数数量是 785，000，因为 784 个输入单元连接到 1，000 个隐藏单元，对于 1，000 个隐藏单元产生 784 * 1，000 个权重值和 1，000 个偏差值，总共产生 785，000 个参数。

类似地，输出层有 10 个输出，连接到 1，000 个隐藏层中的每一个，产生 1，000 * 10 个参数和 10 个偏差-总共 10，010 个参数。

输出层有 10 个单元，因为输出中有 10 个可能的标签。对于给定的输入图像，输出层现在为我们提供了每个类别的概率值。

6.  按如下方式编译模型:

```
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```

请注意，因为目标变量是一个包含多个类的独热编码向量，所以损失函数将是分类交叉熵损失。

此外，我们正在使用 Adam 优化器来最小化成本函数(更多关于不同优化器的信息，请参见*改变损耗优化器以提高网络精度*方法)。

我们还注意到，在模型接受训练时，我们需要查看准确性指标。

7.  按照以下方式拟合模型:

```
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=32, verbose=1)
```

在前面的代码中，我们已经指定了模型适合的输入(`X_train`)和输出(`y_train`)。此外，我们还指定了测试数据集的输入和输出，模型不会使用它们来训练权重；然而，它将让我们了解训练数据集和测试数据集之间的损失值和准确度值有多大的不同。

8.  提取不同时期的训练和测试损失和准确性指标:

```
history_dict = history.history
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
acc_values = history_dict['acc']
val_acc_values = history_dict['val_acc']
epochs = range(1, len(val_loss_values) + 1)
```

在拟合模型时，对于训练和测试数据集，历史变量将已经存储了对应于每个历元中的模型的精度和损失值。在前面的步骤中，我们将这些值存储在一个列表中，这样我们就可以绘制出在越来越多的时期内，训练和测试数据集中的精度变化和损失。

9.  可视化不同数量的时期内的训练和测试损失以及准确性:

```
import matplotlib.pyplot as plt
%matplotlib inline 

plt.subplot(211)
plt.plot(epochs, history.history['loss'], 'rx', label='Training loss')
plt.plot(epochs, val_loss_values, 'b', label='Test loss')
plt.title('Training and test loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.subplot(212)
plt.plot(epochs, history.history['acc'], 'rx', label='Training accuracy')
plt.plot(epochs, val_acc_values, 'b', label='Test accuracy')
plt.title('Training and test accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.gca().set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()]) 
plt.legend()
plt.show()
```

前面的代码生成了下图，其中第一个图显示了递增时期内的训练和测试损失值，第二个图显示了递增时期内的训练和测试准确性:

![](Images/3d56bf42-6d83-4844-a9b7-06ea6ad79d71.png)

请注意，之前的网络的准确率为 97%。此外，请注意，损失值(以及精度)在不同的历元数上有阶跃变化。在下一节中，我们将把这种损失变化与缩放输入数据集的情况进行对比。

10.  让我们手动计算模型的精确度:

```
preds = model.predict(X_test)
```

在前面的步骤中，我们使用`predict`方法来计算模型给定输入(本例中为`X_test`)的预期输出值。注意，我们将它指定为`model.predict`，因为我们已经初始化了一个名为`model`的顺序模型:

```
import numpy as np
correct = 0
for i in range(len(X_test)):
    pred = np.argmax(preds[i],axis=0)
    act = np.argmax(y_test[i],axis=0)
    if(pred==act):
        correct+=1
    else:
        continue

correct/len(X_test)
```

在前面的代码中，我们一次循环一个测试预测。对于每个测试预测，我们将对`argmax`进行置换，以获得具有最高概率值的索引。

类似地，我们对测试数据集的实际值执行相同的练习。在测试数据集的预测值和实际值中，最高值索引的预测是相同的。

最后，测试数据集中数据点总数的正确预测数就是模型在测试数据集上的准确性。

        

# 它是如何工作的...

我们在前面的代码中执行的关键步骤如下:

*   我们展平了输入数据集，以便使用`reshape`方法将每个像素视为一个变量
*   我们对输出值执行了一次性编码，这样我们就可以使用`np_utils`包中的`to_categorical`方法来区分不同的标签
*   我们建立了一个带有隐藏层的神经网络
*   我们使用`model.compile`方法编译神经网络以最小化分类交叉熵损失(因为输出有 10 个不同的类别)
*   我们使用`model.fit`方法用训练数据拟合模型
*   我们提取了历史中存储的所有时期的训练和测试损失准确度
*   我们使用`model.predict`方法预测了测试数据集中每个类的概率
*   我们遍历了测试数据集中的所有图像，并确定了概率最高的类
*   最后，我们计算准确度(在所有实例中，预测的类别与图像的实际类别相匹配的实例的数量)

在下一节中，我们将探讨损耗和精度值发生阶跃变化的原因，并使变化更加平滑。

        

# 缩放输入数据集

在缩放数据集的过程中，我们会限制数据集中的变量，以确保它们不会有很大范围的不同值。一种方法是将数据集中的每个变量除以该变量的最大值。通常，当我们缩放输入数据集时，神经网络表现良好。

在本节中，让我们了解当数据集被缩放时神经网络性能更好的原因。

        

# 做好准备

为了理解缩放输入对输出的影响，让我们对比一下在输入数据集未缩放时检查输出的场景和在输入数据集缩放时检查输出的场景。

输入数据未缩放:

![](Images/4c549749-a8f8-45e8-8f9c-1ab0f2077506.jpg)

在上表中，请注意，尽管权重值在 0.01 到 0.9 之间变化，但输出(sigmoid)变化不大。sigmoid 函数的计算方法是输入与权重相乘的 sigmoid 值，然后加上偏差:

```
output = 1/(1+np.exp(-(w*x + b))
```

其中`w`是权重，`x`是输入，`b`是偏差值。

sigmoid 输出没有变化的原因是由于`w*x`的乘法运算是一个很大的数(因为 x 是一个很大的数)，导致 sigmoid 值总是落在 sigmoid 曲线的饱和部分(饱和值在 sigmoid 曲线的右上角或左下角)。

在这种情况下，让我们将不同的权重值乘以一个小的输入数，如下所示:

![](Images/7de39d87-5f55-4a9c-ad30-771958d1d0ce.png)

上表中的 sigmoid 输出会发生变化，因为输入和权重值较小，导致输入和权重相乘时的值较小，进而导致 sigmoid 值在输出中发生变化。

在本练习中，我们了解了缩放输入数据集的重要性，以便在权重(假设权重范围不大)乘以输入值时产生较小的值。这种现象导致权重值更新不够快。

因此，为了获得最佳权重值，我们应该在初始化权重时缩放输入数据集，使其不具有很大的范围(通常，权重在初始化期间具有-1 到+1 之间的随机值)。

当重量值也是一个非常大的数字时，这些问题就成立。因此，我们最好将权重值初始化为一个更接近于零的小值。

        

# 怎么做...

让我们来看一下我们在上一节中使用的数据集缩放设置，并比较缩放和不缩放的结果:

1.  导入相关的包和数据集:

```
from keras.datasets import mnist
import numpy as np
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.utils import np_utils

(X_train, y_train), (X_test, y_test) = mnist.load_data()
```

2.  有多种方法可以缩放数据集。一种方法是将所有数据点转换为 0 到 1 之间的值(通过将每个数据点除以总数据集中的最大值，这就是我们在下面的代码中所做的)。在众多其他方法中，另一种流行的方法是归一化数据集，通过减去每个数据点的整体数据集平均值，然后将每个结果数据点除以原始数据集中值的标准偏差，使值介于-1 和+1 之间。

现在，我们将对输入数据集进行展平和缩放，如下所示:

```
# flatten 28*28 images to a 784 vector for each image
num_pixels = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')

X_train = X_train/255
X_test = X_test/255
```

在前面的步骤中，我们通过将每个值除以数据集中的最大可能值(255 ),将定型和测试输入调整为一个介于 0 和 1 之间的值。此外，我们将输出数据集转换为独热编码格式:

```
# one hot encode outputs
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
```

3.  使用以下代码构建模型并编译它:

```
model = Sequential()
model.add(Dense(1000, input_dim=784, activation='relu'))
model.add(Dense(10,  activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```

请注意，前面的模型与我们在上一节中构建的模型完全相同。但是，唯一的区别是，它将在经过缩放的训练数据集上执行，而前一个数据集没有经过缩放。

4.  按照以下方式拟合模型:

```
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=32, verbose=1)
```

您将会注意到，前面的模型的准确度约为 98.25%。

5.  绘制不同时期的训练和测试精度以及损失值(生成以下图的代码与我们在*训练普通神经网络*方法的步骤 8 中使用的代码相同):

![](Images/4affc74a-14b2-4085-9407-5da31fb07f4b.png)

从前面的图表中，您应该注意到，与我们在上一节中看到的非缩放数据集相比，训练和测试损失随着时期的增加而平稳下降。

虽然前面的网络在平滑降低损失值方面给了我们很好的结果，但是我们注意到在训练和测试准确度/损失值之间存在差距，这表明在训练数据集上存在潜在的过度拟合。**过度拟合**是指模型专注于训练数据，而在测试数据集上可能效果不佳的现象。

        

# 它是如何工作的...

我们在前面的代码中执行的关键步骤如下:

*   我们展平了输入数据集，以便使用整形方法将每个像素视为一个变量
*   此外，我们缩放了数据集，使每个变量现在都有一个介于 0 和 1 之间的值
    *   我们通过将一个变量的值除以该变量的最大值来达到上述目的

*   我们对输出值执行了一次性编码，这样我们就可以使用`np_utils`包中的`to_categorical`方法来区分不同的标签
*   我们建立了一个带有隐藏层的神经网络
*   我们使用`model.compile`方法编译神经网络以最小化类别交叉熵损失(因为输出有 10 个不同的类别)
*   我们使用`model.fit`方法用训练数据拟合模型
*   我们提取了存储在历史中的所有时期的训练和测试损失准确度
*   我们还确定了一个我们认为过度拟合的场景

        

# 还有更多...

除了通过将变量值除以变量值中的最大值来缩放变量值之外，其他常用的缩放方法如下:

*   最小-最大归一化
*   均值归一化
*   标准化

关于这些缩放方法的更多信息可以在维基百科上找到:[https://en.wikipedia.org/wiki/Feature_scaling](https://en.wikipedia.org/wiki/Feature_scaling)。

        

# 当大多数输入大于零时对培训的影响

到目前为止，在我们考虑的数据集中，我们还没有查看输入数据集中值的分布。输入的某些值导致更快的训练。在本节中，我们将了解一个场景，其中当训练时间取决于输入值时，权重训练得更快。

        

# 做好准备

在本节中，我们将按照与上一节完全相同的方式来遵循模型构建过程。

但是，我们将对我们的策略做一个小小的改变:

*   我们将反转背景色，以及前景色。本质上，在这个场景中，背景将被涂成白色，标签将被涂成黑色。

影响模型准确性的这种变化的直觉如下。

图像角落中的像素对预测图像的标签没有贡献。假设一个黑色像素(原始场景)的像素值为零，它会被自动处理，因为当这个输入乘以任何权重值时，输出为零。这将导致网络了解到，将该角像素连接到隐藏层的权重值的任何改变都不会对损失值的改变产生影响。

然而，如果我们在角上有一个白色像素(我们已经知道角像素对预测图像的标签没有贡献)，它将对某些隐藏单元值有贡献，因此需要微调权重，直到角像素对预测标签的影响最小。

        

# 怎么做...

1.  加载并缩放输入数据集:

```
(X_train, y_train), (X_test, y_test) = mnist.load_data()
num_pixels = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')
X_train = X_train/255
X_test = X_test/255
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
```

2.  让我们看看输入值的分布:

```
X_train.flatten()
```

前面的代码将所有输入展平到一个列表中，因此，其形状为(47，040，000)，与`28 x 28 x X_train.shape[0]`相同。让我们绘制所有输入值的分布图:

```
plt.hist(X_train.flatten())
plt.grid('off')
plt.title('Histogram of input values')
plt.xlabel('Input values')
plt.ylabel('Frequency of input values')
```

![](Images/aeb37d82-2cee-490f-b933-4cfe855c3975.png)

我们注意到大多数输入为零(您应该注意到所有输入图像的背景都是黑色的，因此大多数值为零，这是黑色的像素值)。

3.  在本节中，我们将探索一种颠倒颜色的场景，其中背景是白色，字母用黑色书写，使用以下代码:

```
X_train = 1-X_train
X_test = 1-X_test
```

让我们绘制图像:

```
import matplotlib.pyplot as plt
%matplotlib inline
plt.subplot(221)
plt.imshow(X_train[0].reshape(28,28), cmap=plt.get_cmap('gray'))
plt.grid('off')
plt.subplot(222)
plt.imshow(X_train[1].reshape(28,28), cmap=plt.get_cmap('gray'))
plt.grid('off')
plt.subplot(223)
plt.imshow(X_train[2].reshape(28,28), cmap=plt.get_cmap('gray'))
plt.grid('off')
plt.subplot(224)
plt.imshow(X_train[3].reshape(28,28), cmap=plt.get_cmap('gray'))
plt.grid('off')
plt.show()
```

它们将如下所示:

![](Images/41d3b675-5acf-4a2a-830c-d911857cb189.png)

结果图像的直方图现在看起来如下:

![](Images/c8f2c774-2d0e-47dd-9324-7e199534c934.png)

您应该注意到，现在大多数输入值的值都是 1。

4.  让我们继续，使用我们在 S*scaling 输入数据集*部分构建的相同模型架构来构建我们的模型:

```
model = Sequential()
model.add(Dense(1000,input_dim=784,activation='relu'))
model.add(Dense(10, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32, verbose=1)
```

5.  绘制不同时期的训练和测试准确度和损失值(生成以下图的代码与我们在*训练普通神经网络*方法的步骤 8 中使用的代码相同):

![](Images/bf81c069-33e3-44f5-905f-76c8bcb34578.png)

我们应该注意到，现在模型精度已经下降到大约 97%,相比之下，当使用相同的模型进行相同的历元数和批量处理时，模型精度为大约 98%,但是数据集上的大多数是 0(而不是 1)。此外，该模型实现了 97%的准确性，比大多数输入像素为零的情况慢得多。

当大多数数据点不为零时，准确性下降的直觉是，当大多数像素为零时，模型的任务更容易(必须微调的权重更少)，因为它必须基于少数像素值(像素值大于零的少数像素值)进行预测。然而，当大多数数据点不为零时，需要微调更多的权重来进行预测。

        

# 批量大小对模型准确性的影响

在前面的部分中，对于我们已经建立的所有模型，我们考虑了 32 的批量大小。在本节中，我们将尝试了解改变批量大小对准确度的影响。

        

# 做好准备

为了理解批量大小对模型准确性产生影响的原因，让我们对比一下数据集总大小为 60，000 的两种情况:

*   批量为 30，000 件
*   批量大小为 32

当批量较大时，与批量较小的情况相比，每个时期的权重更新次数较少。

当批量较小时，每个时期的权重更新数量较高的原因是，计算损失值时考虑的数据点较少。这导致每个时期有更多批次，因为在一个时期中，您将不得不遍历数据集中的所有训练数据点。

因此，批量越小，相同数量的历元的准确度越好。但是，在决定批次大小要考虑的数据点数时，还应该确保批次大小不要太小，否则可能会在少量数据上溢出。

        

# 怎么做...

在前面的配方中，我们构建了一个批量为 32 的模型。在本配方中，我们将继续实施该模型，以对比相同数量的时期的低批量和高批量的情况:

1.  预处理数据集并拟合模型，如下所示:

```
(X_train, y_train), (X_test, y_test) = mnist.load_data()
num_pixels = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')
X_train = X_train/255
X_test = X_test/255
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
model = Sequential()
model.add(Dense(1000,input_dim=784,activation='relu'))
model.add(Dense(10, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=30000, verbose=1)
```

注意，代码中唯一的变化是模型拟合过程中的`batch_size`参数。

2.  绘制不同时期的训练和测试准确度和损失值(生成以下图的代码与我们在*训练普通神经网络*方法的步骤 8 中使用的代码相同):

![](Images/8045caed-04a4-41b0-a58d-5aaafba99a70.png)

在前面的场景中，您应该注意到，与批量更小时达到的模型精度相比，模型精度在更晚的时期达到了约 98%。

        

# 它是如何工作的...

您应该注意到，精度在开始时要低得多，只有在运行了相当多的历元后才能赶上。初始时期准确度低的原因是，与之前的情况(批量较小)相比，在这种情况下，重量更新的次数要少得多。

在这种情况下，当批量大小为 30，000，总数据集大小为 60，000 时，当我们运行 500 个时期的模型时，权重更新发生在时期*(数据集大小/批量大小)= 500 * (60，000/30，000) = 1，000 次。

在前面的场景中，权重更新发生 500 * (60，000/32) = 937，500 次。

因此，批量越小，权重更新的次数就越多，一般来说，对于相同数量的时期，准确度就越好。

同时，您应该注意不要在批量大小中有太少的示例，这可能不仅会导致很长的训练时间，还可能会出现过度拟合的情况。

        

# 构建深度神经网络，提高网络精度

到目前为止，我们已经看到了神经网络在输入层和输出层之间只有一个隐藏层的模型结构。在本节中，我们将查看有多个隐藏层的神经网络(因此是一个深度神经网络)，同时重用经过缩放的相同 MNIST 训练和测试数据集。

        

# 做好准备

深度神经网络意味着有多个隐藏层连接输入和输出层。多个隐藏层确保神经网络学习输入和输出之间的复杂非线性关系，这是简单的神经网络无法学习的(由于隐藏层的数量有限)。

典型的深度前馈神经网络如下所示:

![](Images/5b116810-f567-4e06-9f94-efdbe6f013e0.png)

        

# 怎么做...

通过在输入和输出层之间添加多个隐藏层来构建深度神经网络架构，如下所示:

1.  加载数据集并缩放它:

```
(X_train, y_train), (X_test, y_test) = mnist.load_data()
num_pixels = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')
X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')
X_train = X_train/255
X_test = X_test/255
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_test.shape[1]
```

2.  构建一个具有多个连接输入和输出层的隐藏层的模型:

```
model = Sequential()
model.add(Dense(1000, input_dim=784, activation='relu'))
model.add(Dense(1000,activation='relu'))
model.add(Dense(1000,activation='relu'))
model.add(Dense(10,  activation='softmax'))
```

前面的模型体系结构会产生一个模型摘要，如下所示:

![](Images/f36020ac-67c3-494d-a646-f863b71527dc.png)

请注意，前面的模型会产生更多的参数，这是深层架构的结果(因为模型中有多个隐藏层)。

3.  既然已经建立了模型，让我们编译并拟合模型:

```
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=250, batch_size=1024, verbose=1)
```

前面的结果是一个准确度为 98.6%的模型，比我们之前看到的模型架构的准确度稍好。训练和测试损失和准确性如下(生成下图中的图的代码与我们在*训练普通神经网络*方法的步骤 8 中使用的代码相同):

![](Images/f11ab003-caee-4343-8b8b-eadce1391e26.png)

请注意，在这种情况下，训练和测试损失之间存在相当大的差距，这表明深度前馈神经网络专门处理训练数据。同样，在关于过度拟合的章节中，我们将了解避免训练数据过度拟合的方法。

        

# 改变学习速率以提高网络精度

到目前为止，在前面的配方中，我们使用了 Adam 优化器的默认学习率，即 0.0001。

在本节中，我们将手动将学习率设置为一个较高的数字，并查看更改学习率对模型准确性的影响，同时重用在之前的配方中调整的相同 MNIST 训练和测试数据集。

        

# 做好准备

在前一章构建前馈神经网络中，我们了解到学习率用于更新权重，权重的变化与损失减少量成比例。

此外，权重值的变化等于损失的减少乘以学习率。因此，学习率越低，权重值的变化越小，反之亦然。

实际上，您可以将权重值视为一个连续的频谱，其中权重是随机初始化的。当权重值的变化很大时，很可能不考虑光谱中的各种权重值。然而，当权重值的变化很小时，权重可能达到全局最小值，因为可以考虑更多可能的权重值。

为了进一步理解这一点，让我们考虑拟合 *y = 2x* 线的玩具示例，其中初始权重值为 1.477，初始偏移值为零。前馈和反向传播函数将与我们在前一章中看到的一样:

```
def feed_forward(inputs, outputs, weights):
     hidden = np.dot(inputs,weights[0])
     out = hidden+weights[1]
     squared_error = (np.square(out - outputs))
     return squared_error

def update_weights(inputs, outputs, weights, epochs, lr): 
    for epoch in range(epochs):
        org_loss = feed_forward(inputs, outputs, weights)
        wts_tmp = deepcopy(weights)
        wts_tmp2 = deepcopy(weights)
        for ix, wt in enumerate(weights):
            print(ix, wt)
            wts_tmp[-(ix+1)] += 0.0001
            loss = feed_forward(inputs, outputs, wts_tmp)
            del_loss = np.sum(org_loss - loss)/(0.0001*len(inputs))
            wts_tmp2[-(ix+1)] += del_loss*lr
            wts_tmp = deepcopy(weights)
        weights = deepcopy(wts_tmp2)
    return wts_tmp2
```

请注意，与我们在上一章中看到的反向传播函数的唯一变化是，我们将学习率作为一个参数传递到前面的函数中。当在不同数量的时期内学习率为 0.01 时，权重值如下:

```
w_val = []
b_val = []
for k in range(1000):
     w_new, b_new = update_weights(x,y,w,(k+1),0.01)
     w_val.append(w_new)
     b_val.append(b_new)
```

使用以下代码可以获得不同时期的重量变化图:

```
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(w_val)
plt.title('Weight value over different epochs when learning rate is 0.01')
plt.xlabel('epochs')
plt.ylabel('weight value')
plt.grid('off')
```

上述代码的输出如下:

![](Images/ff23c179-fb57-4c71-905c-512353516f30.png)

以类似的方式，当学习率为 0.1 时，不同数量的时期上的权重值如下:

![](Images/226c0088-34ef-46bc-a399-46ce411fd70e.png)

该屏幕截图显示了当学习率为 0.5 时，不同数量的时期的权重值:

![](Images/c6883c3e-8044-4ba1-97cc-34943ece045f.png)

注意，在前面的场景中，权重值最初有剧烈的变化，0.1 的学习率收敛，而 0.5 的学习率没有收敛到最优解，因此陷入局部最小值。

在学习率为 0.5 的情况下，给定的权重值卡在局部最小值，它不能达到最优值 2。

        

# 怎么做...

既然我们已经了解了学习率是如何影响输出值的，那么让我们来看看学习率对我们之前看到的 MNIST 数据集的影响，其中我们保持了相同的模型架构，但只是更改了学习率参数。

请注意，我们将使用与*缩放输入数据集*配方中步骤 1 和步骤 2 相同的数据预处理步骤。

预处理数据集后，我们将在下一步中通过指定优化器来改变模型的学习速率:

1.  我们将学习率更改如下:

```
from keras import optimizers
adam=optimizers.Adam(lr=0.01)
```

使用前面的代码，我们已经用 0.01 的指定学习率初始化了 Adam 优化器。

2.  我们按如下方式构建、编译和拟合模型:

```
model = Sequential()
model.add(Dense(1000, input_dim=784, activation='relu'))
model.add(Dense(10,  activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy']) 

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=1024, verbose=1)
```

在 500 个历元结束时，前述网络的准确度约为 90%。让我们看看损失函数和精度在不同的时期数上是如何变化的(生成下图中的图的代码与我们在*训练普通神经网络*方法的步骤 8 中使用的代码相同):

![](Images/1b584542-f5c3-48b0-9af0-1855dea2f4c7.png)

注意，与 0.0001(在*缩放输入数据集*配方中考虑的场景中)相比，当学习率较高(在当前场景中为 0.01)时，与低学习率模型相比，损失下降不太平稳。

低学习率模型缓慢地更新权重，从而导致平滑地减少损失函数以及高精度，这是在更高数量的时期上缓慢实现的。

或者，当学习率较高时，损失值的阶跃变化是由于损失值陷入局部最小值，直到权重值变为最佳值。较低的学习速率更有可能更快地达到最佳权重值，因为权重在正确的方向上缓慢但稳定地变化。

以类似的方式，让我们探索当学习率高达 0.1 时的网络准确度:

```
from keras import optimizers
adam=optimizers.Adam(lr=0.1)

model = Sequential()
model.add(Dense(1000, input_dim=784, activation='relu'))
model.add(Dense(10,  activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=1024, verbose=1)
```

要注意的是，损失值不能进一步减少太多，因为学习率很高；也就是说，权重可能陷入局部最小值:

![](Images/2c8659c9-e4e3-4524-9dfd-5f6661fd3b96.png)

因此，一般来说，将学习速率设置为较低的值并让网络学习大量的时期是一个好主意。

        

# 改变损耗优化器以提高网络精度

到目前为止，在前面的配方中，我们认为损失优化器是 Adam 优化器。然而，优化器有多种其他变体，优化器的变化可能会影响模型学习适应输入和输出的速度。

在这个菜谱中，我们将了解更改优化器对模型准确性的影响。

        

# 做好准备

为了了解改变优化器对网络准确性的影响，让我们将前面章节中展示的场景(即 Adam 优化器)与本章节中使用的**随机梯度下降优化器**进行对比，同时重用已缩放的相同 MNIST 训练和测试数据集(与*缩放数据集*方法中的步骤 1 和步骤 2 相同的数据预处理步骤):

```
model = Sequential()
model.add(Dense(1000, input_dim=784, activation='relu'))
model.add(Dense(10,  activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, verbose=1)
```

请注意，当我们在前面的代码中使用随机梯度下降优化器时，100 个时期后的最终精度约为 98%(生成下图中的图的代码与我们在*训练普通神经网络*方法的步骤 8 中使用的代码保持相同):

![](Images/38cdae47-275b-46d5-b662-fc29d5f85df3.png)

但是，我们还应该注意到，与使用 Adam 优化的模型相比，该模型达到高精度水平的速度要慢得多。

        

# 还有更多...

其他一些可用的损耗优化器如下:

*   RMSprop
*   阿达格拉德
*   阿达德尔塔
*   阿达马斯
*   那达慕

你可以在这里了解更多关于各种优化:[https://keras.io/optimizers/](https://keras.io/optimizers/)。

另外，你可以在这里找到每个优化器的源代码:[https://github . com/keras-team/keras/blob/master/keras/optimizer . py](https://github.com/keras-team/keras/blob/master/keras/optimizers.py)。

        

# 了解过度拟合的情况

在前面的一些方法中，我们已经注意到，训练精度约为 100%，而测试精度约为 98%，这是在训练数据集上过度拟合的情况。让我们直观地了解一下训练和测试准确性之间的差异。

为了理解导致过度拟合的现象，让我们对比两个场景，在这两个场景中，我们比较训练和测试精度以及权重直方图:

*   模型运行五个时期
*   模型运行 100 个时期

两种场景之间的训练和测试数据集之间的准确性比较度量如下:

| **场景** | **训练数据集** | **测试数据集** |
| 5 个时代 | 97.59% | 97.1% |
| 100 个时代 | 100% | 98.28% |

一旦我们绘制了连接隐藏层和输出层的权重直方图，我们将会注意到，与五个历元的情况相比，100 个历元的情况具有更高的权重分布:

![](Images/a6f6b04b-293f-4fb2-a76f-211d40b81f0c.png)

![](Images/7c5f76b7-f483-4666-9fc5-ea98573fc588.png)

从前面的图片中，您应该注意到，与五个时期的情况相比，100 个时期的情况具有更高的权重值分散度。这是因为当模型运行 100 个时期时，与当模型运行 5 个时期时相比，模型不得不在训练数据集上过度拟合的机会更高，因为 100 个时期情况下的权重更新的数量高于 5 个时期情况下的权重更新的数量。

高权重值(以及训练和测试数据集中的差异)是模型潜在过度拟合和/或缩放输入/权重以提高模型准确性的潜在机会的良好指示。

此外，还要注意，神经网络可能有数十万个权重(在某些体系结构中有数百万个)需要调整，因此，总有机会将一个或另一个权重更新为非常高的数字，以便对数据集的一个异常值行进行微调。

        

# 使用正则化克服过拟合

在上一节中，我们确定了高重量是过度拟合的原因之一。在本节中，我们将研究解决过度拟合问题的方法，例如对高权重值进行惩罚。

**正则化**对模型中的高权重给出惩罚。L1 和 L2 正则化是最常用的正则化技术，其工作原理如下:

除了最小化损失函数(在下面的公式中是损失的平方和)之外，L2 正则化还最小化神经网络的指定层的权重的加权平方和:

![](Images/64a98b14-388a-4cbb-b604-30c56e739681.png)

其中![](Images/f9fef716-fe17-45b4-8abf-6a669c5d3cc6.png)是与正则项相关联的权重，并且是需要调整的超参数， *y* 是![](Images/28abf306-b171-4bd7-b8de-1bdffe82639c.png)的预测值，![](Images/f552cdd0-2311-4b72-8ec6-29e8c99fcf69.png)是模型所有层的权重值。

L1 正则化除了最小化损失函数(在下面的公式中是损失的平方和)之外，还最小化神经网络的指定层的权重的绝对值的加权和:

![](Images/9e52d2e2-aca1-4e06-a3af-007deacb2468.png)。

这样，我们可以确保权重不会仅针对训练数据集中的极端情况进行定制(因此，不会对测试数据进行概括)。

        

# 怎么做

L1/L2 正规化在喀拉斯州实施，具体如下:

```
model = Sequential()
model.add(Dense(1000,input_dim=784,activation='relu',kernel_regularizer=l2(0.1)))model.add(Dense(10,  activation='softmax',kernel_regularizer=l2(0.1)))
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=500, batch_size=1024, verbose=1)
```

注意，前面涉及到调用一个额外的超参数——`kernel_regularizer`——然后指定它是否是 L1/L2 正则化。此外，我们还指定了赋予正则化权重的 lambda 值。

我们注意到，在正则化之后，训练数据集的准确度不会恰好在 100%左右，而测试数据的准确度在 98%。下图显示了 L2 正则化后的权重直方图。

将隐藏层连接到输出层的权重提取如下:

```
model.get_weights()[0].flatten()
```

一旦提取了权重，它们就被绘制如下:

```
plt.hist(model.get_weights()[0].flatten())
```

![](Images/e672e37d-2059-4520-9564-12ee59698144.png)

我们注意到，与之前的场景相比，现在大多数权重更接近于零，从而提出了避免过度拟合问题的案例。在 L1 正则化的情况下，我们会看到类似的趋势。

请注意，与执行正则化时的权重值相比，正则化时的权重值要低得多。

因此，L1 和 L2 正则化帮助我们避免训练数据集上的过度拟合问题。

        

# 使用 dropout 克服过度拟合

在前面使用正则化克服过拟合的部分中，我们使用 L1/ L2 正则化作为避免过拟合的方法。在本节中，我们将使用另一个有助于实现相同目标的工具— **dropout** 。

丢弃可以被认为是这样一种方式，其中只有特定百分比的权重得到更新，而其他权重在给定的权重更新迭代中没有得到更新。这样，我们就处于这样一个位置，即在权重更新过程中并非所有权重都得到更新，从而避免某些权重与其他权重相比达到非常高的幅度:

```
model = Sequential()
model.add(Dense(1000, input_dim=784, activation='relu'))
model.add(Dropout(0.75))
model.add(Dense(10,  activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=1024, verbose=1)
```

在前面的代码中，我们给出了 0.75 的漏失；也就是说，随机地，75%的权重在某个权重更新迭代中没有得到更新。

前面所述将导致训练和测试准确性之间的差距不如在前面的场景中构建没有丢失的模型时高，在前面的场景中权重的分布更高。

现在注意第一层的权重直方图:

```
plt.hist(model.get_weights()[-2].flatten())
```

![](Images/64d666de-bbb5-4c96-94d6-79946b4e26a6.png)

请注意，在前面的场景中，与 100 个时期的场景相比，权重超过 0.2 或-0.2 的频率计数较少。

        

# 使用批量标准化加速训练过程

在关于缩放数据集的上一节中，我们了解到，当输入数据未缩放时(即，它不在零和一之间)，优化速度很慢。

在下列情况下，隐藏层值可能很高:

*   输入数据值高
*   重量值很高
*   重量和投入的倍增是高的

这些情况中的任何一种都会在隐藏图层上产生较大的输出值。

请注意，隐藏层是输入层到输出层。因此，当隐藏层值也很大时，导致缓慢优化的高输入值现象也成立。

**批量正常化**在这种情况下起了作用。我们已经知道，当输入值较高时，我们执行缩放来减少输入值。此外，我们还了解到，还可以使用不同的方法进行缩放，即减去输入的平均值，然后除以输入的标准偏差。批处理规范化执行这种缩放方法。

通常，所有值都使用以下公式进行缩放:

![](Images/bc09c4fd-85ae-46d7-97ac-2dc608a9c27f.png)

![](Images/e4844bdd-acf2-4e1d-b5f7-f8288413842a.png)

![](Images/110c9b11-033b-4bfc-9b4f-445cc803a93e.png)

![](Images/aa3c6535-7868-42c7-bd2f-4056453df216.png)

请注意， *γ* 和 *β* 是在训练期间学习的，同时还有网络的原始参数。

        

# 怎么做...

在代码中，批处理规范化的应用如下:

请注意，我们将使用与在*缩放输入数据集*配方中的步骤 1 和步骤 2 相同的数据预处理步骤。

1.  导入`BatchNormalization`方法如下:

```
from keras.layers.normalization import BatchNormalization
```

2.  实例化一个模型，并构建与我们使用正则化技术时构建的架构相同的架构。唯一增加的是我们在一个隐藏层中执行批量标准化:

```
model = Sequential()
model.add(Dense(1000, input_dim=784,activation='relu', kernel_regularizer = l2(0.01)))
model.add(BatchNormalization())
model.add(Dense(10, activation='softmax', kernel_regularizer = l2(0.01)))
```

3.  按如下方式构建、编译和拟合模型:

```
from keras.optimizers import Adam
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=1024, verbose=1)
```

上述操作产生的训练比没有批处理规范化时快得多，如下所示:

![](Images/2f5f1e2c-8a0d-45df-8c1a-d1a7918628b9.png)

前面的图表显示了当没有批量归一化，而只有正则化时的训练和测试损失和准确性。下图显示了正则化和批量归一化的训练和测试损失和准确性:

![](Images/32d51718-2639-441a-8155-bd78778bed39.png)

请注意，在前面的两个场景中，我们看到当我们执行批处理规范化时(测试数据集准确度约为 97%)比不执行批处理规范化时(测试数据集准确度约为 91%)训练速度快得多。

因此，批量标准化导致更快的训练。