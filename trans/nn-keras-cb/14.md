<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 端到端学习

在前面的章节中，我们已经学习了使用**循环神经网络** ( **RNN** )分析序列数据(文本)，以及使用**卷积神经网络** ( **CNN** )分析图像数据。

在本章中，我们将学习使用 CNN + RNN 组合来解决以下案例研究:

*   手写文本识别
*   从图像生成标题

此外，在解决手写文本识别问题时，我们还将学习一个新的损失函数，称为**连接主义者时间分类** ( **CTC** )损失。

最后，我们将学习波束搜索，在解决从图像生成标题的问题时，为生成的文本提出合理的替代方案。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 介绍

考虑这样一个场景，我们正在抄写一个手写文本的图像。在这种情况下，我们将处理图像数据和顺序数据(因为图像中的内容需要顺序转录)。

在传统的分析中，我们会手工制作解决方案，例如:我们可能会在图像上滑动一个窗口(窗口的大小是一个字符的平均大小),以便该窗口可以检测每个字符，然后以很高的可信度输出它检测到的字符。

然而，在这种情况下，我们应该滑动的窗口的大小或数量是由我们手工制作的——这变成了特征工程(特征生成)的问题。

更端到端的方法是提取通过 CNN 传递图像获得的特征，然后将这些特征作为输入传递到 RNN 的各个时间步长，以便我们在各个时间步长提取输出。

因此，我们将使用有线电视新闻网和 RNN 的组合，通过这种方式处理问题，我们根本不必构建手工制作的功能，让模型计算出有线电视新闻网和 RNN 的最佳参数。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 联结主义时间分类

在手写文本识别或语音转录的基础上执行监督学习的一个限制是，使用传统方法，我们必须提供图像的哪个部分包含某个字符(在手写识别的情况下)或音频的哪个子段包含某个音素(多个音素组合形成单词发音)的标签。

然而，当构建数据集时，为图像中的每个字符或语音转录中的每个音素提供基础事实是极其昂贵的，因为要转录成千上万个单词或数百个小时的语音。

CTC 在解决不知道图像的不同部分到不同字符的映射的问题上派上了用场。在本节中，我们将了解 CTC 丢失是如何起作用的。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 解码 CTC

比方说，我们正在转录一个包含文本 **ab** 的图像。该示例可能看起来像以下任何一个(在字符 **a** 和 **b** 之间有不同的间距),并且输出标签(地面真相)完全相同 **ab** :

![](Images/cf409f12-0d59-4815-8c13-0abc072eb181.png)

在下一步中，我们将把这些例子分成多个时间步，如下所示(其中每个方框代表一个时间步):

![](Images/77d67303-323b-495a-a8c4-8170d6abca28.png)

在前面的示例中，我们总共有六个时间步长(每个单元代表一个时间步长)。

我们将预测每个时间步长的输出，其中一个时间步长的输出是词汇表中的 softmax。

假设我们正在执行 softmax，假设第一张图片 **ab** 的每个时间步的输出如下:

![](Images/1b3bac58-0c5a-48a2-a533-44f676518f0b.png)

注意，上图中的 **-** 代表空白。此外，如果图像的特征通过双向 LSTM(或 GRU)，则第四和第五时间步中的输出可以是 **b** ，因为在执行双向分析时，下一时间步中的信息也可以影响前一时间步中的输出。

在最后一步中，我们将压缩所有在连续时间步长中具有相同值的 softmax 输出。

前面的结果导致我们的最终输出是: **-a-b-** 。

如果，万一地面事实是 **abb** ，我们将期望在两个 **b** 之间有一个 **-** ，这样连续的 **b** 不会被挤成一个。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 计算 CTC 损失值

对于我们在上一节中解决的问题，让我们考虑以下场景，在下图的圆圈中提供了角色在给定时间步长中的概率(注意，在从 **t0** 到 **t5** 的每个时间步长中，概率加起来为 1):

![](Images/e6ae3e2b-0198-4204-93e2-c3099b144404.png)

然而，为了使计算简单易懂，让我们考虑这样的场景，其中地面真值是 **a** 而不是 **ab** ，并且输出只有三个时间步长而不是六个。跨三个时间步长的修改后的输出如下所示:

![](Images/f49d376c-e7ca-493c-9c01-8642c6ba3ff2.png)

如果每个时间步中的 softmax 是以下任何一种情况，我们可以得到 **a** 的地面真值:

| **每个时间步的输出** | **时间步长 1 中字符的概率** | **第二时间步字符的概率** | **第三时间步字符的概率** | **组合概率** | **最终概率** |
| ——答 | Zero point eight | Zero point one | Zero point one | 0.8 x 0.1 x 0.1 |  0.008 |
| -啊啊 | Zero point eight | Zero point nine | Zero point one | 0.8 x 0.9 x 0.1 | Zero point zero seven two |
| 啊啊啊 | Zero point two | Zero point nine | Zero point one | 0.2 x 0.9 x 0.1 | Zero point zero one eight |
| -一个- | Zero point eight | Zero point nine | Zero point eight | 0.8 x 0.9 x 0.8 | Zero point five seven six |
| -啊啊 | Zero point eight | Zero point nine | Zero point one | 0.8 x 0.9 x 0.1 | Zero point zero seven two |
| 一个.. | Zero point two | Zero point one | Zero point eight | 0.2 x 0.1 x 0.8 | Zero point zero one six |
| 一个 | Zero point two | Zero point nine | Zero point eight | 0.2 x 0.9 x 0.8 | Zero point one four four |
| **总概率** | **0.906** |

从前面的结果中，我们看到获得地面真值 **a** 的总概率是 0.906。

CTC 损失是总体概率的负对数= *-log(0.906) = 0.04* 。

注意，由于在每个时间步中具有最高概率的字符组合指示 **a** 的地面事实，所以 CTC 损失接近于零。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 手写文本识别

在这个案例研究中，我们将努力转录手写图像，以便提取图片中的文本。

笔迹样本如下所示:

![](Images/900d721e-6a43-4de5-9ced-850bc6332af3.png)

注意，在上图中，手写字符具有不同的长度，图像具有不同的尺寸，字符之间的间隔是不同的，并且图像具有不同的质量。

在本节中，我们将学习如何使用 CNN、RNN 和 CTC loss 函数来转录手写示例。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 做好准备

我们将采用的抄写手写示例的策略如下:

*   下载包含手写文本图像的图像:
    *   GitHub 中与该案例研究相关的代码文件中提供了包含手写文本图像的多个数据集
    *   确保你在拍摄图像的同时，也拍摄了与图像相对应的地面真相
*   将所有图像的大小调整为相同，比如 32 x 128
*   在调整大小时，我们还应该确保图片的纵横比不失真:
    *   这是为了确保图像看起来不会非常模糊，因为原始图像的大小已更改为 32 x 128
*   我们将在不扭曲纵横比的情况下调整图像的大小，然后将它们叠加在不同的空白 32 x 128 图像上
*   反转图像的颜色，使背景为黑色，手写内容为白色
*   缩放图像，使其值介于 0 和 1 之间
*   预处理输出(地面实况):
    *   提取输出中所有唯一的字符
    *   为每个字符分配一个索引
    *   找出输出的最大长度，然后确保我们预测时间步长内容的时间步长数大于输出的最大长度
    *   通过填充基本事实，确保所有输出的输出长度相同
*   将处理过的图片通过 CNN，以便我们提取形状为 32 x 256 的特征
*   将从 CNN 提取的特征通过双向的 GRU 单元，以便我们封装相邻时间步骤中存在的信息
*   32 个时间步长中的 256 个特征中的每一个都是相应时间步长的输入
*   将输出通过一个密集层，该层的输出值与 ground truth 中唯一字符的总数一样多(填充值( **-** 在 CTC 丢失部分介绍中给出的示例中)也应是唯一字符之一，其中填充值 **-** 表示字符之间的间距，或图片空白部分的填充
*   在 32 个时间步中的每一步提取 softmax 及其相应的输出字符

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 怎么做...

代码中的上述算法执行如下(GitHub 中的代码文件为`Handwritten_text_recognition.ipynb`):

1.  下载并导入数据集。该数据集将包含手写文本的图像及其相应的基本事实(转录)。
2.  构建一个在不扭曲长宽比的情况下调整图片大小的功能，并填充图片的其余部分，使其具有相同的形状:

```
def extract_img(img):
     target = np.ones((32,128))*255
     new_shape1 = 32/img.shape[0]
     new_shape2 = 128/img.shape[1]
     final_shape = min(new_shape1, new_shape2)
     new_x = int(img.shape[0]*final_shape)
     new_y = int(img.shape[1]*final_shape)
     img2 = cv2.resize(img, (new_y,new_x ))
     target[:new_x,:new_y] = img2[:,:,0]
     target[new_x:,new_y:]=255
     return 255-target
```

在前面的代码中，我们创建了一个空白图片(名为`target`)。在下一步中，我们已经改变了图片的形状以保持它的长宽比。

最后，我们在我们创建的空白图片上覆盖了重新缩放的图片，并返回了背景为黑色的图片(255 目标)。

3.  读取图片并将它们存储在一个列表中，如下面的代码所示:

```
filepath = '/content/words.txt'
f = open(filepath)
import cv2
count = 0
x = []
y = []
x_new = []
chars = set()
for line in f:
     if not line or line[0]=='#':
         continue
     try:
         lineSplit = line.strip().split(' ')
         fileNameSplit = lineSplit[0].split('-')
         img_path = '/content/'+fileNameSplit[0]+'/'+fileNameSplit[0] + '-' +              fileNameSplit[1]+'/'+lineSplit[0]+'.png'
         img_word = lineSplit[-1]
         img = cv2.imread(img_path)
         img2 = extract_img(img)
         x_new.append(img2)
         x.append(img)
         y.append(img_word)
         count+=1
     except:
         continue
```

在前面的代码中，我们提取了每张图片，并根据我们定义的函数对其进行了修改。不同场景的输入和修改示例:

![](Images/cb7b3958-fbd3-4d1c-b6cb-40a4cc1ec9be.png)

4.  提取输出中的唯一字符，如下所示:

```
import itertools
list2d = y
charList = list(set(list(itertools.chain(*list2d))))
```

5.  创建输出地面实况，如以下代码所示:

```
num_images = 50000

import numpy as np
y2 = []
input_lengths = np.ones((num_images,1))*32
label_lengths = np.zeros((num_images,1))
for i in range(num_images):
     val = list(map(lambda x: charList.index(x), y[i]))
     while len(val)<32:
         val.append(79)
     y2.append(val)
     label_lengths[i] = len(y[i])
     input_lengths[i] = 32
```

在前面的代码中，我们将输出中每个字符的索引存储到一个列表中。此外，如果输出小于 32 个字符，我们用 79 填充它，这表示空白值。

最后，我们还存储了标签长度(实际长度)和输入长度(其大小始终为 32)。

6.  将输入和输出转换为 NumPy 数组，如下所示:

```
x = np.asarray(x_new[:num_images])
y2 = np.asarray(y2)
x= x.reshape(x.shape[0], x.shape[1], x.shape[2],1)
```

7.  定义目标，如下所示:

```
outputs = {'ctc': np.zeros([32])}
```

我们正在初始化 32 个零，因为批量大小将是 32。对于批量中的每个值，我们期望损失值为零。

8.  定义 CTC 损失函数如下:

```
def ctc_loss(args):
     y_pred, labels, input_length, label_length = args
     return K.ctc_batch_cost(labels, y_pred, input_length, label_length)
```

前面的函数将预测值、地面实况(标签)和输入、标签长度作为输入，并计算 CTC 损失值。

9.  定义模型，演示如下:

```
input_data = Input(name='the_input', shape = (32, 128,1), dtype='float32')

inner = Conv2D(32, (3,3), padding='same')(input_data)
inner = Activation('relu')(inner)
inner = MaxPooling2D(pool_size=(2,2),name='max1')(inner)
inner = Conv2D(64, (3,3), padding='same')(inner)
inner = Activation('relu')(inner)
inner = MaxPooling2D(pool_size=(2,2),name='max2')(inner)
inner = Conv2D(128, (3,3), padding='same')(input_data)
inner = Activation('relu')(inner)
inner = MaxPooling2D(pool_size=(2,2),name='max3')(inner)
inner = Conv2D(128, (3,3), padding='same')(inner)
inner = Activation('relu')(inner)
inner = MaxPooling2D(pool_size=(2,2),name='max4')(inner)
inner = Conv2D(256, (3,3), padding='same')(inner)
inner = Activation('relu')(inner)
inner = MaxPooling2D(pool_size=(4,2),name='max5')(inner)
inner = Reshape(target_shape = ((32,256)), name='reshape')(inner)
```

在前面的代码中，我们正在构建 CNN，它将一张 32 x 128 形状的图片转换为一张 32 x 256 形状的图片:

```
gru_1 = GRU(256, return_sequences = True, name = 'gru_1')(inner)
gru_2 = GRU(256, return_sequences = True, go_backwards = True, name = 'gru_2')(inner)
mix_1 = add([gru_1, gru_2])
gru_3 = GRU(256, return_sequences = True, name = 'gru_3')(inner)
gru_4 = GRU(256, return_sequences = True, go_backwards = True, name = 'gru_4')(inner)
```

直到先前定义的层的模型架构如下:

![](Images/ceeb523d-37f3-4c1d-9828-32dc1dddfe58.png)

在前面的代码中，我们将从 CNN 获得的功能传递到 GRU 中。前面定义的架构延续了前面的图表，如下所示:

![](Images/3703ea67-229b-4f73-bc63-a71be57a06d0.png)

在下面的代码中，我们连接了两个 GRU 的输出，因此我们同时考虑了双向 GRU 和正常 GRU 生成的特征:

```
merged = concatenate([gru_3, gru_4])
```

添加前一层后的架构如下:

![](Images/d3b78853-dfea-445c-8be8-538b17c2b466.png)

在以下代码中，我们通过密集图层传递 GRU 输出的要素，并应用 softmax 获取 80 个可能值中的一个作为输出:

```
dense = TimeDistributed(Dense(80))(merged)
y_pred = TimeDistributed(Activation('softmax', name='softmax'))(dense)
```

该模型的架构如下:

![](Images/1f69ad73-9c27-40b1-8720-e37833d6fce1.png)

10.  初始化 CTC 损失所需的变量:

```
from keras.optimizers import Adam
Optimizer = Adam()
labels = Input(name = 'the_labels', shape=[32], dtype='float32')
input_length = Input(name='input_length', shape=[1],dtype='int64')
label_length = Input(name='label_length',shape=[1],dtype='int64')
output = Lambda(ctc_loss, output_shape=(1,),name='ctc')([y_pred, labels, input_length, label_length])
```

在前面的代码中，我们提到了`y_pred`(预测字符值)、实际标签、输入长度和标签长度是 CTC 损失函数的输入。

11.  按如下方式构建和编译模型:

```
model = Model(inputs = [input_data, labels, input_length, label_length], outputs= output)
model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = Optimizer)
```

请注意，我们向模型传递了多个输入。CTC 计算如下:

![](Images/8ad595f7-6135-494d-8787-4979a3460b79.png)

12.  创建以下输入和输出向量:

```
x2 = 1-np.array(x_new[:num_images])/255
x2 = x2.reshape(x2.shape[0],x2.shape[1],x2.shape[2],1)
y2 = np.array(y2[:num_images])
input_lengths = input_lengths[:num_images]
label_lengths = label_lengths[:num_images]
```

13.  使模型适合多批图片，如以下代码所示:

```
import random

for i in range(100):
     samp=random.sample(range(x2.shape[0]-100),32)
     x3=[x2[i] for i in samp]
     x3 = np.array(x3)
     y3 = [y2[i] for i in samp]
     y3 = np.array(y3)
     input_lengths2 = [input_lengths[i] for i in samp]
     label_lengths2 = [label_lengths[i] for i in samp]
     input_lengths2 = np.array(input_lengths2)
     label_lengths2 = np.array(label_lengths2)
     inputs = {
     'the_input': x3,
     'the_labels': y3,
     'input_length': input_lengths2,
     'label_length': label_lengths2,
     }
     outputs = {'ctc': np.zeros([32])}
     model.fit(inputs, outputs,batch_size = 32, epochs=1, verbose =2)
```

在前面的代码中，我们一次采样 32 张图片，将它们转换为一个数组，并拟合模型以确保 CTC 损失为零。

请注意，我们排除了最后 100 张图片(在`x2`中)作为模型的输入，这样我们就可以测试我们的模型在这些数据上的准确性。

此外，我们对整个数据集进行了多次循环，因为将所有图片提取到 RAM 中并将其转换为数组很可能会使系统崩溃，这是由于巨大的内存需求。

递增时期的训练损失如下:

![](Images/65786b6a-f385-4f80-a721-9e7111c729f5.png)

14.  使用以下代码预测测试图片在每个时间的输出:

```
model2 = Model(inputs = input_data, outputs = y_pred)
pred = model2.predict(x2[-5].reshape(1,32,128,1))

pred2 = np.argmax(pred[0,:],axis=1)
out = ""
for i in pred2:
  if(i==79):
    continue
  else:
    out += charList[i]
plt.imshow(x2[k].reshape(32,128))
plt.title('Predicted word: '+out)
plt.grid('off')
```

在前面的代码中，如果某个时间步长的预测字符是字符 79，我们将丢弃输出。

一个测试例子及其相应的预测(在标题中)如下:

![](Images/3841110b-83d4-4e17-a9ed-1a43606a7f33.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 图像字幕生成

在之前的案例研究中，我们学习了如何同时使用 CNN、RNN 和 CTC loss 来转录手写数字。

在这个案例研究中，我们将学习如何集成 CNN 和 RNN 架构来为给定图片生成字幕。

这是一张图片的样本:

![](Images/d236b475-d8ec-405c-bc4a-712208d8b564.png)

*   一个穿着红色连衣裙的女孩，背景是圣诞树
*   一个女孩正在展示圣诞树
*   一个女孩正在公园里玩
*   一个女孩正在庆祝圣诞节

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 做好准备

在这一节中，让我们列出我们将采用的转录图片的策略:

*   我们将通过处理一个包含图像以及与图像相关联的标题的数据集来从图片中生成标题。GitHub 中相应的笔记本提供了包含图像及其相应标题的数据集链接。
*   我们将提取每张图片的 VGG16 特征。
*   我们还将预处理标题文本:
    *   将所有单词转换成小写
    *   删除标点符号
    *   为每个标题添加开始和结束标记

*   只保留一只狗或一个女孩的图片(我们进行这种分析只是为了更快地训练我们的模型，因为即使在 GPU 上运行这个模型也需要大约 5 个小时)。
*   为标题词汇表中的每个独特单词指定一个索引。
*   填充所有标题(其中每个单词由一个索引值表示),以便所有标题现在大小相同。
*   为了预测第一个字，该模型将采用 VGG16 特征和开始标记嵌入的组合。
*   类似地，为了预测第二个单词，该模型将采用 VGG16 特征的组合以及起始令牌和第一个单词的嵌入组合。
*   以类似的方式，我们继续获取所有预测的单词。
*   我们继续前面的步骤，直到我们预测结束令牌。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 怎么做...

我们将对之前定义的策略进行编码，如下所示(代码文件在 GitHub 中以`Image_captioning.ipynb`的形式提供):

1.  下载并导入包含图像及其相应标题的数据集。GitHub 中提供了推荐的数据集

2.  导入相关的包，如下所示:

```
import glob
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
import pickle
from tqdm import tqdm
import pandas as pd
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector, merge, Activation, Flatten
from keras.optimizers import Adam, RMSprop
from keras.layers.wrappers import Bidirectional
from keras.applications.inception_v3 import InceptionV3
from keras.preprocessing import image
import nltk
```

3.  加载标题数据集，如以下代码所示:

```
caption_file = '...'
captions = open(caption_file, 'r').read().strip().split('\n')
d = {}
for i, row in enumerate(captions):
     row = row.split('\t')
     row[0] = row[0][:len(row[0])-2]
     if row[0] in d:
         d[row[0]].append(row[1])
     else:
         d[row[0]] = [row[1]]
total_images = list(d.keys())
```

4.  加载图片并存储 VGG16 功能:

```
image_path = '...'
from keras.applications.vgg16 import VGG16
vgg16=VGG16(include_top=False, weights='imagenet', input_shape=(224,224,3))

import cv2
x = []
y = []
x2 = []
tot_images = ''
for i in range(len(d.keys())):
     for j in range(len(d[total_images[i]])):
         img_path = image_path+total_images[i]
         img = cv2.imread(img_path)
         try:
             img2 = cv2.resize(img, (224, 224))/255
             img3 = vgg16.predict(img2.reshape(1,224,224,3))
             x.append(img3)
             y.append(d[total_images[i]][j])
             tot_images = tot_images + ' '+total_images[i]
         except:
             continue
```

5.  将 VGG16 功能转换为 NumPy 数组:

```
x = np.array(x)
x = x.reshape(x.shape[0],7,7,512)
```

6.  构建一个函数，删除标题中的标点符号，并将所有单词转换为小写:

```
def preprocess(text):
     text=text.lower()
     text=re.sub('[^0-9a-zA-Z]+',' ',text)
     words = text.split()
     words2 = words
     words4=' '.join(words2)
     return(words4)
```

在下面的代码中，我们预处理了所有的标题，并附加了开始和结束标记:

```
caps = []
for key, val in d.items():
     if(key in img_path2):
         for i in val:
             i = preprocess(i)
             caps.append('<start> ' + i + ' <end>')
```

7.  仅附加孩子或狗的图片:

```
caps2 = []
x2 = []
img_path3 = []
for i in range(len(caps)):
     if (('girl') in caps[i]):
         caps2.append(caps[i])
         x2.append(x[i])
         img_path2.append(img_path[i])
     elif 'dog' in caps[i]:
         caps2.append(caps[i])
         x2.append(x[i])
         img_path2.append(img_path[i])
     else:
         continue
```

8.  提取标题中所有独特的单词，如下所示:

```
words = [i.split() for i in caps2]
unique = []
for i in words:
     unique.extend(i)
unique = list(set(unique))
vocab_size = len(unique)
```

9.  将索引分配给词汇表中的单词，如以下代码所示:

```
word2idx = {val:(index+1) for index, val in enumerate(unique)}
idx2word = {(index+1):val for index, val in enumerate(unique)}
```

10.  确定标题的最大长度，以便我们将所有标题填充为相同的长度:

```
max_len = 0
for c in caps:
     c = c.split()
     if len(c) > max_len:
         max_len = len(c)
```

11.  填充所有标题，使其长度相同，如下所示:

```
n = np.zeros(vocab_size+1)
y = []
y2 = []
for k in range(len(caps2)):
     t= [word2idx[i] for i in caps2[k].split()]
     y.append(len(t))
     while(len(t)<max_len):
         t.append(word2idx['<end>'])
     y2.append(t)
```

12.  构建将图片作为输入并从中创建要素的模型:

```
from keras.layers import Input
embedding_size = 300
inp = Input(shape=(7,7,512))
inp1 = Conv2D(512, (3,3), activation='relu')(inp)
inp11 = MaxPooling2D(pool_size=(2, 2))(inp1)
inp2 = Flatten()(inp11)
img_emb = Dense(embedding_size, activation='relu') (inp2)
img_emb2 = RepeatVector(max_len)(img_emb)
```

13.  构建一个将标题作为输入并从中创建要素的模型:

```
inp2 = Input(shape=(max_len,))
cap_emb = Embedding((vocab_size+1), embedding_size, input_length=max_len) (inp2)
cap_emb2 = LSTM(256, return_sequences=True)(cap_emb)
cap_emb3 = TimeDistributed(Dense(300)) (cap_emb2)
```

14.  将两个模型连接起来，得出所有可能输出单词的软最大概率:

```
final1 = concatenate([img_emb2, cap_emb3])
final2 = Bidirectional(LSTM(256, return_sequences=False))(final1)
final3 = Dense(vocab_size+1)(final2)
final4 = Activation('softmax')(final3)

final_model = Model([inp, inp2], final4)
```

15.  编译模型，如下所示:

```
from keras.optimizers import Adam
adam = Adam(lr = 0.0001)
final_model.compile(loss='categorical_crossentropy', optimizer = adam, metrics=['accuracy'])
```

16.  拟合模型，如以下代码所示:

```
for i in range(500):
     x3 = []
     x3_sent = []
     y3 = []
     shortlist_y = random.sample(range(len(y)-100),32)
     for j in range(len(shortlist_y)):
         for k in range(y[shortlist_y[j]]-1):
             n = np.zeros(vocab_size+1) 
             x3.append(x2[shortlist_y[j]])
             pad_sent = pad_sequences([y2[shortlist_y[j]][:(k+1)]], maxlen=max_len, padding='post')
             x3_sent.append(pad_sent)
             n[y2[shortlist_y[j]][(k+1)]] = 1
             y3.append(n)
             x3 = np.array(x3)
             x3_sent =np.array(x3_sent)
             x3_sent = x3_sent.reshape(x3_sent.shape[0], x3_sent.shape[2])
             y3 = np.array(y3) 
             final_model.fit([x3/12, x3_sent], y3, batch_size = 32, epochs = 3, verbose = 1)
```

在前面的代码中，我们一次循环 32 张图片。此外，我们以这样的方式创建输入数据集，即字幕中第一个 *n 个*输出单词与图片的 VGG16 特征一起输入，对应的输出是字幕的第 *n+1 个^第个*单词。

此外，我们将 VGG16 特性(`x3`)除以 12，因为我们需要在零和一之间调整输入值。

17.  样本图片的输出字幕可以如下获得:

```
l=-25
im_path = image_path+ img_path3[l]
img1 = cv2.imread(im_path)
plt.imshow(img1)
```

![](Images/4344dfc9-c830-4c95-abbe-a425403cb3d0.jpg)

输出解码如下:

```
p = np.zeros(max_len)
p[0] = word2idx['<start>']
for i in range(max_len-1):
     pred= final_model.predict([x33[l].reshape(1,7,7,512)/12, p.reshape(1,max_len)])
     pred2 = np.argmax(pred)
     print(idx2word[pred2])
     p[i+1] = pred2
     if(idx2word[pred2]=='<end>'):
         break
```

上述代码的输出如下:

![](Images/6ef9c565-3bcb-4d8b-bc54-4776d13e1022.png)

请注意，生成的标题正确地检测到狗是黑色的，并且也在跳跃。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 使用波束搜索生成字幕

在前面关于字幕生成的章节中，我们已经根据在给定时间步长内概率最高的单词进行了解码。在这一节中，我们将通过使用波束搜索来改进预测字幕。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 做好准备

波束搜索的工作方式如下:

*   在第一时间步中提取各种单词的概率(其中图片的 VGG16 特征和开始标记是输入)
*   我们将考虑前三个最有可能的单词，而不是提供最有可能的单词作为输出
*   我们将进入下一个时间步骤，在这个时间步骤中，我们提取前三个字符
*   我们将遍历第一时间步中的前三个预测，作为第二时间步预测的输入，并为输入中可能的前三个预测中的每一个提取前三个预测:
    *   假设 *a* 、 *b* 和 *c* 是时间步长一中的前三个预测
    *   我们将使用 *a* 和 VGG16 特征作为输入来预测时间步长 2 中的前三个可能的角色，对于 *b* 和 *c* 也是如此
    *   在第一时间步和第二时间步之间，我们总共有九种输出组合
    *   除了该组合，我们还将存储所有九个组合中每个预测的置信度:
        *   例如:如果时间步长一中 *a* 的概率为 0.4，时间步长二中 *x* 的概率为 0.5，那么组合的概率为 0.4 x 0.5 = 0.2
    *   我们将保留前三个组合，并丢弃其余的组合
*   我们将重复前面列出前三个组合的步骤，直到句子结束

值 3 是我们在其上搜索组合的波束长度。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 怎么做...

在本节中，我们将编写之前讨论过的波束搜索策略(代码文件在 GitHub 中以`Image_captioning.ipynb`的形式提供):

1.  定义一个函数，该函数将图片的 VGG16 特征作为输入，连同来自先前时间步长的单词序列及其相应的置信度，并返回当前时间步长中的前三个预测:

```
beamsize = 3
def get_top3(img, string_with_conf):
     tokens, confidence = string_with_conf
     p = np.zeros((1, max_len))
     p[0, :len(tokens)] = np.array(tokens)
     pred = final_model.predict([img.reshape(1,7,7,512)/12, p])
     best_pred = list(np.argsort(pred)[0][-beamsize:])
     best_confs = list(pred[0,best_pred])
     top_best = [(tokens + list([best_pred[i]]), confidence*best_confs[i]) for i in range(beamsize)]
     return top_best
```

在前面的步骤中，我们正在分离单词 id 和它们在`string_with_conf`参数中提供的相应置信度。此外，我们将令牌序列存储在一个数组中，并使用它来进行预测。

在下一步中，我们将提取下一时间步中的前三个预测，并将其存储在`best_pred`中。

此外，除了单词 id 的最佳预测，我们还存储了与当前时间步长中的前三个预测相关联的置信度。

最后，我们返回第二时间步的三个预测。

2.  在句子的最大可能长度范围内循环，并提取所有时间步长的前三个可能的单词组合:

```
start_token = word2idx['<start>']
best_strings = [([start_token], 1)]
for i in range(max_len):
     new_best_strings = []
     for string in best_strings:
         strings = get_top3(x33[l], string)
         new_best_strings.extend(strings) 
         best_strings = sorted(new_best_strings, key=lambda x: x[1], reverse=True)[:beamsize]
```

3.  通过前面获得的`best_strings`循环打印输出:

```
for i in range(3):
     string = best_strings[i][0]
     print('============')
     for j in string:
         print(idx2word[j])
         if(idx2word[j]=='<end>'):
             break
```

我们在上一节中测试的同一张图片的输出句子如下:

![](Images/ec9b27db-9d6e-46a2-9716-f0b4e19d595b.png)![](Images/1448ba62-e00b-4a79-a24b-23d879f650c1.png)

请注意，在这个特定的案例中，当涉及到单词`jumping`和`playing`时，第一句和第二句是不同的，而第三句恰好与第一句相同，因为组合的概率要高得多。