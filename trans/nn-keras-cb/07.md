        

# 七、自动驾驶汽车中的图像分析应用

在前几章中，我们学习了对象分类和对象定位。在这一章中，我们将浏览与自动驾驶汽车相关的多个案例研究。

您将了解以下内容:

*   交通标志识别
*   预测汽车需要转弯的角度
*   使用 U-net 架构识别道路上的车辆
*   道路上物体的语义分割

        

# 交通标志识别

在本案例研究中，我们将了解如何将信号分为 43 个可能的类别。

        

# 做好准备

对于本练习，我们将采用以下策略:

1.  下载包含所有可能的交通标志的数据集
2.  在输入图像上执行直方图归一化:
    *   某些照片是在白天拍摄的，而另一些可能是在黄昏时拍摄的
    *   不同的照明条件会导致像素值的变化，这取决于拍摄照片时的照明条件
    *   直方图规范化对像素值执行规范化，以便它们都具有相似的分布
3.  缩放输入图像
4.  构建、编译并拟合一个模型，以降低分类交叉熵损失值

        

# 怎么做...

1.  下载数据集，如下(代码文件在 GitHub 中以`Traffic_signal_detection.ipynb`的形式提供)。该数据集可通过论文获得:J. Stallkamp，M. Schlipsing，J. Salmen，C. Igel，Man vs . computer:benchmark machine learning algorithms for traffic sign recognition:

```py
$ wget http://benchmark.ini.rub.de/Dataset/GTSRB_Final_Training_Images.zip
$ unzip GTSRB_Final_Training_Images.zip
```

2.  将图像路径读入列表，如下所示:

```py
from skimage import io
import os
import glob

root_dir = '/content/GTSRB/Final_Training/Images/'
all_img_paths = glob.glob(os.path.join(root_dir, '*/*.ppm'))
```

图像示例如下所示:

![](Images/12bf31d5-f7a6-4255-afeb-78f1abe0d861.png)

注意，某些图像与其他图像相比具有较小的形状，并且某些图像与其他图像相比具有更多的光照。因此，我们必须对图像进行预处理，这样所有的图像都可以根据光照和形状进行归一化。

3.  对输入数据集执行直方图归一化，如下所示:

```py
import numpy as np
from skimage import color, exposure, transform

NUM_CLASSES = 43
IMG_SIZE = 48

def preprocess_img(img):
     hsv = color.rgb2hsv(img)
     hsv[:, :, 2] = exposure.equalize_hist(hsv[:, :, 2])
     img = color.hsv2rgb(hsv)
     img = transform.resize(img, (IMG_SIZE, IMG_SIZE))
     return img
```

在前面的代码中，我们首先将 RGB 格式的图像转换为**色调饱和度值(HSV)** 格式。通过将图像从 RGB 转换为 HSV 格式，我们实质上是将组合的 RGB 值转换为一个数组，然后该数组可以转换为一维数组。

在那之后，我们正通过使用`equalize_hist`方法将 HSV 格式中获得的值标准化，以便它们属于相同的范围。

一旦图像在 HSV 格式的最后一个通道中被标准化，我们就将它们转换回 RGB 格式。

最后，我们将图像调整到标准大小。

4.  在通过直方图归一化之前检查图像，并与直方图归一化后的图像进行对比(通过`preprocess_img`函数后传递图像)，如下所示:

![](Images/c4e7a40e-ffcd-451d-9a24-7547c009ff94.png) ![](Images/548e4913-54ac-4117-814e-c8b5207d0392.png)

从前面的图片中，我们可以看到直方图归一化后(右图)，图像(左图)的可见性发生了相当大的变化。

5.  准备输入和输出数组，如下所示:

```py
count = 0
imgs = []
labels = []
for img_path in all_img_paths:
     img = preprocess_img(io.imread(img_path))
     label = img_path.split('/')[-2]
     imgs.append(img)
     labels.append(label)

X = np.array(imgs)
Y = to_categorical(labels, num_classes = NUM_CLASSES)
```

6.  构建训练和测试数据集，如下所示:

```py
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state= 42)
```

7.  构建并编译模型，如下所示:

```py
model = Sequential()
model.add(Conv2D(32, (3, 3), padding='same',input_shape=(IMG_SIZE, IMG_SIZE, 3), activation='relu'))
model.add(Conv2D(32, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(128, (3, 3), padding='same',activation='relu'))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(NUM_CLASSES, activation='softmax'))
model.summary()

model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']
```

该模型的摘要如下:

![](Images/23d3c55c-1505-4cf1-a1b8-10214e8d9207.png)

8.  拟合模型，如下所示:

```py
model.fit(X_train, y_train,batch_size=32,epochs=5,validation_data = (X_test, y_test))
```

前面的代码生成了一个准确度约为 99%的模型:

![](Images/fe909540-e0d5-466a-bd5c-bfc9597710c9.png)

此外，如果您像我们一样执行完全相同的分析，但没有直方图归一化(校正曝光)，模型的准确性约为 97%。

        

# 预测汽车需要转弯的角度

在本案例研究中，我们将根据提供的图像了解汽车需要转向的角度。

        

# 做好准备

我们采用的建立转向角预测的策略如下:

1.  收集包含道路图像和相应转向角度的数据集
2.  预处理图像

3.  将图像通过 VGG16 模型来提取特征
4.  建立执行回归的神经网络来预测转向角，该转向角是待预测的连续值

        

# 怎么做...

1.  下载以下数据集。该数据集可从以下链接获得:[https://github.com/SullyChen/driving-datasets](https://github.com/SullyChen/driving-datasets):(代码文件在 GitHub 中可作为`Car_steering_angle_detection.ipynb`获得):

```py
$ pip install PyDrive 

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

file_id = '0B-KJCaaF7elleG1RbzVPZWV4Tlk' # URL id. 
downloaded = drive.CreateFile({'id': file_id})
downloaded.GetContentFile('steering_angle.zip')

$ unzip steering_angle.zip
```

2.  导入相关的包，如下所示:

```py
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import pi
import cv2
import scipy.misc
import tensorflow as tf
```

3.  将图像及其相应的弧度角度读入单独的列表，如下所示:

```py
DATA_FOLDER = "/content/driving_dataset/"
DATA_FILE = os.path.join(DATA_FOLDER, "data.txt")
x = []
y = []

train_batch_pointer = 0
test_batch_pointer = 0

with open(DATA_FILE) as f:
     for line in f:
         image_name, angle = line.split() 
         image_path = os.path.join(DATA_FOLDER, image_name)
         x.append(image_path) 
         angle_radians = float(angle) * (pi / 180) #converting angle into radians
         y.append(angle_radians)
y = np.array(y)
```

4.  创建训练和测试数据集，如下所示:

```py
split_ratio = int(len(x) * 0.8)
train_x = x[:split_ratio]
train_y = y[:split_ratio]
test_x = x[split_ratio:]
test_y = y[split_ratio:]
```

5.  检查训练和测试数据集中的输出标签值，如下所示:

```py
fig = plt.figure(figsize = (10, 7))
plt.hist(train_y, bins = 50, histtype = "step",color='r')
plt.hist(test_y, bins = 50, histtype = "step",color='b')
plt.title("Steering Wheel angle in train and test")
plt.xlabel("Angle")
plt.ylabel("Bin count")
plt.grid('off')
plt.show()
```

![](Images/40a861cd-fd88-473f-a389-2d38d2e18b25.png)

6.  删除前 100 行中的像素，因为它们与道路图像不对应，然后将结果图像传递给 VGG16 模型。此外，在本练习中，我们将只处理数据集中的前 10，000 幅图像，以便能够更快地构建模型。删除前 100 行中的像素，如下所示:

```py
x = []
y = []
for i in range(10000):
     im = cv2.imread(train_x[i])
     im = im[100:,:,:]/255
     vgg_im = vgg16_model.predict(im.reshape(1,im.shape[0],im.shape[1],3))
     x.append(vgg_im)
     y.append(train_y[i])
x1 = np.array(x)
x1 = x1.reshape(x1.shape[0],4,14,512)
y1 = np.array(y)
```

7.  构建并编译模型，如下所示:

```py
model = Sequential()
model.add(Flatten(input_shape=(4,14,512)))
model.add(Dense(512, activation='relu'))
model.add(Dropout(.5))
model.add(Dense(100, activation='linear'))
model.add(Dropout(.2))
model.add(Dense(50, activation='linear'))
model.add(Dropout(.1))
model.add(Dense(10, activation='linear'))
model.add(Dense(1, activation='linear'))
model.summary()
```

请注意，输出层具有线性激活，因为输出是范围从-9 到+9 的连续值。该模型的摘要如下:

![](Images/edb25ad9-1735-49e8-8eac-039a1b89c67e.png)

现在，我们将编译我们定义的模型，如下所示:

```py
model.compile(loss='mean_squared_error',optimizer='adam')
```

8.  拟合模型，如下所示:

```py
model.fit(x1/11, y1,batch_size=32,epochs=10, validation_split = 0.1, verbose = 1)
```

![](Images/971589a3-0d22-4c0f-8fa9-30f7742401d4.png)

测试损耗是上图中损耗较低的线。

请注意，我们已经将输入数据集除以 11，这样我们就可以将其缩放为 0 到 1 之间的值。现在，我们应该能够根据预测的角度来模拟汽车的运动。

由图像样本的模型获得的转向角预测如下:

![](Images/6d08b8a4-ce50-464a-9b5d-46afa3198668.png)

![](Images/342b5540-0861-4d72-b58c-908420c84c6f.png)

请注意，在采用类似前面的模型并实现它时，您应该非常小心。在最终投入生产之前，应该首先在多种日光条件下进行测试。

        

# 使用 U-net 架构的实例分割

到目前为止，在前两章中，我们已经学习了如何检测对象，以及如何识别图像中对象所在的边界框。在本节中，我们将了解如何执行实例分割，其中属于某个对象的所有像素都将高亮显示，而其他所有像素都不会高亮显示(这类似于用 0 屏蔽不属于某个对象的所有其他像素，并用像素值 1 屏蔽属于该对象的像素)。

        

# 做好准备

为了执行实例分段，我们将执行以下操作:

1.  处理包含输入图像和图像中对象所在像素的相应掩膜图像的数据集:
    *   图像及其遮罩图像
2.  我们将通过预训练的 VGG16 模型传递图像，以提取每个卷积层的特征
3.  我们将逐渐对卷积层进行上采样，这样我们得到的输出图像的形状为 224 x 224 x 3
4.  我们将冻结使用 VGG16 权重的层
5.  将上采样卷积层与下采样卷积层连接起来:
    *   这形成了 U 形连接
    *   U 形连接以类似于 ResNet 的方式帮助模型具有上下文(除了上采样层之外，先前的下采样层还提供上下文)
    *   如果我们采用第一层的输出，重建图像会容易得多，因为第一层中的大部分图像是完整的(早期的层学习轮廓)。如果我们试图通过对最后几层进行上采样来重建图像，则很可能会丢失图像的大部分信息
6.  拟合将输入图像映射到遮罩图像的模型:
    *   请注意，被遮罩的图像本质上是二进制的，其中黑色值对应于像素值 0，而白色像素值为 1
7.  最小化所有 224×224×1 像素的二进制交叉熵损失函数

这个模型被称为 **U-net 架构**的原因是因为模型的可视化看起来如下——一个旋转的 U 型结构:

![](Images/b4e97c5a-c390-4983-92f7-c9da19ef8318.png)

该模型的 U 型结构是由于早期层连接到下采样层的上采样版本。

        

# 怎么做...

在下面的代码中，我们将执行实例分割来检测图像中的汽车:

1.  从[https://github.com/divamgupta/image-segmentation-keras](https://github.com/divamgupta/image-segmentation-keras)下载并导入文件，如下所示:

```py
$ wget https://www.dropbox.com/s/0pigmmmynbf9xwq/dataset1.zip
$ unzip dataset1.zip
dir_data = "/content/dataset1"
dir_seg = dir_data + "/annotations_prepped_train/"
dir_img = dir_data + "/images_prepped_train/"
import glob, os
all_img_paths = glob.glob(os.path.join(dir_img, '*.png'))
all_mask_paths = glob.glob(os.path.join(dir_seg, '*.png'))
```

2.  将图像及其对应的遮罩读入数组，如下所示:

```py
import cv2
from scipy import ndimage
x = []
y = []
for i in range(len(all_img_paths)):
  img = cv2.imread(all_img_paths[i])
  img = cv2.resize(img,(224,224))
  mask_path = dir_seg+all_img_paths[i].split('/')[4]
  img_mask = ndimage.imread(mask_path)
  img_mask = cv2.resize(img_mask,(224,224))
  x.append(img)
  y.append(img_mask)

x = np.array(x)/255
y = np.array(y)/255
y2 = np.where(y==8,1,0)
```

在前面的步骤中，我们已经创建了输入和输出数组，并对输入数组进行了规范化。最后，我们将汽车的遮罩与其他事物分开，因为该数据集有 12 个唯一的类，其中汽车被像素值为 8 的遮罩。

输入和遮罩图像的示例如下:

![](Images/1c1e655a-3c45-45ce-bfda-8863dbc5dbf8.png)

此外，我们还创建了输入和输出数组，其中我们对输入数组进行了缩放，并对输出数组进行了整形(以便它可以传递到网络)，如下所示:

```py
x = np.array(x)
x = x/255
y2 = np.array(y2)
y2 = y2.reshape(y2.shape[0],y2.shape[1],y2.shape[2],1)
```

4.  构建模型，其中图像首先通过 VGG16 模型层，然后提取卷积特征，如下所示:

在下面的代码中，我们将导入预训练的 VGG16 模型:

```py
from keras.applications.vgg16 import VGG16 as PTModel
from keras.layers import Input, Conv2D, concatenate, UpSampling2D, BatchNormalization, Activation, Cropping2D, ZeroPadding2D
from keras.layers import Input, merge, Conv2D, MaxPooling2D,UpSampling2D, Dropout, Cropping2D, merge, concatenate
from keras.optimizers import Adam
from keras.callbacks import ModelCheckpoint, LearningRateScheduler
from keras import backend as K
from keras.models import Model
```

```py
base_pretrained_model = PTModel(input_shape = (224,224,3), include_top = False, weights = 'imagenet')
base_pretrained_model.trainable = False
```

在以下代码中，提取了通过 VGG16 模型时各种卷积层的特征:

```py
conv1 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block1_conv2').output).output
conv2 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block2_conv2').output).output
conv3 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block3_conv3').output).output
conv4 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block4_conv3').output).output
drop4 = Dropout(0.5)(conv4)
conv5 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block5_conv3').output).output
drop5 = Dropout(0.5)(conv5)
```

在以下代码中，我们使用`UpSampling`方法放大了特征，然后在每一层与缩小的 VGG16 卷积特征连接:

```py
up6 = Conv2D(512, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(drop5))
merge6 = concatenate([drop4,up6], axis = 3) 

conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge6)
conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv6)
conv6 = BatchNormalization()(conv6)
up7 = Conv2D(256, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv6))
merge7 = concatenate([conv3,up7], axis = 3)
conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge7)
conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv7)
conv7 = BatchNormalization()(conv7)
up8 = Conv2D(128, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv7))
merge8 = concatenate([conv2,up8],axis = 3)
conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge8)
conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv8)
conv8 = BatchNormalization()(conv8)
up9 = Conv2D(64, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv8))
merge9 = concatenate([conv1,up9], axis = 3)
conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge9)
conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv9)
conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv9)
conv9 = BatchNormalization()(conv9)
conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)
```

在下面的代码中，我们定义了模型的输入和输出，其中输入首先传递给`base_pretrained_model`，输出是`conv10`(其形状为 224 x 224 x 1，这是我们输出的预期形状):

```py
model = Model(input = base_pretrained_model.input, output = conv10)
```

5.  冻结从训练的 VGG16 模型的乘法中获得的卷积层，如下所示:

```py
for layer in model.layers[:18]:
     layer.trainable = False
```

6.  为数据集中的前 1，000 幅图像编译并拟合模型，如下所示:

```py
from keras import optimizers
adam = optimizers.Adam(1e-3, decay = 1e-6)
model.compile(loss='binary_crossentropy',optimizer=adam,metrics=['accuracy'])
history = model.fit(x,y,validation_split = 0.1,batch_size=1,epochs=5,verbose=1)
```

![](Images/3027213c-790c-4e63-8d37-c02db987f53a.png)

7.  在测试图像上测试前面的模型(我们数据集的最后 2 个图像，它们是具有`validtion_split = 0.1`的测试图像)，如下所示:

```py
y_pred = model.predict(x[-2:].reshape(2,224,224,3))
```

![](Images/35f3e4be-1c52-4657-a157-aaf44df7d756.png)

我们可以看到，对于给定的道路输入，生成的掩膜是真实的，并且在某种程度上比我们之前所做的要好，因为预测的掩膜图像中不存在噪声点。

        

# 图像中对象的语义分割

在上一节中，我们学习了如何在只包含一个对象的图像上执行分割。在这个分段中，我们将学习如何执行分段，以便能够区分道路图像中的多个对象。

        

# 做好准备

我们将采用以下策略对道路图像进行语义分割:

1.  收集一个数据集，该数据集具有图像中多个对象所在位置的注记:
    *   语义图像的示例如下:

![](Images/9bee9bc6-479d-41cb-a8f7-25a68aa90b73.png)

2.  将输出掩码转换为多维数组，其中列的数量与所有可能的唯一对象的数量一样多
3.  如果有 12 个可能的唯一值(12 个唯一对象)，则将输出图像转换为形状为 224 x 224 x 12 的图像:
    *   通道的值表示对应于该通道的对象存在于图像的该位置
4.  利用我们在前面章节中看到的模型架构来训练具有 12 个可能输出值的模型
5.  通过将所有三个通道指定为具有相同的输出，将预测整形为三个通道:
    *   输出是 12 个可能类别的概率预测的 argmax

        

# 怎么做...

代码中的语义分割执行如下(代码文件在 GitHub 中以`Semantic_segmentation.ipynb`的形式提供):

1.  下载数据集，如下所示:

```py
!wget https://www.dropbox.com/s/0pigmmmynbf9xwq/dataset1.zip
!unzip dataset1.zip
dir_data = "/content/dataset1"
dir_seg = dir_data + "/annotations_prepped_train/"
dir_img = dir_data + "/images_prepped_train/"
import glob, os
all_img_paths = glob.glob(os.path.join(dir_img, '*.png'))
all_mask_paths = glob.glob(os.path.join(dir_seg, '*.png'))
```

2.  将图像及其相应的标签读入单独的列表，如下所示:

```py
import cv2
from scipy import ndimage
for i in range(len(all_img_paths)):
     img = cv2.imread(all_img_paths[i])
     img = cv2.resize(img,(224,224))
     mask_path = dir_seg+all_img_paths[i].split('/')[4]
     img_mask = ndimage.imread(mask_path)
     img_mask = cv2.resize(img_mask,(224,224))
     x.append(img)
     y.append(img_mask)
```

3.  定义一个函数，将三个通道的输出图像转换为 12 个通道，其中有 12 个唯一的输出值:
    1.  提取输出中存在的唯一值(对象)的数量，如下所示:

```py
n_classes = len(set(np.array(y).flatten()))
```

```py
def getSegmentationArr(img):
      seg_labels = np.zeros(( 224, 224, n_classes ))
      for c in range(n_classes):
            seg_labels[: , : , c ] = (img == c ).astype(int)
      return seg_labels

y2 = []
for i in range(len(y)):
     y2.append(getSegmentationArr(y[i]))

y2 = np.array(y2)
x = x/255
```

4.  构建模型:
    1.  将图像通过预训练的 VGG16 模型，如下所示:

```py
from keras.applications.vgg16 import VGG16 as PTModel
base_pretrained_model = PTModel(input_shape = (224,224,3), include_top = False, weights = 'imagenet')
base_pretrained_model.trainable = False
```

```py
conv1 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block1_conv2').output).output
conv2 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block2_conv2').output).output
conv3 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block3_conv3').output).output
conv4 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block4_conv3').output).output
drop4 = Dropout(0.5)(conv4)
conv5 = Model(inputs=base_pretrained_model.input,outputs=base_pretrained_model.get_layer('block5_conv3').output).output
drop5 = Dropout(0.5)(conv5)
```

```py
conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge6)
conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv6)
conv6 = BatchNormalization()(conv6)
up7 = Conv2D(256, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv6))
merge7 = concatenate([conv3,up7], axis = 3)
conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge7)
conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv7)
conv7 = BatchNormalization()(conv7)
up8 = Conv2D(128, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv7))
merge8 = concatenate([conv2,up8],axis = 3)
conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge8)
conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv8)
conv8 = BatchNormalization()(conv8)
up9 = Conv2D(64, 2, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(UpSampling2D(size =(2,2))(conv8))
merge9 = concatenate([conv1,up9], axis = 3)
conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(merge9)
conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv9)
conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same',kernel_initializer = 'he_normal')(conv9)
conv9 = BatchNormalization()(conv9)
conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)

model = Model(input = base_pretrained_model.input, output = conv10)
```

5.  冻结 VGG16 层，如下所示:

```py
for layer in model.layers[:18]:
     layer.trainable = False
```

6.  编译并拟合模型，如下所示:

```py
model.compile(optimizer=Adam(1e-3, decay = 1e-6), 
 loss='categorical_crossentropy', metrics = ['accuracy'])

history = model.fit(x,y2,epochs=15,batch_size=1,validation_split=0.1)
```

![](Images/cae3a4fb-af64-4933-b45b-f0ace18dc9c3.png)

7.  对测试图像进行预测，如下所示:

```py
y_pred = model.predict(x[-2:].reshape(2,224,224,3))
y_predi = np.argmax(y_pred, axis=3)
y_testi = np.argmax(y2[-2:].reshape(2,224,224,12), axis=3)

import matplotlib.pyplot as plt
%matplotlib inline
plt.subplot(231)
plt.imshow(x[-1])
plt.axis('off')
plt.title('Original image')
plt.grid('off')
plt.subplot(232)
plt.imshow(y[-1])
plt.axis('off')
plt.title('Masked image')
plt.grid('off')
plt.subplot(233)
plt.imshow(y_predi[-1])
plt.axis('off')
plt.title('Predicted masked image')
plt.grid('off')
plt.subplot(234)
plt.imshow(x[-2])
plt.axis('off')
plt.grid('off')
plt.subplot(235)
plt.imshow(y[-2])
plt.axis('off')
plt.grid('off')
plt.subplot(236)
plt.imshow(y_predi[-2])
plt.axis('off')
plt.grid('off')
plt.show()
```

前面的代码生成了一个图像，其中预测的和实际的语义图像如下:

![](Images/9a872688-dd90-47ec-8d9c-675650960d84.png)

从前面的图像中，我们可以看到，我们能够以很高的准确度(对于我们训练的模型，大约 90%)准确地识别图像中的语义结构。