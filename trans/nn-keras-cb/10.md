        

# 使用词向量的文本分析

在前一章中，我们学习了为推荐系统编码图像或编码用户或电影，其中相似的项目具有相似的向量。在本章中，我们将讨论如何对文本数据进行编码。

您将了解以下主题:

*   用 Python 从头开始构建单词向量
*   使用 skip-gram 和 CBOW 模型构建单词向量
*   使用预训练的单词向量执行向量算术
*   创建文档向量
*   使用 fastText 构建单词向量
*   使用 GloVe 构建单词向量
*   使用词向量建立情感分类

        

# 介绍

在解决文本相关问题的传统方法中，我们会对单词进行一次性编码。然而，如果数据集具有数千个唯一的单词，则得到的一个热编码向量将具有数千个维度，这很可能导致计算问题。此外，在这种情况下，相似的单词将不具有相似的向量。Word2Vec 是一种帮助我们实现相似单词的相似向量的方法。

为了理解 Word2Vec 是如何有用的，让我们来探讨下面这个问题。

假设我们有两个输入句子:

![](Images/2ffc33e5-9913-45da-96d2-64b6c54b31b6.png)

直观上，我们知道**享受**和**喜欢**是相近的词。然而，在传统的文本挖掘中，当我们对单词进行一次性编码时，我们的输出如下所示:

![](Images/06066bdf-95ff-41d5-85cb-e1789acc850e.png)

注意，一键编码导致每个单词被分配一列。像这样的单热编码的主要问题是，在 **I** 和 **enjoy** 之间的欧几里德距离与 **enjoy** 和**之间的欧几里德距离相同，就像**一样。

但是，凭直觉，我们知道**享受**和**喜欢**之间的距离应该低于**本人**和**享受**之间的距离，因为**享受**和**喜欢**彼此相似。

        

# 用 Python 从头开始构建单词向量

我们构建单词向量的原则是*相关单词周围有相似的单词*。

比如:*女王*和*公主*这两个词的周围会更频繁地出现类似的词(与一个*王国*有关)。在某种程度上，这些单词的上下文(周围的单词)是相似的。

        

# 做好准备

当我们将周围的单词作为输入，剩余的(中间的)单词作为输出时，我们的数据集(由两个句子组成)看起来如下:

![](Images/d5a3f1d0-5066-49a1-80a8-70c53b0979e8.png)

请注意，我们使用中间的单词作为输出，其余的单词作为输入。这种输入和输出的矢量化形式如下所示(回想一下我们在[第 9 章](18e82d39-d5b2-40fe-ae2f-df222c2e1ffe.xhtml)、*编码输入*中的*文本分析*部分将句子转换成矢量的方法):

![](Images/17715c08-bfa2-4f12-a2dd-c8b30b67a2cc.png)

注意，第一行输入的矢量化形式是 *{0，1，1，1，0}* ，因为输入单词的索引是 *{1，2，3}* ，而输出是 *{1，0，0，0，0}* ，因为输出单词的索引是 *{1}* 。

在这种情况下，我们的隐藏层有三个相关的神经元。我们的神经网络看起来如下:

![](Images/536560c0-abd1-4c24-8e19-5e09c079e68f.png)

每层的尺寸如下:

| **层** | **重物的形状** | **解说** |
| 输入层 | 1 x 5 | 每行乘以五个权重。 |
| 隐蔽层 | 5 x 3 | 隐藏层中的三个神经元各有五个输入权重。 |
| 隐藏层输出 | 1 x 3 | 这是输入和隐藏层的矩阵乘法。 |
| 从隐藏到输出的权重 | 3 x 5 | 三个输出隐藏单元映射到五个输出列(因为有五个唯一的单词)。 |
| 输出层 | 1 x 5 | 这是隐藏层的输出和从隐藏层到输出层的权重之间的矩阵乘法。 |

请注意，我们不会在构建单词向量时在隐藏层的顶部应用激活。

输出图层的值不限于特定范围。因此，我们通过 softmax 函数传递它们，以便我们得到单词的概率。此外，我们最小化交叉熵损失，以获得网络中的最佳权重值。现在，当输入是单词的一次性编码版本(不是输入句子)时，给定单词的单词向量是隐藏层单元值。

        

# 怎么做...

现在我们知道了单词向量是如何生成的，让我们编写生成单词向量的过程(代码文件在 GitHub 中以`Word_vector_generation.ipynb`的形式提供):

1.  定义感兴趣的句子:

```
docs = ["I enjoy playing TT", "I like playing TT"]
```

从前面的内容中，我们应该期望`enjoy`和`like`的单词向量是相似的，因为`enjoy`和`like`周围的单词完全相同。

2.  现在让我们创建每个句子的一键编码版本:

```
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(min_df=0, token_pattern=r"\b\w+\b")
vectorizer.fit(docs)
```

请注意，矢量器定义了将文档转换为矢量格式的参数。此外，我们传入更多的参数，这样像`I`这样的词就不会在`CountVectorizer`中被过滤掉。

此外，我们将使我们的文档适合定义的矢量器。

3.  将文档转换为矢量格式:

```
vector = vectorizer.transform(docs)
```

4.  验证执行的转换:

```
print(vectorizer.vocabulary_)
print(vector.shape)
print(vector.toarray())
```

![](Images/5a5b6dc6-d127-48fa-b65e-75253bde3f66.png)

注意，`vocabulary_`返回各种单词的索引，转换`toarray`向量返回句子的一键编码版本。

5.  创建输入和输出数据集:

```
x = []
y = []
for i in range(len(docs)):
     for j in range(len(docs[i].split())):
         t_x = []
         t_y = []
         for k in range(4):
             if(j==k):
                 t_y.append(docs[i].split()[k])
                 continue
             else:
                 t_x.append(docs[i].split()[k])
         x.append(t_x)
         y.append(t_y)

x2 = []
y2 = []
for i in range(len(x)):
     x2.append(' '.join(x[i]))
     y2.append(' '.join(y[i]))
```

根据前面的代码，我们已经创建了输入和输出数据集。以下是输入数据集:

![](Images/0e9852c5-b6c2-45ca-a41f-8ad7a8f4a1d2.jpg)

这是输出数据集:

![](Images/7269b7b9-b223-4ba1-b9cf-0103abd330da.png)

6.  将前面的输入和输出字转换成向量:

```
vector_x = vectorizer.transform(x2)
vector_x.toarray()
vector_y = vectorizer.transform(y2)
vector_y.toarray()
```

以下是输入数组:

![](Images/4ac45077-8dbc-41aa-b6b8-5ad81fbd5a88.png)

以下是输出数组:

![](Images/98d56a01-487d-4a36-8ae1-f040f3cf8a1e.png)

7.  定义神经网络模型，该模型使用具有三个单元的隐藏层来映射输入和输出向量:

```
model = Sequential()
model.add(Dense(3, activation='linear', input_shape=(5,)))
model.add(Dense(5,activation='sigmoid'))
```

8.  编译并拟合模型:

```
model.compile(loss='binary_crossentropy',optimizer='adam')

model.fit(vector_x, vector_y, epochs=1000, batch_size=4,verbose=1)
```

9.  通过提取中间层值来提取单词向量，其中输入是每个单词(而不是句子)的向量:

```
from keras.models import Model
layer_name = 'dense_5'
intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)
```

在前面的代码中，我们从我们感兴趣的层中提取输出:在我们初始化的模型中名为`dense_5`的层。

在下面的代码中，当我们将单词的 one-hot-encoded 版本作为输入传递时，我们提取中间层的输出:

```
for i in range(len(vectorizer.vocabulary_)):
     word = list(vectorizer.vocabulary_.keys())[i]
     word_vec = vectorizer.transform([list(vectorizer.vocabulary_.keys())[i]]).toarray()
     print(word, intermediate_layer_model.predict(word_vec))
```

单个单词的单词向量如下:

![](Images/d0f17447-2ac1-4512-b2be-0c821dd11b4e.png)

注意，单词`enjoy`和`like`比其他单词更相关，因此是单词向量的更好表示。

您运行的模型的名称可能不同，因为我们没有在模型构建中指定层名称。此外，当我们没有在层中明确指定模型名称时，每次新运行模型初始化时，层名称都会改变。

        

# 测量单词向量之间的相似度

词向量之间的相似性可以使用多种指标来衡量，以下是两种比较常见的指标:

*   余弦相似性
*   欧几里得距离

两个不同向量 *A* 和 *B* 之间的余弦相似度计算如下:

![](Images/cf0c3808-dfe4-473c-be3a-b0fd82932a7e.png)

在上一节的示例中， *enjoy* 和 *like* 之间的余弦相似度计算如下:

*享受= (-1.43，-0.94，-2.49)*

*喜欢= (-1.43，-0.94，-2.66)*

下面是*享受*和*喜欢*的向量之间的相似性:

*(-1.43 *-1.43+-0.94 *-0.94+-2.49 *-2.66)/sqrt((-1.43)²+(-0.94)²+(-2.49)²)* sqrt((-1.43)^2+(-0.94)^2+(-2.66)^2)= 0.99*

两个不同向量 *A* 和 *B* 之间的欧几里德距离计算如下:

*距离= sqrt(A-B)^2*

*= sqrt((-1.43-(-1.43))^2+(-0.94-(-0.94))^2+(-2.49-(-2.66))^2))*

*= 0.03*

        

# 使用 skip-gram 和 CBOW 模型构建单词向量

在前一个食谱中，我们建立了一个单词向量。在这个菜谱中，我们将使用`gensim`库构建 skip-gram 和 CBOW 模型。

        

# 做好准备

我们在这个菜谱中采用的构建单词向量的方法叫做**连续单词包** ( **CBOW** )模型。其被称为 CBOW 的原因解释如下:

让我们以这句话为例:*我喜欢扮演 TT* 。

CBOW 模型是这样处理这句话的:

1.  固定一个特定大小的窗口—比如说 1。
    *   通过指定窗口大小，我们指定了给定单词左边和右边的单词数。
2.  给定窗口大小，输入和输出向量将如下所示:

| **输入单词** | **输出字** |
| *{我，在玩}* | *{享受}* |
| *{享受吧，TT}* | *{播放}* |

构建单词向量的另一种方法是跳格模型，其中前面的步骤是相反的，如下所示:

| **输入单词** | **输出字** |
| *{享受}* | *{我，在玩}* |
| *{播放}* | *{享受吧，TT}* |

无论是 skip-gram 模型还是 CBOW 模型，获得单词的隐藏层值的方法与我们在上一节中讨论的方法相同。

        

# 怎么做

既然我们已经了解了如何构建单词向量的后端，让我们使用 skip-gram 和 CBOW 模型来构建单词向量。为了建立模型，我们将使用航空公司情感数据集，其中给出了推文文本，并提供了与推文对应的情感。为了生成单词向量，我们将使用`gensim`包，如下所示(代码文件在 GitHub 中作为`word2vec.ipynb`提供):

1.  安装`gensim`包:

```
$pip install gensim
```

2.  导入相关包:

```
import gensim
import pandas as pd
```

3.  读取航空公司推文情绪数据集，其中包含与航空公司及其相应情绪相关的评论(文本)。数据集可从[https://D1 p 17 r 2m 4 rz LBO . cloudfront . net/WP-content/uploads/2016/03/Airline-sensation-2-w-aa . CSV](https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/Airline-Sentiment-2-w-AA.csv):

```
data=pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv')
data.head()                   
```

数据集的示例如下所示:

![](Images/7cdad718-62b5-41b1-8158-947dd9313b85.png)

4.  预处理前面的文本以执行以下操作:
    *   将每个单词规范化为小写。
    *   去掉标点符号，只保留数字和字母。
    *   删除停用词:

```
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop = set(stopwords.words('english'))
def preprocess(text):
    text=text.lower()
    text=re.sub('[^0-9a-zA-Z]+',' ',text)
    words = text.split()
    words2 = [i for i in words if i not in stop]
    words3=' '.join(words2)
    return(words3)
data['text'] = data['text'].apply(preprocess)
```

5.  将句子分割成一个标记列表，这样它们就可以被传递给`gensim`。第一句的输出应该如下所示:

```
data['text'][0].split()
```

上面的代码按空格分割句子，如下所示:

![](Images/20e6f8c6-be19-41b0-a33e-6c2574611a1d.png)

我们将遍历所有的文本，并将其添加到一个列表中，如下所示:

```
list_words=[]
for i in range(len(data)):
     list_words.append(data['text'][i].split())
```

让我们检查列表列表中的前三个列表:

```
list_words[:3]
```

前三句的列表列表如下:

![](Images/b06668c0-b3d5-4475-afce-4f836f7cf292.png)

6.  构建`Word2Vec`模型:

```
from gensim.models import Word2Vec
```

定义向量大小、要查看的上下文窗口大小，以及有资格拥有单词向量的单词的最小计数，如下所示:

```
model = Word2Vec(size=100,window=5,min_count=30, sg=0, alpha = 0.025)
```

在前面的代码中，`size`表示单词向量的大小(维度)，window 表示将被考虑的单词的上下文大小，`min_count`指定单词被考虑的最小频率，`sg`表示是否将使用 skip-gram(当`sg=1`时)或 CBOW(当`sg = 0`时)，alpha 表示模型的学习速率。

一旦定义了模型，我们将传递我们的列表列表来构建词汇表，如下所示:

```
model.build_vocab(list_words)
```

一旦建立了词汇，在过滤掉在整个语料库中出现少于 30 次的单词之后，可以如下找到最后剩下的单词:

```
model.wv.vocab.keys()
```

7.  通过指定需要考虑的示例(列表)总数和要运行的时期数来训练模型，如下所示:

```
model.train(list_words, total_examples=model.corpus_count, epochs=100)
```

在前面的代码中，`list_words`(单词列表)是输入，`total_examples`表示要考虑的列表总数，epochs 是要运行的 epochs 数。

或者，您也可以通过在`Word2Vec`方法中指定`iter`参数来训练模型，如下所示:

```
model = Word2Vec(list_words,size=100,window=5,min_count=30, iter = 100)
```

8.  提取给定单词(`month`)的单词向量，如下所示:

```
model['month']
```

与单词“月”相对应的单词向量如下:

![](Images/074b3142-43d7-44e8-9b3d-0875022063eb.png)

两个单词之间的相似度可以计算如下:

```
model.similarity('month','year')
0.48
```

与给定单词最相似的单词计算如下:

```
model.most_similar('month')
```

单词`month`最相似的单词如下:

![](Images/d45f1891-0a22-4ffe-b943-338221155ccd.png)

请注意，虽然这些相似性看起来很低，一些最相似的词看起来不直观，但一旦我们在一个比我们现有的 11，000 条推文数据集更大的数据集上进行训练，这将更加现实。

在前面的场景中，让我们来看看当我们运行几个时期的模型时，与单词“month”最相似的单词的输出:

```
model = Word2Vec(size=100,window=5,min_count=30, sg=0)
model.build_vocab(list_words)
model.train(list_words, total_examples=model.corpus_count, epochs=5)
model.most_similar('month')
```

与单词“月”最相似的单词如下:

![](Images/ccf6b20e-4013-4fc5-aea4-643a97f97cb9.png)

我们可以看到，如果我们有几个时期，与单词`month`最相似的单词是不直观的，而当有许多时期时，结果是直观的，特别是当权重没有为几个时期完全优化时。

通过用`1`替换`sg`参数的值，可以为 skip-gram 重复相同的操作。

        

# 使用预训练的单词向量执行向量算术

在前面的部分中，我们看到的一个限制是句子的数量太少，我们无法建立一个健壮的模型(我们在前面的部分中看到月和年的相关性在 0.4 左右，这是相对较低的，因为它们属于同一类型的单词)。

为了克服这种情况，我们将使用谷歌训练的单词向量。来自谷歌的预训练单词向量包括针对来自谷歌新闻数据集的单词训练的 300 万单词和短语的词汇的单词向量。

        

# 怎么做...

1.  从 Google News 下载预先训练好的单词向量(代码文件在 GitHub 中以`word2vec.ipynb`的形式提供):

```
$wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz
```

解压缩下载的文件:

```
$gunzip '/content/GoogleNews-vectors-negative300.bin.gz'
```

该命令解压`bin`文件，这是模型的保存版本。

2.  加载模型:

```
from gensim.models import KeyedVectors
filename = '/content/GoogleNews-vectors-negative300.bin'
model = KeyedVectors.load_word2vec_format(filename, binary=True)
```

3.  加载与给定单词最相似的单词，`month`:

```
model.most_similar('month')
```

与`month`最相似的词如下:

![](Images/df73a48b-f995-4ad3-98a2-97506f9bad8d.png)

4.  我们将执行矢量运算；也就是说，我们将试着回答下面的类比:女人之于男人就像什么之于国王？查看以下代码:

```
result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
print(result)
```

上述算法的输出如下:

![](Images/568bc06c-e395-4066-895d-74c8d070fd04.png)

在这种情况下，从`man`的单词向量中减去`woman`的单词向量，并将其添加到`king`的单词向量中，从而得到最接近单词`queen`的向量。

        

# 创建文档向量

为了理解拥有文档向量的原因，让我们通过下面的直觉。

*bank* 这个词用在金融的上下文中，也用在河流的上下文中。我们如何识别给定句子或文档中的单词 *bank* 是否与河流话题或金融话题相关？

这个问题可以通过添加文档向量来解决，其工作方式类似于单词向量生成，但是添加了段落 ID 的一次性编码版本，如下所示:

![](Images/d1f47a77-220b-459a-9c00-2e9d68fb81d9.png)

在前面的场景中，段落 ID 包含了不仅仅由单词捕获的增量。例如，在句子 *on the bank of river* 中，其中的 bank*是输入，而 *river* 是输出，单词 *on、*和 *of* 对预测没有贡献，因为它们是频繁出现的单词，而单词 *bank* 混淆了输出预测是 river 还是 America。该特定文档/句子的文档 ID 将有助于识别该文档是与河流主题相关还是与金融主题相关。这个模型叫做**段落向量分布式内存模型** ( **PV-DM** )。*

例如，如果文档的数量是 100，则段落 ID 的一键编码版本将是 100 维的。类似地，如果满足单词的最小频率的唯一单词的数量是 1000，则单词的独热编码版本的大小是 1000。当隐藏层大小(即单词向量大小)为 300 时，参数总数将为 100 * 300 + 1，000 * 300 = 330，000

当所有输入单词的独热编码版本为 0 时，文档向量将是隐藏层的值(即，单词的影响被中和，并且仅考虑文档/段落 ID 的影响)。

类似于输入和输出在 skip-gram 和 CBOW 模型之间切换的方式，即使对于文档向量，输出和输入也可以如下切换:

![](Images/4cc8cbac-3d82-4fe8-ad28-0b5ac55e1631.png)

这个模型的表示被称为**段落向量，带有一个分布式单词包** ( **PVDBOW** )。

        

# 做好准备

我们将采用以下策略来构建文档向量:

*   对输入的句子进行预处理，去除标点符号以及所有单词的小写，并去除停用词(出现频率非常高且不会给句子添加上下文的单词，例如，*和*以及*和*
*   用句子 ID 标记每个句子。
    *   我们为每个句子分配一个 ID。
*   使用 Doc2Vec 方法提取文档 id 和单词的向量。
    *   在大量的时期内训练 Doc2Vec 方法，以便训练模型。

        

# 怎么做...

既然我们已经直观地了解了文档向量是如何生成的，并且有了构建文档向量的策略，那么让我们来生成航空公司 tweets 数据集的文档向量(代码文件在 GitHub 中以`word2vec.ipynb`的形式提供):

1.  导入相关包:

```
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from nltk.tokenize import word_tokenize
```

2.  预处理推文文本:

```
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop = set(stopwords.words('english'))
def preprocess(text):
    text=text.lower()
    text=re.sub('[^0-9a-zA-Z]+',' ',text)
    words = text.split()
    words2 = [i for i in words if i not in stop]
    words3=' '.join(words2)
    return(words3)
data['text'] = data['text'].apply(preprocess)
```

3.  创建一个标记文档的字典，其中文档 ID 与文本一起生成(tweet):

```
import nltk
nltk.download('punkt')
tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(data['text'])]
```

标记的文档数据如下所示:

![](Images/cfc4df1f-d5d2-4bbd-94a5-b9e08757d5c3.png)

在前面的代码中，我们提取了一个句子(文档)中所有组成单词的列表。

4.  用参数初始化模型，如下所示:

```
max_epochs = 100
vec_size = 300
alpha = 0.025
model = Doc2Vec(size=vec_size,
                alpha=alpha,
                min_alpha=0.00025,
                min_count=30,
                dm =1)
```

在前面的代码片段中，`size`表示文档的向量大小，`alpha`表示学习率，`min_count`表示要考虑的单词的最小频率，`dm = 1`表示 PV-DM

建立一个词汇表:

```
model.build_vocab(tagged_data)
```

6.  针对标记数据的大量历元训练模型:

```
model.train(tagged_data,epochs=100,total_examples=model.corpus_count)
```

7.  训练过程将为单词以及文档/段落 ID 生成向量。

可以像我们在上一节中那样获取单词向量，如下所示:

```
model['wife']
```

可以按如下方式提取文档向量:

```
model.docvecs[0]
```

前面的代码片段为第一个文档的文档向量生成代码片段。

8.  提取与给定文档 ID 最相似的文档:

```
similar_doc = model.docvecs.most_similar('457')
print(similar_doc)
```

![](Images/45742cdd-3b4c-44a5-8459-8e4833b21286.png)

在前面的代码片段中，我们提取了与文档 ID 号 457(827)最相似的文档 ID。

让我们看看 457 号和 827 号文件的内容:

```
data['text'][457]
```

![](Images/87db2662-06c1-414d-a69d-25c9b5e0bacd.png)

```
data['text'][827]
```

![](Images/2c6c1158-93c8-4324-8de6-169ac90b85d4.png)

如果我们检查模型的词汇表，我们会看到除了单词`just`，所有其他单词都出现在两个句子之间——因此很明显，文档 ID `457`与文档 ID `827`最相似。

        

# 使用 fastText 构建单词向量

fastText 是由脸书研究团队创建的一个库，用于高效学习单词表示和句子分类。

fastText 与 word2vec 的不同之处在于，word2vec 将每一个单词都视为其矢量表示的最小单位，但 fastText 假定一个单词由 n 个字符组成；例如，sunny 由*【sun，sunn，sunny】*、*【sunny，unny，nny】*等组成，其中我们看到大小为 *n* 的原始单词的子集，其中 *n* 的范围可以从 *1* 到原始单词的长度。

使用 fastText 的另一个原因是单词不符合 skip-gram 或 CBOW 模型中的最小频率截止值。例如，单词*附加*与*附加*不会有很大不同。然而，如果*附加*频繁出现，并且在新句子中我们将单词*附加*而不是*附加*，我们就不能将*的向量附加*。在这种情况下，fastText 的 n-gram 考虑就派上了用场。

实际上，fastText 使用 skip-gram/CBOW 模型，但是，它增加了输入数据集，以便将未看到的单词也考虑在内。

        

# 做好准备

我们将采用的使用 fastText 提取单词向量的策略如下:

1.  使用 gensim 库中的 fastText 方法
2.  预处理输入数据
3.  将每个输入句子分成一系列列表
4.  在输入列表的顶部建立一个词汇表
5.  使用多个时期的先前输入数据训练模型
6.  计算单词之间的相似度

        

# 怎么做...

在下面的代码中，我们来看看如何使用 fastText 生成单词向量(代码文件在 GitHub 中以`word2vec.ipynb`的形式提供):

1.  导入相关包:

```
from gensim.models.fasttext import FastText
```

2.  将数据集预处理并准备到一个列表列表中，就像我们对 word2vec 模型所做的那样:

```
import re
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop = set(stopwords.words('english'))
def preprocess(text):
    text=text.lower()
    text=re.sub('[^0-9a-zA-Z]+',' ',text)
    words = text.split()
    words2 = [i for i in words if i not in stop]
    words3=' '.join(words2)
    return(words3)
data['text'] = data['text'].apply(preprocess)
```

在前面的代码中，我们对输入文本进行了预处理。接下来，让我们将输入文本转换为列表列表:

```
list_words=[]
for i in range(len(data)):
     list_words.append(data['text'][i].split())
```

3.  定义模型(指定每个单词的向量数量)并构建词汇表:

```
ft_model = FastText(size=100)
ft_model.build_vocab(list_words)
```

4.  训练模型:

```
ft_model.train(list_words, total_examples=ft_model.corpus_count,epochs=100)
```

5.  检查模型词汇表中不存在的单词的单词向量。比如词汇里有`first`这个词；但是，词汇中没有单词`firstli`。在这种情况下，检查`first`和`firstli`的词向量之间的相似性:

```
ft_model.similarity('first','firstli')
```

前面代码片段的输出是 0.97，这表明两个单词之间的相关性非常高。

因此，我们可以看到 fastText 单词向量帮助我们为词汇表中不存在的单词生成单词向量。

如果在我们的数据语料库中有拼写错误的话，也可以利用前述方法来纠正拼写错误，因为拼写错误的单词可能很少出现，并且具有最高频率的最相似的单词更可能是拼写错误的单词的正确拼写版本。

可以使用矢量算法进行拼写纠正，如下所示:

```
result = ft_model.most_similar(positive=['exprience', 'prmise'], negative=['experience'], topn=1)
print(result)
```

注意，在前面的代码中，肯定词有拼写错误，而否定词没有。代码的输出是`promise`。所以这有可能纠正我们的拼写错误。

此外，还可以按如下方式执行:

```
ft_model.most_similar('exprience', topn=1)
```

*[('experience', 0.9027844071388245)]*

但是，请注意，当有多个拼写错误时，这不起作用。

        

# 使用 GloVe 构建单词向量

与 word2vec 生成单词向量的方式类似， **GloVe** (单词表示的**全局向量**的缩写)也生成单词向量，但使用不同的方法。在这一节，我们将探索 GloVe 如何工作，然后进入 GloVe 的实现细节。

        

# 做好准备

GloVe 旨在实现两个目标:

*   创建在向量空间中捕捉意义的词向量
*   利用全局计数统计信息，而不仅仅是本地信息

GloVe 通过查看单词的共现矩阵并优化损失函数来学习单词向量。从下面的例子可以理解手套算法的工作细节:

让我们考虑一个有两个句子的场景，如下所示:

| **句子** |
| 这是测试 |
| 这也是一个 |

让我们试着建立一个单词共现矩阵。在我们的句子玩具数据集中，总共有五个独特的单词，从这里开始，单词共现矩阵看起来如下:

![](Images/adf142cf-c7c0-4105-83bb-56260c16ab48.png)

在上表中，单词 *this* 和 *is* 在数据集的两行中同时出现，因此共现值为 2。类似地，单词 *this* 和 *test* 在数据集中只一起出现一次，因此具有同现值 1。

然而，在前面的矩阵中，我们没有考虑两个单词之间的距离。考虑两个单词之间的距离的直觉是，同现单词彼此距离越远，它们对于同现的相关性可能越小。

我们将引入一个新的度量——*offset*，它对给定单词和共现单词之间的距离过大进行惩罚。例如，*测试*发生在距离第一句中的*这个*为 2 的地方，所以我们将同现次数除以值 2。

变换后的共生矩阵现在看起来如下:

|  | **这个** | **是** | **测试** | **亦作** | **答** |
| **这个** | Zero | Two | Zero point five | Zero point five | Zero point three three |
| **是** | Zero | Zero | one | one | Zero point five |
| **测试** | Zero | Zero | Zero | Zero | Zero |
| **亦作** | Zero | Zero | Zero | Zero | one |
| **答** | Zero | Zero | Zero | Zero | Zero |

现在我们已经构建了矩阵，让我们引入一个额外的参数:要考虑的单词的*上下文*。例如，如果窗口大小是 2，对应于单词 *this* 和 *a* 的同现值将是值 0，因为这两个单词之间的距离大于 2。当上下文窗口大小为 2 时，变换后的共生矩阵看起来如下:

|  | **这个** | **是** | **测试** | **亦作** | **答** |
| **这个** | Zero | Two | Zero point five | Zero point five | Zero |
| **是** | Zero | Zero | one | one | Zero point five |
| **测试** | Zero | Zero | Zero | Zero | Zero |
| **亦作** | Zero | Zero | Zero | Zero | one |
| **答** | Zero | Zero | Zero | Zero | Zero |

现在我们已经得到了一个修改的共现矩阵，在这个例子中，我们随机初始化每个单词的单词向量，其维数为 2。每个单词的随机初始化的权重和偏差值如下所示，其中每个单词的向量大小为 3:

| **字** | **权重 1** | **重量 2** | **重量 3** | **偏置** |
| **这个** | -0.64 | Zero point eight two | -0.08 | Zero point one six |
| **是** | -0.89 | -0.31 | Zero point seven nine | -0.34 |
| **测试** | -0.01 | Zero point one four | Zero point eight two | -0.35 |
| **亦作** | -0.1 | -0.67 | Zero point eight nine | Zero point two six |
| **答** | -0.1 | -0.84 | Zero point three five | Zero point three six |

假设前面的权重和偏差是随机初始化的，我们修改权重以优化损失函数。为此，让我们定义利息损失函数，如下所示:

![](Images/7d0d0021-4a99-46aa-9ec9-6a8121574215.png)

在前面的等式中，*w[I]表示第 *i* ^(th) 字的字向量， *w [j]* 表示第 *j* ^(th) 字的字向量；*b[I]和*b[j]分别是与*I*th 和*j*th 字相对应的偏置。 *X [ij]* 代表我们之前定义的最终同现值中的值。***

例如，*X[ij]的值其中 *i* 是字*这个*和 *j* 是字*也是*是 0.5*

当*X[ij]的值为 0 时，f(*X[ij]的值为 0；否则，如下所示:**

![](Images/e3cdf189-d305-4b20-88c4-6c4ba834f4b1.png)

在前面的等式中，根据经验发现 alpha 为 0.75，*x[max]为 100， *x* 为*x[ij]的值。**

既然方程已经定义好了，让我们将它应用到矩阵中，如下所示:

![](Images/d9c693ff-588a-47e5-b951-5503216d9586.png)

第一个表表示单词共现矩阵和随机初始化的权重和偏差。

第二个表表示损失值计算，在这里我们计算总的加权损失值。

我们优化权重和偏差，直到总加权损失值最小。

        

# 怎么做...

现在我们知道了如何使用 GloVe 生成单词向量，让我们用 Python 来实现它(代码文件在 GitHub 中以`word2vec.ipynb`的形式提供):

1.  安装手套:

```
$pip install glove_python
```

2.  导入相关包:

```
from glove import Corpus, Glove
```

3.  按照我们在 word2vec、skip-gram 和 CBOW 算法中预处理的方式预处理数据集，如下所示:

```
import re
import nltk
from nltk.corpus import stopwords
import pandas as pd
nltk.download('stopwords')
stop = set(stopwords.words('english'))
data = pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv?dl=1')
def preprocess(text):
    text=text.lower()
    text=re.sub('[^0-9a-zA-Z]+',' ',text)
    words = text.split()
    words2 = [i for i in words if i not in stop]
    words3=' '.join(words2)
    return(words3)
data['text'] = data['text'].apply(preprocess)
list_words=[]
for i in range(len(data)):
      list_words.append(data['text'][i].split())
```

4.  创建一个语料库，并为其配备一个词汇表:

```
corpus.fit(list_words, window=5)
```

语料库的字典可以如下找到:

```
corpus.dictionary
```

唯一单词及其对应的单词 id 如下获得:

![](Images/a15b2447-73bd-42bd-97e8-5bb1d630f4e9.png)

前面的屏幕截图显示了单词的键值及其对应的索引。

以下代码片段为我们提供了共现矩阵:

```
corpus.matrix.todense()
```

![](Images/0248814c-3205-47f8-b94d-79788deb21df.png)

5.  让我们定义模型参数，即向量的维数、学习速率和要运行的时期数，如下所示:

```
glove = Glove(no_components=100, learning_rate=0.025)
glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)
```

6.  一旦模型被拟合，可以如下找到词向量的权重和偏差:

```
glove.word_biases.tolist()
glove.word_vectors.tolist()
```

7.  给定单词的单词向量可以如下确定:

```
glove.word_vectors[glove.dictionary['united']]
```

8.  给定单词的最相似单词可以如下确定:

```
glove.most_similar('united')
```

与“united”最相似的单词的输出如下:

![](Images/f88b5cb8-7fc0-4a9f-97cc-13fdc15c6c0e.png)

注意和`united`这个词最相似的词是属于其他航空公司的词。

        

# 使用词向量建立情感分类

在前面的章节中，我们学习了如何使用多种模型生成单词向量。在这一部分，我们将学习如何为一个给定的句子建立一个情感分类器。在本次练习中，我们将继续使用航空公司情绪推特数据集。

        

# 怎么做...

像我们在以前的食谱中提取的那样生成单词向量(代码文件在 GitHub 中以`word2vec.ipynb`的形式提供):

1.  导入包并下载数据集:

```
import re
import nltk
from nltk.corpus import stopwords
import pandas as pd
nltk.download('stopwords')
stop = set(stopwords.words('english'))
data=pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv?dl=1')
```

2.  预处理输入文本:

```
def preprocess(text):
 text=text.lower()
 text=re.sub('[^0-9a-zA-Z]+',' ',text)
 words = text.split()
 words2 = [i for i in words if i not in stop]
 words3=' '.join(words2)
 return(words3)
data['text'] = data['text'].apply(preprocess)
```

3.  提取数据集中所有句子的列表列表:

```
t=[]
for i in range(len(data)):
 t.append(data['text'][i].split())
```

4.  建立 CBOW 模型，上下文窗口`size`为`5`，向量长度为 100:

```
from gensim.models import Word2Vec
model = Word2Vec(size=100,window=5,min_count=30, sg=0)
```

5.  指定要建模的词汇，然后对其进行训练:

```
model.build_vocab(t)
model.train(t, total_examples=model.corpus_count, epochs=100)
```

6.  提取给定推文的平均向量:

```
import numpy as np
features= []
for i in range(len(t)):
      t2 = t[i]
      z = np.zeros((1,100))
      k=0
      for j in range(len(t2)):
            try:
              z = z+model[t2[j]]
              k= k+1
            except KeyError:
              continue
      features.append(z/k)
```

我们取输入句子中所有单词的单词向量的平均值。此外，将会有某些单词不在词汇表中(出现频率较低的单词),如果我们试图提取它们的向量，将会导致错误。我们已经为这个特定场景部署了`try`和`catch`错误。

7.  预处理要素以将其转换为数组，将数据集分割为训练数据集和测试数据集，并对数据集进行整形，以便将其传递给模型:

```
features = np.array(features)

from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(features, data['airline_sentiment'], test_size=0.30,random_state=10)
X_train = X_train.reshape(X_train.shape[0],100)
X_test = X_test.reshape(X_test.shape[0],100)
```

8.  编译和构建神经网络来预测推文的情绪:

```
from keras.layers import Dense, Activation
from keras.models import Sequential
from keras.utils import to_categorical
from keras.layers.embeddings import Embedding
model = Sequential()
model.add(Dense(1000,input_dim = 100,activation='relu'))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
```

上面定义的模型总结如下:

![](Images/f192b1ad-1bfb-48b4-aec9-fe09290cf0ad.png)

在前面的模型中，我们有一个 1000 维的隐藏层，它将 100 个输入的平均单词向量值连接到输出，输出的值为 1 (1 或 0 分别表示积极或消极的情绪):

```
model.fit(X_train, y_train, batch_size=128, nb_epoch=5, validation_data=(X_test, y_test),verbose = 1)
```

我们可以看到，我们的模型在预测一条推文的情绪方面的准确率约为 90%。

9.  绘制预测的混淆矩阵:

```
pred = model.predict(X_test)
pred2 = np.where(pred>0.5,1,0)
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, pred2)
```

混淆矩阵的输出如下:

![](Images/2c7b07d2-adcd-4ee5-bcaa-b8bfd5201169.png)

从上面，我们看到在 2644 个句子中，我们预测它们是积极的，它们实际上是积极的。125 句被预测为否定，碰巧为肯定。209 个句子被预测为肯定的，碰巧是否定的，最后，485 个句子被预测为否定的，实际上是否定的。

        

# 还有更多...

虽然我们使用 CBOW 模型和推文中出现的所有词向量的平均值实现了情感分类，但我们可以采用的其他方法如下:

*   使用跳格模型。
*   使用 doc2vec 模型来构建使用文档向量的模型。
*   使用基于 fastText 模型的单词向量。
*   使用基于手套的单词向量。
*   使用预训练模型的单词向量值。

虽然这些方法以相似的方式工作，但前面模型的一个限制是它没有考虑词序。还有更复杂的算法解决词序问题，这将在下一章讨论。