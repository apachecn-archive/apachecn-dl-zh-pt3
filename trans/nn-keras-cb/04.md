        

# 四、构建深度卷积神经网络

在本章中，我们将介绍以下配方:

*   传统神经网络在图像翻译时的不精确性
*   使用 Python 从头开始构建 CNN
*   CNN 提高图像翻译的准确性
*   使用 CNN 的性别分类
*   提高网络精度的数据扩充

        

# 介绍

在前一章，我们看了一个传统的深度前馈神经网络。传统的深度前馈神经网络的限制之一是它不是平移不变的，也就是说，图像右上角的猫图像将被认为不同于图像中心有猫的图像。此外，传统的神经网络受到对象规模的影响。如果对象在大多数图像中是大的，并且新图像中具有相同的对象，但是具有较小的比例(占据图像的较小部分)，则传统的神经网络很可能无法对图像进行分类。

**卷积神经网络**(**CNN**)就是用来处理这类问题的。鉴于 CNN 能够处理图像中的平移以及图像的缩放，它被认为在对象分类/检测中更有用。

在本章中，您将了解以下内容:

*   传统神经网络在图像翻译时的不精确性
*   使用 Python 从头开始构建 CNN
*   使用 CNN 改进 MNIST 数据集上的图像分类
*   实施数据扩充以提高网络精度
*   使用 CNN 的性别分类

        

# 传统神经网络在图像翻译时的不准确性

为了进一步理解 CNN 的需要，我们将首先理解为什么前馈**神经网络** ( **NN** )在图像被翻译时不起作用，然后看看 CNN 如何改进传统的前馈 NN。

让我们看一下下面的场景:

*   我们将构建一个神经网络模型来预测来自 MNIST 数据集的标签
*   我们将考虑标签为 1 的所有图像，并取所有图像的平均值(生成 1 个图像的平均值)
*   我们将使用传统的神经网络预测我们在上一步中生成的平均 1 图像的标签
*   我们将把平均 1 幅图像向左或向右平移 1 个像素
*   我们将使用传统的神经网络模型对翻译后的图像进行预测

        

# 怎么做...

上面定义的策略编码如下(实现代码时请参考 GitHub 中的`Issue_with_image translation.ipynb`文件)

1.  下载数据集并提取训练和测试 MNIST 数据集:

```py
from keras.datasets import mnist
from keras.layers import Flatten, Dense
from keras.models import Sequential
import matplotlib.pyplot as plt
%matplotlib inline
(X_train, y_train), (X_test, y_test) = mnist.load_data()
```

2.  仅获取标签`1`对应的训练集:

```py
X_train1 = X_train[y_train==1]
```

3.  重塑和规范化原始训练数据集:

```py
num_pixels = X_train.shape[1] * X_train.shape[2]
X_train = X_train.reshape(X_train.shape[0],num_pixels).astype('float32')
X_test = X_test.reshape(X_test.shape[0],num_pixels).astype('float32')
X_train = X_train / 255
X_test = X_test / 255
```

4.  对输出标签进行一次热编码:

```py
y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)
num_classes = y_train.shape[1]
```

5.  建立一个模型并进行拟合:

```py
model = Sequential()
model.add(Dense(1000, input_dim=num_pixels, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])
model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=5, batch_size=1024, verbose=1)
```

6.  让我们绘制在步骤 2 中获得的平均 1 幅图像:

```py
pic=np.zeros((28,28))
pic2=np.copy(pic)
for i in range(X_train1.shape[0]):
    pic2=X_train1[i,:,:]
    pic=pic+pic2
pic=(pic/X_train1.shape[0])
plt.imshow(pic)
```

在前面的代码中，我们初始化了一个尺寸为 28 x 28 的空图片，并通过循环遍历`X_train1`对象中的所有值，在标签为 1 的图像(`X_train1`对象)的各个像素位置上取平均像素值。

平均 1 幅图像的曲线如下所示:

![](Images/11fffca9-ea5a-4ea8-8fc4-59bfc3402c22.png)

要注意的是，像素越黄(粗)，人们越经常在像素上书写，而像素越黄(越蓝/越粗)，人们越不经常在像素上书写。此外，要注意的是，中间的像素是最黄/最厚的(这是因为大多数人会在中间像素上书写，而不管整个数字是以垂直线书写还是向左或向右倾斜)。

        

# 传统神经网络的问题

**场景 1** :让我们创建一个新图像，其中原始图像向左平移 1 个像素。在下面的代码中，我们遍历图像的列，并将下一列的像素值复制到当前列:

```py
for i in range(pic.shape[0]):
     if i<20:
         pic[:,i]=pic[:,i+1]
     plt.imshow(pic)
```

左平移平均 1 图像如下所示:

![](Images/d18d3b21-d722-4c43-9860-79b25e008b2e.png)

让我们继续使用构建的模型预测图像的标签:

```py
model.predict(pic.reshape(1,784)/255)
```

模型对翻译图像的预测如下:

![](Images/a8682fa3-d0d9-4dec-9ba8-bf00aea80760.png)

我们可以看到预测值为 1，尽管概率低于像素未被平移时的概率。

**场景 2** :创建新图像，其中原始平均 1 图像的像素向右移动 2 个像素:

```py
pic=np.zeros((28,28))
pic2=np.copy(pic)
for i in range(X_train1.shape[0]):
    pic2=X_train1[i,:,:]
    pic=pic+pic2
pic=(pic/X_train1.shape[0])
pic2=np.copy(pic)
for i in range(pic.shape[0]):
    if ((i>6) and (i<26)):
    pic[:,i]=pic2[:,(i-1)]
plt.imshow(pic)
```

右平移平均 1 图像如下所示:

![](Images/e2fed1a8-3e6d-4a03-ad3e-3ff32ce0d59c.png)

这个图像的预测如下:

```py
model.predict(pic.reshape(1,784)/255)
```

模型对翻译图像的预测如下:

![](Images/a2d7d8ca-e1ab-44d3-b039-0199a5ef6b43.png)

我们可以看到预测是不正确的，输出为 3。这是我们将通过 CNN 解决的问题。

        

# 使用 Python 从头开始构建 CNN

在这一节中，我们将学习 CNN 如何使用 NumPy 从头开始构建前馈网络。

        

# 做好准备

典型的 CNN 有多个组件。在本节中，我们将先了解 CNN 的各个组成部分，然后再了解在图像被翻译时，CNN 是如何提高预测精度的。

        

# 理解卷积

我们已经知道典型的神经网络是如何工作的。在这一节中，我们来了解一下 CNN 中卷积过程的工作细节。

        

# 过滤器

卷积是两个矩阵之间的乘法，一个矩阵大，另一个矩阵小。为了理解卷积，考虑下面的例子:

矩阵 *A* 如下:

![](Images/2dd6021a-9a7e-45c5-9e81-94a543cb720d.png)

矩阵 *B* 如下:

![](Images/e6ad679b-8864-42af-948f-745bea146353.png)

当执行卷积时，可以把它想象成我们在较大的矩阵上滑动较小的矩阵，也就是说，当较小的矩阵在较大的矩阵的整个区域上滑动时，我们可能会得到 9 次这样的乘法。注意不是矩阵乘法。

较大和较小矩阵之间发生的各种乘法如下:

1.  大矩阵的 *{1，2，5，6}* 乘以小矩阵的 *{1，2，3，4}* :

*1*1 + 2*2 + 5*3 + 6*4 = 44*

2.  大矩阵的 *{2，3，6，7}* 乘以小矩阵的 *{1，2，3，4}* :

*2*1 + 3*2 + 6*3 + 7*4 = 54*

3.  大矩阵的 *{3，4，7，8}* 乘以小矩阵的 *{1，2，3，4}* :

*3*1 + 4*2 + 7*3 + 8*4 = 64*

4.  大矩阵的 *{5，6，9，10}* 乘以小矩阵的 *{1，2，3，4}* :

*5*1 + 6*2 + 9*3 + 10*4 = 84*

5.  大矩阵的 *{6，7，10，11}* 与小矩阵的 *{1，2，3，4}* 相乘:

*6*1 + 7*2 + 10*3 + 11*4 = 94*

6.  大矩阵的 *{7，8，11，12}* 与小矩阵的 *{1，2，3，4}* 相乘:

*7*1 + 8*2 + 11*3 + 12*4 = 104*

7.  大矩阵的 *{9，10，13，14}* 乘以小矩阵的 *{1，2，3，4}* :

*9*1 + 10*2 + 13*3 + 14*4 = 124*

8.  大矩阵的 *{10，11，14，15}* 乘以小矩阵的 *{1，2，3，4}* :

*10*1 + 11*2 + 14*3 + 15*4 = 134*

9.  大矩阵的 *{11，12，15，16}* 与小矩阵的 *{1，2，3，4}* 相乘:

*11*1 + 12*2 + 15*3 + 16*4 = 144*

前面步骤的结果将是下面的矩阵:

![](Images/d341e2cc-5f69-4bb6-9d34-f6cb88e9ded8.png)

传统上，较小的矩阵被称为滤波器或内核，并且较小的矩阵值是通过梯度下降统计得到的。过滤器中的值是成分权重。

实际上，当图像输入形状是 224×224×3 时，其中有 3 个通道，具有 3×3 形状的滤波器也将有 3 个通道，使得能够执行矩阵乘法(和积)。

一个滤波器的通道数与它所相乘的矩阵中的通道数一样多。

        

# 大步

在前面的步骤中，假设过滤器在水平和垂直方向上一次移动一步，则过滤器的步幅为(1，1)。步长数越大，从矩阵乘法中跳过的值的数量就越多。

        

# 填料

在前面的步骤中，我们忽略了将滤波器最左边的值与原始矩阵最右边的值相乘。如果我们要执行这样的操作，我们将确保在原始矩阵的边缘(用零填充的图像边缘)周围有零填充。这种形式的填充称为**有效**填充。我们在*理解卷积*的*滤波器*部分执行的矩阵乘法是**相同的**填充的结果。

        

# 从卷积到激活

在传统的神经网络中，隐藏层不仅将输入值乘以权重，还对数据应用非线性，也就是说，它通过激活函数传递值。

类似的活动也发生在典型的 CNN 中，卷积通过激活函数传递。CNN 支持我们目前看到的传统激活函数:sigmoid、ReLU、tanh 和 leaky ReLU。

对于前面的输出，我们可以看到，当通过 ReLU 激活函数时，输出保持不变，因为所有的数字都是正数。

        

# 从卷积激活到池化

在上一节中，我们看了卷积是如何工作的。在本节中，我们将了解卷积后的典型下一步:池化。

假设卷积步骤的输出如下(我们不考虑前面的例子，这是一个新的例子，只是为了说明池是如何工作的):

![](Images/1262742f-f600-4d1e-96f8-8abe2641e16f.png)

在前一种情况下，卷积步骤的输出是一个 2×2 矩阵。最大池考虑 2 x 2 块，并给出最大值作为输出。同样，假设卷积步骤的输出是一个更大的矩阵，如下所示:

![](Images/9e1ef209-932a-43ac-beec-86f01d8aa9e5.png)

最大池将大矩阵划分为 2 x 2 的非重叠块(当跨距值为 2 时)，如下所示:

![](Images/7e811be5-036f-47ac-a50c-2a6c992987dd.png)

从每个块中，只选择具有最高值的元素。因此，对前面矩阵的最大池化操作的输出如下:

![](Images/b947e987-a657-4837-bde2-4ea0c4cdae85.png)

在实践中，并不是在所有情况下都需要 2 x 2 的窗口，但它经常被使用。

涉及的其他类型的池包括总和池和平均池，同样，在实践中，与其他类型的池相比，我们会看到很多最大池。

        

# 卷积和池化有什么帮助？

在 MNIST 的例子中，传统神经网络的缺点之一是每个像素都与不同的权重相关联。

因此，如果除了原始像素之外的相邻像素被高亮显示，而不是原始像素，则输出将不会非常准确(例如*场景 1* ，其中平均像素稍微偏左)。

现在解决这种情况，因为像素共享在每个滤波器内构成的权重。

所有像素乘以构成滤波器的所有权重。在汇集层中，仅选择卷积后具有高值的值。

这样，无论高亮显示的像素是在中心还是稍微远离中心，输出通常都不是预期值。

然而，当突出显示的像素距离中心非常远时，问题仍然存在。

        

# 怎么做...

为了获得可靠的理解，我们将使用 Keras 构建一个基于 CNN 的架构，并通过将从头构建 CNN 的前馈传播部分获得的输出与从 Keras 获得的输出进行匹配，来验证我们对 CNN 如何工作的理解。

让我们用一个定义了输入和预期输出数据的玩具示例来实现 CNN(代码文件在 GitHub 中以`CNN_working_details.ipynb`的形式提供):

1.  创建输入和输出数据集:

```py
import numpy as np
X_train=np.array([[[1,2,3,4],[2,3,4,5],[5,6,7,8],[1,3,4,5]],
[[-1,2,3,-4],[2,-3,4,5],[-5,6,-7,8],[-1,-3,-4,-5]]])
y_train=np.array([0,1])
```

在前面的代码中，我们创建了数据，其中正输入给出了输出`0`，负输入给出了输出`1`。

2.  缩放输入数据集:

```py
X_train = X_train / 8
```

3.  重塑输入数据集，使每个输入图像以宽度`x`高度`x`通道数的格式表示:

```py
X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[1],1 ).astype('float32')
```

4.  构建模型架构:

导入相关方法后实例化模型:

```py
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from keras.models import Sequential
model = Sequential()
```

在下一步中，我们将执行卷积运算:

```py
model.add(Conv2D(1, (3,3), input_shape=(4,4,1),activation='relu'))
```

在前面的步骤中，我们对输入数据执行 2D 卷积(我们在*了解卷积*一节中看到的矩阵乘法)，其中我们有一个大小为 3 x 3 的滤波器。

此外，假设这是实例化模型后的第一层，我们指定输入形状为(4，4，1)

最后，我们在卷积的输出之上执行 ReLu 激活。

在这种情况下，卷积运算的输出在形状上是 2 x 2 x 1，因为权重与输入的矩阵乘法将产生 2 x 2 矩阵(假定默认步幅是 1 x 1)。

此外，输出的大小会缩小，因为我们没有填充输入(在输入图像周围放置零)。

在以下步骤中，我们将添加一个执行最大池化操作的层，如下所示:

```py
model.add(MaxPooling2D(pool_size=(2, 2))) 
```

我们在从上一层获得的输出之上执行最大池化，其中池大小为 2 x 2。这意味着计算图像的 2×2 部分的子集中的最大值。

请注意，在这种情况下，池层中 2 × 2 的步长不会影响输出，因为上一步的输出是 2 × 2。然而，一般来说，大于 1 × 1 的步长会影响输出形状。

让我们展平池层的输出:

```py
model.add(Flatten())
```

一旦我们执行展平，该过程就变得非常类似于我们在标准前馈神经网络中执行的过程，在标准前馈神经网络中，输入连接到隐藏层，然后连接到输出层(我们也可以将输入连接到更多隐藏层！).

我们使用 sigmoid 激活将展平层的输出直接连接到输出层:

```py
model.add(Dense(1, activation='sigmoid'))
```

可以获得该模型的摘要，如下所示:

```py
model.summary()
```

输出的摘要如下:

![](Images/6e708f38-84ee-4c40-9b05-299cf9c52816.png)

注意，卷积层中有 10 个参数，因为一个 3×3 滤波器将具有 9 个权重和 1 个偏置项。池化图层和展平图层没有任何参数，因为它们要么提取特定区域中的最大值(最大池化)，要么展平前一图层的输出(展平)，因此没有需要在这些图层中修改权重的操作。

输出图层有两个参数，因为展平图层有一个输出，该输出连接到具有一个值的输出图层-因此我们将有一个权重和一个偏置项连接展平图层和输出图层。

5.  编译并拟合模型:

```py
model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])
```

在前面的代码中，我们将损失指定为二进制交叉熵，因为结果要么是`1`要么是`0`。

6.  符合模型:

```py
model.fit(X_train, y_train, epochs = 500)
```

我们将模型拟合为具有连接输入层和输出层的最佳权重。

        

# 验证 CNN 输出

现在我们已经拟合了模型，让我们通过实现 CNN 的前馈部分来验证我们从模型获得的输出:

1.  让我们提取权重和偏差出现的顺序:

```py
model.weights
```

![](Images/bc1369d0-c303-4cbe-8080-4e302592a96a.png)

您可以看到，首先显示的是卷积层的权重，然后是偏差，最后是输出层中的权重和偏差。

还要注意，卷积层中权重的形状是(3，3，1，1)，因为过滤器的形状是 3 x 3 x 1(因为图像是三维的:形状是 28 x 28 x 1)，最后的 1(形状的第四个值)是卷积层中指定的过滤器数量。

如果我们将卷积中的滤波器数量指定为 64，权重的形状将会是 3 x 3 x 1 x 64。

类似地，如果对具有 3 个通道的图像执行卷积操作，每个滤镜的形状将是 3 x 3 x 3。

2.  提取各层的权重值:

```py
model.get_weights()
```

3.  让我们提取第一个输入的输出，以便我们可以用前馈传播来验证它:

```py
model.predict(X_train[0].reshape(1,4,4,1))
```

![](Images/cd6f5149-946f-4c97-8499-a4c6c7d6fe6c.png)

我们运行的迭代的输出是 0.0428(当您运行模型时，这可能不同，因为权重的随机初始化可能不同)，我们将通过执行矩阵乘法来验证它。

我们在将输入传递给 predict 方法的同时对其进行整形，因为它希望输入的形状为(None，4，4，1)，其中 None 指定批处理大小可以是任何数字。

4.  执行滤波器与输入图像的卷积。注意，输入图像的形状是 4×4，而过滤器的形状是 3×3。我们将沿着代码中的行和列执行矩阵乘法(卷积):

```py
sumprod = []
for i in range(X_train[0].shape[0]-model.get_weights()[0].shape[0]+1):
     for j in range(X_train[0].shape[0]-model.get_weights()[0].shape[0]+1):
         img_subset = np.array(X_train[0,i:(i+3),j:(j+3),0])
         filter = model.get_weights()[0].reshape(3,3)
         val = np.sum(img_subset*filter) + model.get_weights()[1]
         sumprod.append(val)
```

在前面的代码中，我们正在初始化一个名为`sumprod`的空列表，该列表存储过滤器与图像子集(图像子集的大小与过滤器相同)的每次矩阵乘法的输出。

5.  重塑`sumprod`的输出，使其可以传递到池层:

```py
sumprod= np.array(sumprod).reshape(2,2,1)
```

6.  在卷积的输出传递到池层之前，在卷积的输出顶部执行激活:

```py
sumprod = np.where(sumprod>0,sumprod,0)
```

7.  将卷积输出传递给池层。然而，在当前情况下，假设卷积的输出为 2 x 2，我们将保持简单，只取我们在*步骤 6* 中获得的输出中的最大值:

```py
pooling_layer_output = np.max(sumprod)
```

8.  将池层的输出连接到输出层:

```py
intermediate_output_value = pooling_layer_output*model.get_weights()[2]+model.get_weights()[3]
```

我们将池层的输出乘以输出层中的权重，并在输出层中添加偏差。

9.  计算 sigmoid 输出:

```py
1/(1+np.exp(-intermediate_output_value))
```

前面操作的输出如下:

![](Images/9f3d6105-094e-441a-9139-3c885798733d.png)

您在这里看到的输出将与我们使用`model.predict`方法获得的输出相同，从而验证了我们对 CNN 工作原理的理解。

        

# CNN 提高图像翻译的准确性

在前面的章节中，我们了解了图像翻译的问题以及 CNN 是如何工作的。在这一节中，我们将利用这些知识来了解 CNN 如何在翻译图像时提高预测准确性。

        

# 做好准备

我们将采用以下策略来构建 CNN 模型:

*   假设输入形状为 28 x 28 x 1，过滤器的大小应为 3 x 3 x 1:
    *   请注意，过滤器的大小可以改变，但是通道的数量不能改变
*   让我们初始化 10 个过滤器
*   我们将在输入图像上卷积 10 个滤波器的上一步中获得的输出之上执行池化:
    *   这将导致图像尺寸减半
*   我们将拉平合并时获得的输出
*   展平的图层将连接到另一个具有 1000 个单位的隐藏图层
*   最后，我们将隐藏层连接到输出层，这里有 10 个可能的类(因为有 10 个数字，从 0 到 9)

一旦我们建立了模型，我们将把平均 1 幅图像平移 1 个像素，然后在平移的图像上测试 CNN 模型的预测。注意，前馈神经网络结构在这种情况下不能预测正确的类别。

        

# 怎么做...

让我们来了解一下在代码中使用 CNN 对 MNIST 的数据(代码文件在 GitHub 中以`CNN_image_translation.ipynb`的形式提供):

1.  加载和预处理数据:

```py
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[1],1 ).astype('float32')
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[1],1).astype('float32')

X_train = X_train / 255
X_test = X_test / 255

y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)

num_classes = y_test.shape[1]
```

请注意，我们在此步骤中执行的所有步骤与我们在[第 2 章](750fdf81-d758-47c9-a3b3-7cae6aae1576.xhtml)、*构建深度前馈神经网络*中执行的步骤相同。

2.  构建和编译模型:

```py
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from keras.models import Sequential
model = Sequential()
model.add(Conv2D(10, (3,3), input_shape=(28, 28,1),activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Flatten())
model.add(Dense(1000, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
```

可以获得我们在前面的代码中初始化的模型的摘要，如下所示:

```py
model.summary()
```

该模型的摘要如下:

![](Images/3e4273c7-43a7-4785-9fcb-a5a0e92fc1bb.png)

我们在卷积层中总共有 100 个参数，因为有 10 个 3×3×1 滤波器，导致总共 90 个权重参数。此外，10 个偏置项(每个滤波器一个)加起来在卷积层中形成总共 100 个参数。

请注意，max pooling 没有任何参数，因为它是关于在大小为 2 x 2 的补丁中提取最大值。

3.  符合模型:

```py
model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=5, batch_size=1024, verbose=1)
```

前面的模型在 5 个时期内给出了 98%的准确度:

![](Images/2356a9a5-1d51-429c-b4ea-5ca798823335.png)

4.  先确定平均 1 张图像，然后用`1`单位平移:

```py
X_test1 = X_test[y_test[:,1]==1]
```

在前面的代码中，我们过滤了标签为`1`的所有图像输入:

```py
import numpy as np
pic=np.zeros((28,28))
pic2=np.copy(pic)
for i in range(X_test1.shape[0]):
     pic2=X_test1[i,:,:,0]
     pic=pic+pic2
pic=(pic/X_test1.shape[0])
```

在前面的代码中，我们取了平均 1 幅图像:

```py
for i in range(pic.shape[0]):
     if i<20:
         pic[:,i]=pic[:,i+1]
```

在前面的代码中，我们将平均 1 幅图像中的每个像素向左平移 1 个单位。

5.  在翻译的 1 图像上预测:

```py
model.predict(pic.reshape(1,28,28,1))
```

上一步的输出如下:

![](Images/5d530d8c-c505-4b13-aafa-5d0c85335cdd.png)

注意，与深度前馈 NN 模型的情况相比，现在的预测(当我们使用 CNN 时)具有更大的概率(0.9541)为 1，深度前馈 NN 模型在传统神经网络的*图像被翻译时的不准确性*部分中被预测为翻译图像的标签中的(0.6335)。

        

# 使用 CNN 的性别分类

在前面的章节中，我们了解了 CNN 是如何工作的，以及 CNN 是如何解决图像翻译问题的。

在本节中，我们将通过建立一个模型来检测图像中出现的人的性别，进一步了解 CNN 是如何工作的。

        

# 做好准备

在这一部分中，让我们制定如何解决这个问题的策略:

*   我们将收集一个图像数据集，并根据图像中出现的人的性别来标记每张图像
*   我们将只处理 2，000 张图片，因为获取数据的过程对于我们的数据集来说需要相当长的时间(在本案例研究中，我们是从一个网站手动下载图片)
*   此外，我们将确保在数据集中男性和女性图像有相等的表示
*   一旦数据集就位，我们将把图像重新调整成同样的大小，这样它们就可以输入到 CNN 模型中
*   我们将构建 CNN 模型，其中输出层的类别数与标签数 2 一样多
*   假设这是预测数据集中两个可能标签之一的情况，我们将最小化二进制交叉熵损失

        

# 怎么做...

在本节中，我们将对之前定义的策略进行编码(代码文件在 GitHub 中以`Gender classification.ipynb`的形式提供):

1.  下载数据集:

```py
$ wget https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2017/04/a943287.csv
```

2.  加载数据集并检查其内容:

```py
import pandas as pd, numpy as np
from skimage import io
# Location of file is /content/a943287.csv
# be sure to change to location of downloaded file on your machine
data = pd.read_csv('/content/a943287.csv')
data.head() 
```

数据集中一些关键字段的示例如下:

![](Images/6bbc8d52-1050-4284-9dc9-ac82eb5a039c.png)

3.  从数据集中提供的 URL 链接中获取 1，000 张男性图像和 1，000 张女性图像:

```py
data_male = data[data['please_select_the_gender_of_the_person_in_the_picture']=="male"].reset_index(drop='index')
data_female = data[data['please_select_the_gender_of_the_person_in_the_picture']=="female"].reset_index(drop='index')
final_data = pd.concat([data_male[:1000],data_female[:1000]],axis=0).reset_index(drop='index')
```

在前面的代码中，`final_data`包含 1000 张男性图片和 1000 张女性图片的 URL 链接。读取 URL 链接并获取对应于 URL 链接的图像。确保所有图像的形状都是 300 × 300 × 3(因为他的数据集中的大多数图像都是这种形状),并确保我们注意任何禁止访问的问题:

```py
x = []
y = []
for i in range(final_data.shape[0]):
     try:
         image = io.imread(final_data.loc[i]['image_url'])
         if(image.shape==(300,300,3)):
             x.append(image)
             y.append(final_data.loc[i]['please_select_the_gender_of_the_person_in_the_picture'])
     except:
         continue
```

输入及其相应情感标签的示例如下:

![](Images/d08b8aaf-175f-470f-88f4-49b7f728ff26.png)

4.  创建输入和输出数组:

```py
x2 = []
y2 = []
for i in range(len(x)):
      img = cv2.cvtColor(x[i], cv2.COLOR_BGR2GRAY)
      img2 = cv2.resize(img,(50,50))
      x2.append(img2)
      img_label = np.where(y[i]=="male",1,0)
      y2.append(img_label)
```

在前面的步骤中，我们已经将彩色图像转换为灰度图像，因为图像的颜色可能会添加额外的信息(我们将在第五章、*的[迁移学习](c50d0373-e7d4-47d9-a514-df766f575a47.xhtml)*中验证这一假设)。

此外，我们还将图像的大小调整为较小的形状(50 x 50 x 1)。这样做的结果如下:

![](Images/014738bb-62bc-4d19-86cf-1a5e419d57b6.png)

最后，我们将输出转换成一次性编码的版本。

5.  创建训练和测试数据集。首先，我们将输入和输出列表转换成数组，然后对输入进行整形，使其形状可以作为输入提供给 CNN:

```py
x2 = np.array(x2)
x2 = x2.reshape(x2.shape[0],x2.shape[1],x2.shape[2],1)
Y = np.array(y2)
```

第一个值`x2`的输出如下:

![](Images/f75c95fd-9415-4496-9936-f1045c6aa1c3.png)

请注意，输入的值在`0`到`255`之间，因此我们必须对其进行缩放:

```py
X = np.array(x2)/255
Y = np.array(y2)
```

最后，我们将输入和输出数组分成训练和测试数据集:

```py
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.1, random_state=42)
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)
```

训练和测试输入、输出阵列的形状如下:

![](Images/dabfed13-ae88-4956-aa54-d93b53a6477a.png)

6.  构建和编译模型:

```py
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense
from keras.models import Sequential
model = Sequential()
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu',input_shape=(50,50,1)))
model.add(MaxPooling2D(pool_size=(5, 5)))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(256, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(512, kernel_size=(3, 3), activation='relu',padding='same'))
model.add(Flatten())
model.add(Dense(100, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.summary()
```

该模型的摘要如下:

![](Images/e5f69cca-98f7-42f6-a04d-0f86e829cbe0.png)

请注意，卷积层输出中的通道数将等于该层中指定的过滤器数。此外，我们在第一个卷积层的输出上执行了一个稍微更积极的汇集。

现在，我们将编译模型以最小化二进制交叉熵损失(因为输出只有两个类)，如下所示:

```py
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
```

7.  符合模型:

```py
history = model.fit(X_train, y_train, batch_size=32, epochs=50,verbose=1,validation_data = (X_test, y_test))
```

![](Images/8efa1177-0525-46b7-a427-c60dcefea2f0.png)

一旦我们拟合了模型，我们可以看到，前面的代码在预测图像中正确的性别方面的准确率约为 80%。

        

# 还有更多...

分类的准确性可以通过以下方式进一步提高:

*   处理更多图像
*   处理用于训练更大网络的更大图像(而不是 50 x 50 图像)
*   利用迁移学习(将在[第 5 章](c50d0373-e7d4-47d9-a514-df766f575a47.xhtml)、*迁移学习*中讨论)
*   使用正则化和剔除避免过拟合

        

# 提高网络精度的数据扩充

如果图像是从原始位置平移过来的，则很难对其进行准确分类。然而，给定一个图像，图像的标签将保持不变，即使我们平移、旋转或缩放图像。数据扩充是一种从给定的一组图像中创建更多图像的方法，即通过旋转、平移或缩放图像并将它们映射到原始图像的标签。

对此的直觉如下:一个人的图像将仍然对应于该人，即使该图像被轻微旋转或者该图像中的人从该图像的中间移动到该图像的最右边。

因此，我们应该能够通过旋转和平移原始图像来创建更多的训练数据，其中我们已经知道对应于每个图像的标签。

        

# 做好准备

在这个菜谱中，我们将使用 CIFAR-10 数据集，它包含 10 个不同类别的对象的图像。

我们将使用的策略如下:

*   下载 CIFAR-10 数据集
*   预处理数据集
    *   缩放输入值
    *   对输出类进行一次热编码
*   构建具有多个卷积和池层的深度 CNN
*   编译并拟合模型，以在测试数据集上测试其准确性
*   在训练数据集中生成原始图像集的随机翻译
*   在所有图像(生成的图像，加上原始图像)上安装在上一步中构建的相同模型架构
*   在测试数据集上检查模型的准确性

我们将使用`keras.preprocessing.image`包中的`ImageDataGenerator`方法实现数据扩充。

        

# 怎么做...

为了理解数据扩充的好处，让我们来看一个在有数据扩充和没有数据扩充的 CIFAR-10 数据集上计算精度的例子(代码文件在 GitHub 中作为`Data_augmentation_to_improve_network_accuracy.ipynb`提供)。

        

# 无数据扩充的模型精度

让我们在以下步骤中计算没有数据扩充的精度:

1.  导入包和数据:

```py
from matplotlib import pyplot as plt
%matplotlib inline
import numpy as np
from keras.utils import np_utils
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.layers.normalization import BatchNormalization
from keras import regularizers

from keras.datasets import cifar10
(X_train, y_train), (X_val, y_val) = cifar10.load_data()
```

2.  预处理数据:

```py
X_train = X_train.astype('float32')/255.
X_val = X_val.astype('float32')/255.

n_classes = 10
y_train = np_utils.to_categorical(y_train, n_classes)
y_val = np_utils.to_categorical(y_val, n_classes)
```

图像及其相应标签的示例如下:

![](Images/dff754a8-553a-49ad-bd16-b9b339e04ec1.png)

3.  构建和编译模型:

```py
input_shape = X_train[0].shape

model = Sequential()
model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=X_train.shape[1:]))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.2)) 
model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.3)) 
model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.4)) 
model.add(Flatten())
model.add(Dense(10, activation='softmax'))

from keras.optimizers import Adam
adam = Adam(lr = 0.01)
model.compile(loss='categorical_crossentropy', optimizer=adam,metrics=['accuracy'])
```

我们有一个更高的学习率，只是为了使模型在更少的时期内收敛得更快。这使得能够更快地比较非数据扩充情形和数据扩充情形。理想情况下，我们会让模型以较低的学习速率运行更多的时期。

4.  符合模型:

```py
model.fit(X_train, y_train, batch_size=32,epochs=10, verbose=1, validation_data=(X_val, y_val))
```

该网络的精度约为 66%:

![](Images/92ccfaf3-5c22-4811-bab8-237bf334af4d.png)

        

# 数据扩充的模型精度

在下面的代码中，我们将实现数据扩充:

1.  使用`keras.preprocessing.image`包中的`ImageDataGenerator`方法:

```py
from keras.preprocessing.image import ImageDataGenerator 
datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0,
    height_shift_range=0,
    fill_mode = 'nearest')

datagen.fit(X_train)
```

在前面的代码中，我们正在生成新的图像，这些图像在 0 到 20 度之间随机旋转。通过数据生成器后的图像示例如下:

![](Images/a17f5da5-2d6d-4b24-b090-3399a1b49dbd.png)

请注意，与之前的一组图像相比，这些图像略微倾斜。

2.  现在，我们将通过数据生成器传递总数据，如下所示:

```py
batch_size = 32
model = Sequential()
model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=X_train.shape[1:]))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.2)) 
model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.3)) 
model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('relu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.4)) 
model.add(Flatten())
model.add(Dense(10, activation='softmax'))
from keras.optimizers import Adam
adam = Adam(lr = 0.01)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
```

3.  请注意，我们正在重建模型，以便在比较数据扩充和非数据扩充场景时，再次初始化权重:

```py
model.fit_generator(datagen.flow(X_train, y_train, batch_size=batch_size),steps_per_epoch=X_train.shape[0] // batch_size, epochs=10,validation_data=(X_val,y_val))
```

请注意，`fit_generator`方法在生成新图像时适合模型。

4.  此外，`datagen.flow`指定需要根据我们在步骤 *1* 中初始化的 datagen 策略生成新的训练数据点。除此之外，我们还将每个时期的步骤数指定为数据点总数与批处理大小的比率:

![](Images/8ce964fc-7aec-48ba-a6a6-485f4850a457.png)

该代码的精度约为 80%，优于仅使用给定数据集(无数据扩充)的 66%的精度。