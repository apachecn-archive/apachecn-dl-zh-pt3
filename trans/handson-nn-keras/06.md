        

# 六、卷积神经网络

在上一章中，我们看到了如何在利用前馈神经网络的预测能力的同时执行几个信号处理任务。这个基础架构允许我们引入许多基本特性，这些特性构成了**人工神经网络**(**ann**)的学习机制。

在这一章中，我们深入探讨另一种类型的人工神经网络，即**卷积神经网络** ( **CNN** )，以擅长图像识别、对象检测和语义分割等视觉任务而闻名。事实上，这些特定架构的灵感也可以追溯到我们自己的生物学。很快，我们将回顾人类的实验和发现，这些实验和发现激发了这些表现如此出色的复杂系统的灵感。这个想法的最新迭代可以追溯到 ImageNet 分类挑战，其中 AlexNet 能够在超大数据集上的图像分类任务中胜过当时最先进的计算机视觉系统。然而，正如我们将很快看到的，CNN 背后的想法是多学科科学研究的产物，有数百万年的实验支持。

在本章中，我们将讨论以下主题:

*   为什么是 CNN？
*   视觉的诞生
*   理解生物视觉
*   现代 CNN 的诞生
*   设计 CNN
*   密集层与卷积层
*   卷积运算
*   保留图像的空间结构
*   使用过滤器的特征提取

        

# 为什么是 CNN？

CNN 非常类似于普通的神经网络。正如我们在前一章所看到的，神经网络是由具有可学习的权重和偏差的神经元组成的。每个神经元仍然使用点积计算其输入的加权和，添加一个偏差项，并将其传递给一个非线性方程。网络将只显示一个可区分的分数函数，从一端的原始图像到另一端的类分数。

并且它们还将具有损失函数，例如在最后一层上的 softmax 或 SVM。此外，我们学习的所有开发神经网络的技术都将适用。

但是你可能会问，ConvNets 有什么不同。需要注意的要点是，ConvNet 架构明确假设接收到的输入都是镜像，这一假设实际上有助于我们对架构本身的其他属性进行编码。从实现的角度来看，这样做允许网络更有效，极大地减少了网络中所需的参数数量。我们称一个网络为*卷积*，因为除了其他类型的层之外，它还有卷积层。很快，我们将探索这些特殊的层，以及其他一些数学运算，如何帮助计算机直观地理解我们周围的世界。

因此，这种特定的神经网络架构擅长各种视觉处理任务，包括对象检测、人脸识别、视频分类、语义分割、图像字幕、人类姿势估计等等。这些网络让一系列计算机视觉任务得以有效执行，其中一些对我们人类的进步至关重要(如医疗诊断)，另一些则近乎娱乐(在给定图像上叠加某种艺术风格)。在我们深入研究它的概念和当代实现之前，通过快速浏览视觉这个如此复杂但对我们人类来说如此与生俱来的东西是如何产生的，理解我们试图复制的更广泛的范围是非常有用的。

        

# 视觉的诞生

下面是一个史诗般的故事，一个发生在近 5.4 亿年前的史诗般的故事。

大约在这个时候，在后来被称为地球的淡蓝色宇宙点上，生活相当平静，没有烦恼。那时候，几乎我们所有的祖先都是水上居民，他们只是在平静的海洋中漂浮，只有在漂浮的时候才会大嚼食物。是的，这与今天这个掠夺成性、充满压力和刺激的世界截然不同。

突然，很奇怪的事情发生了。在随后相对较短的时间内，我们星球上的动物种类和数量都出现了爆炸。在此后仅仅 2000 万年的时间里，你能在我们的水地球上找到的生物种类发生了巨大的变化。它们从你偶尔会遇到的、以松散连接的群体形式组织起来的单细胞生物，变成了复杂的多细胞生物，出现在每条小溪和每个角落。

生物学家们困惑了很长时间，争论是什么导致了进化加速的大爆炸。我们真正发现的是生物视觉系统的诞生。通过研究那个时代的生物化石记录，动物学家能够提出决定性的证据，将物种的爆炸与感光细胞的首次出现联系起来。这些细胞使生物体能够感知和响应光线，引发了一场进化军备竞赛，最终导致了复杂的哺乳动物视觉皮层，你现在可能正在用它来解释这段文字。事实上，视觉的天赋让生命变得更有活力和主动性，因为现在生物体能够感知和应对它们的环境。

今天，视觉是几乎所有生物的主要感觉系统之一，无论是否有智能。事实上，我们人类几乎将一半的神经元能力用于视觉处理，这使它成为我们用来定位自己、识别人和物以及处理日常生活的最大的感觉系统。事实证明，视觉是认知系统的一个非常重要的组成部分，不管是生物的还是其他的。因此，研究自然创造的视觉系统的发展和实现是非常合理的。毕竟，没有必要重新发明轮子。

        

# 理解生物视觉

我们对生物视觉系统的下一个认识来自哈佛大学科学家在 20 世纪 50 年代末进行的一系列实验。诺贝尔奖获得者 David Hubel 和 Torstein Wiesel 向世界展示了哺乳动物视觉皮层的内部工作原理，通过绘制受体细胞的活动以及猫的视觉路径，从视网膜到视觉皮层。这些科学家使用电生理学来准确理解我们的感觉器官如何吸收、处理和解释电磁辐射，以产生我们在周围看到的现实。这使他们能够更好地理解刺激的流动和在单个神经元水平上发生的相关反应:

![](Images/a057c3ee-63ef-4454-b8e4-1acbc968f2a9.png)

下面的截图描述了细胞对光的反应:

![](Images/55e51fb4-6c25-4605-aec1-3b30a1ba6fba.png)

由于他们在神经科学领域的实验，我们能够与你分享他们研究的几个关键因素，这些因素直接影响了对视觉信号处理的科学理解，导致了一系列学术贡献，使我们到了今天。这些元素启发了我们大脑利用视觉信息处理的机制，启发了 CNN 的设计，这是现代视觉智能系统的基石。

        

# 概念化空间不变性

这些概念中的第一个来自于空间不变性的概念。研究人员注意到，不管图案在屏幕上的确切位置如何，猫对特定图案的神经激活是一致的。直觉上，同一组神经元会对给定的模式(即一条线段)产生反应，即使该模式出现在屏幕的顶部或底部。这表明神经元的激活是空间不变的，这意味着它们的激活不依赖于给定模式的空间位置。

        

# 定义神经元的感受野

其次，他们还注意到神经元*负责*对给定输入的特定区域做出反应。他们将神经元的这种特性命名为其**感受野**。换句话说，某些神经元只对给定输入的某些区域做出反应，而其他神经元则对同一输入的不同区域做出反应。神经元的感受野简单地表示神经元可能响应的输入范围。

        

# 实现神经元的层次结构

最后，研究人员能够证明视觉皮层中存在神经元的层次结构。他们表明，低级细胞的任务是检测简单的视觉模式，如线段。这些神经元的输出被用于后续的神经元层，以构建越来越复杂的模式，形成我们看到的物体和与之互动的人。事实上，现代神经科学证实，视觉皮层的结构是分层组织的，可以利用前几层的输出进行越来越复杂的推理，如图所示:

![](Images/fb3d45e9-1025-4e42-a472-06eef7410b21.png)

前面的图表意味着识别一个朋友包括检测组成他们脸的线段( **V1** )，使用这些线段来构建形状和边缘( **V2** )，使用这些形状和边缘来形成复杂的形状，如眼睛和鼻子( **V3** )，然后利用先前的知识来推断你的朋友中谁的眼睛和鼻子最像( **IT-posterior** )。基于这样的推理，甚至与朋友的个性、魅力等相关的更高层次的激活也可能出现。

        

# 现代 CNN 的诞生

直到 20 世纪 80 年代，Heubel 和 Wiesel 的发现才被重新用于计算机科学领域。*神经认知器*(福岛，1980:【https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf】T2)通过将一层层的细胞夹在中间，利用了简单和复杂细胞的概念。这种现代神经网络的祖先使用上述交替层来顺序地包括可修改的参数(或简单单元)，同时使用汇集层(或复杂单元)来使网络不受简单单元的微小变化的影响。虽然直观，但这种架构仍然不足以捕捉视觉信号中存在的错综复杂。

重大突破之一发生在 1998 年，当时著名的人工智能研究人员 Yan Lecun 和 Yoshua Bengio 能够利用基于梯度的权重更新来训练 CNN，以执行文档识别。这个网络在识别邮政编码方面做得非常好。类似的网络很快被美国邮政局等组织采用，以自动化分拣邮件的繁琐任务(不，不是电子邮件)。虽然这些结果足以引起狭窄领域的商业兴趣，但这些网络仍然无法处理更具挑战性和复杂的数据，如人脸、汽车和其他现实世界的物体。然而，这些研究人员和许多其他人的集体工作导致了更大更深的 CNN 的现代化身，首先出现在 ImageNet 分类挑战赛上。这些网络现在已经主宰了计算机视觉领域，出现在当今最复杂的人工视觉智能系统中。这些网络现在被用于医疗图像诊断、探测外层空间的天体以及让计算机玩老派雅达利游戏等任务。

        

# 设计 CNN

现在，有了生物视觉的直觉，我们理解了神经元必须如何分层组织，以检测简单的模式，并使用这些模式逐步构建与现实世界对象相对应的更复杂的模式。我们还知道，我们必须实现空间不变性的机制，以允许神经元处理在给定图像的不同空间位置出现的类似输入。最后，我们意识到，为每个神经元实现感受野对于实现神经元到真实世界中的空间位置的地形映射是有用的，使得附近的神经元可以代表视野中的附近区域。

        

# 密集层与卷积层

您会记得，在前一章中，我们使用了一个前馈神经网络来执行手写数字识别任务，该网络由完全连接的密集神经元层组成。在构建我们的网络时，我们被迫将每个图像的 28 x 28 的输入像素展平为 784 像素的向量。这样做导致我们丢失了任何空间相关的信息，而我们的网络可以利用这些信息对显示的数字进行分类。我们只是为数据集中的每张图像显示了一个 784 维的向量，并期望它此后能识别数字。虽然这种方法足以在从漂亮和干净的 MNIST 数据集中分类简单的手写数字时获得相当高的准确度，但它很快在我们可能想要处理的更复杂的数据中变得不切实际，这些数据涉及不同空间方向的大量局部模式。

理想情况下，我们希望保留空间信息，并重用神经元来检测不同空间区域中出现的相似模式。这将使我们的卷积网络更加高效。通过重用其神经元来识别我们数据中的特定模式，而不管它们的位置，CNN 在视觉任务中的使用是有利的。另一方面，如果一个密集连接的网络出现在图像的另一个位置，它将被迫重新学习一个模式。鉴于视觉数据中存在的自然空间层次，使用卷积图层是检测微小局部模式并使用它们逐步构建更复杂模式的理想方式。

下面的屏幕截图说明了可视数据的分层性质:

![](Images/896eb13d-29a1-4df1-baa0-657440c08e48.png)

我们提到的密集层的另一个问题是它们在从数据中捕获局部模式方面的弱点。众所周知，致密层可以捕捉图像中所有像素的全局模式。然而，正如 Hubel 和 Wiesel 的发现所表明的，我们希望将神经元的感受域限制在数据中存在的局部模式，并使用这些模式逐步形成更复杂的模式。这将允许我们的网络处理非常不同形式的可视数据，每种数据都包含不同类型的本地模式。为了克服这些问题，CNN 的核心部分被开发出来，称为**卷积运算**。

        

# 卷积运算

单词 *convolvere* 来自拉丁语，翻译成 *convolve* 或 *roll together* 。从数学的角度来看，您可以将卷积定义为基于微积分的积分，表示两个给定函数重叠的量，因为两个函数中的一个滑过另一个。换句话说，对两个函数( *f* 和 *g* )执行卷积运算将产生第三个函数，该函数表示一个函数的形状如何被另一个函数修改。术语*卷积*指结果函数和计算过程，源于信号处理的数学子领域，如下图所示:

![](Images/98d0d59a-e7b7-454c-9bba-6fe684609c99.png)

那么，我们如何利用这次行动为我们带来优势呢？

        

# 保留图像的空间结构

首先，我们将使用图像的固有空间结构，简单地将它作为一个 *n* 维张量输入到我们的神经网络中。这意味着网络将在原始矩阵位置接收每个像素，而不是像我们之前所做的那样，减少到单个向量内的位置。在 MNIST 的例子中，一个卷积网络将接收一个 28×28×1 的张量作为输入，代表每个图像。这里，28×28 表示二维网格，在其上排列像素以形成手写数字，而末尾的`1`表示图像的颜色通道(即，给定图像中每个像素的像素值的数量)。正如我们所知，彩色数据集可能具有 28 x 28 x 3 的图像尺寸，其中`3`表示形成单个像素颜色的不同红色、绿色和蓝色值。因为我们只处理 MNIST 数据集中每个像素的单个灰度值，所以颜色通道表示为 1。因此，重要的是要理解图像作为三维张量进入我们的网络，保留我们图像数据的空间结构:

![](Images/e0c88e93-5c70-4ca0-a02c-cc73dbbce452.png)

        

# 感受野

现在，我们可以在网络架构中利用图像固有的额外空间信息的属性。换句话说，我们现在准备执行卷积运算。这仅仅意味着我们将使用更小的空间片段，并将其滑过我们的输入图像，作为检测局部模式的过滤器。这里的直觉是将我们输入数据空间的小块连接到隐藏层中相应的神经元。这样做允许神经元在每次卷积时只观察图像的局部区域，从而限制了它的感受野。限制神经元的感受野有两个主要原因。由于我们认为图像中邻近的像素更有可能彼此相关，因此限制我们网络中神经元的感受域可以使这些神经元更好地区分给定图像中像素之间的局部差异。此外，这种做法还允许我们大幅减少网络中可学习参数(或权重)的数量。以这种方式，我们使用一个**过滤器**，它本质上是一个权重矩阵，并从图像的左上角开始在输入空间上迭代地应用它。在我们的输入空间中，通过在每个像素的顶部将我们的滤波器居中，我们在每个卷积处向右移动一大步。在卷积了对应于顶部像素行的图像片段之后，我们再次从图像的左侧开始重复该操作，这一次是针对下面的行。这就是我们如何在整个输入图像上滑动过滤器，通过计算输入区域和所谓的**过滤器**的点积，提取每个卷积的局部特征。

下图描述了卷积运算的初始步骤。此处显示的三维蓝色矩形将逐渐滑过整个图像的各个部分(红色)，这就是卷积运算的名称。矩形本身被称为一个滤波器，或一个卷积核:

![](Images/89364f65-c2de-43ec-ba69-99f8f3a67982.png)        

# 使用过滤器的特征提取

每个滤波器本质上都可以被认为是神经元的排列，在精神上类似于我们在[第 3 章](46e25614-bb5a-4cca-ac3e-b6dfbe29eea5.xhtml)、*信号处理-用神经网络进行数据分析*中遇到的那些。这里，滤波器的神经元用随机权重初始化，并在训练期间使用反向传播算法逐步更新。过滤器本身负责检测特定类型的图案，从线段到曲线和更复杂的形状。当滤镜经过输入图像的一个区域时，滤镜权重与该位置的像素值相乘(按元素),生成一个输出向量。然后，我们使用求和运算将这个向量简化为标量值，就像我们之前对前馈神经网络所做的一样。这些标量值在滤波器移动到的每个新位置生成，跨越整个输入图像。对于给定的滤波器，这些值被存储在称为**激活图**(也称为**特征图**或**响应图**的东西中。激活图本身通常在尺寸上小于输入图像，并且它体现了输入图像的新表示，同时强调了其中的某些模式。

滤波器本身的权重可以被认为是卷积层的可学习参数，并且被更新以捕捉与手边的任务相关的模式，同时网络训练:

![](Images/3fd980af-7e27-4b63-9338-0dbbad1f50ef.png)

        

# 细胞神经网络中误差的反向传播

当我们的网络训练寻找理想的过滤器权重时，我们希望给定过滤器的激活图能够捕捉可能存在于我们的数据中的最具信息性的视觉模式。本质上，这些激活图是使用矩阵乘法生成的。这些激活图作为输入被输入到下一层，因此信息向前传播，直到我们模型的最后一层，执行分类。此时，我们的`loss`函数评估网络预测与实际输出之间的差异，并反向传播预测误差以调整每层的网络权重。

这基本上是一个 ConvNet 如何被训练，在一个非常高的水平。卷积运算包括迭代计算转置滤波器矩阵(保存我们的卷积滤波器的权重)与相应的像素输入空间的点积，以从训练示例中提取可概括的特征。然后，这些特征图(或激活图)首先被输入池层以减少其维数，随后被输入完全连接的层以确定哪种过滤器组合最能代表给定的输出类。当模型权重在向后传递期间更新时，新的激活图在下一次向前传递时生成，其理想地编码我们数据的更有代表性的特征。在这里，我们对 ConvNet 架构进行了简要的视觉总结:

![](Images/b288bdad-fdfc-45d1-a7a5-be63bb8a9a9f.png)

它遵循以下步骤:

1.  通过卷积学习输入图像的特征
2.  通过激活函数引入非线性(真实世界数据是非线性的)
3.  通过池化降低维度并保持空间不变性

        

# 使用多个过滤器

由于滤镜是特定于模式的(也就是说，每个滤镜擅长拾取特定类型的模式)，我们需要不止一个滤镜来拾取图像中可能存在的所有不同类型的模式。这意味着我们可以为网络中的给定卷积层使用多个滤波器，从而允许我们为输入空间的给定区域提取多个不同的局部特征。这些存储在特定过滤器的激活图中的局部特征可以传递给后续层，以构建更复杂的图案。渐进卷积层将使用不同的滤波器来卷积来自前一层的输入激活图，再次提取输入图并将其转换成对应于所使用的每个滤波器的激活图的三维张量输出。新的激活图将再次是三维张量，并且可以类似地在后续层上传递。回想一下，当图像进入我们的网络时，它是作为一个三维张量进入的，其(宽度、高度和深度)对应于输入图像的维度(其中深度由像素通道表示)。另一方面，在我们的输出张量中，深度轴表示在前一卷积层中使用的滤波器的数量，其中每个滤波器产生它自己的激活图。本质上，这就是数据在 CNN 中向前传播的方式，以图像的形式进入，以三维激活图的形式退出，随着数据在更深的层中传播，通过各种过滤器逐渐转换。当我们构建一个 ConvNet 时，转换的确切性质将很快变得更加清晰。我们将进一步讨论理论，然后我们将准备继续。

        

# 卷积的步幅

下图描述了您现在已经熟悉的二维卷积运算(为简单起见)。它显示了一个 4 x 4 滤镜(红框)滑过一个更大的图像(14 x 14)，一次移动两个像素:

![](Images/45490089-e87f-47df-8725-450ecaefce34.png)

滤波器在每次迭代中移动的像素数称为其**步距**。在每一步，使用来自相应输入区域的像素矩阵以及滤波器权重来计算点积。这些点积作为标量值存储在该过滤器的激活图中(如下所示，显示为一个 6 x 6 的正方形)。因此，激活图表示层输入的简化表示，本质上是一个由求和点积组成的矩阵，我们通过在输入数据段上卷积我们的滤波器来获得该矩阵。在更高的水平上，激活图表示相应过滤器正在检测的特定模式的神经元的激活。这些滤波器的步幅越大，相应输入区域的采样越不详细，而步幅越短，单个像素的采样越频繁，从而允许更高清晰度的激活图。

        

# 什么是特性？

虽然使用不同类型的过滤器收集要素的总体机制可能很清楚，但您可能很想知道要素实际上是什么样子，以及我们如何从过滤器中提取它们。

让我们考虑一个简单的例子来阐明我们的理解。假设您希望从一堆字母的灰度图像中检测出字母 X。CNN 会怎么做呢？让我们首先考虑 X 的图像。如下图所示，我们可以认为具有正值的像素构成了 X 的线条，而具有负值的像素只是表示图像中的空白，如下图所示:

![](Images/63093442-c5bf-41be-87b7-e09edd13968c.png)

但是随之而来的是位置变化的问题:如果 X 以任何其他方式稍微旋转或扭曲了呢？在现实世界中，字母 X 有许多大小、形状等等。我们如何使用更小的过滤器来捕捉潜在的图案，从而分解 X 的图像？好吧，这里有一个方法:

![](Images/ba63b741-18d9-40a1-ae8c-6d7bd9d98f13.png)

您可能已经注意到，我们实际上可以将图像分成更小的片段，每个片段(用绿色、橙色和紫色的方框表示)代表图像中的重复模式。在我们的例子中，我们能够使用两个对角线方向的过滤器和一个交叉过滤器，并在我们的图像中滑动它们，以拾取形成 x 的线段。本质上，您可以将每个过滤器视为各种模式检测器。随着这些不同的过滤器在我们的输入图像上卷积，我们剩下的激活图开始类似于长的水平线和十字，网络学习将它们组合起来以识别字母 x。

        

# 用过滤器可视化特征提取

让我们考虑另一个例子，以巩固我们对滤波器如何检测模式的理解。考虑这个从 MNIST 数据集中提取的数字 7 的描述。我们使用这张 28 x 28 像素化的图像来展示滤镜实际上是如何拾取不同图案的:

![](Images/e3666371-2cdb-4ae5-975f-f3e73b6e5a56.png)

直观上，我们注意到这个 7 由两条水平线和一条倾斜的垂直线组成。本质上，我们需要用可以拾取这些独立模式的值来初始化我们的过滤器。接下来，我们观察一些 3 x 3 滤波器矩阵，对于手头的任务，ConvNet 通常会学习这些矩阵:

![](Images/08cb92c2-c8cb-466c-a60f-edfc86715ef0.png)

虽然看起来不太直观，但这些滤波器实际上是复杂的边缘检测器。为了了解它们是如何工作的，让我们将过滤器权重中的每个 0 描绘成灰色，而每个值 1 取白色，剩下-1 取黑色。当这些滤波器在输入图像上卷积时，使用滤波器值和下面的像素来执行逐元素乘法。这种操作的产物是另一个矩阵，称为**激活图**，表示对于给定的输入图像，由它们各自的过滤器拾取的特定特征。现在，让我们来看看使用这四种滤波器中的每一种，对 a 7 的输入图像执行卷积运算的效果，以准确理解它们分别拾取哪种模式。下图描绘了卷积层中每个滤波器的激活图，图中显示了一个 7:

![](Images/53e3fdc9-124c-4381-8cf4-cb75e93adafc.png)

我们观察到，通过简单地计算每个空间位置的像素值和滤波器权重的点积和，每个滤波器能够拾取输入图像中的特定模式。拾取的模式可以由前述激活图中的白色区域表示。我们看到，前两个滤波器分别熟练地拾取 7 的图像中的上下水平线。我们还注意到后两个过滤器如何分别拾取形成数字 7 主体的内部和外部垂直线。虽然这些仍然是相当简单的模式检测的例子，但通过区分颜色和形状(本质上是数字的模式)，ConvNets 中的渐进层往往能够拾取更多信息的结构。

        

# 查看复杂滤波器

下图显示了第二层 ConvNet 中与特定输入相关联的每个网格的前九个激活图。在左边，你可以把迷你网格想象成给定输入下单个神经元的激活。右边相应的彩色网格与这些神经元显示的输入相关。我们在这里视觉化的是那种*最大化*激活这些神经元的输入。我们注意到，已经可以看到一些非常清晰的圆形检测器神经元(网格 2，2)，它们被激活用于输入，如灯罩顶部和动物眼睛:

![](Images/2bcd19b0-5d97-436d-8491-fffc1f3d477e.png)

类似地，我们注意到一些类似正方形的图案检测器(网格 4，4 ),它们似乎会为包含门框和窗框的图像激活。随着我们逐渐可视化 CNN 中更深层的激活图，我们观察到甚至更复杂的几何图案被拾取，代表狗的脸(网格 1，1)，鸟的腿(网格 2，4)，等等:

![](Images/99378b16-e96a-4e0f-8614-46d48a6d3f34.png)

        

# 总结卷积运算

我们在这里所做的只是将一组权重(即过滤器)应用于局部输入空间以进行特征提取。我们迭代地这样做，以固定的步长在输入空间中移动我们的滤波器，称为**步幅**。此外，使用不同的过滤器允许我们从给定的输入中捕获不同的模式。最后，由于滤波器在整个图像上卷积，我们能够在空间上共享给定滤波器的参数。这允许我们使用*相同的*滤波器来检测图像的不同位置中的相似图案，这与前面讨论的空间不变性的概念有关。然而，卷积层输出的这些激活图本质上是抽象的高维表示。在我们继续进行分类之前，我们需要实现一种机制来将这些表示减少到更易于管理的维度。这就把我们带到了**池层**。

        

# 了解池层

使用卷积层时的最后一个考虑事项是堆叠简单细胞以检测局部模式，堆叠复杂细胞以缩减采样表示，正如我们之前在猫脑实验和 neocognitron 中看到的那样。我们看到的卷积滤波器就像简单的细胞一样，在输入图像的局部区域提供一些刺激的情况下，聚焦于输入的特定位置并训练神经元触发。另一方面，复杂细胞对刺激位置的特异性要求较低。这就是池层的用武之地。这种池技术旨在将 CNN 层的输出减少到更易于管理的表示形式。在卷积层之间定期添加池层，以便对卷积层的输出进行空间下采样。所有这一切都是为了逐步减小卷积层输出的大小，从而实现更高效的表示，如下所示:

![](Images/9e3eb8a8-0a3a-4565-b004-318a4fba670b.png)

如您所见，体积的深度得以保留，因为我们已经用大小为 2 的过滤器和大小为 2 的步幅汇集了大小为 224 x 224 x 64 的输入体积。这给出了 112×112×64 的输出体积。

        

# 联营业务的类型

这同样减少了我们的网络中相同任务所需的可学习参数的数量，并防止我们的网络过度拟合。最后，对给定层生成的每个激活图(即输入张量的每个深度切片)执行池化，并使用其自己的过滤器在空间上调整其输入的大小。在每个深度切片上，使用 2 x 2 的过滤器以及 2 的跨距来合并层是非常常见的。有许多方法可以执行这种下采样。最常见的是，这是通过一个 **max pooling** 操作来实现的，这仅仅意味着我们在我们的池过滤器下保留输入区域中所有像素中具有最高值的像素。下图说明了在输入张量的给定切片上的最大池化操作:

![](Images/dc8b9aa0-1ece-47cb-9762-fbeef39a3276.png)

汇集层在输入体积的每个深度切片中独立地对激活体积进行空间下采样。最常见的缩减采样操作是最大化，导致最大池化，这里显示的步长为 2。也就是说，每个最大值由四个数字(小的 2×2 平方)组成。

您也可以通过获取 2 x 2 个正方形的平均值来进行缩减采样，如图所示。这个操作被称为**平均池**。正如我们将在后面的章节中看到的，Keras 提供了相当多的池层，每个池层对前一层的输出执行不同类型的缩减采样操作。选择将在很大程度上取决于您的特定用例。在影像分类的情况下，二维或三维 max pooling 图层是最常用的。层的维度仅仅是指它接受的输入类型。例如，二维图层用于在处理灰度图像时进行下采样，三维图层用于彩色图像。可以参考维护良好的文档自己研究一下。

        

# 在 Keras 中实现 CNN

已经对 CNN 的关键组件有了较高层次的理解，我们现在可以开始自己实际实现一个了。这将使我们熟悉构建卷积网络时的关键架构考虑因素，并了解使这些网络性能如此出色的实施细节。很快，我们将在 Keras 中实现卷积层，并探索下采样技术，如池化层，以了解我们如何利用卷积层、池化层和密集连接层的组合来完成各种图像分类任务。

对于这个例子，我们将采用一个简单的用例。比方说，我们想让 CNN 以微笑或皱眉的形式来探测人类的情感。这是一个简单的二元分类任务。我们如何进行？首先，我们需要一个人类微笑和皱眉的标记数据集。虽然有很多方法可以做到这一点，但我们选择了**快乐之家数据集**来实现这一目的。这个数据集有大约 750 个人的图像，每个人或者微笑或者皱眉，存储在`h5py`文件中。要跟进，你需要做的就是下载 Kaggle 网站上的数据集，你可以通过这个链接免费访问:[https://www.kaggle.com/iarunava/happy-house-dataset](https://www.kaggle.com/iarunava/happy-house-dataset)

        

# 探测我们的数据

让我们从加载和探测我们的数据集开始，以了解我们正在处理什么。我们创建一个简单的函数来读取我们的`h5py`文件，提取训练和测试数据，并将其放入标准的 NumPy 数组中，如下所示:

```
import numpy as np
import h5py
import matplotlib.pyplot as plt
# Function to load data
def load_dataset():
# use h5py module and specify file path and mode (read) all_train_data=h5py.File('C:/Users/npurk/Desktop/Chapter_3_CNN/train_happy.h5', "r")
all_test_data=h5py.File('C:/Users/npurk/Desktop/Chapter_3_CNN/test_happy.h5', "r")
# Collect all train and test data from file as numpy arrays
x_train = np.array(all_train_data["train_set_x"][:]) 
y_train = np.array(all_train_data["train_set_y"][:]) 
x_test = np.array(all_test_data["test_set_x"][:])
y_test = np.array(all_test_data["test_set_y"][:]) 
# Reshape data
y_train = y_train.reshape((1, y_train.shape[0]))
y_test = y_test.reshape((1, y_test.shape[0]))    
return x_train, y_train, x_test, y_test
# Load the data 
X_train, Y_train, X_test, Y_test = load_dataset()
```

        

# 验证数据形状

接下来，我们将打印出我们的训练和测试数据的形状。在下面的代码块中，我们注意到我们正在处理 64 x 64 像素的彩色图像。我们的训练集中有 600 个，测试集中有`150`。我们还可以看看图像实际上是什么样子，就像我们在前面使用 Matplotlib 的示例中所做的那样:

```
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

(600, 64, 64, 3)
(150, 64, 64, 3)
(1, 600)
(1, 150)

# Plot out a single image plt.imshow(X_train[0]) # Print label for image (smiling = 1, frowning = 0) print ("y = " + str(np.squeeze(Y_train[:, 0])))
y = 0
```

—瞧！女士们先生们，我们有一张皱眉头的脸:

![](Images/1ab7a4ab-dbf3-4581-8103-06808ab90da1.png)

        

# 标准化我们的数据

现在，我们将通过在 0 和 1 之间重新调整像素值来准备我们的图像。我们还转置我们的标签矩阵，因为我们希望它们的方向是(600，1)，而不是(1，600)，参考我们的训练标签，如先前在训练标签中所示。最后，我们打印出训练集和测试集的特征和标签的形状:

```
# Normalize pixels using max channel value, 255 (Rescale data)

X_train = X_train/255.
X_test = X_test/255.

# Transpose labels
Y_train = Y_train.T
Y_test = Y_test.T

# Print stats
print ("Number of training examples : " + str(X_train.shape[0]))
print ("Number of test examples : " + str(X_test.shape[0]))
print ("X_train shape: " + str(X_train.shape))
print ("Y_train shape: " + str(Y_train.shape))
print ("X_test shape: " + str(X_test.shape))
print ("Y_test shape: " + str(Y_test.shape))
-----------------------------------------------------------------------
Output:
Number of training examples : 600
Number of test examples : 150
X_train shape: (600, 64, 64, 3)
Y_train shape: (600, 1)
X_test shape: (150, 64, 64, 3)
Y_test shape: (150, 1)
```

然后，我们将 NumPy 数组转换为浮点算术值，这是我们的网络更喜欢的:

```
#convert to float 32 ndarrays
from keras.utils import to_categorical
X_train = X_train.astype('float32') X_test = X_test.astype('float32') Y_train = Y_train.astype('float32') Y_test = Y_test.astype('float32')
```

        

# 做一些进口

最后，我们开始导入新的层，我们将用于我们的情感分类任务。在前面代码的块的底部，我们导入一个二维卷积层。卷积层的维数是一个特定于你希望执行的任务的属性。因为我们处理的是图像，所以二维卷积层是最佳选择。如果我们要处理时间序列传感器数据(如生物医学数据，例如脑电图或股票市场等金融数据)，那么一维卷积层将是更合适的选择。同样，如果视频是输入数据，我们会使用三维卷积层:

```
import keras
from keras.models import Sequential
from keras.layers import Flatten
from keras.layers import Dense
from keras.layers import Activation, Dropout
from keras.optimizers import Adam
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers.normalization import BatchNormalization 
```

类似地，我们还导入了一个二维最大池层，以及一个批量标准化器。当数据通过我们的网络传播时，批量标准化简单地允许我们处理层输出的变化值。

“内部协变量偏移”的问题确实是
CNN 以及其他 ANN 架构中一个众所周知的现象，指的是在几次训练迭代后输入的统计
分布的变化，减缓了我们的模型收敛到理想权重的速度。这个问题可以通过使用均值和方差参考简单地在小批量中标准化我们的数据来避免。虽然我们鼓励您进一步研究内部协变量转换的问题和批量标准化背后的数学，但现在只要知道这有助于我们更快地训练我们的网络并允许更高的学习速率，同时使我们的网络权重更容易初始化就足够了

        

# 卷积层

Keras 中的卷积层有两个主要的架构考虑因素。第一个与给定层中使用的过滤器数量有关，而第二个表示过滤器本身的大小。让我们来看看如何通过初始化一个空白序列模型并添加第一个卷积层来实现这一点:

```
model=sequential()
#First Convolutional layer 
model.add(Conv2D(16,(5,5), padding = 'same', activation = 'relu', input_shape = (64,64,3)))
model.add(BatchNormalization())
```

        

# 定义过滤器的数量和尺寸

正如我们之前看到的，我们用 16 个滤镜来定义图层，每个滤镜的高度和宽度都是 5 x 5。实际上，我们的过滤器的合适尺寸是 5×5×3。然而，所有滤波器的深度跨越给定输入张量的全部深度，因此永远不需要指定。由于这是第一层，接收我们的训练图像的张量表示作为输入，对于覆盖的每个像素的红色、绿色和蓝色值，我们的过滤器的深度将是 3。

        

# 填充输入张量

直观地思考卷积运算，很明显，当我们在输入张量上滑动我们的滤波器时，最终发生的情况是，我们的滤波器通过边界和边缘的频率低于通过输入张量的其他部分。这仅仅是因为当不位于输入边缘的每个像素跨越图像时，过滤器可能会对其进行多次重新采样。这给我们的输出表示留下了来自输入的边界和边缘的不均匀采样，称为**边界效应**。

我们可以通过简单地用零填充输入张量来避免这种情况，如下所示:

![](Images/c1ad6a02-499a-4aaf-8654-16637e6cc1bb.png)

以这种方式，通常出现在我们的输入张量边缘的像素现在稍后出现，允许对输入中的所有像素执行均匀采样。我们在第一个卷积层中指定，我们希望保持输入的长度和宽度，以确保输出张量相对于其长度和宽度具有相同的空间维度。这是通过将层的填充参数定义为`same`来实现的。如前所述，卷积层输出的深度由我们选择使用的滤波器数量表示。在我们的例子中，这将是 16，表示当我们的 16 个滤波器中的每一个在输入空间上卷积时产生的 16 个激活图。最后，我们将输入形状定义为任何单个输入图像的尺寸。对我们来说，这相当于我们数据集中的 64 x 64 x 3 彩色像素，如下所示:

```
model = Sequential()
#First Convolutional layer 
model.add(Conv2D(16,(5,5), padding = 'same', activation = 'relu', input_shape = (64,64,3)))
model.add(BatchNormalization())
```

        

# 最大池层

来自我们的第一个卷积层的激活映射被归一化，并馈入下面的最大池层。与卷积运算类似，池化一次应用于一个输入区域。对于最大池的情况，我们简单地取我们的像素网格中的最大值，其代表与每个特征最强相关的像素，并且组合这些最大值以形成输入图像的较低维度表示。以这种方式，我们保留了更重要的值，并丢弃了相应激活图的给定网格中的剩余值。

这种下采样操作自然会导致一定程度的信息丢失，但却大大减少了网络所需的存储空间，从而显著提高了效率:

```
#First Pooling layer 
model.add(MaxPooling2D(pool_size = (2,2)))
model.add(Dropout(0.1))
```

        

# 利用完全连接的图层进行分类

然后，我们简单地添加几层卷积、批量标准化和漏失，逐步构建我们的网络，直到我们到达最终层。就像在 MNIST 的例子中，我们将利用密集连接的层在我们的网络中实现分类机制。在我们这样做之前，我们必须将前一层(16 x 16 x 32)的输入展平为维度(8，192)的 1D 向量。我们这样做是因为基于密集层的分类器更喜欢接收 1D 向量，而不像来自前一层的输出。我们继续添加两个紧密连接的层，第一层有 128 个神经元(任意选择)，第二层只有一个神经元，因为我们正在处理二元分类问题。如果一切都按计划进行，这一个神经元将得到来自前几层的神经元内阁的支持，并学会在看到某个输出类(例如，微笑的脸)时触发，而在看到来自另一个类的图像(例如，皱眉的脸)时不触发。注意，我们再次为最后一层使用 sigmoid 激活函数，它计算每个给定输入图像的分类概率:

```
#Second Convolutional layer 
model.add(Conv2D(32, (5,5), padding = 'same', activation = 'relu'))
model.add(BatchNormalization())
#Second Pooling layer 
model.add(MaxPooling2D(pool_size = (2,2)))
#Dropout layer
model.add(Dropout(0.1))
#Flattening layer
model.add(Flatten())
#First densely connected layer
model.add(Dense(128, activation = 'relu'))
#Final output layer
model.add(Dense(1, activation = 'sigmoid'))
```

        

# 总结我们的模型

让我们将模型可视化，以便更好地理解我们刚刚构建的内容。您会注意到，激活图的数量(由后续层输出的深度表示)在整个网络中逐渐增加。另一方面，激活图的长度和宽度趋向于减小，在到达脱落层时，从(64×64)减小到(16×16)。这两种模式在大多数(如果不是全部的话)CNN 的现代迭代中是常规的。

各层之间输入和输出尺寸差异背后的原因可能取决于您选择如何解决我们之前讨论过的边界效应，或者您为卷积层中的滤波器实现了什么样的 T2 步长。较小的步幅将导致较高的维度，而较大的步幅将导致较低的维度。这只是为了计算点积的位置数量，同时将结果存储在激活图中。较大的滤波器步幅(或步长)将更早到达图像的末端，在较小滤波器步幅的相同输入空间上计算较少的点积值。卷积层的步幅可以通过定义步幅参数来设置，其中给定层具有整数或单个整数的元组/列表。整数值指的是每次卷积运算的步距长度:

```
model.summary()
```

以下是摘要:

![](Images/219aa4f8-8c91-4f49-8679-6a85e4769a3b.png)

如前所述，您会注意到卷积层和最大池层都会生成一个三维张量，其维度对应于输出高度、输出宽度和输出深度。层输出的深度实质上是初始化的每个滤波器的激活图。我们用 16 个滤波器实现了第一个卷积层，用 32 个滤波器实现了第二个卷积层，因此各层将产生如此多的激活图，如前所述。

        

# 编译模型

目前，我们已经涵盖了设计 ConvNet 所涉及的所有关键架构决策，并准备编译我们正在构建的网络。我们选择了`adam`优化器和`binary_crossentropy`损失函数，就像我们在上一章中为二元情感分析任务所做的那样。类似地，我们还导入了`EarlyStopping`回调来监控我们在验证集上的损失，以了解我们的模型在每个时期对看不见的数据做得如何:

![](Images/8d2424db-d215-464b-8371-1754e63cba37.png)

        

# 检查模型准确性

正如我们之前看到的，在我们培训课程的最后一个时期，我们达到了 88%的测试准确率。通过解释我们的分类器的精度和召回分数，让我们看看这到底意味着什么:

![](Images/bc8a4fd2-d20e-4f13-91a9-525cdde053fb.png)

正如我们之前注意到的，在我们的测试集中，正确预测的正面观察与正面观察总数的比率(也称为**精度分数**)非常高，为 0.98。召回分数稍低，表示正确预测的结果数除以应该返回的结果数。最后，F-measure 简单地将精确度和召回分数组合为调和平均值。

为了补充我们的理解，我们在测试集上绘制了分类器的混淆矩阵，如下所示。这本质上是一个误差矩阵，让我们直观地看到我们的模型是如何执行的。 *x* 轴表示我们的分类器的预测类，而 *y* 轴表示我们的测试示例的实际类。正如我们所看到的，我们的分类器错误地检测到大约 17 幅图像，它认为这个人在微笑，而实际上他们在皱眉(也称为**假阳性**)。

另一方面，我们的分类器只犯了一个错误，将微笑的脸分类为皱眉的脸(也称为**假阴性**)。考虑假阳性和假阴性有助于我们评估我们的分类器在现实世界场景中的效用，并让我们对部署这样的系统进行成本效益分析。在我们的例子中，考虑到我们分类任务的主题，这是不必要的；然而，在使用这样的学习系统之前，其他场景(例如皮肤癌检测)将需要仔细考虑和评估:

![](Images/20d88cab-a52e-4927-9bab-8febf0d5501a.png)

每当您对模型的准确性感到满意时，您可以保存如下所示的模型。请注意，这不仅构成了最佳实践，因为它为您提供了以前的尝试、采取的步骤和取得的结果的详细记录，而且如果您想要进一步探索该模型，通过查看它的中间层来了解它实际上了解了什么，这也是非常有用的，我们马上就要这样做:

![](Images/e06e4e2d-c845-4935-8774-122927e3916e.png)

        

# 察觉微笑的问题是

在这一点上，我们必须注意，外部有效性的问题(也就是我们模型的可推广性)在像微笑检测器这样的数据集上仍然存在。鉴于收集数据的方式受到限制，期望我们的 CNN 在其他数据上做得更好是不合理的。首先，用低分辨率输入图像训练网络。此外，它每次只能在同一位置看到一个微笑或皱眉的人的图像。比方说，向这个网络输入国际足联管理委员会的图像，不会让它察觉到笑容，不管这些笑容有多灿烂。我们需要重新调整我们的方法。一种方式可以是通过对输入图像应用与对训练数据所做的变换相同的变换，对每张脸的输入图像进行分割和调整大小。更好的方法是收集更多不同的数据，并通过旋转和扭曲训练样本来扩充训练集，我们将在后面的章节中看到这一点。这里的关键是在您的数据集中包括不同姿势和方向、不同光照条件下微笑的人，以真实地捕捉微笑的所有有用的视觉表现。如果收集更多数据的成本太高，生成合成图像(使用 Keras 图像生成器)也是一个可行的选择。数据质量可以极大地提高网络性能。现在，我们将探索一些技巧来了解 CNN 的内部运作。

        

# 在黑盒子里

从相似图像的狭窄数据集中训练一个**微笑检测器**是一回事。你不仅可以直接验证你的预测，而且每个错误的预测也不会让你损失一大笔钱。现在，如果您使用类似的系统来监控精神病患者的行为反应(作为一个假设的例子)，您可能希望通过确保您的模型真正理解微笑的含义，而不是在您的训练集中拾取一些不相关的模式，来确保高准确性。在医疗保健或能源等高风险行业，任何误解都可能带来灾难性的后果，从生命损失到资源损失。因此，我们希望能够确保我们部署的模型确实已经获得了数据中真正具有预测性的趋势，并且没有记住一些没有超出设定预测性的随机特征。在使用神经网络的历史中，这种情况一直存在。在接下来的部分，我们从神经网络民间传说中挑选了一些故事来说明这些困境。

        

# 神经网络失效

曾几何时，美国陆军有了使用神经网络自动探测伪装的敌人坦克的想法。研究人员被委托设计和训练一个神经网络来检测伪装的坦克图像，从空中拍摄敌人的位置。研究人员只是微调了模型权重，以反映每个训练示例的正确输出标签，然后在他们隐蔽的测试示例上测试该模型。幸运的是(至少看起来如此)，他们的网络能够对所有测试图像进行充分分类，向研究人员确认他们的任务已经结束。然而，很快，研究人员就收到了愤怒的五角大楼官员的反馈，声称他们移交的网络在对迷彩坦克进行分类方面并不比随机机会好多少。困惑的是，研究人员探测了他们的训练数据，并将其与五角大楼测试网络的数据进行了比较。他们发现用于训练网络的伪装坦克的照片都是在阴天拍摄的，而底片(没有伪装坦克的照片)都是在晴天拍摄的。其结果是，他们的网络只学会了辨别天气(通过像素的亮度),而不是着手进行预期的分类任务:

![](Images/b4494810-16c1-4784-9bed-19f1652ea8bc.png)

在大量的训练迭代之后，神经网络经常在它们的训练集上达到超人的精度。例如，一名研究人员在试图训练一个网络来对不同类型的陆地和海洋哺乳动物进行分类时观察到了这种现象。已经取得了巨大的成绩，研究人员试图进一步挖掘，以解码我们人类在这项任务中可能仍然忽略的任何分类规则。事实证明，他们复杂的网络所学会的很大一部分是图像中是否存在蓝色像素，这在陆地哺乳动物的照片中自然不会经常出现。

在我们简短选择的神经网络失败故事中，最后一个是自动驾驶汽车从桥上自动驾驶的案例。困惑的自动化工程师试图探查训练有素的网络，以了解哪里出了问题。令他们惊讶的是，他们发现了一些非常奇怪的事情。由于某种原因，该网络不是检测街道上的道路，而是依靠连续的绿色草地将道路与人行道分开来确定方向。当遇到桥时，这片绿草消失了，导致网络以看似不可预测的方式运行。

        

# 可视化转换学习

这些故事促使我们需要确保我们的模型不会过度适应随机噪声，而是实际捕捉代表性的预测特征。我们知道粗心的数据考虑、任务的固有性质或建模中固有的随机性会如何引入预测误差。关于神经网络的传统普及叙述通常包括诸如**黑盒**之类的术语来描述其学习机制。虽然理解单个神经元学到了什么对所有种类的神经网络来说可能不是直观的，但对 CNN 来说却不是这样。有趣的是，ConvNets 允许我们，从字面上，可视化他们的学习特征。正如我们之前看到的，我们可以可视化给定输入图像的神经激活。但是我们可以做得更多。事实上，近年来已经开发了多种方法来探测 CNN，以更好地了解它学到了什么。虽然我们没有时间涵盖所有这些，但我们将能够涵盖最实际有用的。

        

# 可视化中间层的神经激活

首先，通过观察它们的激活图，我们可以看到 CNN 中的渐进层是如何转换输入的。回想一下，这些只是网络看到数据时通过其架构传播的输入的简化表示。将中间层(卷积层或池层)可视化，可以让我们了解在输入被各种学习过滤器分解的每个阶段，网络中神经元的激活情况。由于每个二维激活图存储由给定过滤器提取的特征，我们必须将这些图可视化为二维图像，其中每个图像对应于一个学习到的特征。这种方法通常被称为可视化中间激活。

为了能够提取我们网络的已知特征，我们必须对我们的模型做一些小的架构调整。这就把我们带到了 Keras 的函数式 API。回想一下，以前，我们使用 Keras 的顺序 API 定义了一个顺序模型，它本质上让我们顺序堆叠神经元层来执行我们的分类任务。这些模型摄取图像或单词表示的输入张量，并给出分配给每个输入的类别概率。现在我们将使用 functional API，它允许构建多输出模型、有向无环图，甚至具有共享层的模型。我们将使用这个 API 来深入了解我们的卷积网络。

        

# 对输入图像的预测

首先也是最重要的，我们准备了一个图像(尽管您可能会使用几个)供我们的多输出模型接收，这样我们就能够看到这个图像通过我们的新模型传播时发生的中间层激活。为此，我们从测试集中随机抽取一幅图像，并将其准备为一个四维张量(批量大小为 1，因为我们只向网络提供一幅图像):

![](Images/ffc11494-ab65-4fe9-951f-de179a99ec72.png)

接下来，我们初始化一个多输出模型来对我们的输入图像进行预测。这样做的目的是捕捉网络每一层的中间激活，这样我们就可以直观地绘制出由不同过滤器生成的激活图。这有助于我们理解我们的模型实际上已经学习了哪些特性。

        

# 介绍 Keras 的功能 API

我们到底要怎么做？我们从从函数式 API 导入`Model`类开始。这让我们定义了一个新的模型。我们新模型的关键区别在于，这个模型能够返回多个输出，与中间层的输出相关。这是通过使用来自经过训练的 CNN(例如我们的微笑检测器)的层输出并将其馈送到这个新的多输出模型中来实现的。本质上，我们的多输出模型将获取一个输入图像，并返回我们之前训练的微笑检测器模型中八个层中每一层的过滤激活。

您还可以通过在`model.layers`上使用的列表切片符号来限制可视化的层数，如下所示:

![](Images/2bd743e5-6b33-4288-989d-9e3410362c5e.png)

前面代码的最后一行定义了 activations 变量，让我们的多输出模型对我们输入的图像进行推理。该操作返回对应于 CNN 每一层的多个输出，现在存储为一组 NumPy 数组:

![](Images/3a828fd7-0c27-498c-a448-95966af6bd45.png)

如您所见，activations 变量存储了一个由`8` NumPy *n* 维数组组成的列表。这些`8`数组中的每一个都代表了我们的微笑检测器 CNN 中特定层的张量输出。每个层输出表示来自所使用的多个过滤器的激活。因此，我们观察到每层有多个激活图。这些激活图本质上是编码来自输入图像的不同特征的二维张量。

        

# 验证每层的通道数量

我们看到每一层都有一个深度，表示激活图的数量。这些也被称为通道，其中每个通道包含一个激活图，高度和宽度为( *n* x *n* )。例如，我们的第一层有 16 张大小为 64 x 64 的不同地图。类似地，第四层具有 16 个大小为 32×32 的激活图。第八层有 32 个激活图，每个大小为 16 x 16。这些激活图中的每一个都是由特定的过滤器从其各自的层生成的，并且被向前传递到后续层以编码更高级别的特征。这将与我们的微笑探测器模型的架构相一致，我们可以随时验证，如下所示:

![](Images/ef30f0c5-299c-4aa7-a50a-0ae05e5ea016.png)

        

# 可视化激活图

现在是有趣的部分！我们将为给定层中的不同过滤器绘制激活图。让我们从第一层开始。我们可以绘制出 16 个激活图，如下所示:

虽然我们不会展示所有 16 个激活图，但我们发现了一些有趣的激活图:

![](Images/3096988d-4caa-4871-9e88-e462458fd77d.png)

正如我们可以清楚地看到的，每个滤波器都从输入图像中捕获了不同的特征，涉及到面部的水平和垂直边缘，以及图像的背景。当你想象更深层的激活图时，你会注意到激活在本质上变得越来越抽象，人眼越来越难以理解。这些激活被认为是对与一张脸的位置以及里面的眼睛和耳朵相关的更高层次的概念进行编码。你还会注意到，越深入网络，越多的激活图保持空白。这意味着在较深的层中较少的过滤器被激活，因为输入图像不具有与过滤器编码的图案相对应的图案。这很常见，因为随着图像在网络的更深层传播，我们会期望激活模式与所显示的图像类别越来越相关。

        

# 理解显著性

我们之前看到，我们的 ConvNet 的中间层似乎编码了一些非常清晰的面部边缘检测器。然而，很难区分我们的网络是否理解微笑到底是什么。你会注意到，在我们的笑脸数据集中，所有的照片都是在相同的背景下，以相同的角度拍摄的。此外，你会注意到，在我们的数据集中，当人们把头抬得又高又大时，他们往往会微笑，但在皱眉时，他们的头大多会向下倾斜。对于我们的网络来说，这是一个在一些不相关的模式上过度适应的机会。因此，我们实际上如何知道我们的网络理解微笑与一个人嘴唇的运动有更多的关系，而不是与某人的脸倾斜的角度有更多的关系？正如我们在《神经网络故障》中所看到的，网络获取不相关模式的情况经常发生。在我们实验的这一部分中，我们将可视化给定网络输入的**显著图**。

由牛津大学视觉几何小组在一篇论文中首次介绍的显著图背后的思想是简单地计算期望的输出类别相对于输入图像变化的梯度。换句话说，我们试图确定图像像素值的微小变化如何影响我们的网络对给定图像所见内容的信念:

![](Images/14ff46b8-8287-417c-b26b-7b84338678f4.png)

直观地说，让我们假设我们已经训练了一个关于各种动物图像的卷积网络:长颈鹿、豹子、狗、猫等等。然后，为了测试它学到了什么，我们给它看一张豹子的图片，并问它，*你认为豹子在这张图片的什么位置？*从技术上讲，我们正在对输入图像的像素进行排序，基于每个像素对我们的网络为该图像得出的**类概率得分**的影响。然后，我们可以简单地可视化在给定图像的分类中具有最大影响的像素，因为这些像素是积极变化导致增加我们的网络的类概率得分或置信度的像素，即给定图像属于某个类。

        

# 用 ResNet50 可视化显著图

为了保持有趣，我们将结束我们的微笑检测器实验，并实际使用一个预先训练好的、非常深入的 CNN 来演示我们的 leopard 示例。我们还使用 Keras `vis`，这是一个很棒的高级工具包，用于可视化和调试基于 Keras 构建的 CNN。您可以使用`pip`软件包管理器安装这个软件包:

![](Images/b8b36506-43aa-459e-883f-c5573d6c10de.png)

这里，我们导入 ResNet50 CNN 架构，并为 ImageNet 数据集预训练权重。我们鼓励您探索存储在 Keras 中的其他模型，可以通过`keras.applications`访问。我们还使用`utils.apply_modifications`在该网络的最后一层中为线性激活函数切换出 Softmax 激活，这重建了网络图以帮助我们更好地可视化地图的显著性。

ResNet50 首次作为 ILSVRC 比赛推出，并在 2015 年获得第一名。它很好地避免了与非常深的神经网络相关的精度下降问题。它在 ImageNet 数据集的大约一千个输出类上进行训练。它被认为是高性能、最先进的 CNN 架构，由它的创造者免费提供。虽然它使用了一些有趣的机制，被称为**剩余块**，但是我们将在后面的章节中对它的架构做进一步的评论。现在，让我们看看如何使用这个模型的预训练权重来可视化一些豹纹图片的显著性图。

        

# 从本地目录加载图片

如果你想继续，只需谷歌一些漂亮的豹纹图片，并将它们存储在本地目录中。您可以使用 Keras `vis`中`utils`模块的图像加载器将图像调整到 ResNet50 模型接受的目标尺寸(即 224 x 224 像素的图像):

![](Images/8c5243eb-cb45-4ffc-9411-dc00ce627250.png)

由于我们希望让我们的网络的实验变得相当艰巨，我们有目的地选择了伪装的豹子的照片，以观察这个网络在检测自然界中一些最复杂的试图隐藏这些掠食动物使其不被猎物(如我们自己)发现方面做得有多好:

![](Images/5b333364-2b72-4d3b-8452-67c0986b289d.png)

        

# 使用 Keras 的可视化模块

乍看之下，即使我们的生物神经网络遍布我们的视觉皮层，似乎也很难在每张图像中找到豹子。让我们看看它的人工对应物在这项任务中表现如何。在下面的代码片段中，我们从`keras-vis`模块导入了显著性可视化器对象，以及一个让我们按名称搜索图层的 utils 工具。请注意，标准的 Keras 安装不附带该模块。但是，可以使用 Python 上的`pip`包管理器轻松安装它。您甚至可以通过 Jupyter 环境执行安装:

```
! pip install keras-vis
```

        

# 在层中搜索

接下来，我们执行效用搜索来定义模型中最后一个密集连接的层。我们需要这一层，因为它输出每个输出类别的类别概率得分，我们需要能够在输入图像上可视化显著性。层的名称可以在模型概要(`model.summary()`)中找到。我们将向`visualize_salency()`函数传递四个特定的参数:

![](Images/bcf27d3e-44ec-4237-bd72-d6bdac2afdd0.png)

这将返回我们的输出相对于我们的输入的梯度，这直观地告诉我们哪些像素对我们的模型的预测有最大的影响。gradient 变量存储六幅 224 x 224 的图像(对应于 ResNet50 体系结构的输入大小)，每幅图像对应六幅豹子的输入图像。正如我们注意到的，这些图像是由`visualize_salency`函数生成的，它接受四个参数作为输入:

*   要对其执行预测的种子输入图像(`seed_input`)
*   一个 Keras CNN 模型(`model`)
*   模型输出层的标识符(`layer_idx`)
*   我们想要可视化的输出类的索引(`filter_indices`)

我们在这里使用的索引引用(288)指的是 ImageNet 数据集上标签 *leopard* 的索引。回想一下，之前我们为当前初始化的模型导入了预训练的层权重。这些权重是通过在 ImageNet 数据集上训练 ResNet50 模型获得的。如果你对不同的输出类感到好奇，你可以找到它们以及它们各自的索引，在这里:【https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a。

可视化前三幅图像的显著性图，我们实际上可以看到网络正在关注我们在图像中发现豹的位置——完美！这确实是我们想要看到的，因为它表明我们的网络真正了解(大致)豹子在我们图像中的位置，尽管我们尽最大努力向它显示伪装豹子的嘈杂图像:

![](Images/bad43907-8885-42ab-bbd1-998bdea3b0ca.png)

        

# 锻炼

*   探测网络中的所有层。你注意到了什么？

        

# 梯度加权类激活映射

另一个漂亮的基于梯度的方法是**梯度加权类激活图** ( **Grad-CAM** )。如果您的输入影像中的实体属于多个输出类，并且您希望可视化输入图片中网络与特定输出类关联最密切的区域，则此功能特别有用。这种技术利用流入 CNN 的最终卷积层的特定于类别的梯度信息来产生图像中重要区域的粗略定位图。换句话说，我们向我们的网络提供输入图像，并通过用输出类相对于通道的梯度对输出的每个通道(即，激活图)进行加权来获得卷积层的输出激活图。这允许我们更好地利用与我们的网络最关注的内容相对应的空间信息，该空间信息在网络的最后一个卷积层中表示。我们可以将这些梯度加权激活图叠加在输入图像之上，以了解网络将输入的哪些部分与给定的输出类(即 leopard)高度关联。

        

# 用 Keras-vis 可视化类激活

为此，我们使用了`visualize_cam`函数，该函数本质上生成了一个 Grad-CAM，它最大化了给定输入和指定输出类的层激活。

`visualize_cam`函数采用我们前面看到的四个参数，外加一个参数。我们向它传递对应于 Keras 模型的参数、一个**种子输入**图像、一个**过滤器索引**对应于我们的输出类(leopard 的 ImageNet 索引)以及两个模型层。其中一层仍然是完全连接的密集输出播放器，而另一层是指 ResNet50 模型中的最终卷积层。该方法实质上利用这两个参考点来生成梯度加权的类激活图，如下所示:

![](Images/d831d8b4-2ef4-45c1-be17-9da168645641.png)

正如我们所看到的，网络正确地识别了两幅图像中的豹子。此外，我们注意到网络依赖于豹子的黑点图案来识别其类别。显而易见，网络使用这种模式来识别豹，因为这在同类中非常独特。我们可以通过热图看到网络的注意力，热图主要集中在豹子身体上清晰的点状区域，而不一定是豹子的脸，就像我们自己面对豹子时可能会做的那样。也许数百万年的生物进化已经使我们大脑梭状回区域的层权重适应了不同的面孔，因为这是我们生存的重要模式:

*   **Grad-CAM 上的论文**:【https://arxiv.org/pdf/1610.02391.pdf 

        

# 使用预训练的模型进行预测

顺便说一句，您实际上可以使用 ResNet50 架构对预先训练的 ImageNet 权重在给定的图像上运行推理，就像我们在这里初始化的那样。要做到这一点，您可以首先将想要进行推理的图像预处理成适当的四维张量格式，如下所示。当然，这同样适用于您可能拥有的任何图像数据集，只要它们被调整到适当的格式:

![](Images/48cf2cc8-abb6-4b70-a71a-2e3035c84f80.png)

前面的代码通过沿 0 轴扩展其维度，将我们的一个 leopard 图像重塑为 4D 张量，然后将该张量馈送到我们初始化的 ResNet50 模型，以获得分类概率预测。然后，我们继续将预测类解码成人类可读的输出。为了好玩，我们还定义了`labels`变量，它包括了我们的网络为该图像预测的所有可能的标签，按照概率的降序排列。让我们看看我们的网络对我们的输入图像还有哪些其他的标签:

![](Images/05b3700c-fb5e-4470-889c-cb4136f98d50.png)

        

# 可视化每个输出类的最大激活

在最后一种方法中，我们简单地可视化与特定输出类相关联的全部激活，而不显式地向模型传递输入图像。这种方法非常直观，同时也非常美观。为了我们最后一个实验的目的，我们导入了另一个预训练模型，**vgg 16 网络**。这个网络是基于 2014 年赢得 ImageNet 分类挑战的模型的另一个深度架构。类似于我们的最后一个例子，我们用一个线性层来切换最后一层的 Softmax 激活:

![](Images/eefcd9de-8c03-48b9-89f6-4a65d52ac5ef.png)

然后，我们简单地从在`keras-vis`中实现的可视化模块导入激活可视化器对象。我们绘制出 leopard 类的全部激活，通过传递`visualize_activation`函数我们的模型、输出层和对应于我们的输出类 leopard 的索引。正如我们在这里看到的，网络实际上已经捕捉到了图像中不同方位和位置的豹子的大致形状。一些看起来被放大了，另一些则远没有那么明显，然而猫一样的耳朵和黑色点状图案在整幅图像中非常明显——整洁，对吗？我们来看看下面这张截图:

![](Images/2a32de60-9ffb-4348-92f2-68bc963acdde.png)

        

# 收敛模型

接下来，您可以让模型收敛到这个输出类上，以在多次迭代收敛后可视化模型认为是豹(或另一个输出类)的内容。您可以通过`max_iter`参数定义您希望模型收敛多长时间，如下所示:

![](Images/3f916cf1-b7e6-4352-95e8-7ab7078977a3.png)

        

# 使用多重过滤指数产生幻觉

您还可以通过向`filter_indices`参数传递对应于 ImageNet 数据集中不同输出类的不同索引来进行试验。您还可以向它传递两个整数的列表，对应于两个不同的输出类。这基本上让你的神经网络*通过同时可视化与两个输出类相关的激活来想象*两个独立输出类的视觉组合。这些有时会变得非常有趣，所以让你们的想象力尽情发挥吧！值得注意的是，谷歌的 DeepDream 利用了类似的概念，展示了过度兴奋的激活图如何叠加在输入图像上，以生成艺术图案和图像。这些模式的错综复杂有时令人惊叹不已:

![](Images/8dc35402-607f-4839-bd40-119114172faf.png)

本书作者的照片，摄于巴黎迪士尼乐园鬼屋前。该图像已经使用开源的 DeepDream 生成器进行了处理，我们鼓励您使用它，而不仅仅是惊叹它的美丽。在节日期间，它还能为艺术亲戚带来非常方便的礼物。

        

# CNN 的问题

许多人可能会声称，CNN 利用的分层嵌套模式识别技术非常类似于我们自己的视觉皮层的功能。这在一定程度上可能是真的。然而，视觉皮层实现了一个更复杂的架构，并使其在大约 10 瓦的能量下有效运行。我们的视觉皮层也不容易被出现类似面部特征的图像所迷惑，(尽管这种现象经常发生，在现代神经科学中已经有了一个合适的术语。Pareidolia 是一个术语，与人类大脑以产生更高层次概念的方式解读信号有关，而实际上并不存在更高层次的概念。科学家已经证明了这种现象与位于视觉皮层梭状回区域的神经元的早期激活有关，这些神经元负责几种视觉识别和分类任务。在幻觉妄想的情况下，这些神经元*可以说是*过早地让我们察觉面孔或听到声音，即使事实并非如此。著名的火星表面照片说明了这一点，因为我们大多数人都可以清楚地辨认出一张脸的轮廓和特征，而照片上实际上包含的只是一堆红色的灰尘:

![](Images/8345fe93-9fd7-497e-9c8f-5ea27130aa43.png)

图片由美国宇航局/JPL 加州理工学院提供

        

# 神经网络幻觉

这个问题自然不是我们生物大脑独有的。事实上，尽管 CNN 在许多视觉任务中表现出色，但神经网络幻觉是计算机视觉研究人员一直试图解决的问题。正如我们所提到的，CNN 通过学习各种挑选有用特征的过滤器来学习分类图像，能够以概率的方式分解输入图像。然而，由这些滤波器学习的特征并不代表给定图像中存在的所有信息。这些特征相对于彼此的方向也同样重要！两只眼睛、嘴唇和一个鼻子的存在本身并不构成一张脸的本质。更确切地说，是这些元素在图像中的空间排列造就了这张脸:

![](Images/e7bb30e2-2ccf-4512-aec5-32d92c82758e.png)

        

# 摘要

在本章中，首先，我们使用卷积层，它能够将给定的视觉输入空间分解为卷积滤波器的分层嵌套概率激活，这些卷积滤波器随后连接到执行分类的密集神经元。这些卷积层中的过滤器学习对应于可用概率方式查询的有用表示的权重，以将数据集中存在的输入要素集映射到相应的输出类。此外，我们看到了如何深入我们的卷积网络，以了解它学到了什么。我们看到了四种具体的方法:基于中间激活、基于显著性、梯度加权类激活和激活最大化可视化。每一个都提供了一种独特的直觉，我们网络的不同层次都从中获得了模式。我们为给定的图像以及整个输出类可视化这些模式，以直观地理解我们的网络中的哪些元素在执行推理时会注意。

最后，虽然我们回顾了许多基于神经科学的灵感，这些灵感导致了 CNN 架构的发展，但现代 CNN 根本无法与哺乳动物视觉皮层中实现的复杂机制相竞争。事实上，视觉皮层中许多层的结构设计与我们目前在这里设计的一点也不相似。例如，视觉皮层的各层本身被构造成随后的皮层列，包含具有假定不重叠感受野的神经元，其目的尚不为现代神经科学所知。甚至我们的视网膜也通过使用视杆细胞(接受低强度光)、视锥细胞(接受高强度光)和 ipRGC 细胞(接受时间依赖的刺激)来进行大量的感觉预处理，然后以电脉冲的形式将视觉信号发送到丘脑底部的外侧膝状体核，也称为视觉信号的中继中心。在我们日常生活中，信号就是从这里开始它们的旅程，通过视觉皮层的六个密集互连(而不是卷积)层来回传播。本质上，人类的视觉是相当连续和动态的，与人工实现它有很大不同。总之，虽然我们还远没有像生物学赋予我们的那样赋予机器视觉智能，但 CNN 代表了计算机视觉领域现代成就的顶峰，使其成为无数机器视觉任务的一种适应性极强的架构。

在这里，我们结束了对 CNN 的探索。我们将在后面的章节中重新审视更复杂的架构，并尝试数据增强技术和更复杂的计算机视觉任务。在下一章中，我们将探索另一种称为 RNN 的神经网络体系结构，它对于捕捉和建模时序信息(如时变数据)特别有用，常见于从工业工程到自然语言对话生成的许多领域。