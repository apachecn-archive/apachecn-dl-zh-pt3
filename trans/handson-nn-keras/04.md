<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 信号处理.用神经网络进行数据分析

在获得了大量关于神经网络的知识后，我们现在准备使用它们进行第一次操作。我们将从处理信号开始，看看神经网络是如何输入数据的。你会着迷于增加神经元的层次和复杂性实际上是如何让问题看起来简单的。然后我们会看看语言是如何被处理的。我们将使用数据集进行几次预测。

在本章中，我们将讨论以下主题:

*   处理信号
*   图像作为数字
*   输入神经网络
*   张量的例子
*   建立模型
*   编译模型

*   在 Keras 中实现权重正则化
*   权重正则化实验
*   在 Keras 中实施辍学正规化
*   语言处理
*   互联网电影评论数据集
*   绘制单个训练实例
*   一键编码
*   矢量化特征
*   矢量化标签
*   构建网络
*   回收
*   访问模型预测
*   特征标准化
*   使用 scikit-learn API 进行交叉验证

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 处理信号

我们的宇宙中可能只有四种基本力，但它们都是信号。所谓信号，我们指的是我们可能拥有的现实世界现象的任何类型的特征表示。例如，我们的视觉世界充满了指示运动、颜色和形状的信号。这些是非常动态的信号，生物学能够如此准确地处理这些刺激是一个奇迹，即使我们自己也这么说。当然，从更宏观的角度来看，意识到大自然用了数亿年来完善这一配方可能会让我们变得谦卑，哪怕只是一点点。目前，我们可以欣赏人类视觉皮层的奇迹，它配备了 1.4 亿个密集互连的神经元。事实上，当我们从事越来越复杂的图像处理任务时，整个系列的层(V1-V5)都存在，信息通过这些层传播。眼睛本身利用视杆细胞和视锥细胞来检测不同的光强度和颜色模式，在整合电磁辐射并通过光转导将其转化为电脉冲方面做得非常出色。

当我们看一幅图像时，我们的视觉皮层实际上正在解释电磁信号的特定配置，眼睛正在将这些电磁信号转换为电信号并提供给它。当我们听音乐时，我们的耳膜只是简单地转换和放大连续模式的振动信号，以便我们的听觉皮层可以处理它。事实上，大脑中的神经机制在抽象和表现不同现实世界信号中存在的模式方面非常有效。事实上，神经科学家甚至发现，一些哺乳动物的大脑有能力以某种方式重新布线，允许不同的皮层处理它们原本从未打算遇到的数据类型。最值得注意的是，科学家们发现，重新连接雪貂的听觉皮层使这些生物能够处理来自大脑听觉区域的视觉信号，使它们能够使用非常不同的神经元来*看*，这些神经元先前用于听觉任务。许多科学家引用这样的研究来提出大脑可能正在使用一种主算法的情况，这种算法能够处理任何形式的数据，并将其转化为周围世界的有效表示。

虽然这很有趣，但它自然提出了一千多个关于神经学习的问题，而不是它回答的问题，遗憾的是，我们没有足够的篇幅在本书中解决所有这些问题。可以说，无论是什么算法——或算法集——让我们的大脑实现对我们周围世界的如此有效的表示，自然都会引起神经学家、深度学习工程师和科学界其他人的极大兴趣。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 表征学习

正如我们之前在 TensorFlow 操场上的感知机实验中看到的那样，人工神经元似乎能够学习相当简单的模式。这与我们人类能够完成并希望预测的那种复杂的表达方式相去甚远。然而，我们可以看到，即使在它们的初期简单，这些网络似乎能够适应我们提供给它们的数据类型，有时甚至优于其他统计预测模型。那么，这里发生了什么与以前教机器为我们做事的方法如此不同呢？

简单地向计算机展示我们可能具有的大量医学相关特征，就可以非常有用地教会计算机皮肤癌是什么样子。事实上，这就是我们迄今为止对待机器的方式。我们会手工设计特征，以便我们的机器可以轻松地消化它们并生成相关的预测。但是为什么就此打住呢？为什么不直接给电脑看皮肤癌实际上是什么样子呢？为什么不给它看几百万张图片，让 *it* 搞清楚什么是相关的？事实上，当我们谈到深度学习时，这正是我们试图做的。与传统的**机器学习** ( **ML** )算法相反，在传统算法中，我们以显式处理的表示来表示数据，以供机器学习，我们对神经网络采取了不同的方法。这里，我们实际上希望实现的是网络自己学习这些表示。

如下图所示，网络通过学习简单的表示并使用它们在连续的图层中定义越来越复杂的表示来实现这一点，直到最终的图层学会准确地表示输出类:

![](Images/884630bc-15a4-4cfe-980e-51bbe6ed2471.png)

事实证明，这种方法对于教会你的计算机检测复杂的运动模式和面部表情非常有用，就像我们人类一样。比方说，你想让它在你不在的时候代表你接收包裹，或者检测任何试图闯入你家的潜在劫匪。类似地，如果我们希望我们的计算机安排我们的约会，发现市场上潜在的有利可图的股票，并根据我们发现的有趣的东西让我们了解最新情况，会怎么样？这样做涉及到处理复杂的图像、视频、音频、文本和时间序列数据，所有这些数据都以复杂的维度表示，无法通过几个神经元来建模。那么，我们如何使用神经学习系统，就像我们在上一章看到的那样？我们如何让神经网络学习眼睛、面部和其他现实世界物体中的复杂和分层模式？很明显，答案是我们让它们变得更大。但是，正如我们将看到的，这也带来了自身的复杂性。长话短说，你放入网络的可学习参数越多，它记住一些随机模式的机会就越高，因此不能很好地推广。理想情况下，你希望神经元的配置完全适合手头的学习工作，但如果不进行实验，这几乎不可能事先确定。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 避免随机记忆

另一个答案是不仅控制神经元的总数，还控制这些神经元之间的互联程度。我们可以通过像*退出正则化*和*加权参数*这样的技术来做到这一点，我们很快就会看到。到目前为止，我们已经看到了当数据通过网络传播时，每个神经元可以执行的各种计算。我们还看到了大脑如何利用数亿个密集互连的神经元来完成工作。但是，自然地，我们不能通过任意增加越来越多的神经元来扩大我们的网络。长话短说，模拟一个接近大脑的神经结构很可能需要数千个 **petaflops** (计算速度的单位，等于每秒十亿亿次(10 ^(15) 浮点运算)的计算能力。也许在不久的将来，随着前面提到的大规模并行计算的范例，以及软件和硬件技术的其他进步，这将成为可能。不过现在，我们必须想出聪明的方法来训练我们的网络，以便它可以找到最有效的表示，而不浪费宝贵的计算资源。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 用数字表示信号

在这一章中，我们将看到如何将神经元序列分层，以渐进地表示越来越复杂的模式。我们还将看到正则化和批量学习等概念如何在培训课程中发挥最大作用。我们将学习处理不同类型的真实世界数据，包括图像、文本和时间序列相关信息。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 图像作为数字

对于诸如此类的任务，如果我们希望了解输出类的任何代表性特征，我们需要具有多个隐藏层的深层网络。我们还需要一个好的数据集来实践我们的理解，并熟悉我们将用来设计智能系统的工具。因此，当我们向自己介绍计算机视觉、图像处理和分层表示学习的概念时，我们来到了我们的第一个动手神经网络任务。我们手头的任务是教会计算机阅读数字，而不是像他们已经做的那样，阅读 0 和 1，而是更像我们阅读由我们自己的亲属组成的数字的方式。我们正在谈论手写数字，对于这项任务，我们将使用标志性的 MNIST 数据集，深度学习数据集的真正 *hello world* 。对于我们的第一个例子，我们的选择背后有很好的理论和实践理由。

从理论角度来看，我们需要理解如何使用层神经元来逐步学习更复杂的模式，就像我们自己的大脑一样。由于我们的大脑已经有了大约 2000 至 2500 年的训练数据，它已经非常擅长识别复杂的符号，如手写数字。事实上，我们通常认为这是一项绝对轻松的任务，因为我们早在学龄前就学会了如何区分这些符号。但这实际上是一项相当艰巨的任务。考虑到这些数字可能由不同的人书写的巨大差异，然而我们的大脑仍然能够对这些数字进行分类，好像这是无事生非:

![](Images/cc8d8dcd-ce9c-4f0e-a1d9-180cbafdab03.png)

虽然详尽地编码显式规则会让任何程序员发疯，但当我们看前面的图片时，我们自己的大脑会直观地注意到数据中的一些模式。例如，它发现了 **2** 和 **3** 如何在它们的顶部有一个半环，以及 **1** 、 **4** 和 **7** 如何有一条直线向下。它还理解了一条 **4** 实际上是一条向下的线，一条半向下的线，以及在其他线之间的另一条水平线。因此，我们能够很容易地将一个复杂的模式分解成更小的模式。正如我们刚刚看到的，这对于手写数字来说特别容易做到。因此，我们的任务将是看看我们如何构建一个深度神经网络，并希望我们的每个神经元从我们的数据中捕捉简单的模式，如线段，然后使用我们在前面几层中学习的简单模式，在更深的层中逐步构建更复杂的模式。我们这样做是为了了解对应于输出类的表示的准确组合。

实际上，MNIST 数据集已经被深度学习领域的许多先驱研究了大约二十年。我们从这个数据集中获得了丰富的知识，使它成为探索诸如图层表示、正则化和过拟合等概念的理想工具。一旦我们了解如何训练和测试神经网络，我们就可以将它用于更令人兴奋的任务。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 输入神经网络

本质上，所有进入网络并在网络中传播的数据都由一种称为**张量**的数学结构来表示。这适用于音频数据、图像、视频和我们能想到的任何其他数据，以满足我们数据饥渴的网络。在数学(【https://en.wikipedia.org/wiki/Mathematics】)中，张量被定义为一个抽象且任意的几何(【https://en.wikipedia.org/wiki/Geometry】[)实体，它以多线性(](https://en.wikipedia.org/wiki/Geometry)[【https://en.wikipedia.org/wiki/Linear_map】](https://en.wikipedia.org/wiki/Linear_map))的方式将向量的集合映射到一个结果张量。事实上，向量和标量被认为是张量的简单形式。在 Python 中，张量由三个特定属性定义，如下所示:

*   **等级**:具体表示数轴。据说矩阵的秩为 2，因为它代表一个二维张量。在 Python 库中，这通常表示为`ndim`。
*   **Shape** :张量的形状可以通过调用 NumPy *n* 维数组上的 Shape 属性来检查(这就是张量在 Python 中的表示方式)。这将返回一个整数元组，表示张量沿每个轴的维数。

*   **内容**:这是指存储在张量中的数据类型，可以通过对感兴趣的张量调用`type()`方法来检查。这将返回 float32、uint8、float64 等数据类型，但字符串值除外，字符串值在表示为张量之前首先转换为矢量表示。

下面是一张张量图。不要担心复杂的图表，我们稍后将了解它的含义:

![](Images/6e3eaf25-2ec1-4b81-b958-4b51b38399a4.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 张量的例子

我们之前看到的例子是一个三维张量，然而张量可以以多种形式出现。在下一节中，我们将概述一些不同秩的张量，从秩为零的张量开始:

*   **标量**:值仅仅表示一个单独的数值。这也可以描述为 0 维的张量。这方面的一个例子是通过网络处理图像的单个灰度像素。
*   **向量**:一串标量或者一组数字被称为**向量**，或者秩为 1 的张量。据说 1D 张量只有一个轴。这方面的一个例子是处理单个展平图像。
*   **矩阵**:向量数组是一个矩阵，或 2D 张量。矩阵有两个轴(通常称为行和列)。您可以将矩阵直观地解释为由数字组成的矩形网格。这方面的一个例子是处理单个灰度图像。
*   **三维张量**:通过将几个矩阵打包成一个新的数组，得到一个 3D 张量，在视觉上可以解读为一个数字的立方体。这方面的一个例子是处理灰度图像数据集。

*   **四维张量**:将三维张量打包成一个数组，可以创建一个 4D 张量，等等。这方面的一个例子是处理彩色图像数据集。
*   五维张量:这些是通过将 4D 张量打包成一个数组而创建的。这方面的一个例子是处理视频数据集。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 数据的维度

所以，考虑一个形状的张量(400，600，3)。这是一种常见的输入形状，指的是用于表示 400 x 600 像素彩色图像的三维张量。由于 MNIST 数据集使用二进制灰度像素值，因此在表示图像时，我们只处理 28 x 28 像素的矩阵。这里，每幅图像都是一个二维张量，整个数据集可以用一个三维张量来表示。在彩色图像中，每个像素值实际上有三个数字，代表该像素所代表的红色、绿色和蓝色光强度。因此，对于彩色图像，用于表示图像的二维矩阵现在放大到三维张量。这样的张量由( *x* ， *y* ，3)的元组表示，其中 *x* 和 *y* 表示图像的像素尺寸。因此，彩色图像的数据集可以用一个四维张量来表示，我们将在后面的例子中看到。现在，知道我们可以使用 NumPy *n* 维数组来表示、整形、操作和存储 Python 中的张量是很有用的。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 做一些进口

那么，我们开始吧，好吗？我们将利用我们在前面章节中学到的所有概念进行一些简单的实验，也许还会在工作中遇到一些新的概念。我们将使用 Keras，以及 TensorFlow API，允许我们也探索急切执行范例。我们的第一个任务是实现一个简单的多层感知器。这个版本被称为**前馈神经网络**，是一个基本架构，我们可以用它来进一步探索一些简单的图像分类示例。遵循习惯的深度学习传统，我们将通过使用手写数字的 MNIST 数据集来开始我们的第一个分类任务。这个数据集有 70，000 个数字在 0 到 9 之间的灰度图像。这个数据集的大规模是理想的，因为机器需要每类大约 5，000 张图像，才能在视觉识别任务中接近人类水平的性能。以下代码导入我们将使用的库:

```
import numpy as np
import keras
from keras.datasets import mnist
from keras.utils import np_utils
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# Keras 的顺序 API

众所周知，每个 Python 库通常都有一个核心数据抽象，它定义了库能够操作来执行计算的数据结构。NumPy 有它的数组，而 pandas 有它的数据帧。Keras 的核心数据结构是一个模型，它本质上是一种组织互连神经元层的方式。我们将从最简单的模型开始:序列模型([https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/))。这可以通过顺序 API 以线性层堆栈的形式获得。更复杂的架构也允许我们回顾用于构建定制层的功能 API。我们稍后将讨论这些内容。以下代码导入顺序模型，以及我们将用于构建第一个网络的一些层:

```
from keras.models import Sequential
from keras.layers import Flatten, Dense, Dropout
from keras.layers.core import Activation
from keras import backend as K
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 加载数据

现在，让我们加载数据并将其拆分。幸运的是，MNIST 是 Keras 中已经实现的核心数据集之一，允许一个很好的单行导入，这也让我们可以在训练集和测试集中拆分我们的数据。当然，真实世界的数据没有那么容易移植和分割。在`Keras.utils`中有很多有用的工具可以实现这个目的，我们将在后面简要介绍，但也鼓励您去探索。或者，其他的 **ML** 库，比如 scikit-learn，附带了一些方便的工具(比如`train_test_split`、`MinMaxScaler`和`normalizer`，仅举几个例子)，正如它们的名字所表明的，这些工具可以根据优化神经网络训练的需要，对数据进行拆分、缩放和标准化。让我们导入并加载数据集，如下所示:

```
from keras.datasets import mnist
(x_train, y_train),(x_test, y_test)= fashion_mnist.load_data()
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 检查尺寸

接下来，我们需要检查我们的数据看起来像什么。我们将通过检查它的类型，然后是形状，最后通过使用`matplotlib.pyplot`绘制我们的个人观察来做到这一点，如下所示:

```
type(x_train[0]),x_train.shape,y_train.shape
```

您将获得以下结果:

```
(numpy.ndarray, (60000, 28, 28), (60000,))

```

绘制点:

```
import matplotlib.-pyplot as plt
%matplotlib inline
plt.show(x_train[0], cmap= plt.cm.binary)
<matplotlib.image.AxesImage at 0x24b7f0fa3c8>
```

这将绘制一个类似于下面截图所示的图形:

![](Images/dc711508-0aac-4fb2-8243-f1056e15fef0.png)

正如我们所看到的，我们的训练集有 60，000 张图像，每张图像由一个 28 x 28 的矩阵表示。当我们表示整个数据集时，我们只是表示一个三维张量(60，000 x 28 x 28)。现在，让我们重新调整我们的像素值，通常在 0 到 225 之间。将这些值重新调整为 0 到 1 之间的值，使我们的网络更容易执行计算和学习预测功能。我们鼓励您进行标准化和非标准化实验，以便您可以评估预测能力的差异:

```
x_train=keras.utils.normalize(x_train, axis=1)
x_test=keras.utils.normalize(x_test, axis=1)
plt.imshow(x_train[0], cmap=plt.cm.binary)
```

上述代码生成以下输出:

```
<matplotlib.image.AxesImage at 0x24b00003e48>
```

获得以下图:

![](Images/369416be-0620-432c-8a0f-b9f1e2d10161.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 建立模型

现在我们可以继续前进，建立我们的预测模型。但是在进入有趣的代码之前，我们必须知道围绕一些重要事情的理论。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 介绍 keras 层

Keras 中神经网络模型的核心构建块是它的层。层基本上是数据处理过滤器，*扭曲*它们输入的数据成为更有用的表示。正如我们将会看到的，神经网络的主要架构在设计层的方式以及它们之间神经元的互连方面有很大不同。Keras 的发明者 Francois Chollet 将这种架构描述为对我们的数据进行*渐进蒸馏*。让我们看看这是如何工作的:

```
#Simple Feedforward Neural Network
model = Sequential()

#feeds in the image composed of 28  28 a pixel matrix as one sequence   
 of 784
model.add(Flatten(input_shape=(28,28)))
model.add(Dense(24, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(10, activation='softmax'))
```

我们通过初始化没有层的空白模型的实例来定义我们的模型。然后，我们添加第一层，它总是期望一个与您希望它接收的数据大小相对应的输入维度。在我们的例子中，我们希望模型接收 28 x 28 像素的集合，就像我们之前定义的那样。我们添加的额外逗号指的是网络一次将看到多少个示例，我们很快就会看到。我们还在输入矩阵上调用了`Flatten()`方法。所有这些只是将每个 28 x 28 的图像矩阵转换成一个 784 像素值的向量，每个向量对应于它自己的输入神经元。

我们继续添加层，直到我们到达输出层，它有许多对应于输出类数量的输出神经元——在本例中，是 0 到 9 之间的 10 个数字。请注意，只有输入层需要指定输入数据的输入维度，因为渐进式隐藏层能够执行自动形状推断(并且只有第一个，因为后面的层可以执行自动形状推断)。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 初始化权重

我们还可以选择用特定的权重初始化每一层上的神经元。这不是先决条件，因为如果没有另外指定，它们将自动用小随机数初始化。权重初始化实践实际上是神经网络中一个完全独立的子研究领域。值得注意的是，仔细初始化网络可以显著加快学习过程。

您可以使用`kernel_initializer`和`bais_initializer`参数分别设置每个层的权重和偏差。请记住，这些权重将代表我们的网络所获得的知识，这就是为什么理想的初始化可以显著促进它的学习:

```
#feeds in the image composed of 2828 as one sequence of 784
model.add(Flatten(input_shape=(28,28)))
model.add(Dense(64, activation='relu',   
          kernel_initializer='glorot_uniform',   
          bias_initializer='zeros'))
model.add(Dense(18, activation='relu'))
model.add(Dense(10, activation='softmax'))
```

对不同参数值的全面回顾超出了本章的范围。我们可能会遇到一些用例，在这些用例中，调整这些参数是有益的(参见优化一章)。`kernel_initializer`参数的一些值包括如下:

*   `glorot_uniform`:权重取自`-limit`和`limit`之间均匀分布的样本。这里，术语`limit`被定义为`sqrt(6 / (fan_in + fan_out))`。术语`fan_in`简单地表示权重张量中输入单元的数量，而`fan_out`是权重张量中输出单元的数量。
*   `random_uniform`:权重随机初始化为-0.05 到 0.05 之间的统一小值。
*   `random_normal`:权重初始化为服从高斯分布[1]，平均值为 0，标准偏差为 0.05。
*   `zero`:层权重初始化为零。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# keras 激活

目前，我们的网络由一个扁平的输入层组成，后面是一系列两个致密层，它们是完全连接的神经元层。前两层采用了一个**整流线性单元** ( **ReLU** )激活函数，它与我们在[第 2 章](e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml)、*中看到的 sigmoid 略有不同，是对神经网络*的一个 *的更深入的探究。在下图中，您可以看到 Keras 提供的一些不同的激活功能是如何设计的。请记住，在它们之间进行选择需要对可能的决策边界有一个直观的理解，这些决策边界可能有助于或阻碍对特征空间的划分。在某些情况下，使用适当的激活函数结合理想的初始化偏置可能是至关重要的，但在其他情况下则是微不足道的。尝试总是明智的，不要想尽一切办法:*

![](Images/17d76c00-abee-4ac8-96d2-e2378f0f609b.png)

我们模型中的第四层(也是最后一层)是 10 路 Softmax 层。在我们的例子中，这意味着它将返回一个由十个概率分数组成的数组，所有这些分数的总和为 1。每个分数将是当前数字图像属于我们的输出类之一的概率。因此，对于任何给定的输入，具有 Softmax 激活的层计算并返回该输入相对于每个输出类的类概率。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 可视化总结您的模型

回到我们的模型，让我们总结一下我们将要训练的输出。您可以在 Keras 中通过在模型上使用`summary()`方法来做到这一点，这实际上是更长的`utility`函数的快捷方式(因此更难记住)，如下所示:

```
keras.utils.print_summary(model, line_length=None, positions=None,     
                          print_fn=None)
```

使用此功能，您可以实际可视化神经网络各层的形状，以及每层中的参数:

```
model.summary()
```

上述代码生成以下输出:

```
_________________________________________________________________
Layer (type) Output Shape Param # 
=================================================================
flatten_2 (Flatten) (None, 784) 0 
_________________________________________________________________
dense_4 (Dense) (None, 1024) 803840 
_________________________________________________________________
dense_5 (Dense) (None, 28) 28700 
_________________________________________________________________
dense_6 (Dense) (None, 10) 290 
=================================================================
Total params: 832,830
Trainable params: 832,830
Non-trainable params: 0
_________________________________________________________________
```

正如你所看到的，与我们在[第二章](e8898caf-ebe4-4c1d-b262-5b814276be04.xhtml)、*、*、*中看到的深入研究神经网络*的感知机相反，这个极其简单的模型已经拥有 51，600 个可训练参数，与它的祖先相比，这些参数能够以指数级扩展它的学习。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 编译模型

接下来，我们将编译我们的 Keras 模型。编译基本上是指你的神经网络学习的方式。它让您可以亲自控制学习过程的实现，这是通过使用在我们的`model`对象上调用的`compile`方法来完成的。该方法至少有三个参数:

```
model.compile(optimizer='resprop', #'sgd'
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
```

这里，我们描述以下功能:

*   **A** `loss` **函数**:这只是测量我们在训练数据上的表现，与真实的输出标签相比较。因此，`loss`函数可以用来指示我们模型的误差。正如我们前面看到的，这个度量标准实际上是一个函数，它决定了我们的模型的预测与输出类的实际标注有多远。我们在第二章、*、*、*中看到了**均方误差**、**均方误差** ) `loss`函数对神经网络*的深入研究，其中存在许多变体。这些`loss`功能在 Keras 中实现，这取决于我们的 **ML** 任务的性质。例如，如果您希望执行二元分类(两个输出神经元代表两个输出类)，您最好选择二元交叉熵。对于两个以上的类别，您可以尝试类别交叉熵，或者稀疏类别交叉熵。前者用于一次性编码的输出标注，而后者用于数字分类变量的输出类。对于回归问题，我们经常建议使用 MSE `loss`函数。当处理序列数据时，正如我们将在后面介绍的，那么**连接主义者时态分类** ( **CTC** )被认为是更合适的`loss`函数类型。其他类型的损失可能在测量预测和实际输出标签之间的距离的方式上有所不同(例如，`cosine_proximity`使用余弦距离度量)，或者选择概率分布来模拟预测值(例如，**泊松损失函数**在处理计数数据时可能更好)。
*   **An** `optimizer`:一种直观的思考优化器的方式是，它告诉网络如何达到全局最小损失。这包括你想要优化的目标，以及朝着你的目标前进的步伐大小。从技术上讲，优化器通常被描述为网络用来自我更新的机制，这是通过使用提供给它的数据和`loss`函数来完成的。优化算法用于在减少误差的过程中更新作为模型内部参数的权重和偏差。实际上有两种不同类型的优化函数:具有恒定学习速率的函数(如**随机梯度下降** ( **SGD** ))和具有自适应学习速率的函数(如 Adagrad、Adadelta、RMSprop 和 Adam)。两者中的后一个已知用于实现基于启发式的和预参数化的学习率方法。因此，使用自适应学习率可以减少调整模型超参数的工作量。

*   这只是表示我们在培训和测试期间监控的评估基准。准确性是最常用的，但是如果您愿意，也可以通过 Keras 设计和实现自定义指标。该指标显示的损失和准确性得分之间的主要功能差异是，准确性测量根本不涉及训练过程，而损失则由我们的优化器直接用于训练过程，以反向传播错误。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 拟合模型

`fit`参数启动训练会话，因此应该被认为是训练我们的模型的同义词。它分别将您的训练特征、它们对应的训练标签、模型查看您的数据的次数以及模型在每次训练迭代中看到的学习示例的数量作为训练度量:

```
model.fit(x_train, y_train, epochs=5, batch_size = 2) #other arguments   
                            validation split=0.33, batch_size=10
```

您还可以使用额外的参数来调整数据、创建验证分割或为输出类提供自定义权重。在每个历元之前打乱训练数据可能是有用的，尤其是为了确保您的模型不会在我们的数据中学习任何随机的非预测序列，从而简单地过度适应训练集。为了混洗你的数据，你必须将混洗参数的布尔值设为**真**。最后，如果数据集中存在代表性不足的类，自定义权重会特别有用。设置更高的权重相当于告诉你的模型，*嘿，你，多注意这里的这些例子*。要设置自定义权重，您必须为`class_weight`参数提供一个字典，该字典按照所提供的索引顺序，将类索引映射到与您的输出类相对应的自定义权重。

以下是您在编译模型时将面临的关键架构决策的概述。这些决定与您指导您的模型进行的培训过程有关:

*   `epochs`:该参数必须定义为一个整数值，对应于模型迭代整个数据集的次数。从技术上讲，模型不是针对由历元给定的迭代次数来训练的，而仅仅是直到到达索引历元的历元。你想把这个参数*设置得恰到好处*，这取决于你想要你的模型表现的复杂性的本质。将其设置得太低会导致用于推断的表示过于简单，而将其设置得太高会使模型过度适应训练数据。

*   `batch_size`:`batch_size`定义了每次训练迭代将通过网络传播的样本数量。直观上，这可以被认为是网络在学习时一次看到的例子的数量。从数学上讲，这只是网络在更新模型权重之前将看到的训练实例的数量。到目前为止，我们已经在每个训练示例中更新了我们的模型权重(T2 为 1)，但是这很快会成为计算和内存管理的负担。在数据集太大而无法加载到内存中的情况下，这变得尤其麻烦。设置`batch_size`有助于防止这种情况。神经网络在小批量中也训练得更快。事实上，在反向传播过程中，批量甚至会影响梯度估计的准确性，如下图所示。使用三种不同的批量大小来训练相同的网络。随机表示随机，或一批大小为一。如您所见，与较大的全批次梯度(蓝色)的稳定方向相比，随机和小批次梯度(绿色)的方向波动更大:

![](Images/5ba338bd-2549-4d34-9bc5-a36c8b994344.png)

*   **迭代**的**次数(不需要明确定义)简单表示遍数，其中每遍包含由`batch_size`表示的训练样本数。明确地说，一遍是指通过我们的层向前过滤数据，以及错误的反向传播。假设我们将批量大小设置为 32。一次迭代通过查看 32 个训练示例来包含我们的模型，然后相应地更新其权重。在一个包含 64 个示例、批量大小为 32 的数据集中，只需两次迭代，您的模型就可以遍历它。**

既然我们已经在训练样本上调用了`fit`方法来启动学习过程，我们将观察输出，它简单地显示了估计的训练时间、损失(误差)和训练数据上每个时期的精度:

```
Epoch 1/5
60000/60000 [==========] - 12s 192us/step - loss: 0.3596 - acc: 0.9177
Epoch 2/5
60000/60000 [==========] - 10s 172us/step - loss: 0.1822 - acc: 0.9664
Epoch 3/5
60000/60000 [==========] - 10s 173us/step - loss: 0.1505 - acc: 0.9759
Epoch 4/5
60000/60000 [==========] - 11s 177us/step - loss: 0.1369 - acc:  
                           0.97841s - loss: 
Epoch 5/5
60000/60000 [==========] - 11s 175us/step - loss: 0.1245 - acc: 0.9822
```

在训练过程中，仅通过我们的数据的五次完整运行，我们就实现了 0.96 (96.01%)的准确度。现在，我们必须通过在我们隐蔽的测试集上测试来验证我们的模型是否真正学习了我们希望它学习的东西，我们的模型到目前为止还没有看到:

```
model.evaluation(x_test, y_test)

10000/10000 [==============================] - 1s 98us/step
[0.1425468367099762, 0.9759]
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 评估模型性能

每当我们评估一个网络时，我们实际上感兴趣的是我们在测试集中对图像进行分类的准确性。这对于任何 ML 模型都是正确的，因为我们在训练集上的准确性并不是我们模型可推广性的可靠指标。

在我们的例子中，测试集的准确率为 95.78%，略低于我们的训练集准确率 96%。这是一个经典的过度拟合案例，我们的模型似乎已经捕获了数据中不相关的噪声来预测训练图像。由于我们随机选择的测试集上的固有噪声是不同的，我们的网络不能依赖于它以前拾取的无用表示，因此在测试期间表现很差。正如我们将在本书中看到的，当测试神经网络时，确保它已经学习了我们数据的正确和有效的表示是很重要的。换句话说，我们需要确保我们的网络不会过度适应我们的训练数据。

顺便说一下，您可以通过打印出给定测试主题的最高概率值的标签，并使用 Matplotlib 绘制所述测试主题，在测试集上可视化您的预测。这里，我们打印出测试对象`110`的最大概率标签。我们的模型认为它是一个`8`。通过绘制主题，我们看到我们的模型在这种情况下是正确的:

```
predictions= load_model.predict([x_test])

#predict use the inference graph generated in the model to predict class labels on our test set
#print maximum value for prediction of x_test subject no. 110)

import numpy as np
print(np.argmax(predictions[110]))
-------------------------------------------
8
------------------------------------------
plt.imshow(x_test[110]))
<matplotlib.image.AxesImage at 0x174dd374240>

```

上述代码生成以下输出:

![](Images/2ae3e1bf-72b5-43c4-8cf9-f8680081245d.png)

一旦满意，您可以保存并加载您的模型以备后用，如下所示:

```
model.save('mnist_nn.model')
load_model=kera.models.load_model('mnist_nn.model')
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 正规化

那么，您可以做些什么来防止模型从训练数据中学习误导或不相关的模式呢？嗯，对于神经网络，最好的解决方案是几乎总是获得更多的训练数据。根据更多数据训练的模型确实会让你的模型具有更好的偏差预测能力。当然，获取更多数据并不总是那么简单，甚至是不可能的。在这种情况下，您可以使用其他几种技术来实现类似的效果。其中之一是根据模型可能存储的信息量来约束模型。正如我们在[第一章](e54db312-2f54-4eab-a2c2-91b5a38d13f2.xhtml)、*神经网络概述*中的*敌后*示例中所看到的，找到最有效的信息表示或熵最低的表示是很有用的。类似地，如果我们只能让我们的模型记住少量的模式，我们实际上是在强迫它寻找最有效的表示，这些表示可以更好地概括我们的模型以后可能遇到的其他数据。这种通过减少过度拟合来提高模型可推广性的过程被称为**正则化**，在我们将其用于实践之前，我们将对其进行更详细的讨论。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 调整网络大小

当我们谈到网络的规模时，我们只是指网络中可训练参数的数量。这些参数由网络的层数以及每层的神经元数量来定义。从本质上讲，网络的规模是对其复杂性的一种度量。我们提到过大的网络规模会适得其反并导致过度拟合。思考这个问题的一种直观方式是，我们应该更喜欢简单的表示，而不是复杂的表示，只要它们达到相同的目的——如果你愿意，可以说是一种 *lex parsimoniae* 。设计这种学习系统的工程师确实是有深度的思考者。这里的直觉是，根据网络的深度和每层神经元的数量，您可能有不同的数据表示，但我们倾向于更简单的配置，只在需要时逐步扩展网络，以防止它使用任何额外的学习能力来记忆随机性。然而，让我们的模型拥有太少的参数很可能会导致它不足，使它忘记我们试图在数据中捕捉的潜在趋势。通过实验，您可以根据您的使用情况找到一个合适的网络大小。我们迫使我们的网络有效地表示我们的数据，允许它更好地概括我们的训练数据。下面，我们展示了在改变网络规模时进行的一些实验。这让我们可以比较每个时期我们在验证集上的损失是如何不同的。正如我们将看到的，较大的模型更快地偏离最小损失值，并且它们将几乎立即开始过度拟合我们的训练数据:

![](Images/ae411f1a-470f-40de-9077-037c56a5be17.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 尺寸实验

现在，我们将通过改变网络的规模和评估我们的性能来进行一些简短的实验。我们将在 Keras 上训练六个简单的神经网络，每个都逐渐大于另一个，以观察这些独立的网络如何学习对手写数字进行分类。我们也将展示一些实验结果。为了这个实验的目的，所有这些模型都用恒定批量(`batch_size=100`)、`adam`优化器和作为`loss`函数的`sparse_categorical_crossentropy`来训练。

下面的拟合图显示了增加我们的神经网络的复杂性(就大小而言)如何影响我们对数据的训练和测试集的性能。请注意，我们的目标始终是建立一个模型，使训练和测试准确性/损失之间的差异最小化，因为这表明过度拟合的量最小。直观地说，这简单地向我们展示了如果我们分配更多的神经元，我们的网络学习会受益多少。通过观察测试集上准确性的提高，我们可以看到，添加更多的神经元确实有助于我们的网络更好地对以前从未遇到过的图像进行分类。这可以注意到，直到*最佳点*，这是训练和测试值彼此最接近的地方。然而，最终，复杂性的增加将导致收益递减。在我们的例子中，我们的模型似乎在辍学率为 0.5 左右时过拟合最少，在此之后，训练集和测试集的准确性开始偏离:

![](Images/b6d1ddb0-1ab8-46e3-936e-8690f653ea00.png) ![](Images/a153a0d8-4f37-47d0-bcfa-ef6e702f760d.png)

为了通过增加我们网络的规模来复制这些结果，我们可以调整网络的宽度(每层神经元的数量)和深度(网络的层数)。在 Keras 中，通过使用`model.add()`将层添加到初始化的模型中，可以增加网络的深度。`add`方法将图层的类型(例如，`Dense()`)作为参数。`Dense`函数将在该特定层中初始化的神经元的数量，以及用于所述层的激活函数作为自变量。以下是这方面的一个例子:

```
model.add(Dense(512,  activation=’softmax’))
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 调整重量

另一种确保你的网络不会获取不相关特征的方法是通过调整我们模型的权重。这使得我们可以通过限制网络的层权重来限制网络的复杂性，使其仅取较小的值。所有这一切都是为了使层权重的分布更加规则。我们如何做到这一点？通过简单地增加我们网络的`loss`功能的成本。这个成本实际上代表了对权重较大的神经元的惩罚。按照惯例，我们以三种方式实现这个成本，即 L1、L2 和弹性网正则化:

*   **L1 正则化**:我们添加一个与我们的加权系数的绝对值成比例的成本。
*   **L2 正则化**:我们增加一个与加权系数的平方成正比的成本。这也被称为**权重衰减**，因为如果没有安排其他更新，权重将指数衰减到零。
*   **弹性网络正则化**:这种正则化方法允许我们通过结合使用 L1 和 L2 正则化来捕捉模型的复杂性。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 使用脱落层

最后，将脱落的神经元添加到层中是一种广泛用于调整神经网络并防止它们过度拟合的技术。在这里，我们，相当确切地说，从我们的模型中随机去掉一些神经元。为什么？这导致了双重效用。首先，在通过我们的网络向前传递数据的过程中，这些神经元对我们网络更下游的神经元的激活的贡献被随机忽略。此外，反向传播过程中的任何权重调整都不会应用于神经元。虽然看起来很奇怪，但这背后有很好的直觉。直观地说，神经元权重会在每次反向传递时进行调整，以专门化训练数据中的特定特征。但专业化滋生依赖。最终经常发生的是，周围的神经元开始依赖附近某个神经元的专业化，而不是自己做一些代表性的工作。这种依赖模式通常被称为复杂的共同适应，这个术语是由**人工智能** ( **AI** )研究人员创造的。杰弗里·辛顿(Geoffrey Hinton)就是其中之一，他是反向传播论文的原始合著者，被誉为深度学习的教父。辛顿开玩笑地将这种复杂的协同适应行为描述为神经元之间的*阴谋*，并表示他受到了他所在银行的欺诈防范系统的启发。这家银行不断轮换员工，因此每当辛顿拜访这家银行时，他总能在办公桌后遇到不同的人。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 关于辍学的直觉思考

对于那些熟悉莱昂纳多·迪卡普里奥的电影《如果你能抓住我就抓住我》的人来说，你会回忆起莱昂纳多是如何通过邀请银行职员约会并给他们买零食来迷惑他们的，这样他就可以通过兑现他的假机票来欺骗银行。事实上，由于员工之间的频繁交往和迪卡普里奥的性格，银行员工更加关注无关紧要的特征，如迪卡普里奥的魅力。他们实际上应该注意的是，迪卡普里奥每个月兑现他的月薪支票超过三次。不用说，商家通常不会表现得这么大方。放弃一些神经元等同于旋转它们，以确保它们不会偷懒，让一个肮脏的达芬奇欺骗你的网络。

当我们对一个层应用 dropout 时，我们只是简单地丢弃了它原本会给出的一些输出。假设一个层产生向量[3，5，7，8，1]作为给定输入的输出。向该层添加辍学率(0.4)将简单地将该输出转换为[0，5，7，0，1]。我们所做的只是将向量中 40%的标量初始化为零。

辍学只发生在培训期间。在测试过程中，有漏失的图层的输出会根据之前使用的漏失率系数按比例缩小。这实际上是为了调整这样一个事实，即由于退出机制，在测试期间比训练期间活跃的神经元更多。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 在 Keras 中实现权重正则化

到目前为止，我们已经探讨了三种具体方法背后的理论，这三种方法使我们能够提高我们的模型对未知数据的推广能力。首先，我们可以改变我们的网络规模，以确保它没有额外的学习能力。我们还可以通过初始化加权参数来惩罚低效的表示。最后，我们可以添加辍学层，以防止我们的网络变得懒惰。正如我们之前提到的，眼见为实。

现在，让我们使用 MNIST 数据集和一些 Keras 代码来实现我们的理解。正如我们之前看到的，要改变网络的大小，只需要改变每层神经元的数量。这可以在添加层的过程中在 Keras 中完成，如下所示:

```
import keras.regularizers
model=Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(Dense(1024, kernel_regularizer=  
                      regularizers.12(0.0001),activation ='relu'))
model.add(Dense(28, kernel_regularizer=regularizers.12(0.0001), 
          activation='relu'))
model.add(Dense(10, activation='softmax'))
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 权重正则化实验

简而言之，正则化让我们在优化过程中对层参数施加惩罚。这些惩罚被合并到网络优化的`loss`函数中。在 Keras 中，我们通过向层传递一个`kernel_regularizer`实例来调整层的权重:

```
import keras.regularizers
model=Sequential()
model.add(Flatten(input_shape=(28,28)))
model.add(Dense(1024, kernel_regularizer=regularizers.12(0.0001), 
          activation='relu'))
model.add(Dense(10, activation='softmax'))
```

正如我们之前提到的，我们将 L2 正则化添加到两个图层中，每个图层的 alpha 值都是(0.0001)。正则项的 alpha 值只是指在添加到网络的总损失之前，应用于图层权重矩阵中每个系数的变换。本质上，alpha 值用于乘以权重矩阵中的每个系数(在我们的例子中是 0.0001)。Keras 中的不同正则项可以在`keras.regularizers`中找到。下图显示了正则化如何影响大小相同的两个模型的每个历元的验证损失。我们观察到，我们的正则化模型更不容易过度拟合，因为验证损失不会随着时间的函数而显著增加。在没有正则化的模型上，我们可以清楚地看到情况并非如此，并且在大约七个时期之后，模型开始过度拟合，因此在验证集上表现更差:

![](Images/b4c91389-24a7-4a3b-b6ba-d1febaf38000.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 在 Keras 中实施辍学正规化

在 Keras 中，添加一个 dropout 层也非常简单。你需要做的就是再次使用`model.add()`参数，然后指定一个要添加的 dropout 层(而不是我们目前使用的 dense 层)。Keras 中的`Dropout`参数采用一个浮点值，该值表示其预测将被丢弃的神经元的比例。非常低的辍学率可能无法提供我们所寻求的稳健性，而高辍学率仅仅意味着我们的网络容易失忆，无法记住任何有用的表示。再一次，我们争取一个刚刚好的辍学值；传统上，辍学率设定在 0.2 和 0.4 之间:

```
#Simple feed forward neural network
model=Sequential()

#feeds in the image composed of 28  28 a pixel matrix as one sequence of 784
model.add(Flatten(input_shape=(28,28)))
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.3)
model.add(Dense(28, activation='relu'))
model.add(Dense(10, activation='softmax'))
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 辍学正规化实验

下面是我们使用相同大小的网络进行的两个实验，具有不同的退出率，以观察性能的差异。我们从 0.1 的辍学率开始，逐步扩大到 0.6，以观察这如何影响我们识别手写数字的性能。正如我们在下图中看到的，随着模型在训练集上的表面精度逐渐下降，缩放我们的辍学率似乎可以减少过度拟合。我们可以看到，我们的训练和测试精度都收敛于 0.5 的辍学率附近，之后它们表现出发散的行为。这只是告诉我们，当添加速率为 0.5 的丢弃层时，网络似乎过拟合最少:

![](Images/dc7da382-5d59-4ffe-90e8-5960e26cd6ff.png)

![](Images/b9f13e80-67a8-47eb-b72b-275a841d8dbc.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 复杂性和时间

现在，你已经看到了我们的剧目中通过调整减少过度拟合的一些最突出的技巧。本质上，规范化只是控制我们网络复杂性的一种方式。复杂性控制不仅有助于限制你的网络记忆随机性；也带来了更直接的好处。本质上，更复杂的网络在计算上是昂贵的。他们需要更长的时间来训练，因此会消耗你更多的资源。虽然在处理手头的任务时，这种差异微不足道，但这种差异仍然非常明显。下图是时间复杂性图表。这是一种将训练时间可视化为网络复杂性函数的有用方法。我们可以看到，网络复杂性的增加似乎对每次训练迭代所用的平均时间的增加具有准指数效应:

![](Images/44b86b50-9754-4361-a6c5-6b9c9b979971.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# MNIST 概述

到目前为止，我们已经向您介绍了控制神经网络功能的基本学习机制和过程。您了解了神经网络需要输入数据的张量表示，以便能够针对预测用例对其进行处理。您还了解了在我们的世界中发现的不同类型的数据，如图像、视频、文本等等，是如何被表示为 n 维张量的。此外，您还了解了如何在 Keras 中实现顺序模型，该模型本质上允许您构建相互连接的神经元的顺序层。您使用此模型结构构建了一个简单的前馈神经网络，用于使用 MNIST 数据集对手写数字进行分类。通过这样做，您了解了在模型开发的每个阶段要考虑的关键架构决策。

在模型构建过程中，主要决策涉及定义数据的正确输入大小、选择每层的相关激活函数，以及根据数据中输出类的数量定义最后一层中输出神经元的数量。在编译过程中，您需要选择优化技术、`loss`函数和一个指标来监控您的训练进度。然后，您通过使用`.fit()`参数来启动新创建的模型的训练会话，并在启动训练过程之前向模型传递要做出的最后两个架构决策。这些决定与一次要查看的数据的批量大小以及训练模型的总时期数有关。

最后，您看到了如何测试您的预测，并了解了正则化的关键概念。我们通过试验正则化技术来修改我们的模型的大小、层权重和添加丢弃层，从而帮助我们提高模型对未知数据的概化能力，从而完成了分类任务。最后，我们看到增加模型复杂性是不利的，除非我们的任务性质明确要求:

*   **练习 x** :初始化不同的加权参数，看看这对模型性能有什么影响
*   **练习 y** :初始化每层不同的权重，看看这会如何影响模型性能

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 语言处理

到目前为止，我们已经看到了如何在 Keras 上为图像分类任务训练简单的前馈神经网络。我们还看到了如何用数学方法将图像数据表示为高维几何形状，即张量。我们看到一个高阶张量只是由一个较小阶的张量组成。像素组合起来代表一幅图像，而图像又组合起来代表整个数据集。本质上，每当我们想要使用神经网络的学习机制时，我们有一种方法将我们的训练数据表示为张量。但是语言呢？我们怎样才能像通过语言一样表达人类错综复杂的思想呢？你猜对了——我们将再次使用数字。我们将简单地把我们的文本翻译成数学的通用语言，这些文本是由句子组成的，而句子本身是由单词组成的。这是通过一个被称为**矢量化**的过程来完成的，我们将在使用**互联网电影数据库** ( **IMDB** )数据集对电影评论的情感进行分类的任务中探索这一过程。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 情感分析

随着这些年来我们的计算能力的提高，我们开始将计算技术应用到以前只有语言学家和定性学者经常光顾的领域。事实证明，随着处理器性能的提高，最初被认为太耗时而无法完成的任务变得非常适合计算机进行优化。这导致了计算机辅助文本分析的爆炸式发展，不仅在学术界，在工业界也是如此。诸如计算机辅助情感分析之类的任务对于各种用例来说尤其有益。如果你是一家试图跟踪你的在线客户评论的公司，或者是一位想在社交媒体平台上做一些身份管理的雇主，这可以使用。事实上，就连政治运动也越来越多地咨询那些监测公众情绪并对各种政治话题进行意见挖掘的服务。这有助于政治家准备他们的竞选要点，并了解人们所持观点的总体氛围。虽然这种技术的使用可能很有争议，但它可以极大地帮助组织了解他们在产品、服务和营销策略方面的缺陷，同时以更相关的方式迎合他们的受众。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 互联网电影评论数据集

最简单的情感分析任务是对一段文字代表积极还是消极的观点进行分类。这通常被称为*极地*或*二元情绪分类任务*，其中 0 表示负面情绪，1 表示正面情绪。当然，我们可以有更复杂的情感模型(也许使用我们在[第一章](e54db312-2f54-4eab-a2c2-91b5a38d13f2.xhtml)、*神经网络概述*中看到的五大人格指标)，但目前，我们将专注于这个简单但概念上加载的二元示例。所讨论的例子指的是对来自互联网电影数据库或 IMDB 的电影评论进行分类。

IMDB 数据集由 50，000 条二元评论组成，这些评论平均分为正面和负面意见。每个评论包含一个整数列表，每个整数代表评论中的一个单词。再一次，Keras 的守护者们深思熟虑地将这个数据集用于实践，因此可以在 Keras 的`keras.datasets`下找到。我们鼓励您享受使用 Keras 导入数据的过程，因为我们在以后的练习中不会这样做(您在现实世界中也无法做到):

```
import keras
from keras.datasets import imdb
(x_train,y_train), (x_test,y_test)=imdb.load_data(num_words=12000)
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 加载数据集

正如我们之前所做的，我们通过定义我们的训练实例和标签，以及我们的测试实例和标签来加载我们的数据集。我们能够使用`imdb`上的`load_data`参数将我们预处理的数据加载到 50/50 训练-测试分割中。我们还可以指出我们希望保留在数据集中的最频繁出现的单词的数量。当我们使用合理大小的评审向量时，这有助于我们控制任务的内在复杂性。可以肯定的是，评论中出现的生僻字与特定电影的特定主题有更大的关系，因此它们对该评论的*情绪*几乎没有影响。鉴于此，我们将字数限制在 12000 字以内。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 检查形状和类型

您可以通过检查`x_train`的`.shape`参数来检查每次拆分的评论数量，这实际上是一个由 *n* 维组成的 NumPy 数组:

```
x_train.shape, x_test.shape, type(x_train)
((25000,), (25000,), numpy.ndarray)
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 绘制单个训练实例

如我们所见，有 25，000 个训练和测试样本。我们还可以设计出一个单独的训练样本，看看我们如何代表一个单独的评论。在这里，我们可以看到每个评论只包含一个整数列表，其中每个整数对应于字典中的一个单词:

```
x_train[1]

[1,
 194,
 1153,
 194,
 8255,
 78,
 228,
 5,
 6,
 1463,
 4369,
 5012,
 134,
 26,
 4,
 715,
 8,
 118,
 1634,
 14,
 394,
 20,
 13,
 119,
 954,
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 解读评论

如果你很好奇(我们也很好奇)，我们当然可以标出这些数字对应的确切单词，这样我们就可以阅读评论实际上说了什么。为此，我们必须备份我们的标签。虽然这一步不是必需的，但如果我们稍后想要直观地验证我们网络的预测，这一步是有用的:

```
#backup labels, so we can verify our networks prediction after vectorization
xtrain = x_train
xtest = x_test
```

然后，我们需要恢复代表评论的整数所对应的单词，这是我们前面看到的。IMDB 数据集中包含了用于对这些评论进行编码的单词词典。我们将简单地将它们恢复为`word_index`变量，并颠倒它们的存储顺序。这基本上允许我们将每个整数索引映射到其对应的单词:

```
word_index =imdb.get_word_index()
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])
```

下面的函数有两个参数。第一个(`n`)表示一个整数，表示集合中的第 n 个^个评论。第二个参数定义了第 n ^次检查是来自我们的训练数据还是测试数据。然后，它简单地返回我们指定的评论的字符串版本。

这允许我们读出一个评论者实际上写了什么。正如我们所看到的，在我们的函数中，我们需要调整索引的位置，这些位置偏移了三个位置。这只是 IMDB 数据集的设计者选择实现他们的编码方案的方式，因此这对于其他任务没有实际意义。出现这三个位置的偏移是因为位置 0、1 和 2 被用于填充的索引占用，分别表示序列的开始和未知值:

```
def decode_review(n, split= 'train'):
if split=='train':
    decoded_review=' '.join([reverse_word_index.get(i-3,'?')for i in 
                   ctrain[n]])
elif split=='test':
    decoded_review=' '.join([reverse_word_index.get(i-3,'?')for i in   
                   xtest[n]])
return decoded_review
```

使用这个函数，我们可以从训练集中解码出第五条评论，如下面的代码所示。事实证明，这是一个负面的评论，正如其培训标签所表明的那样，并从其内容中推断出来。请注意，问号只是表示未知值。未知值可能在评论中固有地出现(例如，由于表情符号的使用)或由于我们施加的限制(即，如果一个词不在语料库中使用最频繁的前 12，000 个词中，如前所述):

```
print('Training label:',y_train[5])
decode_review(5, split='train'),
Training label: 0.0
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 准备数据

那么，我们还在等什么？我们有一系列的数字代表每一个电影评论，并有相应的标签，表示(1)为积极或(0)为消极。这听起来像一个经典的结构化数据集，那么为什么不开始把它输入网络呢？嗯，没那么简单。之前我们提到过，神经网络有非常特定的饮食。它们几乎完全是张量涡，所以给它们一个整数列表对我们没什么好处。相反，在我们试图将数据集传递给我们的网络进行训练之前，我们必须将数据集表示为一个 n*维的张量。现在，你会注意到我们的每个电影评论都是由一个单独的整数列表表示的。当然，这些列表的大小各不相同，因为有些评论比其他的要小。另一方面，我们的网络要求输入要素大小相同。因此，我们必须找到一种方法来*填充*我们的评论，使每个评论代表一个相同长度的向量。*

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 一键编码

因为我们知道我们整个语料库中唯一单词的最大数量是 12，000，所以我们可以假设最长可能的评论长度只能是 12，000。因此，我们可以让每个评论成为一个长度为 12，000 的向量，包含二进制值。这是如何工作的？假设我们有两个词的回顾:*不好的*和*电影*。在我们的数据集中包含这些单词的列表可能看起来像[6，49]。相反，我们可以将同一篇评论表示为一个由 0 填充的 12，000 维向量，除了索引 6 和 49，它们将改为 1。实际上，您要做的是创建 12，000 个虚拟特征来代表每个评论。这些虚拟特征中的每一个都代表给定评论中 12，000 个单词中的任何一个的存在或不存在。这种方法也被称为**一键编码**。它通常用于在各种深度学习场景中对特征和分类标签进行编码。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 矢量化特征

下面的函数将获取我们的 25，000 个整数列表的训练数据，其中每个列表都是一个评论。作为回报，它为从我们的训练集中收到的每个整数列表抛出一个热码编码向量。然后，我们简单地重新定义我们的训练和测试特征，通过使用这个函数将我们的整数列表转换成一个一键编码的评论向量的 2D 张量:

```
import numpy as np
def vectorize_features(features):

#Define the number of total words in our corpus 
#make an empty 2D tensor of shape (25000, 12000)
dimension=12000
review_vectors=np.zeros((len(features), dimension))

#interate over each review 
#set the indices of our empty tensor to 1s
for location, feature in enumerate(features):
    review_vectors[location, feature]=1
return review_vectors

x_train = vectorize_features(x_train)
x_test = vectorize_features(x_test)
```

通过检查我们的训练特征和标签的类型和形状，您可以看到我们转换的结果。您还可以检查一个单独的向量看起来像什么，如下面的代码所示。我们可以看到，我们的每个评论现在都是一个长度为`12000`的向量:

```
type(x_train),x_train.shape, y_train.shape
(numpy.ndarray, (25000, 12000), (25000,))

x_train[0].shape, x_train[0]
((12000,), array([0., 1., 1., ..., 0., 0., 0.]), 12000)
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 矢量化标签

我们还可以对我们的训练标签进行矢量化，这只是帮助我们的网络更好地处理我们的数据。您可以将矢量化视为向计算机表示信息的有效方式。就像人类不太擅长使用罗马数字进行计算一样，众所周知，计算机在处理非矢量化数据时表现更差。在下面的代码中，我们将标签转换为 NumPy 数组，其中包含 32 位浮点算术值 0.0 或 1.0:

```
y_train= np.asarray(y_train).astype('float32')
y_test = np.asarray(y_test).astype('float32')
```

最后，我们有了张量，可以被神经网络使用了。这个 2D 张量实际上是 25000 个堆叠的向量，每个向量都有自己的标签。剩下要做的就是建立我们的网络。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 构建网络

在构建具有密集图层的网络时，您必须考虑的第一个架构约束是其深度和宽度。然后，您需要定义一个具有适当形状的输入层，并依次从每个层中选择不同的激活函数来使用。

正如我们在 MNIST 的例子中所做的那样，我们简单地导入顺序模型和密集层结构。然后，我们通过初始化一个空的顺序模型，并逐步添加隐藏层，直到我们到达输出层。请注意，我们的输入层总是需要一个特定的输入形状，对于我们来说，它对应于我们将提供给它的 12，000 维独热编码向量。在我们目前的模型中，输出层只有一个神经元，如果给定评论中的情绪是积极的，它将理想地激活；否则不会。我们将为我们的隐藏层选择**整流线性单元** ( **ReLU** )激活函数，并为最终层选择一个 sigmoid 激活函数。回想一下，sigmoid 激活函数只是将概率值压缩在 0 和 1 之间，这使得它非常适合我们的二元分类任务。ReLU 激活函数只是帮助我们清除负值，因此在许多深度学习任务中可以被视为一个很好的默认设置。总之，我们选择了一个具有三个密集互连的隐藏层的模型，分别包含 18、12 和 4 个神经元，以及一个具有 1 个神经元的输出层:

```
from keras.models import sequential 
from keras.layers import Dense
model=Sequential()
model.add(Dense(6, activation='relu', input_shape=(12000)))
model.add(Dense(6, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 编译模型

现在我们可以编译我们新构建的模型，这是深度学习的传统。回想一下，在编译过程中，两个关键的架构决策是选择`loss`函数和优化器。`loss`函数只是帮助我们在每次迭代中测量我们的模型离实际标签有多远，而优化器决定我们如何收敛到我们模型的理想预测权重。在[第 10 章](cd18f9ea-65ed-4ebd-af06-0403d3774be1.xhtml)、*思考当前和未来发展*中，我们将回顾高级优化器及其在各种数据处理任务中的相关性。现在，我们将展示如何手动调整优化器的学习率。

出于演示目的，我们在**均方根** ( **RMS** )支柱上选择了一个非常小的学习率 0.001。回想一下，学习率的大小仅仅决定了我们希望网络在每次训练迭代中朝着正确输出的方向前进的步长。正如我们之前提到的，大步会导致我们的网络*越过*损失多维空间中的全局最小值，而小的学习率会导致您的模型花费很长时间才能收敛到最小损失值:

```
from keras import optimizers
model.compile(optimizer=optimizers.RMSprop(1r=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']) 
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 拟合模型

在我们之前的 MNIST 例子中，我们检查了最少数量的架构决策来运行我们的代码。这让我们能够非常快速地覆盖深度学习工作流，但代价是效率。您还记得，我们只是在模型上使用了`fit`参数，并向它传递了我们的训练特性和标签，以及两个整数，分别表示训练模型的时期和每次训练迭代的批量大小。前者简单地定义了我们的数据通过模型的次数，而后者定义了在更新其权重之前，我们的模型一次将看到多少个学习示例。这是两个最重要的架构考虑因素，必须定义并适应当前的情况。然而，`fit`参数还可以使用其他一些有用的参数。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 验证数据

您可能想知道为什么我们盲目地对模型进行任意次数的迭代训练，然后在我们的维持数据上测试它。在每个时期之后，用一些看不见的数据来衡量我们的模型，只是为了看看我们做得有多好，这不是更有效率吗？通过这种方式，我们能够准确地评估我们的模型何时开始过度拟合，从而结束训练并节省一些昂贵的计算时间。我们可以在每个时期之后向我们的模型显示测试集，而不更新它的权重，纯粹是为了看看它在那个时期之后在我们的测试数据上做得如何。因为我们不会在每次测试运行时更新我们的模型权重，所以我们不会冒模型过度拟合测试数据的风险。这让我们能够在训练过程中，而不是之后，真正理解我们的模型有多一般化。要在验证分割上测试您的模型，您可以简单地将验证特性和标签作为参数传递给`fit`参数，就像您对训练数据所做的那样。

在我们的例子中，我们简单地使用测试特性和标签作为我们的验证数据。在高风险的严格深度学习场景中，您很可能会选择使用单独的测试和验证集，其中一个用于培训期间的验证，另一个用于在您将模型部署到生产之前的后续评估。这显示在以下代码中:

```
network_metadata=model.fit(x_train, y_train,
                           validation_data=(x_test, y_test),
                           epochs=20,
                           batch_size=100)
```

现在，当您执行前面的单元时，您将看到培训会话开始。此外，在每个训练时期结束时，您会看到我们的模型会暂停一会儿来计算验证集的准确性和损失，然后显示出来。然后，在该验证通过之后，不更新其权重，该模型进行到下一个时期，用于另一轮训练。前面的模型将运行 20 个时期，其中每个时期将迭代我们的 25，000 个*训练*示例，每批 100 个，在每批之后更新模型权重。注意，在我们的情况下，每个时期更新模型权重 250 次，或者在 20 个时期的先前训练期间更新 5000 次。因此，现在我们可以更好地评估我们的模型何时开始记忆我们训练集的随机特征，但我们如何在这一点上实际中断训练会话？嗯，你可能已经注意到了，我们没有仅仅执行`model.fit()`，而是将其定义为`network_metadata`。碰巧的是，`fit()`参数实际上返回了一个历史对象，它包含了我们模型的相关训练统计数据，我们有兴趣恢复这些数据。这个历史对象是由 Keras 中一个叫做**回调**的东西记录的。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 回收

A `callback`基本上是一个 Keras 库函数，它可以在训练会话期间与我们的模型进行交互，以检查其内部状态，并保存相关的训练统计数据以供以后检查。虽然`keras.callbacks`中有相当多的回调函数，但我们将介绍几个关键的。对于那些更注重技术的人来说，Keras 甚至允许您构造自定义回调。要使用回调，只需使用关键字参数`callbacks`将其传递给`fit`参数。请注意，历史回调会自动应用于每个 Keras 模型，因此只要将拟合过程定义为变量，就不需要指定它。这允许您恢复关联的历史对象。

重要的是，如果您之前在 Jupyter 笔记本中启动了一个训练课程，那么调用模型上的`fit()`参数将继续训练同一个模型。相反，您希望在继续另一个训练运行之前重新初始化一个空白模型。这可以通过简单地重新运行先前定义和编译顺序模型的单元来完成。然后，您可以通过使用`callbacks`关键字参数将回调传递给`fit()`参数来实现回调，如下所示:

```
early_stopping= keras.callbacks.EarlyStopping(monitor='loss')
network_metadata=model.fit(x_train, y_train, validation_data=(x_test,  
                           y_test), epochs=20, batch_size=100,  
                           callbacks=[early_stopping]) 
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 提前停止和历史回调

在前一个单元格中，我们使用了一个称为**提前停止**的回调。这个回调允许我们监控一个特定的训练指标。我们的选择是在训练集或验证集上的准确性或损失之间，这些都存储在与我们的模型历史相关的字典中:

```
history_dict = network_metadata.history
history_dict.keys()
dict_keys(['val_loss','val_acc','loss','acc'])
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 选择要监控的指标

理想的选择总是*验证损失*或*验证准确性*，因为这些指标最好地代表了我们模型的可预测性偏差。这仅仅是因为我们只在训练过程中更新我们的模型权重，而不是在验证过程中。选择我们的*训练精度*或*损失*作为度量标准(如下面的代码所示)是次优的，因为您是通过自己的基准定义对模型进行基准测试。换句话说，你的模型可能会不断减少损失并提高准确性，但它是通过死记硬背做到这一点的——而不是因为它正在按照我们的要求学习一般的预测规则。正如我们在以下代码中看到的，通过监控我们的*训练*损失，我们的模型继续减少训练集的损失，即使验证集的损失实际上在第一个时期后不久就开始增加:

```
import matplotlib.pyplot as plt

acc=history_dict['acc']
loss_values=history_dict['loss']
val_loss_values=history_dict['loss']
val_loss_values=history_dict['val_loss']

epochs = range(1, len(acc) + 1)
plt.plot(epochs, loss_values,'r',label='Training loss')
plt.plot(epochs, val_loss_valuesm, 'rD', label-'Validation loss')
plt.title('Training and validation loss')plt.xlabel('Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

上述代码生成以下输出:

![](Images/15feadd5-d291-4653-8c98-d90b611a6984.png)

我们使用 Matplotlib 绘制出前面的图形。类似地，您可以清除以前的损失图，并绘制出我们的培训会话的新的准确性图，如下面的代码所示。如果我们使用验证准确性作为指标来跟踪我们的早期停止回调，我们的培训课程将在第*个*时期后结束，因为*这个*是我们的模型似乎最能概括未见过的数据的时间点:

```
plt.clf()
acc_values=history_dict['acc']
val_acc_values=history_dict['val_acct']
plt.plot(epochs, history_dict.get('acc'),'g',label='Training acc')
plt.plot(epochs, history_dict.get('val_acc'),'gD',label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

上述代码生成以下输出:

![](Images/a902e7f2-2276-4c1a-ad14-22e84927d07d.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 访问模型预测

在 MNIST 的例子中，我们使用了 *Softmax* 激活函数作为最后一层。您可能还记得，该层生成了一个由 10 个概率分值组成的数组，对于给定的输入，这些分值的总和为 1。这 10 个分数中的每一个都是指图像呈现给我们的网络的可能性，对应于一个输出类(也就是说，例如，它有 90%的把握看到 1，10%的把握看到 7)。这种方法对于有 10 个类别的分类任务是有意义的。在我们的情感分析问题中，我们选择了 sigmoid 激活函数，因为我们正在处理二元类别。在这里使用 sigmoid 只是强制我们的网络对于任何给定的数据实例输出 0 到 1 之间的预测。因此，接近 1 的值意味着我们的网络认为给定的信息更可能是正面的评论，而接近 0 的值表明我们的网络确信已经找到负面的评论。为了查看我们模型的预测，我们简单地定义了一个名为`predictions`的变量，方法是在我们训练过的模型上使用`predict()`参数，并将其传递给我们的测试集。现在，我们可以对该集合中的给定示例检查我们的网络预测，如下所示:

```
predictions=model.predict([x_test])
predictions[5]
```

在这种情况下，看起来我们的网络很有信心来自我们测试集的评论`5`是积极的评论。我们不仅可以通过检查存储在`y_test[5]`中的标签来检查是否确实如此，还可以通过我们之前构建的解码器函数来解码评论本身。让我们通过解码评论`5`并检查其标签来测试我们网络的预测:

```
y_test[5], decode_review(5, split='test')
```

事实证明我们的网络是正确的。这是一个复杂的语言模式的例子，需要对语言语法、现实世界的实体、关系逻辑以及人类漫无目的喋喋不休的倾向有更高层次的理解。然而，只有 12 个神经元，我们的网络似乎已经理解了编码在这条信息中的潜在情绪。它以很高的确定度(99.99%)做出预测，尽管存在*恶心*等极有可能出现在负面评论中的词语。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 探索预测

让我们来看看另一篇评论。为了探究我们的预测，我们将创建几个函数来帮助我们更好地可视化我们的结果。如果您希望将模型的预测限制在最确定的情况下，也可以使用这种测量函数:

```
def gauge_predictions(n):
if (predictions[n]<=0.4) and (y_test[n]==0):
    print('Network correctly predicts that review %d is negative' %(n))
elif (predictions[n] <=0.7) and (y_test[n]==1);
elif (predictions[n]>-0.7) and (y_test[n]==0):
else:
    print('Network is not so sure. Review mp. %d has a probability score of %(n),   
           predictions[n])
def verify_predictions(n):
    return gauge_predictions(n), predictions[n], decode_review(n, split='test')
```

我们将创建两个函数来帮助我们更好地可视化我们的网络误差，同时也将我们的预测精度限制在上限和下限。我们将使用第一个函数将好的预测任意定义为网络的概率分数高于 0.7 的情况，以及分数低于 0.4 的坏的情况，用于*正面评论*。我们简单地为负面评论反转这个方案(对一个*负面评论*的好的预测分数低于 0.4，坏的预测分数高于 0.7)。我们还留了一个介于 40%和 70%之间的中间值，标记为不确定预测，以便我们可以更好地理解其准确和不准确猜测背后的原因。第二个函数是为简单起见而设计的，取一个整数值作为输入，该整数值指的是您想要调查和验证的第*篇评论，并返回对网络所想的评估、实际的概率分数以及正在讨论的评论所读的内容。让我们使用这些新构建的函数来探索另一个评论:*

```
verify-predictions(22)
network falsely predicts that review 22 is negative
```

正如我们所看到的，我们的网络似乎很确定来自我们测试集的评论`22`是负面的。它产生了 0.169 的概率分数。您也可以将此分数解释为我们的网络以 16.9%的置信度认为此评论是正面的，因此它肯定是负面的(因为这是我们用来训练我们的网络的仅有的两个类)。原来我们的网络把这个搞错了。阅读评论，你会注意到评论者实际上表达了他们对他们认为被低估的电影的赞赏。注意开头语气相当暧昧，有*傻*和*摔扁*之类的话。然而，句子后面的语境变价词让我们的生物神经网络确定评论实际上表达了积极的情绪。可悲的是，我们的人工网络似乎没有赶上这种特殊的模式。让我们用另一个例子继续我们的探索性分析:

```
verify_predictions(19999)
Network is not so sure. Review no. 19999 has a probability score of [0.5916141]
```

这里可以看到，我们的网络对评论并不是太有把握，尽管它实际上已经猜出了评论中的正确情绪，概率得分为 0.59，更接近于 1(正面)而不是 0(负面)。对我们来说，这篇评论显然是积极的——甚至有点宣传的冲动。直觉上不清楚为什么我们的网络不确定这种情绪。在本书的后面，我们将学习如何使用网络层来可视化词嵌入。现在，让我们用最后一个例子来继续我们的探索:

```
verify_predictions(4)
Network correctly predicts that review 4 is positive
```

这一次，我们的网络又做对了。事实上，我们的网络 99.9%确定这是一个积极的评论。在阅读这篇评论时，你会注意到它实际上做了一件体面的工作，因为这篇评论包含像*无聊*、*一般*这样的词，以及像*嘴硬*这样的暗示性语言，这些都很容易出现在其他负面评论中，潜在地误导我们的网络。正如我们所看到的，我们通过提供一个简短的函数来结束这个探查会话，您可以通过随机检查您的网络对给定数量的评论的预测来玩这个函数。然后，我们打印出我们的网络对从测试集中随机选择的两个评论的预测:

```
from random import randint
def random_predict(n_reviews):
for i in range(n_reviews):
print(verify_predictions(randint(0, 24000)))
random_predict(2)
Network correctly predicts that review 20092 is positive
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# IMDB 摘要

现在，您应该对如何通过简单的前馈神经网络处理自然语言文本和对话有了更好的了解。在我们旅程的这一小节中，您学习了如何使用前馈神经网络执行二元情感分类任务。在此过程中，您学习了如何填充和矢量化自然语言数据，为用神经网络进行处理做准备。您还了解了二元分类中涉及的关键架构变化，例如在网络的最后一层使用输出神经元和 sigmoid 激活函数。您还了解了如何利用数据中的验证分割来了解您的模型在每个训练时期后对未知数据的表现。此外，您还学习了如何通过使用 Keras 回调在培训过程中间接地与您的模型进行交互。回调对于各种用例都很有用，从在某个检查点保存您的模型，到当期望的指标达到某个点时终止培训会话。我们可以使用历史回调来可视化训练统计数据，并且可以使用提前停止回调来指定终止当前训练会话的时刻。最后，您看到了如何探查您的网络对每个评论的预测，以更好地了解它会犯什么样的错误:

*   **练习**:通过正则化提高性能，就像我们在 MNIST 的例子中所做的那样。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 预测连续变量

到目前为止，我们已经使用神经网络执行了两个分类任务。对于我们的第一个任务，我们对手写数字进行了分类。对于我们的第二个任务，我们对电影评论中的情感进行了分类。但是如果我们想预测一个连续值而不是一个分类值呢？如果我们想预测一个事件发生的可能性有多大，或者一个给定物品的未来价格会怎么样？对于这样的任务，例如预测给定市场的价格可能会浮现在脑海中。因此，我们将通过使用波士顿房价数据集编码另一个简单的前馈网络来结束本章。

这个数据集类似于数据科学家和机器学习实践者会遇到的大多数真实世界的数据集。您将看到 13 个与位于波士顿的特定地理区域相关的特征。有了这些特征，手头的任务就是预测房屋的中值价格。要素本身包括各种指标，包括住宅和工业活动、空气中有毒化学物质的水平、财产税、受教育机会以及其他与位置相关的社会经济指标。这些数据是在 20 世纪 70 年代中期收集的，似乎带有当时的一些偏见。你会注意到有些功能看起来非常微妙，甚至可能不合适。在机器学习项目中使用第 12 号功能可能会引起很大争议。当使用特定的数据源或数据类型时，您必须始终考虑更高层次的含义。作为一名机器学习实践者，你有责任确保你的模型不会引入或强化任何类型的社会偏见，或者以任何方式导致人们的差异和不适。请记住，我们的工作是利用技术减轻人类负担，而不是增加负担。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 波士顿房价数据集

正如我们在上一节中提到的，该数据集包含 13 个在观察到的地理区域上表示的训练特征。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 加载数据

我们有兴趣预测的因变量是每个位置的房价，它被表示为一个连续变量，以千美元为单位表示房价。

因此，我们的每一次观察都可以用一个 13 维的向量来表示，并有一个相应的标量标签。在下面的代码中，我们将绘制出训练集中的第二个观察值及其对应的标签:

```
import keras
from keras.datasets import boston_housing.load_data()
(x_train, y_train),(x_test,y_test)=boston_housing.load_data()
x_train[1], y_train[1]
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 探索数据

与我们到目前为止处理过的数据集相比，这个数据集要小得多。我们只能看到 404 训练观察和`102`测试观察:

```
print(type(x_train),'training data:',x_train.shape,'test data:',x_test.shape)
<class 'numpy.ndarray'>training data:(403, 13) test data: (102, 13)
```

我们还将生成一个包含我们的特性描述的字典，以便我们能够理解每个特性实际编码的内容:

```
column_names=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LST  
  AT']

key= ['Per capita crime rate.',
    'The proportion of residential land zoned for lots over 25,000   
     square feet.',
    'The proportion of non-retail business acres per town.',
    'Charles River dummy variable (=1 if tract bounds river; 0 
     otherwise).',
    'Nitric oxides concentration (parts per 10 million).',
    'The average number of rooms per dwelling.',
    'The porportion of owner-occupied units built before 1940.',
    'Weighted distances to five Boston employment centers.',
    'Index of accessibility to radial highways.',
    'Full-value property tax rate per $10,000.',
    'Pupil-Teacher ratio by town.',
    '1000*(Bk-0.63)**2 where Bk is the proportion of Black people by 
     town.',
    'Percentage lower status of the population.'}
```

现在让我们创建一只熊猫`DataFrame`，看看我们训练集中的前五个观察结果。我们将简单地传递训练数据，以及之前定义的列名，作为 pandas `DataFrame`构造函数的参数。然后，我们将在新伪造的`.DataFrame`对象上使用`.head()`参数来获得一个漂亮的显示，如下所示:

```
import pandas as pd
df= pd.DataFrame(x_train, columns=column_names)
df.head()
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 特征标准化

我们可以看到，在我们的观察中，每个特征似乎在不同的尺度上。有些值在数百范围内，而其他值在 1 到 12 之间，甚至是二进制的。虽然神经网络可能仍然会摄取未缩放的特征，但它几乎只喜欢处理相同尺度的特征。在实践中，网络可以从非均匀缩放的要素中学习，但这样做可能需要更长的时间，而且无法保证找到损失情况下的理想最小值。为了让我们的网络以一种改进的方式学习这个数据集，我们必须通过特征标准化的过程来使我们的数据均匀化。我们可以通过减去特定于要素的平均值并除以数据集中每个要素的特定于要素的标准差来实现这一点。请注意，在实时部署的模型中(例如，对于证券交易所)，这样的缩放度量是不切实际的，因为平均值和标准偏差值可能会根据新的输入数据不断变化。在这种情况下，最好使用其他规范化和标准化技术(例如日志规范化):

```
mean=x_train.mean(axis=0)
std=x_train.std(axis=0)
x_train=(x_train-mean)/std
x_test=(x_test-mean)/std
print(x_train[0]) #First Training sample, normalized
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 构建模型

与我们之前构建的分类模型相比，这个回归模型的主要架构差异在于我们构建该网络最后一层的方式。回想一下，在一个经典的标量回归问题中，比如我们手头的这个问题，我们的目标是预测一个连续变量。为了实现这一点，我们避免在最后一层使用激活函数，而只使用一个输出神经元。

我们放弃激活函数的原因是因为我们不想限制该层的输出值的范围。由于我们实施的是纯线性层，因此我们的网络能够学习预测标量连续值，正如我们所希望的那样:

```
from keras.layers import Dense, Dropout
from keras.models import Sequential
model= Sequential()
model.add(Dense(26, activation='relu',input_shape=(13,)))
model.add(Dense(26, activation='relu'))
model.add(Dense(12, activation='relu'))
model.add(Dense(1))
```

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 编译模型

这里编译期间的主要架构差异与我们选择实现的`loss`函数和度量有关。我们将使用 MSE `loss`函数来惩罚较高的预测误差，同时使用**平均绝对误差** ( **MAE** )度量来监控我们模型的训练进度:

```
from keras import optimizers
model.compile(optimizer= opimizers.RMSprop(lr=0.001),
              loss-'mse',
              metrics=['mae'])
model.summary()
__________________________________________________________
Layer (type)                 Output Shape              Param #   
==========================================================
dense_1 (Dense)              (None, 6)                 72006     
__________________________________________________________
dense_2 (Dense)              (None, 6)                 42        
__________________________________________________________
dense_3 (Dense)              (None, 1)                 7         
==========================================================
Total params: 72,055
Trainable params: 72,055
Non-trainable params: 0
__________________________________________________________
```

正如我们之前看到的，MSE 函数测量网络预测误差的平方平均值。简单地说，我们只是简单地测量估计和实际房价标签之间的平均平方差。平方项通过惩罚远离平均值的误差来强调我们预测误差的传播。这种方法对于回归任务尤其有用，因为在回归任务中，小的误差值仍然会对预测准确性产生重大影响。

在我们的情况下，我们的房价标签在 5 到 50 之间，以千美元计。因此，1 的绝对误差实际上意味着 1000 美元的预测差异。因此，使用基于绝对误差的`loss`函数可能不会给网络提供最佳的反馈机制。

另一方面，选择 MAE 作为度量标准是衡量我们训练进度本身的理想方法。事实证明，可视化平方误差对我们人类来说不是很直观。最好是简单地看到我们模型预测中的绝对误差，因为它在视觉上更能提供信息。我们选择的指标对模型的训练机制没有实际影响，它只是为我们提供了一个反馈统计数据，以直观显示我们的模型在训练过程中表现得好坏。MAE 度量本身本质上是两个连续变量之间差异的度量。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 标绘训练和测试误差

在下图中，我们可以看到平均误差约为 2.5 美元(或 2500 美元)。虽然在预测价值 50，000 美元的房子的价格时，这可能是一个很小的差异，但如果房子本身的价格为 5，000 美元，这就变得很重要了:

![](Images/bb27c237-7146-42c8-935c-ec9d9825c23c.png)

最后，让我们使用测试集的数据来预测一些房价。我们将使用散点图来绘制测试集的预测和实际标签。在下图中，我们可以看到一条最佳拟合线，以及数据点。我们的模型似乎捕捉到了数据的总体趋势，尽管对某些点有一些古怪的预测:

![](Images/e6f365c3-5ccb-4e46-b5ac-1c14c9d5819b.png)

此外，我们可以绘制一个直方图，显示我们的预测误差的分布。我们的模型似乎在大多数方面都做得很好，但在预测某些值时有一些问题，同时对少数观察值有过冲和欠冲，如下图所示:

![](Images/ab62f15f-c7ec-4917-9000-a2143ca576fb.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 使用 k 折验证来验证您的方法

我们之前提到过，我们的数据集比我们之前处理的数据集要小得多。这导致了训练和测试中的一些复杂情况。首先，正如我们所做的那样，将数据分为训练样本和测试样本，这样我们只剩下 100 个验证样本。即使我们想部署我们的模型，这对我们来说也是不够的。此外，我们的测试分数可能会有很大的变化，这取决于数据的哪一部分出现在测试集中。因此，为了减少我们对测试模型的任何特定数据段的依赖，我们采用了一种常见的机器学习方法，称为 **k 倍交叉验证**。本质上，我们将我们的数据分成 *n* 个更小的分区，并使用相同数量的神经网络对我们数据的每个更小的分区进行训练。因此，具有五个折叠的 k-fold 交叉验证将把我们的 506 个样本的整个训练数据分成 101 个样本的五个分裂(以及一个具有 102 个样本)。然后，我们使用五个不同的神经网络，每个网络在五个数据分割中的四个分割上进行训练，并在剩余的数据分割上进行自我测试。然后，我们简单地对五个模型的预测进行平均，以生成一个估计值:

![](Images/32ce8785-1897-46ee-8e63-c0c4a34e6a87.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 使用 scikit-learn API 进行交叉验证

与重复随机子采样相比，交叉验证的优势在于，所有的观察值都用于训练和验证，并且每个观察值只用于验证一次。

以下代码向您展示了如何在 Keras 中实现五重交叉验证，其中我们使用了整个数据集(一起训练和测试),并在每次交叉验证运行时打印出网络的平均预测。正如我们所看到的，这是通过在四个随机分割上训练模型，并在每个交叉验证运行的剩余分割上测试它来实现的。我们使用 Keras 提供的 scikit-learn API 包装器，并利用 Keras 回归器，以及 sklearn 的标准缩放器、k-fold 交叉验证器创建器和分数评估器:

```
import numpy as np
import pandas as pd
​
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
​
​
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
​
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

from keras.datasets import boston_housing
(x_train,y_train),(x_test,y_test) = boston_housing.load_data()

x_train.shape, x_test.shape

---------------------------------------------------------------
((404, 13), (102, 13)) ----------------------------------------------------------------

import numpy as np

x_train = np.concatenate((x_train,x_test), axis=0)
y_train = np.concatenate((y_train,y_test), axis=0)

x_train.shape, y_train.shape

-----------------------------------------------------------------
((506, 13), (506,))
-----------------------------------------------------------------
```

您会注意到我们构造了一个名为`baseline_model()`的函数来构建我们的网络。在许多场景中，这是一种构建网络的有用方式，但在这里，它帮助我们将模型对象提供给我们正在使用的来自 Keras 提供的 scikit-learn API 包装器的`KerasRegressor`函数。正如你们许多人可能都知道的那样，scikit-learn 是 ML 的首选 Python 库，提供了各种预处理、缩放、规范化和算法实现。Keras 创建者实现了一个 scikit-learn 包装器，以在这些库之间实现一定程度的交叉功能:

```
def baseline_model():
    model = Sequential()
    model.add(Dense(13, input_dim=13, kernel_initializer='normal', 
              activation='relu'))
 model.add(Dense(1, kernel_initializer='normal'))
 model.compile(loss='mean_squared_error', optimizer='adam')
 return model
```

我们将利用这种交叉功能来执行 k 倍交叉验证，就像我们之前所做的那样。首先，我们将用一个恒定的随机种子初始化一个随机数发生器。这为我们初始化模型权重提供了一致性，帮助我们确保可以一致地比较未来的模型:

```
#set seed for reproducability 
seed = 7
numpy.random.seed(seed)

# Add a data Scaler and the keras regressor containing our model function to a list of estimators

estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('mlp', KerasRegressor(build_fn=baseline_model,   
                    epochs=100, batch_size=5, verbose=0)))

#add our estimator list to a Sklearn pipeline

pipeline = Pipeline(estimators)

#initialize instance of k-fold validation from sklearn api

kfold = KFold(n_splits=5, random_state=seed)

#pass pipeline instance, training data and labels, and k-fold crossvalidator instance to evaluate score

results = cross_val_score(pipeline, x_train, y_train, cv=kfold)

#The results variable contains the mean squared errors for each of our     
 5 cross validation runs.
print("Average MSE of all 5 runs: %.2f, with standard dev: (%.2f)" %   
      (-1*(results.mean()), results.std()))

------------------------------------------------------------------
Model Type: <function larger_model at 0x000001454959CB70>
MSE per fold:
[-11.07775911 -12.70752338 -17.85225084 -14.55760158 -17.3656806 ]
Average MSE of all 5 runs: 14.71, with standard dev: (2.61) 
```

我们将创建一个估计器列表，传递给 sklearn 转换管道，这对于按顺序缩放和处理我们的数据很有用。这次为了调整我们的值，我们简单地使用了 sklearn 的预处理函数`StandardScaler()`,并将其添加到我们的列表中。我们还将 Keras 包装器对象附加到同一个列表中。这个 Keras 包装器对象实际上是一个名为`KerasRegressor`的回归估计器，它接受我们创建的模型函数，以及所需的批量大小和训练时期数作为参数。**冗长**简单来说就是你想在培训过程中看到多少反馈。通过将它设置为`0`，我们要求我们的模型安静地训练。

请注意，这些参数与您要传递给模型的`.fit()`函数的参数是相同的，就像我们之前在开始培训课程时所做的那样。

运行前面的代码，我们可以估算出我们执行的五次交叉验证运行的网络平均性能。`results`变量存储了交叉验证器每次运行时我们网络的 MSE 分数。然后，我们打印出所有五次运行的平均值和标准偏差(平均方差)。请注意，我们将平均值乘以了`-1`。这只是一个实现问题，因为 scikit-learn 的统一评分 API 总是最大化给定的分数。然而，在我们的例子中，我们试图最小化我们的 MSE。因此，需要最小化的分数被否定，以便统一评分 API 可以正确工作。返回的分数是实际 MSE 的负值。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 摘要

在本章中，我们看到了如何使用神经网络执行回归任务。这涉及到对我们以前的分类模型进行一些简单的架构更改，涉及到模型构造(一个没有激活函数的输出层)和选择`loss`函数(MSE)。我们还将 MAE 作为一个指标进行了跟踪，因为平方误差不太直观。最后，我们使用散点图绘制出我们的模型预测与实际预测标签的对比，以更好地可视化网络的表现。我们还使用直方图来了解我们的模型中预测误差的分布。

最后，我们介绍了 k-fold 交叉验证的方法，在我们处理非常少的数据观察的情况下，这比我们的数据的显式训练测试分裂更受欢迎。我们所做的不是将我们的数据分割成训练和测试分割，而是将它分割成 k 个更小的分区。然后，我们通过使用与我们的数据子集相同数量的模型来生成预测的单一估计。这些模型中的每一个都在数量为 *k* -1 的数据分区上进行训练，并在剩余的一个数据分区上进行测试，之后对它们的预测分数进行平均。这样做可以防止我们依赖任何特定的数据分割来进行测试，因此我们可以得到一个更一般化的预测估计。

下一章，我们将学习**卷积神经网络**(**CNN**)。我们将实现 CNN 并使用它们检测对象。我们还将解决一些图像识别问题。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 练习

*   实现三个不同的函数，每个函数返回一个大小(深度和宽度)不同的网络。使用这些函数中的每一个并执行 k 重交叉验证。评估哪个尺码最合适。
*   尝试使用 MAE 和 MSE `loss`函数，并在训练过程中注意差异。
*   尝试不同的`loss`功能，并在训练过程中注意差异。
*   尝试不同的正则化技术，并注意训练中的差异。