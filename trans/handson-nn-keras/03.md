<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 对神经网络的深入探究

在本章中，我们将遇到神经网络的更深入的细节。我们将从构建一个感知机开始。接下来，我们将学习激活功能。我们还将训练我们的第一台感知机。

在本章中，我们将讨论以下主题:

*   从生物神经元到人工神经元——感知机
*   构建感知机
*   从错误中学习
*   训练感知器
*   反向传播
*   缩放感知器
*   单层网络

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 从生物神经元到人工神经元——感知机

现在我们已经简单地熟悉了一些关于数据处理本质的见解，是时候看看我们自己的生物神经元的人工表亲是如何工作的了。我们从弗兰克·罗森布拉特的创作开始，可以追溯到 20 世纪 50 年代。他把他的这个发明叫做**感知器**(【http://citeseerx.ist.psu.edu/viewdoc/download?】T2)doi = 10 . 1 . 1 . 335 . 3398&rep = re P1&type = pdf。本质上，你可以把感知机想象成一个**人工神经网络** ( **ANN** )中的单个神经元。理解单个感知器如何向前传播信息将成为理解我们在后面章节中将要面对的更先进的网络的极好的垫脚石:

![](Images/70ac79de-d77e-4e4d-8cc4-516a38d34757.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 构建感知机

现在，我们将使用六种特定的数学表示来定义感知器，以展示其学习机制。这些表示是输入、权重、偏差项、求和以及激活函数。下面将从功能上详细阐述该输出。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 投入

还记得生物神经元是如何从树突接收电脉冲的吗？嗯，感知机的行为方式类似，但它更喜欢摄取数字，而不是电。本质上，它接受要素输入，如上图所示。这个特定的感知器只有三个输入通道，它们是*x[1]T3、*x[2]和*x[3]。这些特征输入( *x [1]* 、 *x [2]* 和 *x [3]* )可以是您选择用来表示您的观察的任何自变量。简单地说，如果我们想预测某一天是晴天还是雨天，我们可以记录每天的温度和气压等独立变量，以及当天的适当输出类(当天本身是晴天还是雨天)。然后，我们将这些独立变量作为输入特征，一天一次地输入到我们的感知机模型中。***

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 砝码

因此，我们知道数据如何流入我们简单的神经元，但我们如何将这些数据转化为可操作的知识？我们如何建立一个模型来获取这些输入特征，并以一种有助于我们预测任何一天天气的方式来表示它们？

给我们两个特征，我们可以在我们的模型中用作输入，用于确定*雨天*或*晴天*的二元分类任务。

第一步是将每个输入要素与各自的权重配对。您可以将该权重视为该特定输入要素相对于我们试图预测的输出类的相对重要性。换句话说，输入要素温度的权重应该准确反映该输入要素与输出类的相关程度。这些权重最初是随机初始化的，随着我们的模型看到越来越多的数据而学习。我们这样做的希望是，在通过我们的数据进行足够的迭代之后，这些权重将被推向正确的方向，并了解对应于雨天和晴天的温度和压力值的理想配置。根据领域知识，我们实际上知道，温度与天气高度相关，因此，在理想情况下，当数据通过它传播时，我们的模型会学习该特征的更大权重。这有点类似于生物神经元中覆盖轴突的髓鞘。如果一个特定的神经元频繁放电，它的髓鞘据说会增厚，这使轴突绝缘，并允许神经元下次更快地交流。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 总和

所以，现在我们有输入特征流入我们的感知器，每个输入特征与一个随机初始化的权重配对。下一步相当容易。首先，我们将所有三个特征及其权重表示为两个不同的 3×1 矩阵。我们希望使用这两个矩阵来表示输入要素及其权重的综合效果。你应该还记得高中数学，你不能把两个 3×1 的矩阵相乘。因此，我们必须执行一点数学技巧，将两个矩阵简化为一个值。我们简单地转置我们的特征矩阵，如下:

![](Images/6fc54096-4dd9-4917-84e3-b125cae8fde0.png)

我们可以使用这个新的转置特征矩阵(维数为 3×1 ),并将其乘以权重矩阵(维数为 1×3)。当我们执行逐矩阵乘法时，我们获得的结果被称为这两个矩阵的**点积**。在我们的例子中，我们计算转置特征矩阵和权重矩阵的点积。这样，我们可以将两个矩阵简化为一个标量值，该值代表所有输入要素及其各自权重的综合影响。现在，我们将了解如何使用这种集体表示，并根据某个阈值对其进行衡量，以评估这种表示的质量。换句话说，我们将使用一个函数来评估这个标量表示是否编码了一个需要记忆的有用模式。理想情况下，一个有用的模式可以帮助我们的模型区分数据中的不同类别，从而输出正确的预测。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 引入非线性

现在，我们知道数据如何进入感知器单元，以及相关权重如何与每个输入特征配对。我们还知道如何将我们的输入特征及其各自的权重表示为 *n* x 1 矩阵，其中 *n* 是输入特征的数量。最后，我们看到了如何转置我们的特征矩阵，以便能够用包含其权重的矩阵来计算其点积。这个操作留给我们一个标量值。那么，下一步是什么？这不是一个后退一步，思考我们试图实现什么的坏时机，因为这将帮助我们理解为什么我们要使用类似激活功能的东西背后的想法。

你看，真实世界的数据通常是非线性的。我们这样说的意思是，每当我们试图将一个观察结果建模为不同输入的函数时，这个函数本身不能被线性地表示，或者在一条直线上。

如果数据中的所有模式都只是直线，我们可能根本不会讨论神经网络。诸如**支持向量机** ( **支持向量机**)甚至线性回归等技术已经在这项任务中表现出色:

![](Images/f2d549a7-a668-4df6-bc93-beabd654adb4.png)

例如，用温度模拟晴天和雨天会产生一条非线性曲线。实际上，这仅仅意味着我们不可能用一条直线来划分我们的决策边界。换句话说，在一些日子里，尽管气温高，但可能会下雨，而在另一些日子里，尽管气温低，但可能仍然是晴天。

这是因为温度与天气不是线性相关的。任何一天的天气结果都很可能是一个复杂的函数，涉及诸如风速、气压等交互变量。因此，在任何一天，13 度的温度在德国柏林可能意味着晴天，但在英国伦敦可能是雨天:

![](Images/59bf7c16-f222-4ddf-b179-f6e2ef347415.png)

当然，在某些情况下，一种现象可以用线性表示。以物理学为例，物体的质量与其体积之间的关系可以线性定义，如下图截图所示:

![](Images/56356469-bfba-4e93-a959-43701ceade75.png)

这是一个非线性函数的例子:

![](Images/13255c57-c769-455a-bd7a-69e9929d72b0.png)

| **一个线性函数** | **非线性函数** |
| *Y = mx + b* | *Y = mx ² + b* |

这里， *m* 是直线的斜率， *x* 是直线上的任意一点(一个输入或一个*x*-值)，而 *b* 是直线与 *y* 轴的交点。

不幸的是，现实世界中的数据往往不能保证线性，因为我们使用多种特征对观测值进行建模，其中每一种特征都可能对确定我们的输出类产生不同的、不成比例的影响。事实上，我们的世界是极端非线性的，因此，为了在我们的感知器模型中捕捉这种非线性，我们需要它包含能够表示这种现象的非线性函数。通过这样做，我们增加了神经元模拟现实世界中实际存在的更复杂模式的能力，并绘制出决策边界，如果我们只使用线性函数，这是不可能的。这些类型的函数，用于建模我们数据中的非线性关系，被称为**激活函数**。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 激活功能

基本上，到目前为止，我们所做的是在一个较低的维度中以标量表示来表示不同的输入特征及其权重。我们可以使用这个简化的表示，并让它通过一个简单的非线性函数，告诉我们我们的表示是高于还是低于某个阈值。类似于我们之前初始化的权重，这个阈值可以被认为是我们的感知器模型的可学习参数。

换句话说，我们希望我们的感知器能够找出权重和阈值的理想组合，让它能够可靠地将我们的输入匹配到正确的输出类别。因此，我们将我们的简化特征表示与阈值进行比较，然后如果我们高于该阈值，则激活我们的感知器单元，否则不做任何事情。这个将我们减少的特征值与阈值进行比较的函数被称为**激活函数**:

![](Images/e39b47b9-f526-41a4-9bb0-c9d9193c74e9.png)

这些非线性函数有不同的形式，将在后面的章节中详细探讨。现在，我们提出两种不同的激活函数；**重设定步骤**和**物流 s 形**激活功能。我们之前展示的感知器单元最初是用这样一个沉重的阶跃函数实现的，导致二进制输出 1(活动)或 0(不活动)。在我们的感知器单元中使用阶跃函数，我们观察到高于曲线的值将导致激活(1)，而低于或在曲线上的值不会导致激活单元触发(0)。这个过程也可以用代数的方式来概括。

以下截图显示了重步功能:

![](Images/960b3966-48d8-4dd9-8ce8-4c83dc1a62a3.png)

输出阈值公式如下:

![](Images/33960858-cb2e-4c20-9ada-1370f805a9bb.jpg)

本质上，阶跃函数并不是真正的非线性函数，因为它可以重写为两个有限的线性组合。因此，这种分段常数函数在对真实世界的数据建模时不是很灵活，因为真实世界的数据通常比二进制数据更具概率性。另一方面，逻辑 sigmoid 确实是一个非线性函数，可以更灵活地对数据建模。该函数因将**的输入压缩为 0 到 1 之间的输出值而闻名，这使其成为表示概率的常用函数，并且是现代神经网络中神经元常用的激活函数:**

![](Images/d6b9bb4c-33a7-4d97-a7ce-4b4c89f9e714.png)

每种类型的激活函数都有自己的优缺点，我们将在后面的章节中深入探讨。现在，您可以根据具体的数据类型，直观地考虑选择不同的激活函数。换句话说，我们理想地尝试试验并选择一个最能捕捉数据中潜在趋势的函数。

因此，我们将使用这样的激活函数来设定神经元输入的阈值。输入被相应地转换，并根据这一激活阈值进行测量，进而导致神经元放电或停止放电。在下图中，我们可以看到由激活函数产生的决策边界。

![](Images/b89a9e7b-9b18-4410-af79-11b3f253cb6e.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 理解偏见术语的作用

所以，现在我们对数据如何进入我们的感知机有了一个很好的想法；它与权重配对，并通过点积减少，只与激活阈值进行比较。在这一点上，你们中的许多人可能会问，*如果我们想让我们的阈值适应不同的数据模式呢？*换句话说，如果激活函数的边界不理想，无法单独识别我们希望模型学习的特定模式，该怎么办？我们需要能够摆弄我们的激活曲线的形式，以便保证每个神经元可能局部捕获的模式的某种灵活性。

我们将如何塑造我们的激活函数？一种方法是在我们的模型中引入一个**偏差项**。这由下图中离开第一个输入节点(标有数字“1”)的箭头表示:

![](Images/3a0db2c1-49f6-4a50-ae58-30be9a9a2dbb.png)

代表性地，我们可以把这种偏见想象成一个虚构的输入*的权重。*这种虚构的输入据说总是存在的，允许我们的激活单元随意触发，而不需要任何输入功能显式存在(如前面的绿色圆圈所示)。这个术语背后的动机是能够操纵我们激活函数的形状，这反过来影响我们模型的学习。我们希望我们的形状能够灵活地适应数据中的不同模式。偏置项的权重以与所有其他权重相同的方式更新。它的不同之处在于，它不受其输入神经元的干扰，后者只是始终保持一个恒定值(如前所示)。

那么，我们如何利用这个偏置项来影响我们的激活阈值呢？好吧，让我们考虑一个简化的例子。假设我们有一些由步进激活函数生成的输出，它为每个输出生成“0”或“1”，如下所示:

![](Images/d7c86ad9-1663-40a0-bdcd-866715c87e7d.png)

然后，我们可以重写该公式，以包括偏差项，如下所示:

![](Images/330ee91e-aa4e-411f-a4e7-add0eb9cb6f6.png)

换句话说，我们使用了另一种数学技巧，将阈值重新定义为偏差项的负值( *Threshold = -(bias)* )。这个偏差项在我们的训练会话开始时被随机初始化，并且随着模型看到更多的例子并从这些例子中学习而迭代更新。因此，重要的是要理解，虽然我们随机初始化模型参数，例如权重和偏差，但我们希望实际上向模型显示足够多的输入示例及其相应的输出类。这样做，我们希望我们的模型从它的错误中学习，搜索对应于正确输出类的权重和偏差的理想参数组合。请注意，当我们初始化不同的权重时，我们实际上是在修改激活函数的陡度。

下图显示了不同权重如何影响 sigmoid 激活函数的陡度:

![](Images/e9682b24-892c-4c74-8889-5664bfba37d4.png)

我们基本上希望，通过修补激活函数的陡度，我们能够理想地捕捉到数据中的某种潜在模式。类似地，当我们初始化不同的偏置项时，我们实际上试图做的是以最佳方式(向左或向右)移动激活函数，以便触发对应于输入和输出特征的特定配置的激活。

下图显示了不同的偏置项如何影响 sigmoid 激活函数的位置:

![](Images/9ebaca6c-ca47-458c-8054-cc4482fc514a.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 输出

在我们简单的感知器模型中，我们将实际输出类表示为 *y* ，将预测输出类表示为![](Images/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png)。输出类只是指我们试图预测的数据中的不同类。具体来说，我们使用某一天的输入特征( *x [ n ]* )，例如温度( *x [ 1 ]* )和气压( *x [ 2 ]* )，来预测该特定天是晴天还是雨天(![](Images/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png))。然后，我们可以将我们的模型的预测与那天的实际输出类进行比较，表明那天确实是下雨还是晴天。我们可以将这个简单的比较表示为(![](Images/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png) - *y* )，这允许我们观察我们的感知机平均错过了多少标记。但稍后会详细介绍。现在，我们可以用迄今为止所学的所有知识，以数学的方式来表示我们的整个预测模型:

![](Images/e4f8f3d6-b751-40c5-b4e1-4bd2a691bdab.png)

下图显示了上述公式的一个示例:

![](Images/dae36234-c075-4bd3-b38a-5522521562b2.png)

如果我们用图形绘制前面所示的预测线(![](Images/f98b8c99-3018-46e8-8c39-03fdd04ca52b.png))，我们将可以看到将整个特征空间分成两个子空间的决策边界。本质上，绘制预测线只是让我们了解模型学到了什么，或者模型如何选择将包含所有数据点的超平面分成我们感兴趣的各种输出类。实际上，通过绘制这条线，我们只需将晴天和雨天的观察结果放在该特征空间上，然后检查我们的决策边界是否理想地分隔了输出类，就可以直观地了解我们的模型表现如何，如下所示:

![](Images/79bf9bfc-0a83-49bc-9c33-867ffd4ff6bc.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 从错误中学习

我们对输入数据所做的基本工作就是计算点积，添加一个偏置项，让它通过一个非线性方程，然后将我们的预测与实际输出值进行比较，朝着实际输出的方向迈出一步。这是人工神经元的一般结构。你将很快看到这种重复配置的结构如何产生一些更复杂的神经网络。

我们如何通过向正确的方向迈出一步来收敛到理想的参数值，这是通过一种称为**误差反向传播**，或简称为**反向传播**的方法来实现的。但是为了向后传播错误，我们需要一个度量来评估我们相对于我们的目标做得有多好。我们将此指标定义为损失，并使用损失函数来计算它。这个函数试图合并我们的模型认为它看到的和实际地面现实之间的剩余差异。从数学上来说，这表现为( *y* - ![](Images/7a282e6b-02ad-450b-bfc8-37467f25c131.png))。重要的是要理解，损失值实际上可以定义为我们的模型参数的函数。因此，调整这些参数允许我们减少损失，并使我们的预测更接近实际输出值。当我们回顾我们的感知机的完整训练过程时，你会明白我们这样说的确切意思。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 均方误差损失函数

一个突出使用的损失函数是**均方误差** ( **MSE** )函数，在下面的公式中用代数方法表示。您会注意到，这个函数的核心是简单地比较实际的模型输出( *y* )和预测的模型输出(![](Images/1a12876d-d701-4c8b-9986-b147145150ab.png))。该函数特别有助于我们评估我们的预测能力，因为该函数对损失进行二次建模。也就是说，如果我们的模型表现不佳，并且我们的预测输出值和实际输出值越来越不一致，则损失会以 2 的指数增加，这使得我们可以更严厉地惩罚更高的误差:

![](Images/97716134-e976-4bdc-a971-85dc25d9b587.png)

输出值*y[I]与预测值![](Images/a8625ea3-68b5-432c-94d1-e0da34ae2337.png)之间的平均 MSE。*

我们将重温这一概念，以理解我们如何使用各种类型的损失函数来减少我们的模型预测与实际输出之间的差异。现在，只要知道我们的模型的损失可以通过称为**梯度下降**的过程最小化就足够了。正如我们将很快看到的，梯度下降只是基于微积分，并通过基于反向传播的算法来实现。通过调整网络参数，以数学方式减少预测输出和实际输出之间的差异的过程，实际上是网络学习的过程。这种调整发生在我们训练我们的模型时，通过向它显示输入和相关输出的新例子。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 训练感知器

到目前为止，我们已经清楚地掌握了数据实际上是如何通过我们的感知机传播的。我们还简要地看到了我们的模型的误差是如何向后传播的。我们使用损失函数来计算每次训练迭代的损失值。这个损失值告诉我们，我们的模型的预测离实际地面真相有多远。但是接下来呢？

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 量化损失

由于损失值为我们提供了预测和实际输出之间差异的指示，因此，如果我们的损失值很高，那么我们的模型预测和实际输出之间就会有很大的差异。相反，低损耗值表明我们的模型正在缩小预测输出和实际输出之间的距离。理想情况下，我们希望我们的损失收敛到零，这意味着实际上我们的模型认为它看到的和实际显示的没有太大区别。我们通过简单地使用另一个基于微积分的数学技巧使我们的损失收敛到零。你会问，怎么会？

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 损失是模型重量的函数

嗯，还记得我们说过，我们也可以把我们的损失值想象成模型参数的*函数*吗？考虑一下这个。我们的损失值告诉我们我们的模型离实际预测有多远。同样的损失也可以重新定义为模型重量(θ)的函数。回想一下，这些权重实际上导致了我们的模型在每次训练迭代中的预测。凭直觉思考这个问题，我们希望能够根据损失改变我们的模型权重，以便尽可能地减少我们的预测误差。

更精确地说，我们希望最小化我们的损失函数，以便迭代地更新我们的模型的权重，并且理想地收敛到可能的最佳权重。这些将是最好的权重，因为它们能够最好地表示预测输出类的特征。这一过程被称为**损耗优化**，可以用数学方法表示如下:

![](Images/7bd1ef52-965a-4e97-9f0d-89a1f67b0f66.png)

梯度下降

注意，我们将我们的理想模型权重(θ ^(***) )表示为整个训练集的损失函数的最小值。换句话说，对于我们显示模型的所有特征输入和标记输出，我们希望它在我们的特征空间中找到一个位置，在那里实际值( *y* )和预测值(![](Images/0577a5df-72ab-422b-9fc6-41d0b83225a7.png))之间的总体差异最小。我们所指的特征空间是模型可以初始化的所有不同的可能的权重组合。为了简化损失函数，我们将其表示为 *J* (θ)。我们现在可以迭代求解我们的损失函数的最小值， *J* (θ)，并且下降超平面以收敛到全局最小值。这个过程就是我们所说的**梯度下降**:

![](Images/34fc1e60-e821-4528-9982-7f694c6bfdae.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 反向传播

对于更注重数学的人来说，你一定想知道我们到底是如何迭代地降低梯度的。如你所知，我们首先初始化模型的随机权重，输入一些数据，计算点积，然后将它和我们的偏差一起传递给我们的激活函数，得到一个预测的输出。我们使用这个预测的输出和实际的输出，使用损失函数来估计我们的模型表示中的误差。现在微积分来了。我们现在能做的是，将损失函数 *J* (θ)相对于模型的权重(θ)进行微分。这个过程本质上让我们比较模型权重的变化如何影响模型损失的变化。这种微分的结果给出了我们的 J(θ)函数在当前模型权重(θ)下的梯度以及最高上升的方向。对于最高上升，我们指的是预测值和输出值之间的差异似乎更大的方向。因此，我们简单地在相反的方向上迈出一步，并且相对于我们的模型权重(θ)，降低我们的损失函数 *J* (θ)的梯度。我们以伪代码的形式给出了这个概念的算法表示，如下所示:

![](Images/fcde7a14-32f2-406f-b062-ac487bd7a92b.png)

下图是梯度下降算法的可视化:

![](Images/a23c2d4c-88b7-44d0-85a8-7513668a9f6f.png)

正如我们看到的，梯度下降算法允许我们沿着损失超平面逐步下降，直到我们的模型收敛到一些最佳参数。在这一点上，我们的模型的预测和现实之间的差异将是可以忽略不计的，我们可以认为我们的模型是经过训练的！：

![](Images/8975d35f-d47b-4475-99f6-4efeaf0f526d.png)

因此，我们相对于由损失函数(即网络权重的梯度)产生的值的变化来计算我们的网络权重的变化。然后，我们在计算梯度的相反方向上按比例更新网络权重，以便调整误差。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 计算梯度

既然我们已经熟悉了反向传播算法以及梯度下降的概念，我们可以解决更多的技术问题。像这样的问题，我们如何计算这个梯度？如你所知，我们的模型没有可视化损失景观的自由，也没有挑选出一条好的下降路径的自由。事实上，我们的模型无法判断什么是上升，什么是下降。它所知道的，也将永远知道的，只有数字。但是，事实证明，数字能说明很多！

让我们重新考虑我们的简单感知器模型，看看我们如何通过迭代计算我们的损失函数的梯度来反向传播它的误差:

![](Images/d983710e-6717-49bc-83c6-45c5464dc97d.png)

如果我们想知道第二层重量的变化如何影响我们损失的变化呢？遵循微积分的规则，我们可以简单地对我们的损失函数， *J* (θ)，相对于第二层的权重(θ [*2*] )进行微分。从数学上来说，我们也可以用不同的方式来表达。使用链式法则，我们可以显示我们的损耗相对于第二层权重的变化实际上是两个不同梯度本身的产物。一个表示我们的损失相对于模型预测的变化，另一个表示我们的模型预测相对于第二层中权重的变化。这可以表示如下:

![](Images/93d30d82-fee8-4614-b9ea-962b1b3ed8aa.png)

如果这还不够复杂的话，我们甚至可以更进一步。假设不是对第二层权重的变化(θ *[2]* )的影响进行建模，而是想一路传播回去，看看我们的损失相对于第一层的权重如何变化。然后我们简单地用链式法则重新定义这个方程，就像我们之前做的那样。同样，我们对模型损耗相对于第一层模型权重的变化感兴趣，(θ [*1*] )。我们用三个不同梯度的乘积来定义它；我们的损失相对于输出的变化，我们的输出相对于我们的隐藏层值的变化，以及最后，我们的隐藏层值相对于我们的第一层权重的变化。我们可以总结如下:

![](Images/58df4217-ce8f-438f-97bb-50e3dcb16802.png)

这就是我们如何使用损失函数，并通过计算损失函数相对于模型中每一个重量的梯度来反向传播误差。这样做，我们能够调整我们的模型在正确的方向，最高下降的方向，正如我们之前看到的。我们对我们的整个数据集这样做，表示为一个时期。那我们的步长呢？这是由我们设定的学习速度决定的。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 学习率

虽然有些直观，但模型的学习速度只是决定了它可以学习的速度。从数学上来说，学习率决定了我们在每次迭代中所采取的步骤的确切大小，因为我们降低了损失，以收敛到理想的权重。为您的问题设定正确的学习率是一项挑战，尤其是当损失情况复杂且充满惊喜时，如下图所示:

![](Images/334bbf3f-b157-4b18-b044-7d05e9a7d981.png)

这是一个非常重要的概念。如果我们将学习率设置得太小，那么自然地，我们的模型在任何给定的训练迭代中学习到的比它潜在的要少。对于低学习率来说，更糟糕的是当我们的模型陷入局部最小值时，认为它已经达到了全局最小值。相反，另一方面，高学习率会阻止我们的模型捕捉预测值的模式。

如果我们的步长太大，我们可能会简单地超过权重特征空间中存在的任何全局最小值，因此，永远不会收敛于我们的理想模型权重。

这个问题的一个解决方案是设置一个自适应的学习速率，以响应在训练中可能遇到的特定损失情况。我们将在后续章节中探讨自适应学习率的各种实现(如 Momentum、Adadelta、Adagrad、RMSProp 等):

![](Images/b3074f96-b948-4089-903d-e4d549c8b997.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 缩放感知器

到目前为止，我们已经看到了单个神经元在接受训练时是如何学会代表一种模式的。现在，假设我们想要并行地利用额外神经元的学习机制。在我们的模型中有两个感知器单元，每个单元可以学习在我们的数据中表示不同的模式。因此，如果我们想通过添加另一个神经元来稍微扩展之前的感知器，我们可能会得到一个具有两个完全连接的神经元层的结构，如下图所示:

![](Images/4a4de24d-5e18-41a8-80b6-f87143eb4f6a.png)

这里要注意的是，特征权重，以及每个神经元代表我们偏见的额外虚拟输入，都消失了。为了简化我们的表示，我们将标量点积和偏置项表示为一个符号。

我们选择用字母 *z* 来表示这个数学函数。然后 *z* 的值被提供给激活函数，就像我们之前做的一样，因此 *y* = *g* ( *z* )。正如您在前面的图表中看到的，我们的输入功能连接到两个不同的神经元，每个神经元都可以调整其权重和偏差，以从输入的数据中学习特定和独特的表示。然后，这些表示用于预测我们的输出类，并在我们训练模型时进行更新。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 单层网络

好了，现在我们已经看到了如何并行利用我们感知单元的两个版本，使每个单独的单元能够学习不同的潜在模式，这种模式可能存在于我们提供给它的数据中。我们自然希望将这些神经元连接到输出神经元，输出神经元会触发以指示特定输出类的存在。在我们的晴天-雨天分类示例中，我们有两个输出类(晴天或雨天)，因此负责解决该问题的预测网络将有两个输出神经元。这些神经元将受到来自前一层的神经元的学习的支持，并且理想地将代表预测雨天或晴天的信息特征。从数学上来说，这里发生的只是我们转换后的输入特征的前向传播，随后是我们预测中的误差的后向传播。思考这个问题的一种方法是将下图中的每个节点想象成特定数字的持有者。类似地，每个箭头可以被视为从一个节点拾取一个数字，对其执行加权计算，并将其向前带到下一层节点:

![](Images/147f084b-24fe-4d7a-a103-fffb516e8af7.png)

现在，我们有一个只有一个隐藏层的神经网络。我们称之为隐藏层，因为这一层的状态不是直接强制的，与输入和输出层相反。它们的表示不是由网络的设计者硬编码的。相反，当数据通过网络传播时，它们是由网络推断出来的。

正如我们看到的，输入层保存我们的输入值。连接输入层和隐藏层的箭头组简单地计算我们的输入特征( *x* )和它们各自的权重(θ[1])的偏差调整点积( *z* )。( *z* )值然后驻留在隐藏层神经元中，直到我们将我们的非线性函数 *g* ( *x* )应用于这些值。在此之后，远离隐藏层的箭头计算 *g* ( *z* )和对应于*隐藏层*(θ*[2]*)的权重的点积，然后将结果向前传送到两个输出神经元![](Images/25c7dba2-7da4-41b4-af22-9d3bdbd503a2.png)和![](Images/1af86b7f-9ad1-45e2-8dbc-8ab4b4dcb6a7.png)。请注意，每一层都有一个相应的权重矩阵，它是通过将我们的损失函数与来自前一次训练迭代的权重矩阵进行微分来迭代更新的。因此，我们通过降低损失函数相对于模型权重的梯度来训练神经网络，从而收敛到全局最小值。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 尝试 TensorFlow 游乐场

让我们用一个虚拟的例子来看看不同的神经元如何在我们的数据中捕捉不同的模式。假设我们的数据中有两个输出类，如下图所示。我们的神经网络的任务是学习区分我们两个输出类的决策边界。绘制这个二维数据集，我们得到类似于下图的东西，其中我们看到几个对不同的可能输出进行分类的决策边界:

![](Images/d0c7325b-b6d8-4f85-9fa0-0f4d098ab377.png)

我们将采用一个非凡的开源工具来可视化我们模型的学习，称为 TensorFlow playground。这个工具只是允许用一些合成数据模拟一个神经网络，并让我们实际上*看到*我们的神经元在*上获得了什么样的模式。*它可以让你修改我们到目前为止概述的所有概念，包括不同类型和形式的输入特征、激活函数、学习率等等。我们强烈建议您尝试他们提供的不同合成数据集，使用输入功能并逐步添加神经元和隐藏层，以了解这如何影响学习。也要尝试不同的激活函数，看看你的模型如何从数据中捕捉各种复杂的模式。眼见为实！(或者更科学地说，verba 中的 nullius)。正如我们在下图中看到的，隐藏层中的两个神经元实际上都在捕捉我们特征空间中的不同曲率，学习数据中的特定模式。您可以通过观察连接各层的线条的粗细来可视化我们模型的权重。您还可以可视化每个神经元的输出(神经元内显示的蓝色和白色阴影区域),以查看特定神经元在我们的数据中捕捉到的潜在模式。通过在操场上的实验，你会看到，给定数据的形式和类型、激活函数和所用的学习率，这种表示会迭代更新并收敛到理想值:

![](Images/5da2a3e3-fe20-4d99-9b70-366fc12aeb3c.png) ![](Images/6256fd9d-ef64-4cc7-9e5c-f83691beb5af.png)

具有一个隐藏层、两个神经元和 sigmoid 激活函数的模型，训练了 1000 个时期

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 分级捕获模式

我们之前看到了一个具有两个神经元的特定模型配置，每个神经元都配备有一个 sigmoid 激活函数，它如何在我们的特征空间中捕获两个不同的曲率，然后将这两个曲率组合起来绘制我们的决策边界，由上述输出表示。然而，这只是一种可能的配置，导致一个可能的决策边界。

下图显示了一个具有两个隐藏层的模型，这两个隐藏层具有 sigmoid 激活函数，经过 1000 个时期的训练:

![](Images/b6b339b1-a220-4cf6-8ff9-3be17075ea68.png)

下图显示了一个具有一个隐藏层的模型，该模型由两个神经元组成，具有校正的线性单元激活函数，在同一数据集上训练了 1，000 个时期:

![](Images/0c033539-6573-48a8-b4c7-3dd577cfb9cd.png)

下图显示了一个具有一个隐藏层的模型，该模型由三个神经元组成，具有校正的线性单元激活函数，也是在同一数据集上:

![](Images/651adca8-95d9-4f8c-bbd6-f7cb212cbb23.png)

请注意，通过使用不同的激活函数，并操纵隐藏层及其神经元的数量，我们可以实现非常不同的决策边界。由我们来评估哪一个是理想的预测，并且适合我们的用例。大多数情况下，这是通过实验完成的，尽管关于您正在建模的数据的领域知识可能需要很长时间。

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 向前迈进一步

恭喜你！在短短几页中，我们已经走了很长的路。现在，您知道了神经网络如何学习，并对允许它从数据中学习的更高级数学结构有了概念。我们看到了单个神经元，即感知器，是如何配置的。我们看到了当数据通过它向前传播时，这个神经单元如何转换它的输入特征。我们还理解了通过激活函数来表示非线性的概念，以及如何在一个层中组织多个神经元，允许该层中的每个单独的神经元表示来自我们数据的不同模式。对于每个神经元，这些学习到的模式在每次训练迭代中被更新。我们现在知道，这是通过计算我们的预测和实际输出值之间的损失，并调整模型中每个神经元的权重，直到我们找到理想的配置来实现的。

事实上，现代神经网络采用各种类型的神经元，以不同的方式配置，用于不同的预测任务。虽然神经网络的基础学习架构总是保持不变，但是神经元的具体配置，就其数量、互连性、所使用的激活函数等而言。是定义你可能遇到的不同类型的神经网络结构的元素。目前，我们留给你一个由阿西莫夫研究所慷慨提供的全面的例证。

在下图中，你可以看到一些突出类型的神经元，或*细胞*，以及它们的配置，形成了一些最常用的最先进的神经网络，这也将贯穿本书的整个课程:

![](Images/64f53973-778e-40c8-b812-05381d4e291a.png)

<link href="Styles/Style00.css" rel="stylesheet" type="text/css"> <link href="Styles/Style01.css" rel="stylesheet" type="text/css"> <link href="Styles/Style02.css" rel="stylesheet" type="text/css"> <link href="Styles/Style03.css" rel="stylesheet" type="text/css">     

# 摘要

现在我们已经对神经学习系统有了全面的了解，我们可以开始动手了。我们将很快实现我们的第一个神经网络，为一个经典的分类任务测试它，并且实际上面对我们在这里讨论的许多概念。为此，我们将详细概述损失优化的确切性质，以及神经网络的评估指标。