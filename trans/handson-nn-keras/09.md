        

# 七、基于深度 Q 网络的强化学习

在上一章中，我们看到了递归循环、信息门和存储单元如何通过神经网络来模拟复杂的时间相关信号。更具体地说，我们看到了**长短期记忆** ( **LSTM** )架构如何利用这些机制来保持预测误差，并在越来越长的时间步长上反向传播它们。这使得我们的系统能够使用短期(即，来自与即时环境相关的信息)和长期表示(即，来自与很久以前观察到的环境相关的信息)来通知预测。

LSTM 的美妙之处在于，它能够在很长的时间内(长达一千个时间步长)学习和保存有用的表示。通过在架构中保持恒定的错误流，我们可以实现一种机制，允许我们的网络学习复杂的因果模式，嵌入到我们每天面对的现实中。事实上，迄今为止，在人工智能领域，让计算机了解因果关系的问题已经成为一个相当大的挑战。碰巧的是，现实世界的环境中充斥着稀疏和延时的奖励，对应于这些奖励的行为越来越复杂。在这种情况下模拟最佳行为包括发现关于给定环境的足够信息，以及可能的行动和相应的奖励，以做出相关的预测。正如我们所知，编码这种复杂的因果关系可能很困难，即使对人类来说也是如此。我们经常屈服于我们的非理性欲望，而没有仔细研究一些非常有益的因果关系。为什么？简单来说，实际的因果关系可能与我们对情况的内部估价不相符。我们可能会根据不同的奖励信号采取行动，这些信号会随着时间分散开来，每一个都会影响我们的总体决策。

我们对某些奖励信号的反应程度差异很大。这是基于特定的个体，并由特定个体所面临的基因构成和环境因素的复杂组合所决定的。在某种程度上，这是我们的天性。我们中的一些人只是天生更受短期奖励(如美味的小吃或娱乐电影)的影响，而不是长期奖励(如拥有健康的身体或有效利用时间)。这还不算太糟，对吧？嗯，不一定。不同的环境需要不同的短期和长期平衡，才能取得成功。考虑到人类可能遇到的环境的多样性(在个体意义上，以及广义的物种意义上)，我们观察到不同个体之间对奖励信号的解释存在差异就不足为奇了。在一个宏大的尺度上，进化只是最大化了我们在许多环境中生存的机会，这种现实可能会给我们留下深刻的印象。然而，正如我们很快就会看到的，这可能会对某些个人(也许还有一些贪婪的机器)产生微小的影响。

        

# 论奖励与满足

有趣的是，一组斯坦福大学的研究人员展示了(心理学家沃尔特·米歇尔在 20 世纪 70 年代领导的棉花糖实验)个人延迟短期满足的能力如何与长期更成功的结果相关联。基本上，这些研究人员召集儿童，观察他们在一系列选择面前的表现。孩子们有两个选择来决定他们在一次互动中总共能得到多少棉花糖。他们可以选择当场兑现一个棉花糖，或者如果他们选择等待 15 分钟，兑现两个棉花糖。这项实验敏锐地洞察了解读奖励信号对特定环境下的表现是有益还是有害，因为选择两个棉花糖的受试者在他们的一生中平均更成功。事实证明，延迟满足可能是最大化长期更有益行为的最重要部分。许多人甚至指出，为了有利的长期结果(如最终升入天堂)，宗教等概念如何成为延迟短期满足(即不偷窃)的集体表现。

        

# 检查学习的新方法

因此，看起来我们发展了一种内在意识，知道该采取什么行动，以及这将如何影响未来的结果。我们有机制使我们能够通过环境的相互作用来调节这些感觉，观察我们在很长一段时间内从不同的行为中得到什么样的回报。这对人类来说似乎是正确的，对居住在我们星球上的大多数生物来说也是如此，不仅包括动物，也包括植物。甚至植物在一天中也在优化某种能量分数，因为它们转动树叶和树枝来捕捉它们生存所需的阳光。那么，是什么机制让这些有机体模拟出最佳结果呢？这些生物系统如何跟踪环境，并及时准确地执行操作以达到有利的目的？好吧，也许行为心理学的一个分支，被称为**强化理论**，可能对这个话题有所启发。

这一观点由哈佛大学心理学家 B.F. Skinner 提出，将强化定义为一个主体(人类、动物，现在是计算机程序)与其环境之间观察到的相互作用的结果。来自这种相互作用的信息编码可能会加强或削弱代理人在类似的未来迭代中以相同方式行动的可能性。简单来说，如果你走在热煤上，你感觉到的疼痛会起到负面强化的作用，降低你未来选择踩在热煤上的可能性。相反，如果你抢劫了一家银行并侥幸逃脱，这种刺激和兴奋可能会强化这种行为，使其成为未来更有可能考虑的行为。事实上，斯金纳展示了如何通过一种简单的强化机制，训练普通的鸽子识别英语单词之间的差异，并玩乒乓球游戏。他展示了如何让鸽子在一段时间内接受足够多的奖励信号，这足以激励鸽子识别出它所听到的话语或要求它做的动作之间的细微差异。由于捕捉到这些变化代表了鸽子吃饱和空肚子之间的差异，斯金纳能够通过递增地奖励鸽子所期望的结果来影响它的行为。从这些实验中，他创造了术语*操作性条件反射*，涉及将任务分解成增量，然后反复奖励有利的行为。

大约半个世纪后的今天，我们在**机器学习** ( **ML** )的领域中操作这些概念，以加强模拟代理在给定环境中执行的有利行为。这一概念被称为强化学习，可以产生复杂的系统，在执行任务时可以与我们自己的智力相媲美(甚至超越)。

        

# 具有强化学习的条件机器

到目前为止，我们一直在处理简单的回归和分类任务。我们针对连续值(即预测股票市场时)回归观察值，并将特征分类到分类标签中(同时进行情绪分析)。这是监督式洗钱的两个基础活动。我们为我们的网络在训练时遇到的每个观察显示了一个特定的目标标签。在这本书的后面，我们将通过使用**生成对抗网络** ( **GANs** )和自编码器来讲述一些神经网络的无监督学习技术。然而，今天，我们将神经网络用于与这两种学习警告完全不同的东西。这种学习的告诫可以被命名为**强化学习**。

强化学习明显不同于前面提到的 ML 的变体。这里，我们不明确地将所有可能的动作序列标记为环境中所有可能的结果(如在监督分类中)，也不尝试基于基于相似性的距离度量(如在无监督聚类中)来划分我们的数据，以分割最佳动作。相反，我们让机器监控它所采取的行动的反应，并在一段时间内将最大可能的回报累积建模为行动的函数。本质上，我们处理面向目标的算法，这些算法学习在给定的时间步骤内实现复杂的目标。目标可以是击败逐渐向屏幕下方移动的太空入侵者，或者是现实世界中的狗形机器人从 A 点移动到 b 点。

        

# 信用转让问题

就像我们的父母用奖励和奖赏来强化我们的行为一样，我们也可以针对我们环境的特定状态(或配置)来强化我们想要的机器行为。这更多地调用了一种*试错*的学习方法，最近的事件表明了这种方法如何能够产生极其强大的学习系统，为一些非常有趣的用例打开了大门。这种相当独特而强大的学习范式确实带来了一些复杂的问题。例如，考虑信用分配问题。也就是说，我们之前的哪些行为对产生奖励负责，以及负责到什么程度？在一个具有稀疏的、延时的奖励的环境中，许多动作可能发生在一些动作之间，这些动作后来产生了所讨论的奖励。很难正确地将应得的信用分配给相应的行为。在缺乏适当的信用分配的情况下，我们的代理在评估不同的策略以实现其目标时毫无头绪。

        

# 探索-开发困境

让我们假设我们的代理人甚至设法找出了一个提供回报的一致策略。下一步是什么？他们应该简单地坚持同样的策略，永远产生同样的回报吗？或者更确切地说，他们应该一直尝试新事物吗？也许，通过不利用一个已知的策略，代理人可以有机会在未来获得更大的回报？这被称为**探索-利用困境**，指的是代理应该探索新策略或利用已知策略的程度。

在极端情况下，我们可以通过理解依赖已知策略来获得长期的直接回报是如何有害的来更好地理解探索-利用的困境。例如，对老鼠的实验表明，如果给这些动物一种机制来触发多巴胺(一种负责调节我们奖励系统的神经递质)的释放，它们会饿死。显然，从长远来看，挨饿并不是正确的选择，不管结局是多么令人欣喜若狂。然而，由于老鼠利用一个简单的策略持续触发奖励信号，长期奖励的前景(如活着)没有被探索。那么，我们如何通过放弃当前的机会来弥补我们的环境可能在以后提供更好的机会这一事实呢？不知何故，如果我们想让我们的代理人理解延迟满足的概念，以恰当地解决复杂的环境。很快，我们将看到深度强化学习如何试图解决这类问题，从而产生更复杂、更强大的系统，有些人甚至会称之为极其敏锐。

        

# 通往人工智能的道路

以 AlphaGo 系统为例，该系统由英国初创企业 DeepMind 开发，它利用深度强化学习的敏锐味道来通知其预测。谷歌以 5 亿美元收购它的举动背后有一个很好的理由，因为许多人声称 DeepMind 已经朝着所谓的**人工通用智能** ( **AGI** )迈出了第一步——如果你愿意的话，这可以算是人工智能的圣杯。这个概念指的是人工智能系统在各种任务上表现良好的能力，而不是我们的网络迄今为止所采用的狭窄应用范围。一个通过观察自身在环境中的行为来学习的系统在精神上与我们人类学习自己的方式相似(并且可能更快)。

我们在前几章中建立的网络在狭窄的分类或回归任务中表现良好，但是必须进行重大的重新设计和重新训练以在任何其他任务中表现。然而，DeepMind 展示了他们如何训练单个网络在几个不同的(尽管范围狭窄)任务中表现良好，包括玩几个老派的 Atari 2600 游戏。虽然有点过时，但这些游戏最初被设计成对人类具有挑战性，这使得这一壮举成为人工智能领域的一项非凡成就。在他们的研究([https://deepmind.com/research/dqn/](https://deepmind.com/research/dqn/))中，DeepMind 展示了他们的**深度 Q 网络** ( **DQN** )如何可以用来让人工智能体只通过观察屏幕上的像素来玩不同的游戏，而无需任何关于游戏本身的事先信息。他们的工作激发了新一波研究人员，他们开始使用基于强化学习的算法来训练深度学习网络，从而诞生了深度强化学习。从那以后，研究人员和企业家都试图利用这些技术进行一系列的用例，包括但不限于让机器像动物和人类一样移动，为药物生成分子化合物，甚至制造可以在股票市场交易的机器人。

不用说，这种系统在模拟真实世界事件时可以更加灵活，并且可以应用于一系列任务，从而减少花费在训练单独的狭窄系统上的资源。有一天，他们甚至能够揭示复杂和高维的因果关系，利用来自几个领域的训练样本来编码协同表示，这反过来帮助我们解决更复杂的问题。我们自己的发现经常受到来自不同科学领域的信息的启发。这有助于增强我们对这些情况以及控制这些情况的复杂动态的理解。那么，为什么不让机器来做这件事呢？在给定的环境中，如果对可能的行动给出正确的奖励信号，它甚至可能超越我们自己的直觉！也许有一天你能帮上忙。现在，让我们先来看看我们如何着手模拟一个虚拟代理，并让它与环境交互来解决简单的问题。

        

# 模拟环境

首先，我们需要一个模拟环境。环境被定义为学习代理的*交互空间。对人类来说，环境可以是你一天中去的任何一个地方。对于人工智能体，这通常是我们设计的模拟环境。为什么是模拟的？嗯，我们可以要求代理像我们一样实时学习，但事实证明这是非常不切实际的。首先，我们必须为每个代理人设计一个身体，然后精确地设计它的动作以及它们要与之互动的环境。此外，代理人可以在模拟中更快地进行训练，而不需要受人类时间框架的限制。当一台机器在现实中完成一项任务时，它的模拟版本可能已经多次完成了同样的任务，这为它从错误中学习提供了更好的机会。*

接下来，我们将回顾用于描述游戏的基本术语，游戏代表了一个环境，在这个环境中，代理需要执行某些任务来获得奖励和解决环境问题。

        

# 理解状态、行动和奖励

环境本身可以分解成不同状态的集合，所有这些状态都代表代理可能发现自己所处的不同情况。代理可以通过尝试允许它执行的动作的不同组合来浏览这些状态(例如，对于 2D 街机游戏，向左或向右走以及跳跃)。代理所做的动作有效地改变了环境的状态，提供了可用的工具、替代路线、敌人或游戏制作者为了让你更感兴趣而隐藏的任何其他好东西。所有这些对象和事件都代表了代理在学习环境中导航时可能采取的不同状态。作为代理在其先前状态下与其交互的结果，或者由于环境中发生的随机事件，环境生成新的状态。这是一个游戏本质上的进展，直到达到一个终结状态，这意味着游戏不能再继续下去(由于一个胜利或死亡)。

本质上，我们希望代理人追求及时和有利的行动来解决其环境。这些行动必须改变环境的状态，使代理人更接近实现给定的目标(如从 A 点移动到 B 点或最大化得分)。为了能够做到这一点，我们需要设计奖励信号，它是代理人与环境的不同状态交互的结果。我们可以使用奖励的概念作为反馈，允许我们的代理评估其行动在优化给定目标时获得的成功程度:

![](Images/26d4b365-8425-4fac-9733-2ea36e1afcc1.png)

对于那些熟悉经典街机风格视频游戏的人来说，想象一下马里奥游戏。马里奥本人是代理人，受你控制。环境指的是马里奥可以在其中移动的地图。硬币和蘑菇的出现代表了游戏的不同状态。一旦马里奥与这些状态中的任何一个互动，奖励就会以点数的形式触发，一个新的状态随之诞生，这反过来相应地改变了马里奥的环境。马里奥的目标可以是从 A 点移动到 B 点(如果你急着完成游戏)，也可以是最大化他的分数(如果你对解锁成就更感兴趣)。

        

# 自动驾驶出租车

接下来，我们将通过观察人工智能体如何解决环境问题来阐明我们迄今为止所收集的理论理解。我们将看到这是如何实现的，即使是通过从代理的动作空间中随机抽样动作(代理可能执行的动作)。这将有助于我们理解解决哪怕是最简单的环境所涉及的复杂性，以及为什么我们可能希望在短期内调用深度强化学习来帮助我们实现目标。我们要解决的目标是在一个简化的模拟环境中创建一个自动驾驶的出租车。虽然我们将处理的环境比真实世界简单得多，但这种模拟将作为进入强化学习系统的设计架构的优秀垫脚石。

为了做到这一点，我们将使用 OpenAI 的`gym`，这是一个名副其实的模块，用于模拟训练机器的人工环境。你可以通过使用`pip`包管理器来安装 OpenAI gym 依赖项。以下命令将在 Jupyter 笔记本上运行，并启动模块的安装:

```py
 ! pip install gym 
```

`gym`模块带有大量预安装的环境(或测试问题)，从简单的模拟到更复杂的模拟。我们将在下面的例子中使用的测试问题来自于`'TaxiCab-v2'`环境。我们将从所谓的**出租车模拟**开始我们的实验，它只是模拟一个道路网格，供出租车通过，以便它们可以接送客户:

```py
import numpy as np
import gym
from gym import envs

# This will print allavailable environemnts
# print(envs.registry.all())

```

        

# 理解任务

出租车模拟是在(Dietterich 2000)中引入的，以演示以分层方式应用强化学习的问题。然而，在我们继续模拟和解决更复杂的问题之前，我们将使用这个模拟来巩固我们对代理、环境、奖励和目标的理解。目前，我们面临的问题相对简单:在一个给定的地点接载乘客并让他们下车。这些位置(总共四个)用字母表示。我们的代理人所要做的就是前往这些上车地点，搭载一名乘客，然后前往一个指定的下车地点，乘客可以在那里下车。假设成功下船，代理人将获得+20 点(模拟我们的虚拟出租车司机获得的钱)。在到达目的地之前，我们的出租车司机每走一步，就会得到-1 的奖励(直觉上，这是我们的出租车司机因必须补充汽油的成本而受到的惩罚)。最后，另一个惩罚-10 存在于没有预定的取货和卸货。你可以想象惩罚皮卡背后的原因，因为一家出租车公司试图优化其车队部署，以覆盖城市的所有地区，要求我们的虚拟出租车司机只搭载指定的乘客。另一方面，计划外的送货只是反映了顾客的不满和困惑。让我们看看出租车驾驶室环境实际上是什么样子。

        

# 渲染环境

为了可视化我们刚刚加载的环境，我们必须首先通过在我们的环境对象上调用`reset()`来初始化它。然后，我们可以渲染起始帧，对应于出租车的位置(黄色)和四个不同的上车地点(用彩色字母表示):

```py
# running env.reset() returns the initial state of the environment

print('Initial state of environment:' , env.reset())
env.render()
```

我们得到以下输出:

![](Images/96326554-b2e7-4610-bc85-8e8503b9bc64.png)

请注意，前面的截图使用冒号(`:`)描绘了开放的道路，使用符号(`|`)描绘了出租车无法穿越的墙壁。虽然这些障碍和路线的位置保持不变，但每次初始化环境时，表示上车点的字母以及我们的黄色出租车都会不断变化。我们还可以注意到，重置环境会生成一个整数。这指的是初始化时获得的环境的特定状态(即出租车和皮卡的位置)。

你可以用注册表中的其他环境(如`CartPole-v0`或`MountainCar-v0`)替换`Taxi-v2`字符串，并渲染几帧以了解我们在处理什么。还有一些其他命令可以让您更好地理解您正在处理的环境。虽然出租车环境足够简单，可以使用彩色符号模拟，但更复杂的环境可以在单独的窗口中呈现，该窗口在执行时打开。

        

# 参考观察空间

接下来，我们将尝试更好地了解我们的环境和行动空间。出租车环境中的所有状态都由范围在 0 到 499 之间的整数表示。我们可以通过打印出环境中可能状态的总数来验证这一点。让我们来看看我们的环境可能出现的不同状态的数量:

```py
env.observation_space.n

500
```

        

# 参考动作空间

在出租车模拟中，我们的出租车司机代理被给予六个不同的动作，它可以在每个时间步执行。我们可以通过检查环境的操作空间来检查可能操作的总数，如下所示:

```py
env.action_space.n

6
```

我们的司机可以在任何时候做这六个动作中的一个。这些动作对应于向上、向下、向左或向右移动；接人；或者把它们放下。

        

# 与环境互动

为了让我们的代理做一些事情，我们可以在我们的环境对象上使用`step()`方法。`step(i)`方法接受一个整数，该整数表示我们的代理被允许采取的六个可能动作中的一个。在这种情况下，这些操作被标记如下:

*   (0)向下移动
*   (1)向上移动
*   (2)右转
*   (3)对于左转
*   (四)搭载乘客的
*   (5)让乘客下车

下面是代码的表示方式:

```py
#render current position
env.render()

#move down
env.step(0)

#render new position
env.render()
```

我们得到以下输出:

![](Images/14ef2611-c6f5-4c9e-bbc7-93f19dd2688d.png)

正如我们在这里看到的，我们让我们的代理人向下迈了一步。现在，我们了解了如何让我们的代理执行所有必需的步骤来实现其目标。事实上，从代理的角度来看，在环境对象上调用`step(i)`将返回四个特定变量，指的是动作(I)对环境做了什么。这些变量如下:

1.  `observation`:这是观察到的环境状态。这可以是来自游戏截屏的像素数据或向学习代理表示环境状态的另一种方式。
2.  这是对我们代理人的补偿，由于在给定的时间步采取的行动。我们通过简单地要求学习代理在给定的环境中最大化它接收的奖励，来使用奖励为我们的学习代理设定目标。请注意，奖励(浮动)值的比例可能因实验设置而异。
3.  `done`:该布尔(二进制)值表示试验情节是否已经终止。在出租车模拟的情况下，当乘客在给定位置被接载和放下时，一集被认为是`done`。对于 Atari 游戏来说，一集可以定义为代理人的生命，一旦你被太空入侵者击中，它就会终止。
4.  这个字典条目用来存储用于调试我们的代理的行为的信息，并且通常不用于学习过程本身。它确实存储了有价值的信息，例如对于给定的步骤，影响先前状态变化的概率:

```py
# env.step(i) will return four variables
# They are defiend as (in order) the state, reward, done, info
env.step(1)

(204, -1, False, {'prob': 1.0})
```

        

# 随机解决环境问题

有了管理 OpenAI 健身房环境的逻辑，以及如何让人工智能体在其中交互，我们可以继续实现一个随机算法，允许智能体(最终)解决出租车环境。首先，我们定义一个固定的状态来开始我们的模拟。如果您想要重复相同的实验(即，用相同的状态初始化环境)，同时检查代理在每集求解环境时采取了多少随机步骤，这将很有帮助。我们还定义了一个`counter`变量，它简单地记录了我们的代理在剧集进展过程中所采取的时间步数。奖励变量被初始化为`None`，一旦代理迈出第一步，它将被更新。然后，我们简单地启动一个`while`循环，从我们的动作空间中随机重复采样可能的动作，并为每个采样的动作更新各自的`state`、`reward`和`done`变量。为了从环境的动作空间中随机抽取动作，我们在`env.action_space`对象上使用了`.sample()`方法。最后，我们增加我们的`counter`并渲染可视化环境:

```py
# Overriding current state, used for reproducibility
state = env.env.s = 114

# counter tracks number of moves made
counter = 0

#No reward to begin with
reward = None

dropoffs = 0

#loop through random actions until successful dropoff (20 points)

while reward != 20:
    state, reward, done, info = env.step(env.action_space.sample())
    counter += 1
    print(counter)
    env.render()

print(counter, dropoffs)
```

我们得到以下输出:

![](Images/07002f1f-b2af-4e82-b9be-6a0dacef89ba.png)

直到第 2145 次^(尝试，我们的代理人才在驾驶室里找到了正确的乘客(如驾驶室变绿所示)。那是相当长的时间，即使你可能感觉不到时间的流逝。当调用更复杂的模型作为健全性的度量时，随机算法有助于基准测试我们的性能。但是毫无疑问，我们可以做得比 6，011 步更好(如运行随机算法的代理所采取的)来解决这个简单的环境。怎么会？嗯，我们奖励它是正确的。要做到这一点，我们必须首先从数学上定义奖励的概念。)

        

# 当前和未来回报之间的权衡

乍一看，这似乎很简单。我们已经看到了出租车司机是如何被激励的，正确下车奖励+20 分，错误下车奖励-10 分，每完成一集奖励-1 分。那么，从逻辑上讲，你可以计算一个代理为一集收集的总奖励，作为该代理看到的每个时间步的所有个人奖励的累计。我们可以用数学方法表示这一点，并将一集的总报酬表示如下:

![](Images/47449219-aeff-4a74-acb8-0e703c9695f5.png)

这里， *n* 简单表示剧集的时间步长。这似乎很直观。我们现在可以要求我们的代理人在给定的一集里最大化总报酬。但是有一个问题。就像我们自己的现实一样，我们的代理人所面临的环境很大程度上可能是由随机事件决定的。因此，可能无法保证在相似的未来状态下，执行相同的动作会得到相同的回报。事实上，随着我们走向未来，由于存在固有的随机性，回报可能会越来越偏离在每个状态下采取的相应行动。

        

# 贴现未来奖励

那么，我们如何补偿这种差异呢？一种方法是通过贴现未来的奖励，从而放大当前奖励相对于未来时间步骤奖励的相关性。当我们计算给定剧集的总奖励时，我们可以通过在每个时间步生成的奖励中添加折扣因子来实现这一点。这个折扣系数的目的是抑制未来的奖励，放大当前的奖励。在短期内，我们有更多的确定性能够通过使用相应的状态动作对来收集奖励。从长远来看，这是不可能的，因为环境中随机事件的累积效应。因此，为了激励代理专注于相对确定的事件，我们可以修改我们之前的总报酬公式，以包括这个折扣因子，如下所示:

![](Images/8491e1d1-370d-4734-a81c-d522f17ca20f.png)

在我们新的总报酬公式中，γ表示 0 到 1 之间的折扣因子，而 *t* 表示当前时间步长。你可能马上就注意到了，γ项的指数下降使得未来的奖励比当前的要低。直觉上，这仅仅意味着当代理人考虑下一步行动时，远在未来的奖励比更现代的奖励考虑得少。多少钱？这还是取决于我们。接近于零的折扣系数会产生短视的策略，享乐主义倾向于眼前的回报而不是未来的回报。另一方面，将折扣系数设置得太接近于 1 将会违背其最初的目的。实际上，平衡值可以在 0.75-0.9 的范围内，这取决于环境中的随机程度。根据经验，对于确定性更强的环境，您可能需要更高的 gamma (γ)值，而对于随机环境，您可能需要更低的(γ)值。我们甚至可以简化之前给出的总报酬公式，如下所示:

![](Images/5e7e1c95-2c44-4f03-9830-f1bffdd5502e.png)

因此，我们将一集的总奖励形式化为该集每个时间步的累积折扣奖励。通过使用贴现未来回报的概念，我们可以为代理人生成策略，从而控制其行为。执行有益策略的代理将致力于选择在给定情节中最大化未来折扣回报的行动。既然我们已经有了如何为我们的代理人设计奖励信号的好主意，那么是时候继续前进，看看整个学习过程的概况了。

        

# 马尔可夫决策过程

在强化学习中，我们正试图解决即时行动与延迟回报之间的关联问题。这些奖励只是稀疏的、延时的标签，用来控制代理的行为。到目前为止，我们已经讨论了代理如何对环境的不同状态进行操作。我们还看到了交互如何为代理产生各种奖励，并打开环境的新状态。从这里，代理可以恢复与环境的交互，直到一集结束。为了目标优化的目的，现在是我们用数学方法形式化代理和环境之间的这些关系的时候了。为此，我们将调用俄罗斯数学家安德烈·马尔科夫(Andrey Markov)提出的框架，现在称为**马尔科夫决策过程** ( **MDP** )。

这个数学框架允许我们在一个部分随机、部分由代理人控制的环境中对代理人的决策过程进行建模。这个过程依赖于马尔可夫假设，即未来状态( *st+1* )的概率仅取决于当前状态( *st* )。这个假设意味着导致当前状态的所有状态和动作对未来状态的概率没有影响。MDP 由以下五个变量定义:

![](Images/b9c1b7d8-197f-48a1-9192-f157ca72de81.png)

虽然前两个变量是不言自明的，但第三个变量( **R** )指的是给定一个状态-行动对的奖励的概率分布。在这里，状态-动作对只是指对于给定的环境状态要采取的相应动作。列表上的下一个是转移概率( **P** )，它表示在一个时间步长上给定所选状态-动作对的新状态的概率。

最后，贴现因子指的是我们希望将未来奖励贴现为更直接奖励的程度，在下图中有详细说明:

![](Images/8ccdd66f-917a-41a7-b81c-f065858f093b.png)

左:强化学习问题。右图:马尔可夫决策过程

因此，我们可以用 MDP 来描述我们的主体和环境之间的相互作用。MDP 由状态和动作的集合以及指示从一种状态到另一种状态的转换的规则组成。我们现在可以用数学方法将一个情节定义为状态、动作和奖励的有限序列，如下所示:

![](Images/8fe05707-fcf7-4973-b70d-aa6832a6f676.png)

这里，(s [t] )和(a [t] 表示在时间 *t* 的状态和相应的动作。我们也可以把这个状态-动作对对应的回报表示为(r [t+1] )。因此，我们通过从环境(s [o] 中采样初始状态来开始一集。然后，直到我们的目标完成，我们要求我们的代理为它所处的环境的相应状态选择一个动作。一旦代理执行了一个动作，环境就对代理所采取的动作的回报以及随后的下一个状态(s [t+1] )进行采样。然后，代理接收奖励和下一个状态，然后重复该过程，直到它能够解决环境。最后，在一集结束时(也就是当我们的目标完成，或者我们在一个游戏中耗尽了生命)达到一个终结状态(s [n] )。确定代理在每个状态下的动作的规则统称为**策略**，用希腊符号(π)表示。

        

# 了解策略功能

正如我们所看到的，我们的代理解决环境的效率取决于它在每个时间步使用什么策略来匹配状态-动作对。因此，一个被称为**策略函数** (π)的函数可以为代理遇到的每个时间步指定状态-动作对的组合。当模拟运行时，策略负责产生轨迹，轨迹由游戏状态组成；我们的代理作为回应而采取的行动；和环境产生的奖励，以及代理收到的游戏的下一个状态。直观地说，您可以将策略看作是一种启发，它可以生成响应环境生成状态的操作。政策功能本身可以是好的，也可以是坏的。如果你的政策是先开枪再问问题，你最终可能会射杀一名人质。因此，我们现在需要做的就是评估不同的策略(即，它们产生的轨迹，包括它们的状态、动作、奖励和下一个状态的序列)，并挑选出最大化给定游戏的累积折扣奖励的最优策略(π *)。这可以用数学方法说明如下:

![](Images/237dbf57-199c-4267-9521-853d92843d01.png)

因此，如果我们试图从 A 点移动到 B 点，我们的最优策略将包括采取一个行动，允许我们在每个时间步长尽可能靠近 B 点。不幸的是，由于这种环境中存在的随机性，我们可能无法如此肯定地声称我们的政策绝对最大化了折扣奖励的总和。根据定义，我们无法解释某些随机事件，比如地震，比如从 A 点到 B 点的旅行(假设你不是地震学专家)。因此，我们也不能完美地解释由于环境中的随机性而导致的行为所带来的回报。相反，我们可以将最优策略(π *)定义为让我们的代理人最大化折扣奖励的期望总和的策略。这可以通过对前面的等式稍加修改来表示，可以描述如下:

![](Images/d253b709-88ec-4d0d-bcda-79f29ff914c4.png)

这里，我们使用 MDP 框架，从我们的状态概率分布 p(s [o] )中采样初始状态(s [o] )。给定一个状态，我们的代理(a [t] )的动作是从一个策略中抽取的。然后，对与在给定状态下执行的行动的效用相对应的奖励进行采样。最后，环境从当前状态-动作对的转移概率分布中采样下一个状态(s [t+1] )。因此，在每个时间步，我们的目标是更新我们的最优策略，以便我们可以最大化折扣奖励的期望总和。你们中的一些人可能会问，在给定一个状态的情况下，我们如何评估一个行为的效用？这就是 value 和 Q 值函数的用武之地。为了评估不同的策略功能，对于给定的策略，我们需要能够评估不同状态的值，以及对应于这些状态的动作的质量。为此，我们需要定义两个额外的函数，称为值函数和 Q 值函数。

        

# 评估一个国家的价值

首先，我们需要在遵循特定政策(π)的同时，估计状态( *s* )的值( *V* )。这告诉你当遵循从状态( *s* )开始的策略(π)时，在游戏的终端状态的期望累积奖励。这为什么有用？好吧，想象一下，我们的学习代理的环境中充满了不断追逐代理的敌人。它可能已经制定了一个策略，命令它在整个游戏过程中永不停止运行。在这种情况下，代理应该有足够的灵活性来评估游戏状态的值(例如，当它跑到悬崖边缘时，以免跑下来死掉)。我们可以通过定义给定状态下的价值函数*V*π(s)来实现这一点，作为代理从当前状态开始遵循该策略所获得的预期累积(贴现)回报:

![](Images/8cc4b32c-c73b-408b-9a93-17d214b22a7f.png)

因此，我们能够使用价值函数来评估一个国家在遵循某种政策时有多好。然而，这仅仅告诉我们一个国家本身的价值，给定一个政策。我们还希望我们的代理能够根据给定的状态判断一个动作的值。这才是真正允许代理动态地对任何环境(无论是敌人还是悬崖边)做出反应的原因。我们可以通过使用 Q 值函数来操作给定政策的给定状态动作对的*优度*的概念。

        

# 评估行动的质量

如果你走到一堵墙前，你能做的动作并不多。在你所处的环境中，你可能会对这种状态做出反应，选择转身，然后问自己为什么首先要走到墙边。类似地，我们希望我们的代理在遵循一个策略时，根据他们发现自己所处的状态，对不同的行为产生一种良好的感觉。我们可以使用 Q 值函数来实现这一点。这个函数简单地表示在遵循一个政策的同时，在一个特定的状态下，从采取一个特定的行动中得到的预期的累积回报。换句话说，它表示给定策略的状态-动作对的质量。数学上，我们可以将 *Q* *π ( a，s)* 关系表示如下:

![](Images/174dab38-76ed-457e-be41-f29e0daf7ea9.png)

*Q* *π ( s，a)* 函数允许我们表示遵循一个策略的预期累积回报(π)。直观地说，这个函数帮助我们在一个游戏结束时量化我们的总得分，给定在环境(例如，一个马里奥游戏)的每个状态(你观察的游戏屏幕)下采取的动作(即，不同的操纵杆控制移动)，同时遵循一个策略(在跳跃的同时向前移动)。使用这个函数，我们可以定义在一个游戏的最终状态下，给定所遵循的政策，可能的最佳预期累积奖励。这可以表示为 Q 值函数可获得的最大期望值，也称为最佳 Q 值函数。我们可以用数学方法定义如下:

![](Images/7cef80af-3b08-4873-9144-6372f7b1fd09.png)

现在，给定一个策略，我们有一个量化状态动作对的期望最优值的函数。给定一个游戏状态，我们可以用这个函数来预测最佳的行动。然而，我们如何评估我们预测的真实标签呢？我们并没有在游戏屏幕上标注相应的目标行动来评估我们的网络离目标有多远。这就是贝尔曼方程的用武之地，它帮助我们评估一个给定状态动作对的价值，作为当前产生的奖励以及随后游戏状态的价值的函数。然后，我们可以使用该函数来比较我们网络的预测，并反向传播误差以更新模型权重。

        

# 使用贝尔曼方程

由美国数学家理查德·贝尔曼提出的贝尔曼方程是驱动深度 Q 学习战车的主要主力方程之一。它本质上允许我们解决我们之前形式化的马尔可夫决策过程。直觉上，贝尔曼方程做了一个简单的假设。它指出，在一个州执行的给定行为的最大未来奖励是当前奖励加上下一个州的最大未来奖励。为了与棉花糖实验进行比较，两个棉花糖的最大可能奖励是由代理人通过在第一时间步弃权(奖励 0 个棉花糖)然后在第二时间步收集(奖励两个棉花糖)的行为获得的。

换句话说，给定任何状态-动作对，在给定状态(s)执行动作(a)的质量(Q)等于要接收的回报(r)，以及代理最终所处的后续状态(s’)的值。因此，我们可以计算当前状态的最佳动作，只要我们可以估计下一时间步的最佳状态动作值*Q *(s’，a’)*。正如我们刚刚看到的棉花糖的例子，我们的代理需要能够预测未来最大可能的回报(两个棉花糖)，以避免在当前时间只接受一个棉花糖。使用贝尔曼方程，我们希望我们的代理采取最大化即时回报(r)的行动，以及下一个状态-行动对的最优 Q *-值，*Q *(s’，a’)*，由折扣因子γ(y)衰减。更简单地说，我们希望它能够计算当前状态下行动的最大预期未来回报。这转化为以下内容:

![](Images/c753435a-e314-4ca8-b636-8bd4d4dc4b67.png)

现在，我们知道了如何在数学上估计一个动作在给定状态下的预期质量。我们也知道当遵循一个特定的政策时，如何估计国家-行动对的最大期望回报。由此，我们可以将给定状态(s)下的最优策略( π *)重新定义为给定状态下行动的最大预期 Q 值。这可以显示如下:

![](Images/2dc686a1-049a-4431-ade7-2f936882a02d.png)

最后，我们有了拼图的所有部分来实际尝试并找到一个最优策略(π *)来指导我们的代理。这个策略将允许我们的代理人通过为环境产生的每个状态选择理想的行动来最大化期望的折扣奖励(包括环境的随机性)。那么，我们实际上如何着手做这件事呢？一个简单的、非深度学习的解决方案是使用价值迭代算法来计算在未来时间步 *( Qt+1 ( s，a ))* 的行动质量，作为在博弈的下一状态(*γmax a Qt(s’，a’)*的期望当前奖励(r)和最大折扣奖励的函数。从数学上讲，我们可以用公式表示如下:

![](Images/e0d8f2b2-a7ea-43b1-9706-0e1853cdb2cb.png)

这里，我们基本上迭代更新贝尔曼方程，直到 Qt 收敛到 Q*，随着 *t* 的增加，无限增加。实际上，我们可以通过简单地实现 Bellman 方程来解决出租车模拟问题，从而测试它的估计。

        

# 迭代更新贝尔曼方程

您可能还记得，解决出租车模拟的随机方法花费了我们的代理大约 6000 个时间步骤。有时，纯粹出于运气，你可能能够在 2000 个时间步骤之下解决它。然而，我们可以通过实现一个版本的贝尔曼方程来进一步提高我们的胜算。这种方法基本上允许我们的代理通过使用 Q 表来记住它的动作和每个状态对应的奖励。我们可以使用 NumPy 数组在 Python 上实现这个 Q 表，其维度对应于我们在出租车环境中的观察空间(不同可能状态的数量)和动作空间(我们的代理可以做出的不同可能动作的数量)。回想一下，出租车模拟具有 500 的环境空间和 6 的动作空间，使得我们的 Q 表成为 500 行和 6 列的矩阵。我们还可以初始化奖励变量(`R`)和折扣因子 gamma 的值:

```py
#Q-table, functions as agent's memory of state action pairs
Q = np.zeros([env.observation_space.n, env.action_space.n])

#track reward
R = 0

#discount factor
gamma = 0.85

# Track successful dropoffs
dropoffs_done = 0

#Run for 1000 episodes
for episode in range(1,1001):
    done = False

    #Initialize reward
    R, reward = 0,0

    #Initialize state
    state = env.reset()

    counter=0

    while done != True:

            counter+=1

            #Pick action with highest Q value
            action = np.argmax(Q[state]) 

            #Stores future state for compairason
            new_state, reward, done, info = env.step(action)

            #Update state action pair using reward and max Q-value for the new state
            Q[state,action] += gamma * (reward + np.max(Q[new_state]) - Q[state,action]) 

            #Update reward
            R += reward  

            #Update state
            state = new_state

            #Check how many times agent completes task
            if reward == 20:
                dropoffs_done +=1

    #Print reward every 50 episodes        
    if episode % 50 == 0:
        print('Episode {}   Total Reward: {}   Dropoffs done: {}  Time-Steps taken {}'
              .format(episode,R, dropoffs_done, counter))

Episode 50 Total Reward: -30 Dropoffs done: 19 Time-Steps taken 51
Episode 100 Total Reward: 14 Dropoffs done: 66 Time-Steps taken 7
Episode 150 Total Reward: -5 Dropoffs done: 116 Time-Steps taken 26
Episode 200 Total Reward: 14 Dropoffs done: 166 Time-Steps taken 7
Episode 250 Total Reward: 12 Dropoffs done: 216 Time-Steps taken 9
Episode 300 Total Reward: 5 Dropoffs done: 266 Time-Steps taken 16
```

然后，我们简单地循环一千集。每一集，我们初始化环境的状态，一个跟踪已经执行的卸载的计数器，以及奖励变量(total: `R`和 episode-wise: `r`)。在我们的第一个循环中，我们嵌套了另一个循环，指示我们的代理选择一个具有最高 Q 值的动作，执行该动作，并存储环境的未来状态以及收到的奖励。如布尔变量 done 所指示的，该循环被指示运行，直到剧集被认为终止。

接下来，我们更新 Q 表中的状态-动作对，以及全局奖励变量(指示我们的代理整体表现如何)。算法中的α项(α)表示学习率，这有助于在执行 Q 表的更新时控制先前和新生成的 Q 值之间的变化量。因此，我们的算法迭代地更新状态动作对(Q [state，action])的质量，通过在每个时间步近似动作的最佳 Q 值。随着这个过程不断重复，我们的代理最终收敛到最佳状态-动作对，如 Q*所示。

最后，我们更新状态变量，用新的状态变量重新定义当前状态。然后，循环可以重新开始，迭代地更新 Q 值，并且理想地收敛到存储在 Q 表中的最佳状态-动作对。我们每 50 集打印出由环境取样的整体奖励，作为我们代理人行动的结果。我们可以看到，我们的代理最终收敛于手头任务的最佳可能奖励(即，考虑每个时间步的旅行成本的最佳奖励，以及正确下车的奖励)，对于该任务，该奖励在 9 到 13 分之间。你还会注意到，到第 50 集^(时，我们的代理已经在 51 个时间步骤中成功完成了 19 次卸货！这种方法比我们之前实现的随机方法表现得更好。)

        

# 为什么要使用神经网络？

正如我们刚刚看到的，基本值迭代方法可以用来更新贝尔曼方程，并迭代地找到理想的状态-行动对，以最佳地导航给定的环境。这种方法实际上在每个时间步存储新的信息，迭代地使我们的算法更加智能。然而，这种方法也有一个问题。简直不可伸缩！出租车环境足够简单，有 500 个状态和 6 个动作，可以通过迭代更新 Q 值来求解，从而估计每个单独的状态-动作对的值。然而，更复杂的模拟，如视频游戏，可能有数百万个状态和数百个动作，这就是为什么计算每个状态-动作对的质量变得在计算上不可行并且在逻辑上低效。在这种情况下，我们剩下的唯一选择是尝试使用加权参数网络来逼近函数 *Q(a，s)* 。

因此，我们进入了神经网络的领域，正如我们现在所知道的，神经网络是优秀的函数逼近器。我们将很快体验的深度强化学习的特殊味道被称为深度 Q 学习，它的名字自然来自于它在环境中学习给定状态-动作对的最优 Q 值的任务。更正式地说，我们将使用神经网络通过模拟我们的代理的一系列状态、动作和奖励来逼近最优函数 *Q*(s，a)* 。通过这样做，我们可以在与给定环境的最佳状态-动作对最匹配的方向上迭代地更新我们的模型权重(θ):

![](Images/d8250fa4-145f-4881-b4f9-cff3a1fcf568.png)        

# 在 Q 学习中执行向前传递

现在，您理解了使用神经网络来逼近最优函数 *Q*(s，a)* 背后的直觉，找到给定状态下的最佳可能动作。不言而喻，对于一系列状态而言，最佳的行动顺序将产生最佳的回报顺序。因此，我们的神经网络试图估计一个函数，该函数可以将可能的动作映射到状态，从而为整个情节生成最佳回报。您还会记得，我们需要估计的最优质量函数 *Q*(s，a)* 必须满足贝尔曼方程。贝尔曼方程简单地将最大可能未来报酬建模为当前时间的报酬，加上紧接着的时间步长的最大可能报酬:

![](Images/352c3c39-06cb-42d5-8b18-419f59cb8c9d.png)

因此，当我们的目标是预测给定时间的最佳 Q 值时，我们需要确保由贝尔曼方程提出的条件得以保持。为了做到这一点，我们可以将该模型的总损失函数定义为使我们的贝尔曼方程和实际预测中的误差最小化的函数。换句话说，在每次向前传递时，我们计算当前状态-动作质量值 *Q (s，a；θ)* 都是来自当时已经用贝尔曼方程表示的理想的(*Y[t]T5)。由于贝尔曼方程表示的理想预测正在迭代更新，我们实际上是使用移动目标变量(Y [t] )来计算模型的损失。这可以用数学公式表示如下:*

![](Images/069e0250-a679-40a7-b27c-34686e709dc9.png)

因此，我们的网络将通过最小化一系列损失函数来训练，损失函数 *L [t] (θt)* ，在每个时间步长变化。这里，术语*y[t]是我们在时间( *t* )进行预测的目标标签，并且在每个时间步持续更新。还要注意，术语 *ρ(s，a)* 简单地表示序列 s 和我们的模型所采取的动作的内部概率分布，也称为其行为分布。如您所见，在给定时间( *t* )优化损失函数时，前一时间( *t-1* )步骤的模型权重保持冻结。虽然此处所示的实现使用相同的网络进行两次单独的前向传递，但 Q-learning 的后续版本(Mihn 等人，2015 年)使用两个单独的网络:一个用于预测满足贝尔曼方程的移动目标变量(称为目标网络)，另一个用于计算给定时间的模型预测。现在，让我们看看反向传递如何在深度 Q 学习中更新我们的模型权重。*

        

# 在 Q 学习中执行向后传递

现在，我们有了一个定义的损耗度量，它计算最佳 Q 函数(从贝尔曼方程得出)与给定时间的当前 Q 函数之间的误差。然后，当我们的网络在环境中发挥作用时，我们可以通过模型层反向传播 Q 值中的预测误差。正如我们现在所熟知的，这是通过获取损失函数相对于模型权重的梯度，然后在每个学习批次的梯度的相反方向上更新这些权重来实现的。因此，我们可以在最佳 Q 值函数的方向上迭代地更新模型权重。我们可以将反向传播过程公式化，并说明模型权重(θ)的变化，如下所示:

![](Images/b02f39ee-9fea-4576-9a35-b898c0f5deb6.png)

最终，当模型已经看到足够多的状态动作对时，它将充分地反向传播它的错误，并学习最佳表示，以帮助它在给定的环境中导航。换句话说，一个经过训练的模型将具有层权重的理想配置，对应于最佳 Q 值函数，映射代理在给定环境状态下的动作。

简而言之，这些方程描述了估计最佳政体(π*)以解决给定环境的过程。我们使用神经网络来学习给定环境中状态动作对的最佳 Q 值，这反过来可用于计算为我们的代理产生最佳回报的轨迹(即，最佳策略)。这就是我们如何使用强化学习来训练预期的和反应性的代理，这些代理在具有稀疏时间延迟奖励的准随机模拟中操作。现在，我们已经具备了继续实施我们自己的深度强化学习代理所需的所有知识。

        

# 用深度学习取代迭代更新

在我们继续实现之前，让我们澄清一下到目前为止我们在深度 Q 学习方面学到了什么。正如我们在迭代更新方法中看到的，我们可以使用转换(从初始状态、执行的动作、产生的奖励和采样的新状态，< s, a, r, s' >)来更新在每个时间步长保存这些元组的值的 Q 表。然而，正如我们提到的，这种方法在计算上是不可扩展的。相反，我们将替换在 Q 表上执行的这种迭代更新，并尝试使用神经网络来逼近最佳 Q 值函数( *Q*(s，a)* )，如下所示:

1.  使用当前状态( *s* )作为输入，执行前馈过程，然后预测该状态下所有动作的 Q 值。
2.  使用新状态(*s’*)执行前馈传递，以计算我们的网络在下一状态的最大总输出，即*max a’Q(s)，a’)*。
3.  在步骤 2 中计算出最大总输出后，我们将相应动作的目标 Q 值设置为 *r + γmax a' Q(s)，a')* 。我们还将所有其他动作的目标 Q 值设置为与步骤 1 为每个未选择的动作返回的值相同，以便只计算所选动作的预测误差。这有效地抵消了(设置为零)我们的代理在每个时间步没有采取的预测行动的误差影响。
4.  反向传播误差以更新模型权重

我们在这里所做的就是建立一个能够对移动目标进行预测的网络。这是有用的，因为我们的模型在玩游戏时会迭代地遇到更多关于环境物理的信息。这反映在我们的目标输出(在给定的状态下执行什么动作)也在不断变化，不像在监督学习中，我们有固定的输出，我们称之为标签。因此，我们实际上正在尝试学习一个函数 *Q*(s，a)*可以学习不断变化的输入(游戏状态)和输出(要采取的相应动作)之间的映射。

通过这个过程，我们的模型对要执行什么动作有了更好的直觉，并且随着它看到更多的环境，对状态-动作对的正确 Q 值有了更好的了解。理论上，Q-learning 允许我们通过将奖励与先前游戏状态下采取的行动相关联来解决信用分配问题。误差被反向传播，直到我们的模型能够识别决定产生给定奖励的状态-行为对。然而，我们很快就会看到，许多计算和数学技巧被用来使深度 Q 学习系统工作得更好。在我们深入考虑这些因素之前，进一步探讨深 Q 网络中发生的前向和后向传递可能会有所帮助。

        

# Keras 中的深度 Q 学习

既然我们已经了解了如何训练一个智能体选择最佳状态动作对，让我们试着解决一个比我们之前处理的出租车模拟更复杂的环境。为什么不实现一个学习代理来解决一个原本是为人类自己精心制作的问题呢？感谢开源运动的奇迹，这正是我们将要做的。在我们的任务列表中，接下来我们将实现 Mnih 等人(2013 年和 2015 年)的方法，这些方法参考了实现基于 Q-learning 的代理的原始 DeepMind 论文。研究人员使用相同的方法和神经架构来玩七种不同的雅达利游戏。值得注意的是，研究人员在测试的七个不同游戏中的六个游戏中取得了显著的成绩。在这六场游戏中的三场，代理人的表现都超过了人类专家。这就是为什么，今天，我们试图部分复制这些结果，并训练一个神经网络来玩一些老派游戏，如太空入侵者和 Pacman 女士。

这是通过使用一个**卷积神经网络** ( **CNN** )来完成的，它将视频游戏截图作为输入，并估计游戏给定状态下动作的最佳 Q 值。为了跟进，您需要做的就是安装构建在 Keras 之上的强化学习包，称为`keras-rl`。您还需要 OpenAI `gym`模块的 Atari 依赖项，这是我们之前使用过的。Atari dependency 本质上是 Atari 控制台的模拟器，它将生成我们的培训环境。虽然该依赖最初是为在 Ubuntu 操作系统上运行而设计的，但后来它被移植到兼容 Windows 和 Mac 用户的平台上。您可以使用`pip`软件包管理器来安装这两个模块，以便进行下面的实验。

*   您可以使用以下命令安装 Keras 强化学习包:

```py
 ! pip install keras-rl
```

*   您可以使用以下命令安装 Atari dependency for Windows:

```py
! pip install --no-index -f https://github.com/Kojoley/atari- 
  py/releases atari_py
```

*   mnh 等人(2015 年)https://arxiv . org/pdf/1312.5602 v1 . pdf

        

# 做一些进口

在机器智能领域，实现对游戏等任务的人类级控制是一个长期的梦想。自动化一个只对高维感觉输入(如音频、图像等)进行操作的代理所涉及的复杂性，对于强化学习来说是一个相当具有挑战性的任务。以前的方法严重依赖手工制作的特性，结合过于依赖工程特性质量的线性策略表示，以获得良好的性能。不像以前的尝试，这种技术不需要我们的代理人有任何关于游戏的人类工程知识。它将只依赖于它接收的像素输入和编码表示来预测它所经过的环境的每个状态下的每个可能动作的最佳 Q 值。很酷，不是吗？让我们将下面的库导入到我们的工作区中，看看我们如何继续这项任务:

```py
from PIL import Image
import numpy as np
import gym

from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute
from keras.optimizers import Adam
import keras.backend as K

from rl.agents.dqn import DQNAgent
from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy
from rl.memory import SequentialMemory
from rl.core import Processor
from rl.callbacks import FileLogger, ModelIntervalCheckpoint
```

        

# 预处理技术

正如我们之前提到的，我们将使用 CNN 来编码显示给代理的每个州的代表性视觉特征。我们的 CNN 将继续根据最佳 Q 值回归这些更高级别的表示，对应于每个给定状态要采取的最佳行动。因此，在玩 Atari 游戏时，我们必须向我们的网络显示一系列输入，对应于您将看到的一系列屏幕截图。

如果我们在玩太空入侵者(Atari 2600)的游戏，这些截图看起来会像这样:

![](Images/ca81f847-74e0-4b70-8d0d-d60eea49f25a.png)

最初的 Atari 2600 屏幕框架是从 70 年代开始设计的，旨在满足人类味觉的审美要求，尺寸为 210 x 160 像素，配色方案为 128。虽然按顺序处理这些原始帧在计算上可能要求很高，但请注意，有很多机会可以从这些帧中对我们的训练图像进行下采样，以处理更易于管理的表示。事实上，这遵循了 Minh 等人的方法，将输入维度减少到更易于管理的大小。这是通过将原始 RGB 图像下采样为 110 x 84 像素的灰度图像来实现的，然后裁剪掉图像中没有什么变化的末端。这给我们留下了 84 x 84 像素的最终图像尺寸。这种维度的减少有助于我们的 CNN 更好地编码代表性的视觉特征，遵循我们在第 4 章*卷积神经网络*中介绍的理论。

        

# 定义输入参数

最后，我们的卷积网络也将一次四个一批地接收这些裁剪的图像。使用这四个帧，神经网络将被要求估计给定输入帧的最佳 Q 值。因此，我们定义我们的输入形状，参考预处理的 84 x 84 游戏屏幕帧的大小。我们还定义了一个窗口长度`4`，它只是指我们的网络一次看到的图像数量。对于每个图像，网络将对最佳 Q 值进行标量预测，这将使我们的代理可获得的预期未来回报最大化:

![](Images/bf877265-f575-4d99-9b14-5a02378aa151.png)        

# 制作 Atari 游戏状态处理器

由于我们的网络只允许通过输入图像观察游戏的状态，我们必须首先构建一个 Python 类，让我们的**深度 Q 学习代理** ( **DQN** )处理 Atari 模拟器生成的状态和奖励。这个类将接受一个处理器对象，它简单地指代理和它的环境之间的耦合机制，就像在`keras-rl`库中实现的那样。

我们正在创建`AtariProcessor`类，因为我们希望使用相同的网络在不同的环境中执行，每个环境都有不同类型的状态、动作和奖励。这背后的直觉是什么？嗯，想想《太空入侵者》游戏和《吃豆人》游戏在游戏画面和可能的动作上的区别。虽然太空入侵者游戏中的防御者只能侧身滚动并开火，但吃豆人可以上下左右移动，以应对环境的不同状态。自定义处理器类帮助我们简化不同游戏之间的训练过程，而无需对学习代理或观察环境进行太多修改。我们将实现的处理器类将允许我们简化不同游戏状态和奖励的处理，这些游戏状态和奖励是通过对环境起作用的代理生成的:

```py
class AtariProcessor(Processor):

    def process_observation(self, observation):
        # Assert dimension (height, width, channel) 
        assert observation.ndim == 3 
        # Retrieve image from array
        img = Image.fromarray(observation) 
        # Resize and convert to grayscale
        img = img.resize(INPUT_SHAPE).convert('L') 
        # Convert back to array
        processed_observation = np.array(img) 
        # Assert input shape
        assert processed_observation.shape == INPUT_SHAPE 
        # Save processed observation in experience memory (8bit)
        return processed_observation.astype('uint8')
   def process_state_batch(self, batch):
      #Convert the batches of images to float32 datatype
       processed_batch = batch.astype('float32') / 255.
       return processed_batch
   def process_reward(self, reward):
       return np.clip(reward, -1., 1.) # Clip reward
```

        

# 处理单个状态

我们的处理器类包括三个简单的函数。第一个函数(`process_observation`)获取一个表示模拟游戏状态的数组，并将其转换为图像。然后，调整图像的大小，将其转换回数组，并作为可管理的数据类型返回给体验内存(我们稍后将详细阐述这个概念)。

        

# 批处理状态

接下来，我们有(`process_state_batch`)函数，它批量处理图像，并将它们作为一个`float32`数组返回。虽然这一步也可以在第一个函数中完成，但我们分开做的原因是为了获得更高的计算效率。简单的数学表明，存储一个`float32`数组的内存消耗是存储一个 8 位数组的 4 倍。既然我们希望我们的观察结果存储在经验记忆中，我们宁愿将它们存储在可管理的表示中。当处理给定环境的数百万个状态时，这样做变得尤其重要。

        

# 处理奖励

最后，我们类中的最后一个函数让我们通过使用(`process_reward`)函数来裁剪从环境中产生的奖励。为什么要这么做？好吧，让我们考虑一点背景信息。虽然我们让我们的代理人在真实的、未经修改的游戏上训练，但奖励结构的这种变化仅在训练期间执行。我们可以将正奖励和负奖励分别固定为+1 和-1，而不是让我们的代理使用游戏屏幕上的实际分数。奖励值为 0 不受此裁剪操作的影响。这样做实际上是有用的，因为它让我们在反向传播网络误差时限制导数的规模。此外，在不同的学习环境中实现相同的代理变得更容易，因为代理不必为全新类型的游戏学习新的计分方案。

        

# 奖励削减的局限性

正如 DeepMind 的论文(Minh 等人，2015 年)所指出的，削减奖励的一个明显的不利方面是，这种操作禁止我们的代理人能够区分不同大小的奖励。这样的概念肯定会与更复杂的模拟相关。例如，考虑一辆真正的自动驾驶汽车。在给定环境状态的情况下，处于控制中的人工智能体可能需要评估它可能不得不采取的两难行动的奖励/惩罚的幅度。也许代理人面临的行动，如轧过行人，以避免在路上发生更灾难性的事故。然而，这个限制似乎并没有严重影响我们的代理征服 Atari 2600 游戏提供的简单学习环境的能力。

        

# 初始化环境

接下来，我们简单地使用 Atari 依赖项初始化 space invaders 环境(不需要单独导入),这是我们之前添加到可用的`gym`环境中的:

```py
env = gym.make('SpaceInvaders-v0')
np.random.seed(123)
env.seed(123)
nb_actions = env.action_space.n
```

我们还生成一个随机种子来一致地初始化环境的状态，以便它具有可再现的实验。最后，我们定义了一个变量，该变量与我们的代理在任何给定时间可以采取的动作数量有关。

        

# 构建网络

凭直觉思考我们的问题，我们正在设计一个神经网络，它接受从一个环境中采样的一系列游戏状态。在序列的每个状态，我们希望我们的网络预测具有最高 Q 值的动作。因此，对于每个可能的游戏状态，我们网络的输出将涉及每个动作的 Q 值。因此，我们首先定义几个卷积层，随着层数的增加，滤波器的数量增加，步长减小。所有这些卷积层都用一个**整流线性单元** ( **ReLU** )激活函数来实现。在此之后，我们添加一个扁平化层，以减少从卷积层到矢量表示的输出维度。

这些表示然后被馈送到两个紧密连接的层，这两个层针对可用动作的 Q 值执行游戏状态的回归:

```py
input_shape = (WINDOW_LENGTH,) + INPUT_SHAPE
# Build Conv2D model
model = Sequential()
model.add(Permute((2, 3, 1), input_shape=input_shape))
model.add(Convolution2D(32, (8, 8), strides=(4, 4), activation='relu'))
model.add(Convolution2D(64, (4, 4), strides=(2, 2), activation='relu'))
model.add(Convolution2D(64, (3, 3), strides=(1, 1), activation='relu'))
model.add(Flatten())
model.add(Dense(512, activation='relu'))
# Last layer: no. of neurons corresponds to action space
# Linear activation
model.add(Dense(nb_actions, activation='linear'))  
print(model.summary())

_____________________________________________________________
Layer (type) Output Shape Param # 
=============================================================
permute_2 (Permute) (None, 84, 84, 4) 0 
_____________________________________________________________
conv2d_4 (Conv2D) (None, 20, 20, 32) 8224 
_____________________________________________________________
conv2d_5 (Conv2D) (None, 9, 9, 64) 32832 
_____________________________________________________________
conv2d_6 (Conv2D) (None, 7, 7, 64) 36928 
_____________________________________________________________
flatten_2 (Flatten) (None, 3136) 0 
_____________________________________________________________
dense_3 (Dense) (None, 512) 1606144 
_____________________________________________________________
dense_4 (Dense) (None, 6) 3078 
=============================================================
Total params: 1,687,206
Trainable params: 1,687,206
Non-trainable params: 0
______________________________________________________________
None

```

最后，您会注意到我们的输出层是一个紧密连接的层，有许多神经元对应于我们代理的动作空间(即它可能执行的动作的数量)。这一层也有一个线性激活函数，就像我们之前看到的回归例子一样。这是因为我们的网络本质上是在执行一种多变量回归，其中它使用其特征表示来预测代理在给定输入状态下可能采取的每个动作的最高 Q 值。

        

# 缺乏汇集层

您可能已经注意到，与以前的 CNN 示例的另一个不同之处是没有池层。以前，我们使用池层对每个卷积层产生的激活图进行下采样。正如您在第 4 章、*卷积神经网络*中回忆的那样，这些池层帮助我们实现了对 CNN 不同类型输入的空间不变性概念。然而，在为我们的特定用例实现 CNN 时，我们可能不想丢弃特定于表示的空间位置的信息，因为这实际上可能是为我们的代理确定正确移动的不可或缺的一部分:

![](Images/2cada956-3eb7-4acf-a749-4faddb16e3fe.png)

正如你在两张几乎相同的图片中所看到的，太空入侵者发射的炮弹的位置极大地改变了我们代理人的游戏状态。虽然在第一幅图像中，代理人足够远来避开这个投射物，但在第二幅图像中，它可能会因为走错一步(向右移动)而遭遇厄运。因为我们希望它能够显著区分这两种状态，所以我们避免使用池层。

        

# 现场学习的问题

正如我们前面提到的，我们的神经网络将一次处理四帧序列，并将这些输入回归到从 Atari 仿真器采样的每个单独状态(即图像)的最高 Q 值的动作。然而，如果我们不打乱我们的网络接收每批四幅图像的顺序，那么我们的网络在学习过程中就会遇到一些非常棘手的问题。

我们不希望我们的网络从连续批次的样本中学习的原因是因为这些序列是局部相关的。这是一个问题，因为网络参数在任何给定时间都将决定仿真器生成的下一个训练示例。给定马尔可夫假设，未来游戏状态的概率取决于当前游戏状态。因此，如果当前的最大化动作指示我们的代理向右移动，那么批中的后续训练样本将由向右移动的代理支配，从而导致不良和不必要的反馈循环。此外，连续的训练样本通常过于相似，网络无法有效地从中学习。在训练过程中，这些问题可能会导致我们的网络损耗收敛到局部(而不是全局)最小值。那么，我们到底该如何应对呢？

        

# 在重放记忆中存储经验

答案在于为我们的网络打造一个重放记忆的想法。本质上，重放记忆可以充当各种固定长度的*体验* *que* 。它可以用来存储正在玩的游戏的顺序状态，以及所做的动作、产生的奖励和返回给代理的状态。这些经验问题会不断更新，以保持游戏的最新状态。然后，我们的网络将使用存储在重放存储器中的随机批次的经验元组(`state`、`action`、`reward`和`next state`)来执行梯度下降。

在`rl.memory`、`keras-rl`模块中有不同类型的重放存储器实现。我们使用`SequentialMemory`对象来完成我们的目的。这需要两个参数，如下所示:

```py
memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)
```

limit 参数表示要保存在内存中的条目数。一旦超过限制，新条目将替换旧条目。`window_length`参数简单的说就是每批训练样本的数量。

由于经验元组批次的随机顺序，网络不太可能陷入局部最小值，并且将最终收敛以找到最优权重，代表给定环境的最优策略。此外，使用非连续批次执行权重更新意味着我们可以实现更高的数据效率，因为相同的单个图像可以被混洗到不同的批次中，从而有助于多次权重更新。最后，这些经验元组甚至可以从人类游戏数据中收集，而不是由网络执行的先前移动。

其他方法(Schaul 等人，2016:[https://arxiv.org/abs/1511.05952](https://arxiv.org/abs/1511.05952))通过添加额外的数据结构来跟踪每个转换的优先级(*状态- >动作- >奖励- >下一状态*，以更频繁地重放重要的转换，从而实现了体验重放记忆的优先化版本。这背后的直觉是让网络更频繁地从其最佳和最差表现中学习，而不是在没有太多学习的情况下学习。虽然这些是一些帮助我们的模型收敛到相关表示的聪明方法，但我们也希望它不时给我们带来惊喜，并探索它尚未考虑的机会。这就把我们带回到*我们之前讨论过的探索-开发困境*。

        

# 平衡勘探和开发

我们如何确保我们的代理依赖于新旧策略的良好平衡？对于我们的 Q 网络，这个问题由于权重的随机初始化而变得更糟。由于预测的 Q 值是这些随机权重的结果，该模型将在初始训练时期生成次优预测，这反过来导致较差的 Q 值学习。自然，我们不希望我们的网络过于依赖它最初为给定的状态-动作对生成的策略。就像多巴胺上瘾的老鼠一样，如果代理人不探索新的策略并扩大其视野，而不是利用已知的策略，就不能指望它长期表现良好。为了解决这个问题，我们必须实现一种机制，鼓励代理尝试新的动作，忽略学习到的 Q 值。这样做基本上允许我们的学习代理尝试新的策略，这些策略从长远来看可能更有益。

        

# ε-贪婪勘探政策

这可以通过算法修改我们的学习代理用来解决其环境的策略来实现。一种常见的方法是使用ε-贪婪的探索策略。这里，我们定义一个概率(ε)。然后，我们的代理可以忽略学习到的 Q 值，并以(1 - ε)的概率尝试随机动作。因此，如果ε值被设置为 0.5，我们的网络平均来说将忽略其学习的 Q 表所建议的动作，并做一些随机的事情。这是一个相当具有探索性的代理。相反，ε的值为 0.001 将使网络更加一致地依赖于学习的 Q 值，平均仅在一百个时间步长中选择一个随机动作。

很少使用固定的ε值，因为要实现的探索与利用的程度可能基于许多内部(例如，代理的学习速率)和外部因素(例如，给定环境中随机性与确定性的程度)而不同。在 DeepMind 的论文中，研究人员实现了一个随时间衰减的ε项，从 1(完全不依赖于初始随机预测)到 0.1(10 次中有 9 次依赖于预测的 Q 值):

```py
policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), 
                              attr='eps',
                              value_max=1.,
                              value_min=.1,
                              value_test=.05,
                              nb_steps=1000000)
```

因此，衰减的ε确保我们的代理不依赖于初始训练时期的随机预测，只是在以后随着 Q 函数收敛到更一致的预测，更积极地利用它自己的预测。

        

# 初始化深度 Q 学习代理

现在，我们已经以编程方式定义了初始化深度 Q 学习代理所需的所有组件。为此，我们使用从`rl.agents.dqn`导入的`DQNAgent`对象，并定义适当的参数，如下所示:

```py
#Initialize the atari_processor() class

processor = AtariProcessor()

# Initialize the DQN agent 
dqn = DQNAgent(model=model,             #Compiled neural network model
               nb_actions=nb_actions,   #Action space
               policy=policy,   #Policy chosen (Try Boltzman Q policy)
               memory=memory,   #Replay memory (Try Episode Parameter  
                                                memory)
               processor=processor,     #Atari processor class
#Warmup steps to ignore initially (due to random initial weights)
               nb_steps_warmup=50000,   
               gamma=.99,                #Discount factor
               train_interval=4,         #Training intervals
               delta_clip=1.,            #Reward clipping
              )
```

前面的参数是按照最初的 DeepMind 论文初始化的。现在，我们已经准备好最终编译我们的模型，并开始训练过程。为了编译模型，我们可以简单地调用我们的`dqn`模型对象上的 compile 方法:

```py
dqn.compile(optimizer=Adam(lr=.00025), metrics=['mae'])
```

这里的`compile`方法将优化器和我们想要跟踪的指标作为参数。在我们的例子中，我们选择具有低学习率`0.00025`的`Adam`优化器，并跟踪**平均绝对误差** ( **MAE** )指标，如下所示。

        

# 训练模型

现在，我们可以开始深度 Q 学习网络的培训课程了。我们通过在编译后的 DQN 网络对象上调用`fit`方法来实现这一点。`fit`参数将训练的环境(在我们的例子中是 SpaceInvaders-v0)和在这个训练过程中游戏的总步骤数(类似于 epoch，表示从环境中采样的游戏状态总数)作为参数。如果您希望可视化您的代理在训练时的表现，您可以选择将可选参数`visualize`定义为`True`。虽然这很有趣——甚至有点让人着迷——但它会显著影响训练速度，因此将其作为默认设置是不实际的:

```py
dqn.fit(env, nb_steps=1750000)   #visualize=True

Training for 1750000 steps ...
Interval 1 (0 steps performed)
 2697/10000 [=======>....................] - ETA: 26s - reward: 0.0126
```

        

# 测试模型

我们使用以下代码测试该模型:

```py
dqn.test(env, nb_episodes=10, visualize=True)

Testing for 10 episodes ...
Episode 1: reward: 3.000, steps: 654
Episode 2: reward: 11.000, steps: 807
Episode 3: reward: 8.000, steps: 812
Episode 4: reward: 3.000, steps: 475
Episode 5: reward: 4.000, steps: 625
Episode 6: reward: 9.000, steps: 688
Episode 7: reward: 5.000, steps: 652
Episode 8: reward: 12.000, steps: 826
Episode 9: reward: 2.000, steps: 632
Episode 10: reward: 3.000, steps: 643

<keras.callbacks.History at 0x24280aadc50>
```

        

# Q 学习算法综述

恭喜你！现在，您已经对深度 Q-learning 的概念有了详细的理解，并应用这些概念让模拟的代理逐步学习解决其环境问题。下面的伪代码是作为我们刚刚实现的整个深度 Q 学习过程的复习提供的:

```py
initialize replay memory
initialize Q-Value function with random weights
sample initial state from environment
Keep repeating:

     choose an action to perform:
            with probability ε select a random action
            otherwise select action with argmax a Q(s, a')
     execute chosen action
     collect reward and next state
     save experience <s, a, r, s'> in replay memory

     sample random transitions <s, a, r, s'> from replay memory
     compute target variable for each mini-batch transition:

             if s' is terminal state then target = r
             otherwise t = r + γ max a'Q(s', a')
     train the network with loss (target - Q(s,a)`^2)
     s = s'

until done
```

        

# 双 Q 学习

我们刚刚建立的标准 Q 学习模型的另一个增强是双 Q 学习的想法，这是由 Hado van Hasselt (2010 年和 2015 年)提出的。这背后的直觉很简单。回想一下，到目前为止，我们使用贝尔曼方程来估计每个状态-动作对的目标值，并检查在给定状态下我们的预测有多离谱，如下所示:

![](Images/560856e1-103a-48e8-a5a8-f305404472cc.png)

然而，以这种方式估计最大预期未来回报会产生一个问题。您可能已经注意到，目标方程中的 max 运算符( *y [t]* )使用相同的 Q 值来评估给定的动作，这些 Q 值用于预测采样状态的给定动作。这引入了高估 Q 值的倾向，最终甚至失控。为了弥补这种可能性，Van Hasselt 等人(2016)实施了一个模型，将行动的选择与其评估分离开来。这是通过使用两个独立的神经网络实现的，每个神经网络都被参数化以估计整个方程的子集。第一个网络的任务是预测在给定状态下要采取的行动，而第二个网络用于生成目标，在迭代计算损失时，通过该目标来评估第一个网络的预测。尽管每次迭代的损失公式不变，但给定状态的目标标号现在可以用增广的双 DQN 方程来表示，如下所示:

![](Images/c0f29994-cacf-41c3-9c83-7154ab59cac1.png)

正如我们所看到的，目标网络有自己的一组参数需要优化，(θ-)。这种从评估中分离出行动选择的方法已经被证明可以补偿天真的 DQN 所学到的过于乐观的表现。因此，我们能够更快地收敛我们的损失函数，同时实现更稳定的学习。

在实践中，目标网络的权重也可以是固定的，并且缓慢地/周期性地更新，以避免由于不良反馈回路(在目标和预测之间)而使模型不稳定。这种技术在另一篇 DeepMind 论文(Hunt，Pritzel，Heess 等人，2016 年)中得到推广，该方法被发现可以稳定训练过程。

Hunt，Pritzel，Heess 等人的 DeepMind 论文，*深度强化的连续控制* *学习*，2016，可在 https://arxiv.org/pdf/1509.02971.pdf 访问。

你可以通过`keras-rl`模块实现双 DQN，使用我们之前训练太空入侵者代理时使用的相同代码，对定义你的 DQN 代理的部分稍作修改:

```py
double_dqn = DQNAgent(model=model,
               nb_actions=nb_actions,
               policy=policy,
               memory=memory,
               processor=processor,
               nb_steps_warmup=50000,
               gamma=.99, 
               target_model_update=1e-2,
               train_interval=4,
               delta_clip=1.,
               enable_double_dqn=True,
              )
```

我们所要做的就是定义`enable_double_dqn`到`True`的布尔值，我们就可以开始了！或者，您可能还希望试验预热步骤的数量(即，在模型开始学习之前)和目标模型更新的频率。我们可以进一步参考以下论文:

*   **采用双 Q 学习的深度强化学习**:【https://arxiv.org/pdf/1509.06461.pdf】T2

        

# 决斗网络架构

我们将实现的 Q-learning 架构的最后一个变体是决斗网络架构([https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581))。顾名思义，在这里，我们使用两个独立的状态值和状态-动作对值的估计器，形象地进行神经网络决斗。你应该还记得，在本章的前面，我们使用单个卷积流和密集连接层来估计状态-动作对的质量。然而，我们实际上可以将 Q 值函数分解为两个独立项的和。这种分离架构背后的原因是允许我们的模型分别学习可能有价值或可能没有价值的状态，而不必专门学习在每个状态下执行的每个动作的效果:

![](Images/76d2ea1a-a292-4b93-a3c0-a1962a7d8e76.png)

在上图的顶部，我们可以看到标准的 DQN 架构。在底部，我们可以看到决斗 DQN 架构如何分叉成两个独立的流，其中状态和状态动作值是单独估计的，没有任何额外的监督。因此，决斗 dqn 使用单独的估计器(即，密集连接的层)来估计处于某个状态的值 *V(s)* ，以及在给定状态下执行一个动作优于另一个动作的优势 *A(s，a)* 。然后将这两项结合起来预测给定状态-动作对的 Q 值，确保我们的代理从长远来看选择最佳动作。虽然标准的 Q 函数 *Q(s，a)* 只允许我们估计给定状态的选择动作的值，但是我们现在可以分别测量状态的值和动作的相对优势。在执行一个动作没有以足够相关的方式改变环境的情况下，这样做是有帮助的。

值和优势函数都在下面的等式中给出:

![](Images/00e4055e-3390-4e93-a151-d36b8f657edf.png)

DeepMind 的研究人员(王等，2016)在一个早期的赛车游戏(Atari Enduro)上测试了这样的架构，在这个游戏中，智能体被指示在一条有时可能会出现障碍的道路上行驶。研究人员注意到状态价值流如何学习关注道路和屏幕上的分数，而行动优势流只会在游戏屏幕上出现特定障碍时学习关注。自然地，只有当障碍物在其路径上时，代理执行动作(向左或向右移动)才变得重要。否则，向左或向右移动对代理没有重要性。另一方面，对于我们的代理人来说，保持对道路和分数的关注总是很重要的，这是由网络的状态价值流来完成的。因此，在他们的实验中，研究人员展示了这种架构如何能够导致更好的政策评估，特别是当一个代理面临许多具有类似后果的行为时。

我们可以使用`keras-rl`模块实现决斗 dqn，解决我们之前看到的相同的空间入侵者问题。我们需要做的就是重新定义我们的代理，如下所示:

```py
dueling_dqn = DQNAgent(model=model,
               nb_actions=nb_actions,
               policy=policy,
               memory=memory,
               processor=processor,
               nb_steps_warmup=50000,
               gamma=.99, 
               target_model_update=10000,
               train_interval=4,
               delta_clip=1.,
               enable_dueling_network=True,
               dueling_type='avg'
              )
```

这里，我们只需将布尔参数`enabble_dueling_network`定义为`True`，并指定决斗类型。

有关网络架构和潜在使用优势的更多信息，我们鼓励您继续关注在 https://arxiv.org/pdf/1511.06581.pdf 发表的完整研究论文*深度强化学习的决斗网络架构*。

        

# 锻炼

*   在 Atari 环境中使用不同的策略(Boltzman)实施标准 Q-learning，并检查性能指标的差异
*   对同一个问题实现双 DQN，比较性能差异
*   针对相同的问题实现一个决斗 DQN，并比较性能差异

        

# Q 学习的局限性

给定足够的训练时间，这样一个相对简单的算法如何能够产生这样的代理人能够想出的复杂策略，这确实是值得注意的。值得注意的是，研究人员(现在，你也是)能够展示如何通过与环境的足够互动来学习专家策略。例如，在经典游戏《突围》(作为 Atari dependency 中的一个环境)中，你需要移动屏幕底部的一块木板来将球弹回，并打碎屏幕顶部的一些砖块。经过足够长时间的训练后，一名 DQN 经纪人甚至可以想出复杂的策略，比如让球停留在屏幕的顶部，尽可能多地得分:

![](Images/9687c498-c42c-491f-8410-6aa22222b726.png)

这种直觉行为自然会让你想知道——我们能把这种方法学发展到什么程度？用这种方法我们可以控制什么类型的环境，它的限制是什么？

事实上，Q 学习算法的强大之处在于它们解决高维观察空间问题的能力，比如游戏屏幕上的图像。我们使用卷积架构实现了这一点，允许我们将状态-动作对与最佳奖励相关联。然而，到目前为止，我们所关心的行动空间大多是离散的和低维的。向右转或向左转，表示一个离散的动作，相对于连续的动作，如以一个角度向左转。后者是连续动作空间的一个例子，因为代理人向左转弯的动作取决于由某个角度表示的变量，该变量可以取连续值。我们也没有太多的可执行动作(Atari 2600 游戏从 4 到 18 个不等)。其他候选深度强化学习问题，如机器人运动控制或优化车队部署，可能需要对非常高维度和连续的动作空间进行建模，而标准 dqn 往往表现不佳。这仅仅是因为 DQNs 依赖于寻找使 Q 值函数最大化的动作，在连续动作空间的情况下，这将需要在每一步迭代优化。幸运的是，这个问题还有其他解决方法。

        

# 利用策略梯度改进 Q-learning

到目前为止，在我们的方法中，我们一直在迭代地更新我们对状态-动作对的 Q 值的估计，从中我们推断出最优策略。然而，当处理连续动作空间时，这成为一项艰巨的学习任务。例如，在机器人运动控制的情况下，我们的动作空间由连续变量定义，如机器人的关节位置和角度。在这种情况下，估计 Q 值函数变得不切实际，因为我们可以假设函数本身极其复杂。因此，在每个给定的状态下，我们可以尝试不同的方法，而不是学习每个关节位置和角度的最佳 Q 值。如果我们可以直接学习一个策略，而不是通过迭代更新状态-动作对的 Q 值来推断，会怎么样？回想一下，策略只是状态的轨迹，后面是执行的动作、生成的奖励以及返回给代理的状态。因此，我们可以定义一组参数化的策略(由神经网络的权重(θ)来参数化)，其中每个策略的值可以由下面给出的函数来定义:

![](Images/d9a73329-884e-4274-92e5-9013f462ef63.png)

这里，策略的值由函数 *J(θ)* 表示，其中θ表示我们的模型权重。在左侧，我们可以用我们之前看到的熟悉术语来定义给定保单的价值，表示累积的未来回报的预期总和。在这种新的设置下，我们的目标是找到返回保单价值函数的最大值 *J(θ)* 的模型权重，对应于我们的代理人的最佳预期未来报酬。

以前，为了找到函数的全局最小值，我们对该函数的一阶导数进行迭代优化，并采取与梯度的负值成比例的步骤来更新模型权重。这就是我们所说的梯度体面。然而，由于我们想要找到我们的策略值函数的最大值， *J(θ)* ，我们将执行梯度上升，其迭代地更新与我们的梯度的正值成比例的模型权重。因此，我们可以通过评估由给定策略生成的轨迹，而不是单独评估状态-动作对的质量，来获得收敛于最优策略的深度神经网络。在此基础上，我们甚至可以使来自有利政策的行动有更高的概率被我们的代理选择，而来自不利政策的行动可以不那么频繁地被采样，给定博弈状态。这是政策梯度方法背后的主要直觉。很自然，这种方法会带来一整套新的技巧，我们鼓励你仔细阅读。一个这样的例子是*深度强化学习的连续控制，* by，可以在[https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)找到。

一个有趣的策略梯度实现可以是 Actor Critic 模型，它可以在连续的动作空间中实现，以解决涉及高维度动作空间的更复杂的问题，如我们之前讨论的问题。更多关于演员评论家模型的信息可以在 https://arxiv.org/pdf/1509.02971.pdf 找到。

这种相同的演员评论家概念已经在一系列不同任务的不同设置中使用，如自然语言生成和对话建模，甚至玩复杂的实时策略游戏，如星际争霸 2，鼓励感兴趣的读者探索:

*   **自然语言生成和对话建模**:【https://arxiv.org/pdf/1607.07086.pdf】T2
*   **星际争霸 2:强化学习的新挑战**:【https://arxiv.org/pdf/1708.04782.pdf? FB clid = iwar 30 qje 6 kw 16 PHA 949 PEF _ vct brx 582 bdnnwg 2 odmgqtiqpn 4 ypbtdv-xFs

        

# 摘要

在这一章中，我们讲了很多。我们不仅探索了机器学习的一个全新分支，即强化学习，我们还实现了一些最先进的算法，这些算法已经证明可以产生复杂的自主代理。我们看到了如何使用马尔可夫决策过程来模拟环境，以及如何使用贝尔曼方程来评估最佳回报。我们还看到了如何通过使用深度神经网络逼近质量函数来解决信用分配问题。在这样做的同时，我们探索了一整套技巧，如奖励折扣、剪辑和体验回放记忆(仅举几例),这些技巧有助于表示游戏屏幕图像等高维输入，以便在优化目标的同时导航模拟环境。

最后，我们探索了深度 Q 学习领域的一些进展，概述了双 dqn 和决斗 dqn 等架构。最后，我们回顾了在使代理人成功地导航高维行动空间中存在的一些挑战，并看到了不同的方法，如政策梯度，可以帮助解决这些问题。