

# 二、深度前馈网络

在第一章中，你学习了驱动各种神经网络逻辑的数学。在这一章中，我们将关注最基本的神经网络，称为**前馈神经网络**。我们还将研究具有多个隐藏层的深度前馈网络，以提高模型的准确性。

我们将讨论以下主题:

*   定义前馈网络
*   理解反向传播
*   在 TensorFlow 中实现前馈网络
*   分析虹膜数据集
*   为图像分类创建前馈网络



# 定义前馈网络

深度前馈网络，也称前馈神经网络，有时也称为**多层感知器** ( **MLPs** )。前馈网络的目标是逼近*f∫*的函数。例如，对于分类器，*y = f∫(x)*将输入 *x* 映射到标签*y。*前馈网络定义了从输入到标签*y = f(x；θ)* 。它学习导致最佳函数近似的参数 *θ* 的值。

我们在第五章、*中讨论 RNNs，循环神经网络*。前馈网络是通向循环网络的概念性垫脚石，循环网络为许多自然语言应用提供了动力。前馈神经网络之所以称为网络，是因为它们将代表它们的许多不同的函数组合在一起。这些功能由有向无环图组成。

该模型与描述功能如何组合在一起的有向非循环图相关联。比如有三个函数 *f(1)* 、 *f(2)* 、 *f(3)* 连接起来形成 *f(x) =f(3)(f(2)(f(1)(x)))* 。这些链结构是神经网络最常用的结构。在这种情况下， *f(1)* i *s* 称为网络的**第一层**， *f(2)* 称为**第二层**，以此类推。链条的总长度给出了模型的深度。深度学习这个名称就是从这个术语中产生的。前馈网络的最后一层称为**输出层**。

![](Images/e7e58a6b-8f57-4648-9ee4-024e28d62900.png)

显示在输入 x 上激活以形成神经网络的各种功能的图表

这些网络被称为神经网络，因为它们受到神经科学的启发。每个隐藏层都是一个向量。这些隐藏层的维度决定了模型的宽度。



# 理解反向传播

当前馈神经网络用于接受输入 *x* 并产生输出*yˇ*时，信息通过网络元件向前流动。输入 *x* 提供信息，然后传播到每层的隐藏单元，并产生*yˇ*。这被称为**正向传播**。在训练期间，前向传播继续向前，直到它产生标量成本 *J(θ)* 。反向传播算法通常称为反向传播，它允许成本信息通过网络反向传播，以计算梯度。

计算梯度的解析表达式是简单的，但是对这样的表达式进行数值评估在计算上可能是昂贵的。反向传播算法使用简单且廉价的过程来实现这一点。

反向传播仅指计算梯度的方法，而另一种算法，如随机梯度下降，指的是实际的机制。



# 用 TensorFlow 实现前馈网络

通过为隐藏层定义占位符，计算激活值，并使用它们来计算预测，可以使用 TensorFlow 轻松实现前馈网络。让我们以前馈网络的分类为例:

```py
X = tf.placeholder("float", shape=[None, x_size])
y = tf.placeholder("float", shape=[None, y_size])
weights_1 = initialize_weights((x_size, hidden_size), stddev)
weights_2 = initialize_weights((hidden_size, y_size), stddev)
sigmoid = tf.nn.sigmoid(tf.matmul(X, weights_1))
y = tf.matmul(sigmoid, weights_2)
```

一旦定义了预测值张量，我们计算`cost`函数:

```py
cost = tf.reduce_mean(tf.nn.OPERATION_NAME(labels=<actual value>, logits=<predicted value>))
updates_sgd = tf.train.GradientDescentOptimizer(sgd_step).minimize(cost)
```

在这里，`OPERATION_NAME`可以是下列之一:

*   `tf.nn.sigmoid_cross_entropy_with_logits`:计算输入`logits`和`labels`的 sigmoid 交叉熵；

```py
sigmoid_cross_entropy_with_logits(
  _sentinel=None,
  labels=None,
  logits=None,
  name=None
)Formula implemented is max(x, 0) - x * z + log(1 + exp(-abs(x)))
```

`_sentinel`:用于防止位置参数。内部，请勿使用。
`labels`:与 logits 类型和形状相同的张量。
`logits`:一个`float32`或`float64`类型的张量。实现的公式为( *x = logits* ， *z = labels* ) `max(x, 0) - x * z + log(1 + exp(-abs(x)))`。

*   `tf.nn.softmax`:对输入张量执行`softmax`激活。这只是为了确保张量行中的所有概率加起来等于 1。它不能直接用于分类。

```py
softmax = exp(logits) / reduce_sum(exp(logits), dim)
```

`logits`:非空张量。必须是以下类型之一- half、`float32`或`float64`。
`dim`:尺寸`softmax`将被执行。默认为`-1`，表示最后一个尺寸。
`name`:操作的名称(可选)。
`tf.nn.log_softmax`:计算`softmax`函数的对数，帮助归一化欠拟合。这个函数也只是一个归一化函数。

```py
log_softmax(
 logits,
 dim=-1,
 name=None
)
```

`logits`:非空张量。必须是以下类型之一- half、`float32`或`float64`。
`dim`:尺寸`softmax`将被执行。默认为`-1`，表示最后一个尺寸。
`name`:操作的名称(可选)。

*   `tf.nn.softmax_cross_entropy_with_logits`

```py
softmax_cross_entropy_with_logits(
  _sentinel=None,
  labels=None,
  logits=None,
  dim=-1,
  name=None
)
```

`_sentinel`:用于防止位置参数。仅供内部使用。
`labels`:每一行`labels[i]`必须是有效的概率分布。
`logits`:未换算的对数概率。
`dim`:阶级维度。默认为`-1`，是最后一个维度。
`name`:操作的名称(可选)。

前面的代码片段计算了`logits`和`labels.`之间的`softmax`交叉熵。虽然类是互斥的，但是它们的概率不一定是。所需要的只是每行标签都是有效的概率分布。对于排他性标签，使用`sparse_softmax_cross_entropy_with_logits`(其中一次只有一个类别为真)。

*   `tf.nn.sparse_softmax_cross_entropy_with_logits`

```py
sparse_softmax_cross_entropy_with_logits(
  _sentinel=None,
  labels=None,
  logits=None,
  name=None
)
```

`labels`:形状张量*d0*，*D1*，*...*、 *d_(r-1)* (其中 *r* 是标签和结果的排名)和`dtype`、`int32`或`int64`。标签中的每个条目必须是[ *0* 、`num_classes`]中的一个索引。当该操作在 CPU 上运行时，其他值将引发异常，并为 GPU 上的相应损失和梯度行返回 NaN。
`logits`:形状[*d0*，*D1*，*的无标度对数概率...*、 *d_(r-1)* 、`num_classes`、`dtype`、`float32`，或者`float64`。

前面的代码计算了`logits`和`labels`之间的稀疏`softmax`交叉熵。给定标签的概率被认为是唯一的。不允许软类，标签的向量必须为每一行`logits`的真实类提供一个特定的索引。

*   `tf.nn.weighted_cross_entropy_with_logits`

```py
weighted_cross_entropy_with_logits(
  targets,
  logits,
  pos_weight,
  name=None
)
```

`targets`:与逻辑张量类型和形状相同的张量。
`logits`:一个`float32`或`float64`类型的张量。
`pos_weight`:用在正例上的系数。

这与`sigmoid_cross_entropy_with_logits()`类似，除了`pos_weight`允许通过相对于负错误向上或向下加权正错误的成本来权衡召回率和精确度。



# 分析虹膜数据集

让我们看一个使用 Iris 数据集的前馈示例。

可以从[https://github . com/ml-resources/neural network-programming/blob/ed1/ch02/iris/iris . CSV](https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/iris.csv)下载数据集，从[https://github . com/ml-resources/neural network-programming/blob/ed1/ch02/iris/target . CSV](https://github.com/ml-resources/neuralnetwork-programming/blob/ed1/ch02/iris/target.csv)下载目标标签。

在鸢尾数据集中，我们将使用 150 行数据，这些数据由来自三种鸢尾的 50 个样本组成:鸢尾、海滨鸢尾和杂色鸢尾。

三种鸢尾的花瓣几何形状比较:
****海滨鸢尾****杂色鸢尾**。**

**![](Images/ca899bdb-1bb7-4491-b8b4-79db174760e5.png)

在数据集中，每一行都包含每个花样本的数据:萼片长度、萼片宽度、花瓣长度、花瓣宽度和花的种类。花的种类以整数存储，0 表示刚毛鸢尾，1 表示杂色鸢尾，2 表示海滨鸢尾。

首先，我们将创建一个带有三个参数的`run()`函数——隐藏层大小`h_size`，权重的标准偏差`stddev`，以及随机梯度下降的步长`sgd_step`:

```py
def run(h_size, stddev, sgd_step)
```

使用`numpy`中的`genfromtxt`功能加载输入数据。加载的虹膜数据具有 L: 150 和 W: 4 的形状。数据被载入`all_X`变量。从`all_Y`的`target.csv`加载目标标签，形状为 L: 150，W:3:

```py
def load_iris_data():
    from numpy import genfromtxt
    data = genfromtxt('iris.csv', delimiter=',')
    target = genfromtxt('target.csv', delimiter=',').astype(int)
    # Prepend the column of 1s for bias
    L, W  = data.shape
    all_X = np.ones((L, W + 1))
    all_X[:, 1:] = data
    num_labels = len(np.unique(target))
    all_y = np.eye(num_labels)[target]
    return train_test_split(all_X, all_y, test_size=0.33, random_state=RANDOMSEED)
```

加载数据后，我们根据`x_size`、`y_size`和`h_size`初始化权重矩阵，并将标准偏差传递给`run()`方法:

*   `x_size` = 5
*   `y_size` = 3
*   `h_size` = 128(或者为隐藏层中的神经元选择的任何其他数字)

```py
# Size of Layers
x_size = train_x.shape[1] # Input nodes: 4 features and 1 bias
y_size = train_y.shape[1] # Outcomes (3 iris flowers)
# variables
X = tf.placeholder("float", shape=[None, x_size])
y = tf.placeholder("float", shape=[None, y_size])
weights_1 = initialize_weights((x_size, h_size), stddev)
weights_2 = initialize_weights((h_size, y_size), stddev)
```

接下来，我们使用`sigmoid`作为在`forward_propagration()`函数中定义的激活函数进行预测:

```py
def forward_propagation(X, weights_1, weights_2):
    sigmoid = tf.nn.sigmoid(tf.matmul(X, weights_1))
    y = tf.matmul(sigmoid, weights_2)
    return y
```

首先，从输入`X`和`weights_1`计算`sigmoid`输出。然后用它来计算`y`，作为`sigmoid`和`weights_2`的矩阵乘法:

```py
y_pred = forward_propagation(X, weights_1, weights_2)
predict = tf.argmax(y_pred, dimension=1)
```

接下来，我们使用梯度下降定义成本函数和优化。我们来看看正在使用的`GradientDescentOptimizer`。它在`tf.train.GradientDescentOptimizer`类中定义，实现梯度下降算法。

为了构造一个实例，我们使用下面的构造函数并将`sgd_step`作为参数传递:

```py
# constructor for GradientDescentOptimizer
__init__(
  learning_rate,
  use_locking=False,
  name='GradientDescent'
)
```

这里解释了传递的参数:

*   `learning_rate`:张量或浮点值。要使用的学习率。
*   `use_locking`:如果为真，则对更新操作使用锁。
*   `name`:应用渐变时创建的操作的可选名称前缀。默认名称是`"GradientDescent"`。

下面的列表显示了实现`cost`功能的代码:

```py
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_pred))
updates_sgd = tf.train.GradientDescentOptimizer(sgd_step).minimize(cost)
```

接下来，我们将实施以下步骤:

1.  初始化 TensorFlow 会话:

```py
sess = tf.Session()
```

2.  使用`tf.initialize_all_variables()`初始化所有变量；返回对象用于实例化会话。
3.  迭代`steps` (1 到 50)。
4.  对于`train_x`和`train_y`中的每一步，执行`updates_sgd`。
5.  计算`train_accuracy`和`test_accuracy`。

我们将每一步的精度存储在一个列表中，以便绘制图表:

```py
    init = tf.initialize_all_variables()
    steps = 50
    sess.run(init)
    x  = np.arange(steps)
    test_acc = []
    train_acc = []
    print("Step, train accuracy, test accuracy")
    for step in range(steps):
        # Train with each example
        for i in range(len(train_x)):
            sess.run(updates_sgd, feed_dict={X: train_x[i: i + 1], y: train_y[i: i + 1]})

        train_accuracy = np.mean(np.argmax(train_y, axis=1) ==
                                 sess.run(predict, feed_dict={X: train_x, y: train_y}))
        test_accuracy = np.mean(np.argmax(test_y, axis=1) ==
                                sess.run(predict, feed_dict={X: test_x, y: test_y}))

        print("%d, %.2f%%, %.2f%%"
              % (step + 1, 100\. * train_accuracy, 100\. * test_accuracy))

        test_acc.append(100\. * test_accuracy)
        train_acc.append(100\. * train_accuracy)
``` **

# 代码执行

让我们为`128`的`h_size`、`0.1`的标准差和`0.01`的`sgd_step`运行这段代码:

```py
def run(h_size, stddev, sgd_step):
 ...

def main():
run(128,0.1,0.01)

if __name__ == '__main__':
main()
```

前面的代码输出了下图，该图绘制了步骤与测试和训练准确性的关系:

![](Images/6ae79ae5-fae3-4cf2-88ff-a77d94bf6a8e.png)

让我们比较一下 SGD 步骤的变化及其对训练准确性的影响。下面的代码与前面的代码示例非常相似，但是我们将针对多个 SGD 步骤重新运行它，以查看 SGD 步骤如何影响准确性级别。

```py
def run(h_size, stddev, sgd_steps):
    ....
    test_accs = []
    train_accs = []
    time_taken_summary = []
    for sgd_step in sgd_steps:
        start_time = time.time()
        updates_sgd = tf.train.GradientDescentOptimizer(sgd_step).minimize(cost)
        sess = tf.Session()
        init = tf.initialize_all_variables()
        steps = 50
        sess.run(init)
        x  = np.arange(steps)
        test_acc = []
        train_acc = []

        print("Step, train accuracy, test accuracy")

        for step in range(steps):
                # Train with each example
                for i in range(len(train_x)):
                    sess.run(updates_sgd, feed_dict={X: train_x[i: i + 1], 
                              y: train_y[i: i + 1]})

                train_accuracy = np.mean(np.argmax(train_y, axis=1) ==
                                         sess.run(predict, 
                                         feed_dict={X: train_x, y: train_y}))
                test_accuracy = np.mean(np.argmax(test_y, axis=1) ==
                                        sess.run(predict, 
                                        feed_dict={X: test_x, y: test_y}))

                print("%d, %.2f%%, %.2f%%"
                      % (step + 1, 100\. * train_accuracy, 100\. * test_accuracy))
                #x.append(step)
                test_acc.append(100\. * test_accuracy)
                train_acc.append(100\. * train_accuracy)
        end_time = time.time()
        diff = end_time -start_time
        time_taken_summary.append((sgd_step,diff))
        t = [np.array(test_acc)]
        t.append(train_acc)
        train_accs.append(train_acc)
```

前面代码的输出将是一个数组，其中包含每个 SGD 步长值的训练和测试精度。在我们的例子中，我们为 SGD 步长值`[0.01, 0.02, 0.03]`调用函数`sgd_steps`:

```py
def main():
    sgd_steps = [0.01,0.02,0.03]
    run(128,0.1,sgd_steps)

if __name__ == '__main__':
    main()
```

这是显示训练精度如何随`sgd_steps`变化的图。对于 SGD 值`0.03`，步长越大，精度越高。

![](Images/a3106868-376b-44e2-bbe3-c544ef83747d.png)

# 用图像实现前馈网络

现在我们将看看如何使用前馈网络来分类图像。我们将使用`notMNIST`数据。数据集由九个字母 A 到 I 的图像组成。

NotMNIST 数据集类似于 MNIST 数据集，但侧重于字母而不是数字([http://yaroslavvb.blogspot.in/2011/09/notmnist-dataset.html](http://yaroslavvb.blogspot.in/2011/09/notmnist-dataset.html))

我们已经将原始数据集缩减为较小的版本，以便您可以轻松开始培训。下载压缩文件并解压到数据集所在的文件夹[https://1drv.ms/f/s!Av6fk5nQi2j-kniw-8GtP8sdWejs](https://1drv.ms/f/s!Av6fk5nQi2j-kniw-8GtP8sdWejs)。

python 的 pickle 模块实现了一种算法，用于序列化和反序列化 Python 对象结构。**pickle**是 Python 对象层次转换成字节流的过程，unpickling 是逆操作，字节流转换回对象层次。酸洗(和拆线)又称为**序列化**、**整理**、【1】或**展平**。

首先，我们使用`maybe_pickle(..)`方法从下面的文件夹列表中加载`numpy.ndarray`中的图像:

```py
test_folders = ['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D',
'./notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', 
'./notMNIST_small/I', './notMNIST_small/J']
train_folders = ['./notMNIST_large_v2/A', './notMNIST_large_v2/B', './notMNIST_large_v2/C', './notMNIST_large_v2/D',
'./notMNIST_large_v2/E', './notMNIST_large_v2/F', './notMNIST_large_v2/G', './notMNIST_large_v2/H',
'./notMNIST_large_v2/I', './notMNIST_large_v2/J']
maybe_pickle(data_folders, min_num_images_per_class, force=False):
```

`maybe_pickle`使用`load_letter`方法将图像从单个文件夹加载到`ndarray`:

```py
def load_letter(folder, min_num_images):
  image_files = os.listdir(folder)
  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),
                         dtype=np.float32)
  num_images = 0
  for image in image_files:
    image_file = os.path.join(folder, image)
    try:
      image_data = (ndimage.imread(image_file).astype(float) - 
                    pixel_depth / 2) / pixel_depth
      if image_data.shape != (image_size, image_size):
        raise Exception('Unexpected image shape: %s' % str(image_data.shape))
      dataset[num_images, :, :] = image_data
      num_images = num_images + 1
    except IOError as e:
      print('Could not read:', image_file, ':', e, '- it\'s ok, skipping.')

  dataset = dataset[0:num_images, :, :]
  if num_images < min_num_images:
    raise Exception('Fewer images than expected: %d < %d' %
                    (num_images, min_num_images))

  print('Dataset tensor:', dataset.shape)
  print('Mean:', np.mean(dataset))
  print('Standard deviation:', np.std(dataset))
  return dataset
```

对两组文件夹`train_folders`和`test_folders`调用`maybe_pickle`方法:

```py
train_datasets = maybe_pickle(train_folders, 100)
test_datasets = maybe_pickle(test_folders, 50)
```

输出类似于下面的截图。

第一个截图显示了`dataset_names`列表变量值:

![](Images/fe21686e-bc89-45c9-b244-e2bfd65e636f.png)

下面的屏幕截图显示了`notMNIST_small`数据集的`dataset_names`变量的值:

![](Images/d87bb91f-bc8b-4f5c-865c-ce0aa3ab5400.png)

接下来，`merge_datasets`被调用，其中来自每个字符的 pickle 文件被组合成下面的`ndarray`:

*   `valid_dataset`
*   `valid_labels`
*   `train_dataset`
*   `train_labels`

```py
train_size = 1000
valid_size = 500
test_size = 500

valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(
    train_datasets, train_size, valid_size)
  _, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)
```

上述代码的输出如下所示:

```py
Training dataset and labels shape: (1000, 28, 28) (1000,)
Validation dataset and labels shape: (500, 28, 28) (500,)
Testing dataset and labels shape: (500, 28, 28) (500,)
```

最后，通过将这些`ndarray`存储在键-值对中来创建`noMNIST.pickle`文件，其中键是`train_dataset`、`train_labels`、`valid_dataset`、`valid_labels`、`test_dataset`和`test_labels`，值是各自的`ndarray`，如下面的代码所示:

```py

  try:
    f = open(pickle_file, 'wb')
    save = {
      'train_dataset': train_dataset,
      'train_labels': train_labels,
      'valid_dataset': valid_dataset,
      'valid_labels': valid_labels,
      'test_dataset': test_dataset,
      'test_labels': test_labels,
    }
    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)
    f.close()
  except Exception as e:
    print('Unable to save data to', pickle_file, ':', e)
    raise
```

这是生成`notMNIST.pickle`文件的完整代码:

```py
from __future__ import print_function
import numpy as np
import os
from scipy import ndimage
from six.moves import cPickle as pickle

data_root = '.' # Change me to store data elsewhere

num_classes = 10
np.random.seed(133)

test_folders = ['./notMNIST_small/A', './notMNIST_small/B', './notMNIST_small/C', './notMNIST_small/D',
                './notMNIST_small/E', './notMNIST_small/F', './notMNIST_small/G', './notMNIST_small/H', 
                './notMNIST_small/I', './notMNIST_small/J']
train_folders = ['./notMNIST_large_v2/A', './notMNIST_large_v2/B', './notMNIST_large_v2/C', './notMNIST_large_v2/D',
                 './notMNIST_large_v2/E', './notMNIST_large_v2/F', './notMNIST_large_v2/G', './notMNIST_large_v2/H',
                 './notMNIST_large_v2/I', './notMNIST_large_v2/J']

image_size = 28  # Pixel width and height.
pixel_depth = 255.0

def load_letter(folder, min_num_images):
  image_files = os.listdir(folder)
  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),
                         dtype=np.float32)
  num_images = 0
  for image in image_files:
    image_file = os.path.join(folder, image)
    try:
      image_data = (ndimage.imread(image_file).astype(float) - 
                    pixel_depth / 2) / pixel_depth
      if image_data.shape != (image_size, image_size):
        raise Exception('Unexpected image shape: %s' % str(image_data.shape))
      dataset[num_images, :, :] = image_data
      num_images = num_images + 1
    except IOError as e:
      print('Could not read:', image_file, ':', e, '- it\'s ok, skipping.')

  dataset = dataset[0:num_images, :, :]
  if num_images < min_num_images:
    raise Exception('Fewer images than expected: %d < %d' %
                    (num_images, min_num_images))

  print('Dataset tensor:', dataset.shape)
  print('Mean:', np.mean(dataset))
  print('Standard deviation:', np.std(dataset))
  return dataset

def maybe_pickle(data_folders, min_num_images_per_class, force=False):
  dataset_names = []
  for folder in data_folders:
    set_filename = folder + '.pickle'
    dataset_names.append(set_filename)
    if os.path.exists(set_filename) and not force:
      print('%s already present - Skipping pickling.' % set_filename)
    else:
      print('Pickling %s.' % set_filename)
      dataset = load_letter(folder, min_num_images_per_class)
      try:
        with open(set_filename, 'wb') as f:
          #pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)
          print(pickle.HIGHEST_PROTOCOL)
          pickle.dump(dataset, f, 2)
      except Exception as e:
        print('Unable to save data to', set_filename, ':', e)

  return dataset_names

def make_arrays(nb_rows, img_size):
  if nb_rows:
    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)
    labels = np.ndarray(nb_rows, dtype=np.int32)
  else:
    dataset, labels = None, None
```

让我们看看前面创建的 pickle 文件是如何加载数据并运行带有一个隐藏层的网络的。

首先，我们将从`notMNIST.pickle`文件中加载训练、测试和验证数据集(`ndarray`):

```py
with open(pickle_file, 'rb') as f:
 save = pickle.load(f)
 training_dataset = save['train_dataset']
 training_labels = save['train_labels']
 validation_dataset = save['valid_dataset']
 validation_labels = save['valid_labels']
 test_dataset = save['test_dataset']
 test_labels = save['test_labels']

print 'Training set', training_dataset.shape, training_labels.shape
print 'Validation set', validation_dataset.shape, validation_labels.shape
print 'Test set', test_dataset.shape, test_labels.shape
```

您将看到类似于以下清单的输出:

```py
Training set (1000, 28, 28) (1000,)
Validation set (500, 28, 28) (500,)
Test set (500, 28, 28) (500,)
```

接下来，我们将`dataset`转换成一个二维数组，以便用 TensorFlow 更容易处理数据:

```py
def reformat(dataset, labels):
 dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)
 # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]
 labels = (np.arange(num_of_labels) == labels[:, None]).astype(np.float32)
 return dataset, labels
train_dataset, train_labels = reformat(training_dataset, training_labels)
 valid_dataset, valid_labels = reformat(validation_dataset, validation_labels)
 test_dataset, test_labels = reformat(test_dataset, test_labels)

 print 'Training dataset shape', train_dataset.shape, train_labels.shape
 print 'Validation dataset shape', valid_dataset.shape, valid_labels.shape
 print 'Test dataset shape', test_dataset.shape, test_labels.shape
```

您将看到以下输出:

```py
Training dataset shape (1000, 784) (1000, 10)
Validation dataset shape (500, 784) (500, 10)
Test dataset shape (500, 784) (500, 10)
```

接下来，我们定义将返回内容的图，所有变量都将加载到该图中。

这里列出了每个权重和偏差的大小，其中`image_size` = 28，`no_of_neurons` = 1024。

隐藏层中的神经元数量应该是最佳的。神经元太少会导致精度降低，而神经元太多会导致过度拟合。

| **神经网络中的层** | **重量** | **偏置** |
| one | row = 28 x 28 = 784 列数= 1024 | One thousand and twenty-four |
| Two | row = 1024 列= 10 | Ten |

我们将初始化 TensorFlow 图，初始化来自训练、验证和测试数据集和标签的占位符变量。

我们还将定义两个层的权重和偏差:

```py
graph = tf.Graph()
 no_of_neurons = 1024
 with graph.as_default():
   # Placeholder that will be fed
   # at run time with a training minibatch in the session
   tf_train_dataset = tf.placeholder(tf.float32,
     shape=(batch_size, image_size * image_size))
   tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_of_labels))
   tf_valid_dataset = tf.constant(valid_dataset)
   tf_test_dataset = tf.constant(test_dataset)

   # Variables.
   w1 = tf.Variable(tf.truncated_normal([image_size * image_size, no_of_neurons]))
   b1 = tf.Variable(tf.zeros([no_of_neurons]))

   w2 = tf.Variable(
   tf.truncated_normal([no_of_neurons, num_of_labels]))
   b2 = tf.Variable(tf.zeros([num_of_labels]))
```

接下来，我们定义隐藏层张量和计算的 logit:

```py
hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, w1) + b1)
logits = tf.matmul(hidden1, w2) + b2
```

我们网络的损失函数将基于对交叉熵应用的`softmax`函数和`logits`:

```py
loss = tf.reduce_mean(
     tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))
# Training computation.

loss = tf.reduce_mean(
     tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))

   # Optimizer.
   optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)
```

接下来，我们计算`logits`(预测值)；注意，我们使用`softmax`来标准化`logits`:

```py
train_prediction = tf.nn.softmax(logits)
```

计算测试和验证预测。注意这里使用的激活函数`RELU`来计算`w1`和`b1`:

```py
tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1)
 valid_prediction = tf.nn.softmax(
     tf.matmul( tf.nn.relu(tf.matmul(tf_valid_dataset, w1) + b1), 
                w2
              ) + b2)
 test_prediction = tf.nn.softmax(
     tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, w1) + b1), w2) + b2)
```

现在，我们将创建一个 TensorFlow 会话，并通过创建的神经网络传递加载的数据集:

```py
with tf.Session(graph=graph) as session:
  tf.initialize_all_variables().run()
  print("Initialized")
  for step in xrange(num_steps):
   offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
   # Generate a minibatch.
   batch_data = train_dataset[offset:(offset + batch_size), :]
   batch_labels = train_labels[offset:(offset + batch_size), :]
   feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}
   _, l, predictions = session.run(
     [optimizer, loss, train_prediction], feed_dict=feed_dict)
   minibatch_accuracy = accuracy(predictions, batch_labels)
   validation_accuracy = accuracy(
    valid_prediction.eval(), valid_labels)
   if (step % 10 == 0):
     print("Minibatch loss at step", step, ":", l)
     print("Minibatch accuracy: %.1f%%" % accuracy(predictions, batch_labels))
     print("Validation accuracy: %.1f%%" % validation_accuracy)
minibatch_acc.append( minibatch_accuracy)
validation_acc.append( validation_accuracy)
t = [np.array(minibatch_acc)]
t.append(validation_acc)
```

完整代码可以在[https://github . com/rajdeepd/neural network-programming/blob/ed1/ch02/nom NIST/single layer-neural _ network . py](https://github.com/rajdeepd/neuralnetwork-programming/blob/ed1/ch02/nomnist/singlelayer-neural_network.py)找到。

完整的代码可以在前面的 GitHub 链接中找到。请注意，我们将验证和小批量精度附加到我们将要绘制的数组中:

```py
 print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(), test_labels))
 title = "NotMNIST DataSet - Single Hidden Layer - 1024 neurons Activation function: RELU"
 label = ['Minibatch Accuracy', 'Validation Accuracy']
 draw_plot(x, t, title, label)

```

让我们来看看前面代码生成的图:

![](Images/80586153-ad97-4844-b423-d2a50c9046e3.png)

到迭代次数 8 时，小批量准确度达到 100，而验证准确度停止在 60。



# 激活函数对前馈网络精度的影响分析

在前面的例子中，我们使用`RELU`作为激活函数。TensorFlow 支持多种激活功能。让我们看看这些激活函数是如何影响验证准确性的。我们将生成一些随机值:

```py
x_val = np.linspace(start=-10., stop=10., num=1000)
```

然后生成激活输出:

```py
 # ReLU activation
  y_relu = session.run(tf.nn.relu(x_val))
  # ReLU-6 activation
  y_relu6 = session.run(tf.nn.relu6(x_val))
  # Sigmoid activation
  y_sigmoid = session.run(tf.nn.sigmoid(x_val))
  # Hyper Tangent activation
  y_tanh = session.run(tf.nn.tanh(x_val))
  # Softsign activation
  y_softsign = session.run(tf.nn.softsign(x_val))

  # Softplus activation
  y_softplus = session.run(tf.nn.softplus(x_val))
  # Exponential linear activation
  y_elu = session.run(tf.nn.elu(x_val))
```

根据`x_val`绘制激活图:

```py
plt.plot(x_val, y_softplus, 'r--', label='Softplus', linewidth=2)
plt.plot(x_val, y_relu, 'b:', label='RELU', linewidth=2)
plt.plot(x_val, y_relu6, 'g-.', label='RELU6', linewidth=2)
plt.plot(x_val, y_elu, 'k-', label='ELU', linewidth=1)
plt.ylim([-1.5,7])
plt.legend(loc='top left')
plt.title('Activation functions', y=1.05)
plt.show()
plt.plot(x_val, y_sigmoid, 'r--', label='Sigmoid', linewidth=2)
plt.plot(x_val, y_tanh, 'b:', label='tanh', linewidth=2)
plt.plot(x_val, y_softsign, 'g-.', label='Softsign', linewidth=2)
plt.ylim([-1.5,1.5])
plt.legend(loc='top left')
plt.title('Activation functions with Vanishing Gradient', y=1.05)
plt.show()
```

图显示在下面的屏幕截图中:

![](Images/331e4f58-6df4-42eb-9a67-6618c6f7a212.png)

比较**激活函数和消失梯度**的曲线如下:

![](Images/4ed73c91-5b7d-42ac-b254-e7b90f3765c2.png)

现在让我们看看激活函数以及它如何影响 NotMNIST 数据的验证准确性。

我们修改了前面的例子，这样我们可以将激活函数作为参数传递给`main()`:

```py
RELU = 'RELU'
RELU6 = 'RELU6'
CRELU = 'CRELU'
SIGMOID = 'SIGMOID'
ELU = 'ELU'
SOFTPLUS = 'SOFTPLUS'
def activation(name, features):
  if name == RELU:
    return tf.nn.relu(features)
  if name == RELU6:
    return tf.nn.relu6(features)
  if name == SIGMOID:
    return tf.nn.sigmoid(features)
  if name == CRELU:
    return tf.nn.crelu(features)
  if name == ELU:
    return tf.nn.elu(features)
  if name == SOFTPLUS:
    return tf.nn.softplus(features)
```

`run()`函数定义包含了我们之前定义的登录:

```py
batch_size = 128
#activations = [RELU, RELU6, SIGMOID, CRELU, ELU, SOFTPLUS]
activations = [RELU, RELU6, SIGMOID, ELU, SOFTPLUS]
plot_loss = False
def run(name):
 print(name)
 with open(pickle_file, 'rb') as f:
   save = pickle.load(f)
   training_dataset = save['train_dataset']
   training_labels = save['train_labels']
   validation_dataset = save['valid_dataset']
   validation_labels = save['valid_labels']
   test_dataset = save['test_dataset']
   test_labels = save['test_labels'] 
 train_dataset, train_labels = reformat(training_dataset, training_labels)
 valid_dataset, valid_labels = reformat(validation_dataset, 
    validation_labels)
 test_dataset, test_labels = reformat(test_dataset, test_labels)

 graph = tf.Graph()
 no_of_neurons = 1024
 with graph.as_default():

 tf_train_dataset = tf.placeholder(tf.float32,
 shape=(batch_size, image_size * image_size))
 tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, 
   num_of_labels))
 tf_valid_dataset = tf.constant(valid_dataset)
 tf_test_dataset = tf.constant(test_dataset)
 # Define Variables.
 # Training computation...
 # Optimizer ..
 # Predictions for the training, validation, and test data.
 train_prediction = tf.nn.softmax(logits)
 valid_prediction = tf.nn.softmax(
 tf.matmul(activation(name,tf.matmul(tf_valid_dataset, w1) + b1), w2) + b2)
 test_prediction = tf.nn.softmax(
 tf.matmul(activation(name,tf.matmul(tf_test_dataset, w1) + b1), w2) + b2)

 num_steps = 101
 minibatch_acc = []
 validation_acc = []
 loss_array = []
 with tf.Session(graph=graph) as session:
   tf.initialize_all_variables().run()
   print("Initialized")
   for step in xrange(num_steps):
     offset = (step * batch_size) % (train_labels.shape[0] - batch_size)
     # Generate a minibatch.
     batch_data = train_dataset[offset:(offset + batch_size), :]
     batch_labels = train_labels[offset:(offset + batch_size), :]
     feed_dict = {tf_train_dataset: batch_data, tf_train_labels: batch_labels}

     _, l, predictions = session.run(
      [optimizer, loss, train_prediction], feed_dict=feed_dict)
     minibatch_accuracy = accuracy(predictions, batch_labels)
     validation_accuracy = accuracy(
      valid_prediction.eval(), valid_labels)
     if (step % 10 == 0):
       print("Minibatch loss at step", step, ":", l)
       print("Minibatch accuracy: %.1f%%" % accuracy(predictions,
         batch_labels))
       print("Validation accuracy: %.1f%%" % accuracy(
     valid_prediction.eval(), valid_labels))
     minibatch_acc.append(minibatch_accuracy)
     validation_acc.append(validation_accuracy)
     loss_array.append(l)
     print("Test accuracy: %.1f%%" % accuracy(test_prediction.eval(),
       test_labels))
     return validation_acc, loss_array
```

前面列表中的图显示在下面的屏幕截图中:

![](Images/64631b8e-2376-4387-bb86-1321549ddfc8.png)

各种激活功能的验证准确性

从前面的图表中可以看出，RELU 和 RELU6 提供了最高的验证准确性，接近 60%。现在，让我们来看看在各种激活步骤中，培训损失是如何表现的:

![](Images/214cb862-ba33-41ed-adab-3925dd4ac740.png)

作为步骤函数的各种激活的训练损失

对于大多数激活函数，训练损失收敛到零，尽管 RELU 在短期内是最不有效的。



# 摘要

在这一章中，我们建立了我们的第一个神经网络，它仅是前馈的，并将其用于 Iris 数据集和后来的 NotMNIST 数据集的分类。您了解了各种激活函数(如)如何影响预测的验证准确性。

在下一章中，我们将探索一个复杂的神经网络，它对于图像数据集来说更先进、更有效。**