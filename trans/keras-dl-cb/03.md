

# 神经网络的优化

深度学习中的许多应用需要解决优化问题。优化指的是将我们正在处理的任何东西带向它的最终状态。通过使用优化过程解决的问题必须提供数据，提供函数中的模型常数和参数，描述总体目标函数以及一些约束条件。

在本章中，我们将了解 TensorFlow 管道和 TensorFlow 库提供的各种优化模型。涵盖的主题列表如下:

*   优化基础
*   优化器的类型
*   梯度下降
*   选择正确的优化器



# 什么是优化？

寻找最大值或最小值的过程是基于约束的。为您的深度学习模型选择优化算法可能意味着在几分钟、几小时和几天内获得良好结果的差异。

优化是深度学习的核心。大多数学习问题都可以归结为最优化问题。假设我们正在解决一组数据的问题。使用该预处理的数据，我们通过解决优化问题来训练模型，该优化问题关于所选择的损失函数和一些正则化函数来优化模型的权重。

模型的超参数在模型的有效训练中起着重要的作用。因此，有必要使用不同的优化策略和算法来测量模型的超参数的适当和最佳值，这些超参数影响我们的模型的学习过程，并最终影响模型的输出。



# 优化器的类型

首先，我们看看优化算法的高级类别，然后深入到单个优化器。

**一阶优化**算法使用与参数相关的梯度值最小化或最大化损失函数。常用的一阶优化算法是梯度下降法。这里，一阶导数告诉我们函数在特定点是减少还是增加。一阶导数给我们一条与误差面上的一点相切的线。

函数的导数取决于单个变量，而函数的梯度取决于多个变量。

**二阶优化**算法使用二阶导数，也称为**海森**，最小化或最大化给定的损失函数。这里，Hessian 是二阶偏导数的矩阵。计算二阶导数的成本很高。因此，它用得不多。二阶导数告诉我们一阶导数是增加还是减少，给出了函数曲率的概念。二阶导数给了我们一个二次曲面，它与误差曲面的形状相接触。

二阶导数的计算成本很高，但二阶优化方法的优点是它不会忽略或忽略曲面的曲率。此外，步进式性能更好。选择优化方法时需要注意的关键点是，一阶优化方法计算简单，耗时少，在大型数据集上收敛速度相当快。只有当二阶导数已知时，二阶方法才更快，并且这些方法在时间和内存方面计算起来更慢和昂贵。

二阶优化方法有时可以比一阶梯度下降法更好地工作，因为二阶方法在缓慢收敛的路径周围，即在鞍点周围永远不会卡住，而梯度下降有时会卡住并且不收敛。



# 梯度下降

梯度下降是一种最小化函数的算法。一组参数定义了一个函数，梯度下降算法从初始的一组参数值开始，迭代地移向使该函数最小的一组参数值。

这种迭代最小化是使用微积分实现的，在函数梯度的负方向上采取步骤，如下图所示:

![](Images/e3e8215c-9a54-438f-926c-e840fb8dd465.jpeg)

梯度下降是最成功的优化算法。如前所述，它用于在神经网络中进行权重更新，以便我们最小化损失函数。现在让我们来讨论一种称为反向传播的重要神经网络方法，其中我们首先向前传播并计算输入与其相应权重的点积，然后将激活函数应用于乘积之和，将输入转换为输出并向模型添加非线性，这使得模型能够学习几乎任何任意的函数映射。

稍后，我们在神经网络中反向传播，携带误差项并使用梯度下降更新权重值，如下图所示:

![](Images/6408090e-36de-4968-8d52-5868895befb7.png)

# 梯度下降的不同变体

**标准梯度下降**，也称**批量梯度下降**，将计算整个数据集的梯度，但只进行一次更新。因此，对于非常大且不适合内存的数据集来说，控制起来可能非常缓慢和困难。现在让我们看看可以解决这个问题的算法。

**随机梯度下降** ( **SGD** )对每个训练样本进行参数更新，而 mini batch 用每批中的 *n* 个训练样本进行更新。SGD 的问题是，由于频繁的更新和波动，它最终会使收敛到准确的最小值变得复杂，并且会由于有规律的波动而不断超出。小批量梯度下降来拯救这里，它减少了参数更新的方差，导致更好和稳定的收敛。SGD 和小批量可互换使用。

梯度下降的总体问题包括选择适当的学习速率，以便我们避免在小值时缓慢收敛，或在较大值时发散，并将相同的学习速率应用于所有参数更新，其中如果数据稀疏，我们可能不希望将所有参数更新到相同的程度。最后，是处理鞍点。



# 优化梯度下降的算法

我们现在将研究优化梯度下降的各种方法，以便为每个参数计算不同的学习率，计算动量，并防止学习率衰减。

为了解决 SGD 的高方差振荡问题，发现了一种叫做**动量**的方法；这通过沿着适当的方向导航并软化不相关方向的振荡来加速 SGD。基本上，它将过去步骤的更新向量的一部分添加到当前更新向量中。动量值通常设置为 0.9。动量导致更快和更稳定的收敛，减少振荡。

**内斯特罗夫加速梯度**解释说，当我们到达最小值，也就是曲线上的最低点时，动量非常高，它不知道在那个点减速，因为巨大的动量可能会导致它完全错过最小值并继续向上移动。内斯特罗夫建议，我们首先基于之前的动量进行跳远，然后计算梯度，然后进行修正，导致参数更新。现在，这个更新可以防止我们走得太快而错过最小值，并使它对变化更敏感。

**Adagrad** 允许学习速率根据参数进行调整。因此，它对不频繁的参数执行大的更新，对频繁的参数执行小的更新。因此，它非常适合处理稀疏数据。主要的缺陷是它的学习率总是在降低和衰减。学习率下降的问题用 AdaDelta 解决。

**AdaDelta** 解决了 AdaGrad 学习率下降的问题。在 AdaGrad 中，学习率的计算方法是 1 除以平方根之和。在每一个阶段，我们在总和上加另一个平方根，这导致分母不断减小。现在，它使用一个滑动窗口，允许总和减少，而不是对所有先前的平方根求和。

**自适应矩估计** ( **亚当**)计算每个参数的自适应学习率。像 AdaDelta 一样，Adam 不仅存储过去平方梯度的衰减平均值，还存储每个参数的动量变化。Adam 在实践中运行良好，是当今使用最多的优化方法之一。

下面两张图片(图片来源:亚历克·拉德福德)展示了前面描述的优化算法的优化行为。随着时间的推移，我们可以在损失面的轮廓上看到它们的行为。阿达格拉德、RMsprop 和阿达德尔塔几乎很快朝着正确的方向前进，并迅速收敛，而动量和 NAG 则偏离了轨道。NAG 很快就能够纠正它的路线，因为它通过向前看和最小化来提高响应能力。

![](Images/21078e47-ab9b-4dbd-95a9-aaf1f8dfa1eb.png)

第二个图像显示了算法在一个鞍点的行为。 **SGD** 、 **Momentum** 和 **NAG** 发现打破对称很有挑战性，但慢慢地他们设法逃离了鞍点，而 **Adagrad** 、 **Adadelta** 和 **RMsprop** 沿着负斜坡向下，如下图所示:

![](Images/04660bd2-3f81-4c3a-b5bf-94447cf0b8c5.png)

# 选择哪个优化器

在输入数据稀疏的情况下，或者在训练复杂的神经网络时，如果我们希望快速收敛，我们可以使用自适应学习速率方法获得最佳结果。我们也不需要调整学习速度。大多数情况下，Adam 通常是一个不错的选择。



# 示例优化

让我们以线性回归为例，通过最小化从直线到每个数据点的距离的平方，我们试图找到穿过多个数据点的直线的最佳拟合。这就是为什么我们称之为最小二乘回归。本质上，我们把这个问题公式化为一个最优化问题，我们试图最小化一个损失函数。

让我们设置输入数据并查看散点图:

```
# input data
xData = np.arange(100, step=.1)
yData = xData + 20 * np.sin(xData/10)
```

![](Images/c03f289d-0fed-49f7-b77b-519f3e7ed240.png)

定义数据大小和批量大小:

```
# define the data size and batch sizenSamples = 1000
batchSize = 100
```

我们需要调整数据的大小以符合 TensorFlow 输入格式，如下所示:

```
# resize input for tensorflowxData = np.reshape(xData, (nSamples, 1))
 yData = np.reshape(yData, (nSamples, 1)) 
```

以下范围初始化`weights`和`bias`，并描述线性模型和损失函数:

```
with tf.variable_scope("linear-regression-pipeline"):
     W = tf.get_variable("weights", (1,1), initializer=tf.random_normal_initializer())
     b = tf.get_variable("bias", (1, ), initializer=tf.constant_initializer(0.0))

     *#* model
yPred = tf.matmul(X, W) + b
     *#* loss function
loss = tf.reduce_sum((y - yPred)**2/nSamples)
```

然后，我们设置优化器来最小化损失:

```
# set the optimizer
 #optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)
 #optimizer = tf.train.AdamOptimizer(learning_rate=.001).minimize(loss)
 #optimizer = tf.train.AdadeltaOptimizer(learning_rate=.001).minimize(loss)
 #optimizer = tf.train.AdagradOptimizer(learning_rate=.001).minimize(loss)
 #optimizer = tf.train.MomentumOptimizer(learning_rate=.001, momentum=0.9).minimize(loss)
 #optimizer = tf.train.FtrlOptimizer(learning_rate=.001).minimize(loss)
 optimizer = tf.train.RMSPropOptimizer(learning_rate=.001).minimize(loss)  We then select the mini batch and run the optimizers errors = []
 with tf.Session() as sess:
     # init variables
     sess.run(tf.global_variables_initializer())

     for _ in range(1000):
         # select mini batchindices = np.random.choice(nSamples, batchSize)
         xBatch, yBatch = xData[indices], yData[indices]
         # run optimizer_, lossVal = sess.run([optimizer, loss], feed_dict={X: xBatch, y: yBatch})
         errors.append(lossVal)

 plt.plot([np.mean(errors[i-50:i]) for i in range(len(errors))])
 plt.show()
 plt.savefig("errors.png")
```

上述代码的输出如下:

![](Images/e5d26fc6-cbfa-4d72-bbb5-a894c7135c0f.png)

我们还得到一条滑动曲线，如下所示:

![](Images/ba681a85-288f-4c46-90ba-092a214c96f1.png)

# 摘要

在这一章中，我们学习了优化技术和各种类型的基础知识。优化是一个复杂的主题，在很大程度上取决于我们数据的性质和大小。此外，优化取决于权重矩阵。这些优化器中有很多是为图像分类或预测等任务而训练和调整的。然而，对于定制或新的用例，我们需要进行反复试验来确定最佳解决方案。