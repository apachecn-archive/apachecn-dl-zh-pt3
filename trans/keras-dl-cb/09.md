

# 神经网络研究

在这一章中，我们将看看神经网络中一些活跃的研究领域。以下问题从基础研究领域分析到复杂的现实生活问题:

*   神经网络中的过拟合
*   用神经网络进行大规模视频处理
*   基于扭曲神经网络的命名实体识别
*   双向循环神经网络



# 避免神经网络中的过拟合

让我们了解过拟合的构成以及如何在神经网络中避免过拟合。Nitesh Srivastava、Geoffrey Hinton 等人在 2014 年发表了一篇论文[https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)，其中展示了如何避免过度拟合的案例。



# 问题陈述

深度神经网络包含非线性隐藏层，这使它们成为可以学习输入和输出之间非常复杂的关系的表达模型。然而，这些复杂的关系将是采样噪声的结果。这些复杂的关系可能不存在于测试数据中，导致过度拟合。已经开发了许多技术和方法来降低这种噪声。这些措施包括一旦验证集的性能开始变差就停止训练，引入权重惩罚，如 L1 和 L2 正则化，以及软权重共享(诺兰和辛顿，1992)。



# 解决办法

Dropout 是一种解决其他一些技术的性能问题的技术，例如跨多个模型求平均值。它还防止过拟合，并提供了一种有效地指数组合许多不同神经网络结构的方法。术语“辍学”是指放弃神经网络中的单元(隐藏的和可见的)。通过删除一个单元，意味着从网络及其传入和传出连接中删除它，如下图所示。

选择丢弃哪些单元通常是随机的。在简单的情况下，每个单元以独立于其他单元的概率 p 被保留。选择 p 的技术可以是一个验证集或者可以设置为 0.5；对于各种网络和任务，该值接近最佳值。

然而，对于输入单元，最佳保留概率通常更接近于 1 而不是 0.5。

![](Images/a1c5b947-8244-4601-9ba1-2c34e27a6638.png)

漏失神经网络模型；

*   具有两个隐藏层的标准神经网络
*   通过将漏失应用于左边的网络而产生的细化的神经网络；交叉单位已经被放弃

如何在 TensorFlow 中应用 Dropout 的示例

```py
cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)
cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=0.5)
cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)
```

从上面可以看出，0.5 的丢失被应用于`LSTMCell`，其中`output_keep_prob`:单位张量或 0 和 1 之间的浮点数，输出保持概率；如果它是常数且为 1，则不会增加输出压差。



# 结果

让我们看看退出策略如何影响模型的准确性:

![](Images/3933cd47-acda-4760-89ac-019dfaf1d9c8.png)

可以看出，分类误差随着丢失而显著降低。



# 基于神经网络的大规模视频处理

在本文中，[https://static . Google user content . com/media/research . Google . com/en//pubs/archive/42455 . pdf](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42455.pdf)，作者探讨了如何将 CNN 用于大规模视频分类。在这种使用情况下，神经网络不仅可以访问单个静态图像中的外观信息，还可以访问图像的复杂时间演变。在这种情况下扩展和应用 CNN 有几个挑战。

很少有(或没有)视频分类基准能与现有图像数据集的规模和多样性相匹配，因为视频的收集、注释和存储更具挑战性。为了获得训练 CNN 架构所需的足够数量的数据，作者收集了一个新的 Sports-1M 数据集。该数据集包含 100 万个视频(来自 YouTube ),属于 487 个体育分类。Sports-1M 也可用于研究社区，以支持该领域的未来工作。

在这项工作中，作者将每个视频视为一包固定大小的短片。每个片段在时间上包含几个连续的帧，因此网络的连通性可以在时间维度上扩展以学习时空特征。作者描述了三种广泛的连接模式类别(**早期融合**、**晚期融合**和**缓慢融合**)。之后，我们将着眼于解决计算效率的多分辨率架构。

下图解释了各种融合技术:

![](Images/529a83c9-f956-4c19-905b-cacb2c394ba2.png)

各种融合技术来组合按时间分开的帧



# 分辨率提高

作者使用了多分辨率架构，旨在通过在两个空间分辨率上有两个独立的处理流(称为视网膜中央凹和上下文流)来达成妥协(见下图)。178 × 178 帧视频剪辑是网络的输入:

![](Images/a7822995-bf74-4bbe-8995-a213dd0c66f6.png)

多分辨率 CNN

**上下文流**以原始空间分辨率的一半(89 × 89 像素)接收下采样帧。**视网膜中央凹流**以原始分辨率接收中心 89 × 89 区域。这样，总输入维数减半。



# 特征直方图基线

除了相互比较 CNN 架构之外，作者还报告了基于特征的方法的准确性。使用标准的词袋管道提取视频所有帧的几种类型的特征，然后使用 k-means 矢量量化将它们离散化，并使用空间金字塔编码和软量化将词累积到直方图中。



# 定量结果

下表总结了 Sports-1M 数据集测试集的结果(200，000 个视频和 4，000，000 个剪辑)。多个网络的方法持续且显著地优于基于特征的基线。基于特征的方法在视频持续时间内密集地计算视觉单词，并基于完整的视频级特征向量产生预测，而作者的网络仅看到 20 个随机采样的片段:

![](Images/4133e9d9-37f1-44fd-826e-a4e6890ac9d2.png)

Sports-1M 测试集的 200，000 个视频的结果。Hit@k 值表示在前 k 个预测中
包含至少一个基本事实标签的测试样本的比例。

尽管有标签噪声，但网络拓扑采用的方法学习得很好；训练视频会有一些不正确的注释，即使是正确标记的视频也经常包含大量的伪像，如文本、效果、剪辑和徽标，我们试图明确地过滤掉这些伪像。



# 基于扭曲神经网络的命名实体识别

本文中，[http://www . cs . CMU . edu/~李雷/pubs/Lu-bay learn 2015-twinet . pdf，](http://www.cs.cmu.edu/~leili/pubs/lu-baylearn2015-twinet.pdf)作者着眼于自然语言中实体的识别问题。这通常是问答、对话和许多其他 NLP 用例的第一步。对于文本标记序列，命名实体识别器识别属于预定义类别的个人和组织的标记块。



# 命名实体识别示例

IOB 标签系统是 NER 的惯例之一。

**IOB 标签**系统包含以下形式的标签:

*   `B-{CHUNK_TYPE}`:对于 **B** 开始组块中的单词
*   对于单词来说，我和 T2 在一起
*   不在任何区块之内
*   `B-PERSON`:人实体
*   `B-GPE`:地缘政治实体

下面的文字显示了在一个句子中命名实体的例子:

```py
John    has lived in Britain  for    14        years   . 
B-PERSON O    O    O B-GPE     O B-CARDINAL I-CARDINAL O
```

然而，由于两个原因，这是相当具有挑战性的:

*   实体数据库通常不完整(考虑到正在建立的新组织的数量)
*   根据上下文，同一个短语可以指不同的实体(或无实体)



# 定义 Twinet

扭转 RNNs (Twinet)使用两个平行分支。每个分支由一个循环网络层、一个非线性感知器层和一个反向循环网络层组成。分支被*扭曲*:层的顺序在第二个分支中颠倒。所有循环层的输出都在最后收集。

概括地说，**循环神经网络** ( **RNN** )采用一系列输入向量 *x1..T* ，并循环计算隐藏状态(也称为**输出标签**):

*h[t]=σ(U x[t]+W ht1)*

在哪里，

*   *t* 为 1..T
*   *x[t]是外部信号*
*   W 是重量
*   *h[t-1]为时间步长 *t-1* 的隐层权重*
*   *h[t]为时间步长 *t* 计算权重*
*   *U* 是 tan *h* 层，它有助于为时间步长 *t* 创建权重

*σ( )* 是一个非线性激活函数。在作者使用的实验中，我们使用了**整流线性单元** ( **RELU** )。



# 结果

Twinet 与斯坦福大学 NER 分校和伊利诺伊大学 NER 分校进行了比较，结果相当不错。这里 NER 代表 60；(**命名实体识别器**)。

![](Images/e70dfc38-aaec-400c-bd50-ceeb3c45a3c7.png)

从上图可以看出，精确召回率和 F1 分数都更高。



# 双向 RNNs

在这一节中，我们将研究一种新的神经网络拓扑结构，它在 NLP 领域的发展势头越来越大。

舒斯特和帕利瓦尔在 1997 年引入了**双向循环神经网络** ( **BRNN** )。BRNNs 有助于增加网络可用的输入信息量。**多层感知器** ( **MLPs** )和**时间延迟神经网络** ( **TDNNs** )已知对输入数据灵活性有限制。rnn 也要求它们的输入数据是固定的。RNNs 等更高级的拓扑也有限制，因为无法从当前状态预测未来的输入信息。相反，BRNNs 不需要固定它们的输入数据。他们未来的输入信息可以从当前状态获得。BRNNs 的思想是将两个方向相反的隐藏层连接到同一个输出。通过这种结构，输出层能够从过去和未来的状态中获取信息。

当需要输入的上下文时，BRNNs 很有用。例如，在手写识别中，可以通过了解位于当前字母前后的字母来提高性能。

![](Images/807e2fd3-ea0e-4cc3-9554-d7dd4e059fa5.png)

这描绘了一个双向 RNN



# TIMIT 数据集上的 BRNN

在这一节中，我们将研究 BRNN 如何在 TIMIT 数据集上为音素文本分类提供更高精度的结果。

TIMIT 是一个不同性别和不同方言的美国英语使用者的语音和词汇转录的语料库。每个转录的元素都在时间上被描述。TIMIT 旨在进一步丰富声学-语音知识和自动语音识别系统:

![](Images/037a0b0a-62a7-4c7a-a18b-d76acaec0920.png)

从上图中可以看出，无论是训练集还是测试集，BRNN 都比 MLP 给出了更高的精确帧百分比。BLSTM 的精度更高。



# 摘要

在这一章中，我们讨论了一些已经在提高精度和避免过拟合方面进行了研究的领域。我们还研究了一些较新的领域，如视频分类。虽然详细涵盖所有研究领域超出了本书的范围，但我们真诚地建议您除了一级 ACM 和 IEEE 会议之外，还可以浏览谷歌、脸书和百度的研究网站，浏览正在进行的新研究。