<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 生成模型

生成模型是一系列机器学习模型，用于描述如何生成数据。为了训练生成模型，我们首先在任何领域积累大量数据，然后训练模型来创建或生成类似的数据。

换句话说，这些模型可以学习创建与我们给它们的数据相似的数据。其中一种方法是使用**生成对抗网络(GANs)** ，这将作为本章的一部分详细讨论。

本章将涵盖以下主题:

*   生成模型简介
*   甘斯

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 生成模型

有两种机器学习模型:生成模型和判别模型。让我们检查以下分类器列表:决策树、神经网络、随机森林、广义增强模型、逻辑回归、朴素贝叶斯和**支持向量机** ( **SVM** )。其中大多数是分类器和集成模型。奇怪的是朴素贝叶斯。这是列表中唯一的生成模型。其他的都是判别模型的例子。

生成模型和判别模型的根本区别在于潜在的概率推理结构。在这一章中，我们将学习生成模型的关键概念，如类型和 gan，但在此之前，让我们了解一下生成模型和判别模型之间的一些关键差异。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 判别模型与生成模型

判别模型学习目标变量 *Y* 和特征 *X* 之间的条件关系*P(Y | X)】*。这就是最小二乘回归的工作方式，也是最常用的推理模式。这是一种理清变量之间关系的方法。

生成模型旨在对数据集进行完整的概率描述。使用生成模型，目标是开发联合概率分布 *P(X，Y)* ，或者直接或者通过计算 *P(Y | X)* 和 *P(X)* 然后推断分类较新数据所需的条件概率。这种方法比回归要求更可靠的概率思想，但它提供了数据概率结构的完整模型。了解联合分布使您能够生成数据；因此，朴素贝叶斯是一个生成模型。

假设我们有一个监督学习任务，其中*x[I]是数据点的给定特征，*y[I]是对应的标签。预测未来 *x* 的 *y* 的一种方法是从 *(x [i] ，y [i] )* 中学习一个函数 *f()* ，该函数接受 *x* 并输出最可能的 *y* 。这种模型属于辨别模型的范畴，因为你正在学习如何辨别不同类别的 x。像支持向量机和神经网络这样的方法就属于这一类。即使您能够非常准确地对数据进行分类，您也不知道这些数据是如何生成的。**

第二种方法是模拟数据可能是如何生成的，并学习一个函数 *f(x，y)* ，该函数给由 *x* 和 *y* 共同确定的配置打分。然后，您可以通过找到分数 *f(x，y)* 最大的 *y* 来预测新的 *x* 的 *y* 。一个典型的例子是高斯混合模型。

另一个例子是:你可以想象 *x* 是一个图像， *y* 是一种像狗一样的物体，也就是在图像中。写为 *p(y|x)* 的概率告诉我们，给定一个输入图像，与它所知道的所有可能性相比，模型相信有一只狗的程度。试图直接对这个概率图建模的算法被称为**判别模型**。

另一方面，生成模型试图学习一个叫做联合概率 *p(y，x)* 的函数。我们可以将此解读为模型有多相信 *x* 是一个图像，同时还有一只狗 *y* 在其中。这两个概率是相关的，并且可以写成 *p(y，x) = p(x) p(y|x)* ，其中 *p(x)* 是输入 *x* 是图像的可能性。文献中通常将 *p(x)* 概率称为**密度函数**。

将这些模型称为生成性模型的主要原因最终与模型可以同时获得输入和输出的概率这一事实有关。利用这一点，我们可以通过采样动物种类 *y* 和来自 *p(y，x)* 的新图像 *x* 来生成动物的图像。

我们可以主要学习只依赖于输入空间的密度函数 *p(x)* 。

两种模式都有用；然而，相比较而言，生成模型具有优于判别模型的有趣优势，即，即使在没有标签可用时，它们也具有理解和解释输入数据的底层结构的潜力。在现实世界中工作时，这是非常可取的。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 生成模型的类型

判别模型一直处于机器学习领域最近成功的前沿。模型根据给定的输入进行预测，尽管它们不能生成新的样本或数据。

生成建模的最新进展背后的思想是将生成问题转换为预测问题，并使用深度学习算法来学习这样的问题。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 自动编码器

将生成性问题转化为判别性问题的一种方法是从输入空间本身学习映射。例如，我们想要学习一个身份图，对于每个图像 *x* ，它将理想地预测相同的图像，即 *x = f(x)* ，其中 *f* 是预测模型。

这个模型在目前的形式下可能没有用，但我们可以由此创建一个生成模型。

这里，我们创建一个由两个主要组件组成的模型:编码器模型 *q(h|x)* 将输入映射到另一个空间，该空间被称为隐藏空间或由 *h* 表示的潜在空间，以及解码器模型 *q(x|h)* 从隐藏的输入空间学习相反的映射。

这些组件-编码器和解码器-连接在一起，以创建一个端到端的可训练模型。编码器和解码器模型都是不同架构的神经网络，例如，RNNs 和注意力网络，以获得期望的结果。

随着模型的学习，我们可以将解码器从编码器中移除，然后分别使用它们。为了生成新的数据样本，我们可以首先从潜在空间生成样本，然后将其提供给解码器，以从输出空间创建新的样本。

自动编码器在[第 8 章](130cc1c9-51d3-4ba5-95d3-12bb53dee5bd.xhtml)、*自动编码器*中有更详细的介绍。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 开始

正如在自动编码器中看到的那样，我们可以想到一个通用的概念来创建将在关系中一起工作的网络，训练它们将帮助我们了解允许我们生成新数据样本的潜在空间。

另一种类型的生成网络是 GAN，其中我们有一个生成器模型 *q(x|h)* 来将 *h* 的小维潜在空间(通常表示为来自简单分布的噪声样本)映射到 *x* 的输入空间。这与自动编码器中解码器的作用非常相似。

现在的交易是引入一个判别模型*p(y | x)*，它试图将一个输入实例 *x* 关联到一个是/否二元答案 *y* ，关于生成器模型是否生成了输入或者是来自我们正在训练的数据集的真实样本。

让我们使用之前的图像示例。假设生成器模型创建了一个新的图像，并且我们也有来自实际数据集的真实图像。如果发生器模型是正确的，鉴别器模型将不能轻易地区分这两幅图像。如果发电机型号很差，就很容易分辨出哪个是假的，哪个是真的。

当这两个模型耦合时，我们可以通过确保生成器模型随着时间的推移变得越来越好来欺骗鉴别器模型，而鉴别器模型被训练来处理检测欺诈的更困难的问题，从而对它们进行端到端的训练。最后，我们需要一个生成器模型，其输出与我们用于训练的真实数据没有区别。

通过训练的初始部分，鉴别器模型可以很容易地检测来自实际数据集的样本，而不是由生成器模型合成生成的样本，生成器模型刚刚开始学习。随着生成器在数据集建模方面变得越来越好，我们开始看到越来越多的生成样本看起来与数据集相似。下面的例子描述了 GAN 模型随时间学习所生成的图像:

![](Images/ba581ac8-3fa3-42cc-ac63-18e7597d0d06.png)

在接下来的部分中，我们将详细讨论 gan。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 序列模型

如果数据本质上是时间性的，那么我们可以使用称为**序列模型**的专门算法。这些模型可以学习形式为 *p(y|x_n，x_1)* 的概率，其中 *i* 是表示序列中位置的索引， *x_i* 是第 *i ^个* 个输入样本。

举个例子，我们可以把每个单词看成一系列的字符，每个句子看成一系列的单词，每个段落看成一系列的句子。输出 *y* 可以是句子的情感。

使用来自自动编码器的类似技巧，我们可以用系列或序列中的下一个项目替换 *y* ，即 *y =x_n + 1* ，允许模型学习。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 甘斯

由伊恩·古德菲勒领导的蒙特利尔大学的一组研究人员引入了 gan。GAN 模型背后的核心思想是拥有两个相互竞争的神经网络模型。一个网络将噪声作为输入并产生样本(因此被称为**发生器**)。第二个模型(称为**鉴别器**)从生成器和实际训练数据中获取样本，应该能够区分这两个来源。生成性和鉴别性网络正在进行一场持续的游戏，其中生成器模型正在学习生成更现实的样本或示例，鉴别器正在学习越来越好地区分生成的数据和真实的数据。两个网络同时训练，目标是竞争将使生成的样本与真实数据无法区分:

![](Images/746e5e26-9573-47b6-8569-d28866ce5966.png)

用来描述 GANs 的类比是，发电机就像一个试图生产一些伪造材料的伪造者，鉴别器模型是试图检测伪造物品的警察。这似乎有点类似于强化学习，其中生成器从鉴别器获得奖励，允许它知道生成的数据是否准确。与 GANs 的主要区别在于，我们可以将梯度信息从鉴频器网络反向传播回发生器网络，这样发生器就知道如何调整其参数，以产生可以欺骗鉴频器的输出数据。

迄今为止，GANs 主要应用于自然图像的建模。它们在图像生成任务中以及在生成比使用基于最大似然训练目标的其他生成方法训练的图像更清晰的图像中提供最佳结果。

以下是 GANs 生成的一些图像示例:

![](Images/2a4a2461-6921-4252-ad7b-9bbc56cf8ce4.png)<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 甘举了个例子

为了更深入地理解 GAN 如何工作，我们将使用 GAN 来解决 TensorFlow 中的一个简单问题，即学习近似一维高斯分布。

首先，我们将创建实际的数据分布，一个简单的高斯分布，平均值为 4，标准偏差为 0.5。它有一个 sample 函数，从分布中返回给定数量的样本(按值排序)。我们了解到的数据分布如下图所示:

![](Images/066437dd-ffad-443b-8b62-c1429b9d428b.png)

发电机输入噪声分布也用用于实际数据的类似样本函数来定义。

发生器和鉴别器网络都非常简单。生成器是一个通过非线性(`softplus`函数)的线性变换，后面是另一个线性变换。

我们保持鉴别器比发生器强；否则，它将没有足够的学习能力来准确区分生成的样本和真实的样本。因此，我们使它成为一个具有更高维度的更深层次的神经网络。我们在所有层中使用非线性，除了最后一层，它是一个 sigmoid(它的输出可以描述为一个概率)。

我们将这些网络作为张量流图的一部分连接起来，并为每个网络定义损耗函数，这样发电机网络将简单地愚弄鉴别器网络。来自 TensorFlow 的具有指数学习率衰减的梯度下降优化器被用作优化器。

为了训练模型，我们从数据分布和噪声分布中抽取样本，并在优化鉴别器和发生器的参数之间交替。

我们将看到，在训练方法开始时，生成器生成了与真实数据非常不同的分布。在收敛到集中于输入分布平均值的较窄分布之前，网络慢慢地学习非常接近它。训练完网络后，这两个分布如下所示:

![](Images/fdf1eb31-e0b5-4821-ab33-1801f9c0fd20.png)

发电机网络下降到一个参数设置的问题是 GANs 的主要故障之一，在该设置下，发电机网络产生非常窄的点分布或模式。解决方案是允许鉴别器一次查看多个样本，这种技术我们称之为小批量鉴别。小批量鉴别是一种方法，其中鉴别器可以扫视整批样本，以确定它们是来自发生器网络还是来自实际数据。

该方法的总结如下:

*   取鉴频器网络的任何中间层的输出
*   将该输出乘以一个 3D 张量，生成一个大小为*numofkernel ***kernelDim*的矩阵
*   计算一个批次中所有样本的矩阵中各行之间的 L1 距离，然后应用负指数
*   样品的小批特征或性质是这些指数距离的总和
*   将实际输入连接到微型批次图层，即先前或以前的鉴别器图层的输出与创建的微型批次要素，然后将其作为输入传递到鉴别器网络的下一个图层

迷你批次鉴别使批次大小与超参数一样重要:

```
import argparse
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from matplotlib import animation
import seaborn as sns
from tensorflow.python.training.gradient_descent import GradientDescentOptimizer

sns.set(color_codes=True)

seed = 42
np.random.seed(seed)
tf.set_random_seed(seed)

# gaussian data distribution
class DataDist(object):
    def __init__(self):
        self.mue = 4
        self.sigma = 0.5

    def sample(self, N):
        samples = np.random.normal(self.mue, self.sigma, N)
        samples.sort()
        return samples

# data distribution with noise
class GeneratorDist(object):
    def __init__(self, rnge):
        self.rnge = rnge

    def sample(self, N):
        return np.linspace(-self.rnge, self.rnge, N) + \
               np.random.random(N) * 0.01

# linear method
def linearUnit(input, output_dim, scope=None, stddev=1.0):
    with tf.variable_scope(scope or 'linearUnit'):
        weight = tf.get_variable(
            'weight',
            [input.get_shape()[1], output_dim],
            initializer=tf.random_normal_initializer(stddev=stddev)
        )
        bias = tf.get_variable(
            'bias',
            [output_dim],
            initializer=tf.constant_initializer(0.0)
        )
        return tf.matmul(input, weight) + bias

# generator network
def generatorNetwork(input, hidden_size):
    hidd0 = tf.nn.softplus(linearUnit(input, hidden_size, 'g0'))
    hidd1 = linearUnit(hidd0, 1, 'g1')
    return hidd1

# discriminator network
def discriminatorNetwork(input, h_dim, minibatch_layer=True):
    hidd0 = tf.nn.relu(linearUnit(input, h_dim * 2, 'd0'))
    hidd1 = tf.nn.relu(linearUnit(hidd0, h_dim * 2, 'd1'))

    if minibatch_layer:
        hidd2 = miniBatch(hidd1)
    else:
        hidd2 = tf.nn.relu(linearUnit(hidd1, h_dim * 2, scope='d2'))

    hidd3 = tf.sigmoid(linearUnit(hidd2, 1, scope='d3'))
    return hidd3

# minibatch
def miniBatch(input, numKernels=5, kernelDim=3):
    x = linearUnit(input, numKernels * kernelDim, scope='minibatch', stddev=0.02)
    act = tf.reshape(x, (-1, numKernels, kernelDim))
    differences = tf.expand_dims(act, 3) - \
            tf.expand_dims(tf.transpose(act, [1, 2, 0]), 0)
    absDiffs = tf.reduce_sum(tf.abs(differences), 2)
    minibatchFeatures = tf.reduce_sum(tf.exp(-absDiffs), 2)
    return tf.concat([input, minibatchFeatures], 1)

# optimizer
def optimizer(loss, var_list):
    learning_rate = 0.001
    step = tf.Variable(0, trainable=False)
    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(
        loss,
        global_step=step,
        var_list=var_list
    )
    return optimizer

# log
def log(x):
    return tf.log(tf.maximum(x, 1e-5))

class GAN(object):
    def __init__(self, params):
        with tf.variable_scope('Generator'):
            self.zee = tf.placeholder(tf.float32, shape=(params.batchSize, 1))
            self.Gee = generatorNetwork(self.zee, params.hidden_size)

        self.xVal = tf.placeholder(tf.float32, shape=(params.batchSize, 1))
        with tf.variable_scope('Discriminator'):
            self.Dis1 = discriminatorNetwork(
                self.xVal,
                params.hidden_size,
                params.minibatch
            )
        with tf.variable_scope('D', reuse=True):
            self.Dis2 = discriminatorNetwork(
                self.Gee,
                params.hidden_size,
                params.minibatch
            )

        self.lossD = tf.reduce_mean(-log(self.Dis1) - log(1 - self.Dis2))
        self.lossG = tf.reduce_mean(-log(self.Dis2))

        vars = tf.trainable_variables()
        self.dParams = [v for v in vars if v.name.startswith('D/')]
        self.gParams = [v for v in vars if v.name.startswith('G/')]

        self.optD = optimizer(self.lossD, self.dParams)
        self.optG = optimizer(self.lossG, self.gParams)

'''
Train GAN model
'''
def trainGan(model, data, gen, params):
    animFrames = []

    with tf.Session() as session:
        tf.local_variables_initializer().run()
        tf.global_variables_initializer().run()

        for step in range(params.numSteps + 1):
            x = data.sample(params.batchSize)
            z = gen.sample(params.batchSize)
            lossD, _, = session.run([model.lossD, model.optD], {
                model.x: np.reshape(x, (params.batchSize, 1)),
                model.z: np.reshape(z, (params.batchSize, 1))
            })

            z = gen.sample(params.batchSize)
            lossG, _ = session.run([model.lossG, model.optG], {
                model.z: np.reshape(z, (params.batchSize, 1))
            })

            if step % params.log_every == 0:
                print('{}: {:.4f}\t{:.4f}'.format(step, lossD, lossG))

            if params.animPath and (step % params.animEvery == 0):
                animFrames.append(
                    getSamples(model, session, data, gen.range, params.batchSize)
                )

        if params.animPath:
            saveAnimation(animFrames, params.animPath, gen.range)
        else:
            samps = getSamples(model, session, data, gen.range, params.batchSize)
            plotDistributions(samps, gen.range)

def getSamples(
        model,
        session,
        data,
        sampleRange,
        batchSize,
        numPoints=10000,
        numBins=100
):
    xs = np.linspace(-sampleRange, sampleRange, numPoints)
    binss = np.linspace(-sampleRange, sampleRange, numBins)

    # decision boundary
    db = np.zeros((numPoints, 1))
    for i in range(numPoints // batchSize):
        db[batchSize * i:batchSize * (i + 1)] = session.run(
            model.D1,
            {
                model.x: np.reshape(
                    xs[batchSize * i:batchSize * (i + 1)],
                    (batchSize, 1)
                )
            }
        )

    # data distribution
    d = data.sample(numPoints)
    pds, _ = np.histogram(d, bins=binss, density=True)

    zs = np.linspace(-sampleRange, sampleRange, numPoints)
    g = np.zeros((numPoints, 1))
    for i in range(numPoints // batchSize):
        g[batchSize * i:batchSize * (i + 1)] = session.run(
            model.G,
            {
                model.z: np.reshape(
                    zs[batchSize * i:batchSize * (i + 1)],
                    (batchSize, 1)
                )
            }
        )
    pgs, _ = np.histogram(g, bins=binss, density=True)

    return db, pds, pgs

def plotDistributions(samps, sampleRange):
    db, pd, pg = samps
    dbX = np.linspace(-sampleRange, sampleRange, len(db))
    pX = np.linspace(-sampleRange, sampleRange, len(pd))
    f, ax = plt.subplots(1)
    ax.plot(dbX, db, label='Decision Boundary')
    ax.set_ylim(0, 1)
    plt.plot(pX, pd, label='Real Data')
    plt.plot(pX, pg, label='Generated Data')
    plt.title('1D Generative Adversarial Network')
    plt.xlabel('Data Values')
    plt.ylabel('Probability Density')
    plt.legend()
    plt.show()

def saveAnimation(animFrames, animPath, sampleRange):
    f, ax = plt.subplots(figsize=(6, 4))
    f.suptitle('1D GAN', fontsize=15)
    plt.xlabel('dataValues')
    plt.ylabel('probabilityDensity')
    ax.set_xlim(-6, 6)
    ax.set_ylim(0, 1.4)
    lineDb, = ax.plot([], [], label='decision boundary')
    linePd, = ax.plot([], [], label='real data')
    linePg, = ax.plot([], [], label='generated data')
    frameNumber = ax.text(
        0.02,
        0.95,
        '',
        horizontalalignment='left',
        verticalalignment='top',
        transform=ax.transAxes
    )
    ax.legend()

    db, pd, _ = animFrames[0]
    dbX = np.linspace(-sampleRange, sampleRange, len(db))
    pX = np.linspace(-sampleRange, sampleRange, len(pd))

    def init():
        lineDb.set_data([], [])
        linePd.set_data([], [])
        linePg.set_data([], [])
        frameNumber.set_text('')
        return (lineDb, linePd, linePg, frameNumber)

    def animate(i):
        frameNumber.set_text(
            'Frame: {}/{}'.format(i, len(animFrames))
        )
        db, pd, pg = animFrames[i]
        lineDb.set_data(dbX, db)
        linePd.set_data(pX, pd)
        linePg.set_data(pX, pg)
        return (lineDb, linePd, linePg, frameNumber)

    anim = animation.FuncAnimation(
        f,
        animate,
        init_func=init,
        frames=len(animFrames),
        blit=True
    )
    anim.save(animPath, fps=30, extra_args=['-vcodec', 'libx264'])

# start gan modeling
def gan(args):
    model = GAN(args)
    trainGan(model, DataDist(), GeneratorDist(range=8), args)

# input arguments
def parseArguments():
    argParser = argparse.ArgumentParser()
    argParser.add_argument('--num-steps', type=int, default=5000,
                           help='the number of training steps to take')
    argParser.add_argument('--hidden-size', type=int, default=4,
                           help='MLP hidden size')
    argParser.add_argument('--batch-size', type=int, default=8,
                           help='the batch size')
    argParser.add_argument('--minibatch', action='store_true',
                           help='use minibatch discrimination')
    argParser.add_argument('--log-every', type=int, default=10,
                           help='print loss after this many steps')
    argParser.add_argument('--anim-path', type=str, default=None,
                           help='path to the output animation file')
    argParser.add_argument('--anim-every', type=int, default=1,
                           help='save every Nth frame for animation')
    return argParser.parse_args()

# start the gan app
if __name__ == '__main__':
    gan(parseArguments())
```

输出列表:

```
0: 6.6300 0.1449
 10: 6.5390 0.1655
 20: 6.4552 0.1866
 30: 6.3789 0.2106
 40: 6.3190 0.2372
 50: 6.2814 0.2645
 60: 6.2614 0.2884
 70: 6.2556 0.3036
 80: 6.2814 0.3104
 90: 6.2796 0.3113
 100: 6.3008 0.3106
 110: 6.2923 0.3112
 120: 6.2792 0.3153
 130: 6.3299 0.3196
 140: 6.3512 0.3205
 150: 6.2999 0.3197
 160: 6.3513 0.3236
 170: 6.3521 0.3291
 180: 6.3377 0.3292
```

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# gan 的类型

以下部分显示了不同类型的 GAN，例如，普通 GAN、有条件 GAN 等。请参见[https://arxiv.org](https://arxiv.org)了解更多关于文件的信息。以下关于每个 GAN 网络的描述摘自关于 https://arxiv.org 的[的相关论文。](https://arxiv.org)

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 香草甘

Vanilla GANs 有两个网络，称为发电机网络和鉴别器网络。这两个网络同时接受训练，并在一场极小极大游戏中相互竞争或较量。生成器网络被训练或准备，使得它可以通过根据输入创建真实图像来欺骗鉴别器网络，并且鉴别器被训练成不被生成器网络欺骗。

关于香草甘的进一步阅读请参考[https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661)。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 条件 GAN

GANs 是作为一种新颖的生成训练模型的方式开始的。这些是利用额外标签数据的 GAN 网络。它可以产生高质量的图像，并且能够在一定程度上控制生成的图像的外观。该模型可用于学习多模态模型。

关于有条件 GAN 的更多信息，请参考[https://arxiv.org/abs/1411.1784.](https://arxiv.org/abs/1411.1784.)

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 信息甘

能够以无人监管的方式编码或学习重要图像特征或解开表示的 GANs。一个例子是对手指的旋转进行编码。Info GANs 还最大化了潜在变量的一个小子集和观测值之间的交互信息。

欲了解更多信息，请参考[https://arxiv.org/abs/1606.03657](https://arxiv.org/abs/1606.03657)

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 瓦瑟斯坦·甘

WGAN 是常规 GAN 训练的一个选项。WGANs 具有与图像质量相关的损失函数。此外，训练的稳定性得到提高，不再依赖于架构，并提供了对调试有用的重要学习曲线。

欲知更多关于瓦瑟斯坦·甘的内容，请参考[https://arxiv.org/abs/1701.07875](https://arxiv.org/abs/1701.07875)

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 耦合氮化镓

耦合 GANs 用于在两个独立的域中生成多组相似的图像。它由一组 gan 组成，每个 gan 负责在单个域中生成图像。耦合的 GANs 从两个域中的图像中学习联合分布，这两个域是从唯一域的边缘分布中单独绘制的。

有关耦合 GAN 的更多信息，请参考[https://arxiv.org/abs/1606.07536](https://arxiv.org/abs/1606.07536)

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles.css" rel="stylesheet" type="text/css">

# 摘要

生成模型是一个快速发展的学习和研究领域。随着我们继续推进这些模型，增加训练和数据集，我们可以期望最终生成描绘完全可信图像的数据示例。这可以用于多种应用，例如图像去噪、绘画、结构化预测和强化学习中的探索。

这一努力的更深层次的承诺是，在构建生成模型的过程中，我们将通过对世界及其组成元素的理解来丰富计算机。