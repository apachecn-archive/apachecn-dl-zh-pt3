

# 深度学习的 Keras 和 spaCy

在前一章中，我们向您介绍了文本的深度学习技术，为了体验使用神经网络的感觉，我们尝试使用 RNN 生成文本。在这一章中，我们将进一步了解文本的深度学习，特别是如何建立一个可以执行分类的 Keras 模型，以及如何将深度学习纳入 spaCy 管道。

以下是一些有用的链接:

1.  Keras 序列模型[ [1](https://keras.io/getting-started/sequential-model-guide/)
2.  Keras CNN LSTM [ [2](https://github.com/keras-team/keras/blob/master/examples/imdb_cnn_lstm.py) ]
3.  预训练单词嵌入[ [3](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)



# 克拉斯和斯帕西

在前一章中，我们已经讨论了各种深度学习框架——在这一章中，我们将更详细地讨论其中一个，特别是 Keras，同时还将探索我们如何通过 spaCy 使用深度学习。

在我们尝试文本生成的过程中，我们已经使用了 Keras，但是并没有解释使用这个库背后的动机，或者甚至没有解释我们是如何或者为什么以这样的方式构建我们的模型的。我们将试图解开这个谜团，并建立一个神经网络模型来帮助我们进行文本分类。

在我们对 Python 中可用的各种深度学习框架的简要回顾中，我们将 Keras 描述为允许我们轻松构建神经网络的高级库。

![](Images/2318f500-8db5-46d3-bb70-dbd29f880477.png)

图 14.1 arXiv 对 Keras 的提及。arXiv 是一个研究人员在被期刊接受之前上传研究论文的网站。这里，*x*-轴是不同的 Python 深度学习库，*y*-轴是 arXiv 上的论文引用该库的次数

Keras 恰好是所有这些以及更多的东西，并且为生成非常复杂的学习系统提供了最干净的 API 之一。只有 Tensorflow 在 arXiv 上被引用较多，甚至在工业界，Keras 也被广泛使用。它与 Tensorflow 一起打包为`tf.keras`，这意味着它由谷歌支持，其 CNTK [ [4](https://github.com/Microsoft/CNTK) 后端由微软支持。CNTK 是另一个可用于构建神经网络的后端，但我们不会使用它或详细介绍它，因为它不像 Tensorflow 或 Theano 那样受支持或广泛使用。能够使用多个后端(Theano、Tensorflow 和 CNTK)使它成为一个非常灵活的适应框架。有广泛的用户基础和活跃的社区，这意味着在 StackOverflow 或 GitHub 上解决你的问题的可能性很大，并且也很容易将你的模型投入生产。比如 iOS 开发由苹果的 **CoreML** [ [5](https://developer.apple.com/documentation/coreml) ]支持，苹果在其中提供对 Keras 的支持。

但是作为文本分析从业者，我们对 Keras 感兴趣的原因是使用 Keras 执行文本分析任务是多么容易。我们在整本书中都提到了预处理对于文本分析的重要性——Keras 有一个关于预处理的类，甚至还有一个子模块[ [6](https://keras.io/preprocessing/text/) ]更侧重于文本预处理。当出于深度学习的目的清理文本时，上下文可能会略有不同。例如，对于文本生成，我们不删除停用词或词干，因为我们希望模型预测什么看起来像真正的*文本。在这一章中，我们将把重点放在分类上，在这里我们将或多或少地遵循我们以前所做的相同的预处理。*

当我们之前提到神经网络时，我们使用了术语模型，以及这些模型是如何由输入、一个或几个层以及输出组成的。这些层由以不同方式互连的神经元(或节点)组成。不同种类的神经网络有不同的连接方式-例如，**卷积神经网络**被认为是一个密集的网络，在层和节点之间有多个连接。一个**递归神经网络**，我们在上一章中用来生成文本，它是通过添加先前的节点和层来构建上下文的。神经网络的性能很大程度上取决于其架构。对我们来说幸运的是，我们打算用于文本和文档分类的架构得到了很好的研究——我们不必太担心*的超参数和*我们打算如何*建立我们的神经网络，因为大部分已经得到了彻底的研究(尽管如此，关于神经网络还有很多需要了解的！).*

Keras 文档非常全面，值得一看。当我们使用 Keras 时，我们将描述它的一些更重要的部分，但在我们开始我们的例子之前，强烈建议用户浏览以下涉及 Keras 的链接:

1.  **关于 Keras 模型** [ [7](https://keras.io/models/about-keras-models/) ]:这解释了 Keras 中的各种神经网络模型
2.  **关于 Keras 层** [ [8](https://keras.io/layers/about-keras-layers/) ]:这讨论了可以添加到 Keras 神经网络的各种层
3.  **核心层** ( [致密](https://keras.io/layers/core/))【9】:这是 Keras 致密层的记录
4.  Keras 数据集 [ [10](https://keras.io/datasets/) ]:这是解释和记录各种 Keras 数据集的列表
5.  这是关于喀拉斯 LSTM 模块的更多细节
6.  卷积层:这里有更多关于 Keras 卷积层的细节

在本章中，我们将学习如何对序列和文档进行分类，因此阅读前面链接中关于密集层、LSTMs 和 rnn 的内容将为您提供一些背景知识，让您轻松了解接下来的示例。

我们将为我们的分类器使用序列模型——这仅仅意味着它是一个更简单的神经网络，各层按顺序堆叠。也值得看一下顺序模型[ [1](https://keras.io/getting-started/sequential-model-guide/) ]的 Keras 文档。

但在我们进入具体细节和代码之前，让我们简单讨论一下 spaCy 以及它与深度学习的联系。虽然我们之前训练定制空间模型时没有进入细节，但它完全基于深度学习技术。我们在前面提到过空间的后置标记器、NER 标记器和解析器。我们必须在这里信任 spaCy 的平滑训练 APIs 我们被允许将进行训练的模型视为黑盒，并只关注训练数据或我们打算训练的新信息。但话虽如此，我们仍然可以*利用改变各种超参数的能力，比如正则化或辍学率，来玩这个模型。快速提醒:辍学率是控制过度拟合的超参数，并确保我们的神经网络不仅仅在训练数据集上表现良好。*

从纯技术的角度来看，使用 Doc2Vec 对你的文档进行矢量化，然后使用标准的统计分类器(比如朴素贝叶斯分类器)进行分类，也可以认为是一种采用神经网络/深度学习的机器学习系统；然而，在本章中，我们将尝试建立一个分类器系统，其中最终的分类任务是由神经网络执行的。

spaCy 允许我们使用其内置的`TextCategorizer`组件，我们以类似于其其他组件(如 POS 或 ner 标签)的方式训练它。它还可以无缝集成其他单词嵌入，如 Gensim 的 Word2Vec 或 GloVe，如果我们愿意，还可以插入 Keras 模型。串联使用 spaCy 和 Keras 可以让我们利用一个非常强大的分类机器——现在我们已经有了理论和想法，让我们直接进入代码！



# 用 Keras 分类

在我们的实验中，我们将使用 IMDB 情感分类任务。这是一个非常小的数据集——我们使用它是为了方便加载和使用，因为它很容易通过 Keras 获得。在这里理解这一点非常重要，对于我们正在使用的规模的数据集，使用**深度神经网络**进行分类是*而不是*最好的想法——事实上，我们甚至可以通过一个简单的单词包后跟一个**支持向量机** ( **SVM** )进行分类来获得更好的结果。以下示例的目的是让用户理解*如何使用 Keras 构建神经网络，以及如何使用它进行预测。神经网络的微调和对其超参数的研究完全是另一回事，不是本章的重点。使用文本数据和神经网络时要记住的另一件事是，在几乎所有情况下，数据越多越好，神经网络更适合处理有更多数据的问题。*

我们将遵循`Keras/examples`文件夹[ [13](https://github.com/keras-team/keras/tree/master/examples) ]中的代码和示例来帮助我们——这给了读者验证他们的代码或整体运行示例的机会。我们从序列模型开始，你可以在这里找到【17】:

让我们首先设置我们的导入:

```
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Embedding
from keras.layers import LSTM
from keras.datasets import imdb
```

以下是一些注意事项:

1.  在这个例子中，我们没有使用文本预处理模块，因为我们将使用 Keras (IMDB)中包含的数据集。
2.  我们将使用 LSTM 来完成分类任务，它是递归神经网络的一种变体。我们以前在文本生成任务中遇到过这种情况。
3.  我们从模型的模块中导入了`Sequential`。我们这里的 LSTM 仅仅是一个层，模型的类型是一个简单的序列模型。`Dense`是一层规则连接的神经元。

```
max_features = 20000
maxlen = 80  # cut texts after this number of words (among top max_features most common words)
batch_size = 32

print('Loading data...')
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')

print('Pad sequences (samples x time)')
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)
```

这里的`max_features`变量指的是我们希望从数据集中使用的顶部单词——我们在这里将其限制为`20000`单词。这类似于去掉最少使用的单词，这是我们之前在文本预处理中讨论过的一种技术。当我们从数据集创建序列时，使用了`maxlen`变量——我们需要固定序列的长度，因为神经网络接受固定长度的输入。`batch_size`变量稍后用于指定训练期间的批次数量，这是一个经验测量值。代码中的打印语句是为了让用户手动检查数据的大小(我们可以看到它并不是很大！).

然后，我们将数据分为训练和测试两部分，并打印出相应的大小。

```
print('Build model...')
model = Sequential()
model.add(Embedding(max_features, 128))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
```

这就是了——在 4 行代码中，我们建立了一个神经网络！第一行建立了我们的顺序模型，之后我们只是继续堆叠我们需要的东西。在堆叠的第一层，我们已经放置了单词嵌入——这意味着从 20000 个特征，我们已经直接下降到 128 个。我们很快就会发现，我们也可以使用自己的单词嵌入，比如 Word2Vec 或 GloVe vectors。我们的下一层是 LSTM——我们可以看到数字 128，这是神经网络将要处理的维度总数。

这里，dropout 参数是为了防止过度拟合——这里我们使用默认值，恰好是 0.2。由于 LSTM 是一个递归神经网络，我们也有一个递归的辍学值。我们的最后一层是标准的`Dense`层，只有一个输入(恰好是 LSTM 的输出)。我们对这一层使用 sigmoid 激活。这里的一个*激活*指的是用于那个特定层的激活函数[[14](https://en.wikipedia.org/wiki/Activation_function)——你可以在 Keras 的文档页面[ [15](https://keras.io/activations/) ]上读到更多关于激活层的内容。我们可以将它们理解为神经网络决定接受输入的方式，以及它提供何种输出。就这样——我们构建了第一个神经网络！

当然，它还没有准备好让我们开始任何类型的预测或分类——我们仍然必须在任何预测之前编译和拟合它。

```
# try using different optimizers and different optimizer configs
model.compile(loss='binary_crossentropy', 
              optimizer='adam', 
              metrics=['accuracy'])
```

```
print('Train...')
model.fit(x_train, y_train, 
          batch_size=batch_size, 
          epochs=15, 
          validation_data=(x_test, y_test))
```

注意——这是你将要进行的一些高强度训练，如果你在 CPU 上运行，可能需要 30 分钟到一个小时。

瞧啊。我们已经完成了模型的训练和拟合。这意味着我们现在可以预测。你会注意到，当我们运行`compile`方法时，我们使用`binary_crossentropy`作为我们的损失，使用`adam`作为我们的优化器。所有的神经网络都需要一个损失函数和优化器来学习。我们可以将这里的损失理解为它学习神经网络的预测与真实情况有多远的方式，以及优化器调整其权重以获得更好结果的方式。

让我们首先测试我们的模型工作得有多好——Keras 允许我们非常容易地用`evaluate`函数验证这一点。让我们快速看一下我们的模型工作得有多好。

```
score, acc = model.evaluate(x_test, y_test, 
                            batch_size=batch_size)

print('Test score:', score)
print('Test accuracy:', acc)
```

对于一个神经网络来说，我们只用了 4 行代码，还不错吧？我们很快就会看到我们还可以用 Keras 做什么——现在让我们建立一个卷积神经网络，它有一点复杂。我们建立的神经网络是在用于文本分类的 IMDB 数据集上训练的，因此它现在具备了基于情感对文档进行分类的能力。这也是一个序列神经网络-我们现在将建立一个卷积神经网络。

对于卷积神经网络，我们将需要更多的参数来微调我们的网络。我们遵循 Keras GitHub 页面上这个例子[18]中的代码。

```
from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.layers import Embedding
from keras.layers import LSTM
from keras.layers import Conv1D, MaxPooling1D
from keras.datasets import imdb
```

我们在这里有一些新的导入，我们应该密切关注——特别是针对`Dropout`、`Activation`以及我们的卷积网络、`Conv1D`和`MaxPooling1D`的独立模块。

```
# Convolution
kernel_size = 5
filters = 64
pool_size = 4

# Embedding
max_features = 20000
maxlen = 100
embedding_size = 128

# LSTM
lstm_output_size = 70

# Training
batch_size = 30
epochs = 2
```

我们在一开始就注意到了一些新的变量——这些都是针对卷积层的，在这一点上，我们必须请读者相信我们在设置这些常数时的选择——这些变量通常会严重影响训练，并且是在实验后根据经验得出的。在前一个例子中，我们已经遇到了其他变量/参数。

```
print('Build model...')

model = Sequential()
model.add(Embedding(max_features, embedding_size, input_length=maxlen))
model.add(Dropout(0.25))
model.add(Conv1D(filters,
                 kernel_size,
                 padding='valid',
                 activation='relu',
                 strides=1))
model.add(MaxPooling1D(pool_size=pool_size))
model.add(LSTM(lstm_output_size))
model.add(Dense(1))
model.add(Activation('sigmoid'))
```

我们立即看到，这个模型比前一个模型复杂得多。我们可以把复杂性理解为层数，这里我们加起来有 7 层。我们看到这里有一个单独的漏失层，和前面的例子一样，这是为了防止过度拟合。然后我们添加第一个卷积层——这是我们之前提到的变量/参数开始起作用的地方。

我们很快添加的池层也是我们卷积架构的一部分。关于卷积神经网络的斯坦福课程描述了汇集层的功能——它的功能是逐步减少表示的空间大小，以减少网络中的参数和计算的数量，因此也控制过拟合[16]。其余的层我们之前已经见过了，不过我们也为我们的网络显式添加了一个`Activation`函数，和上次一样，是`sigmoid`函数。这就完成了我们网络的设置——我们现在可以开始编译和训练了。我们使用与上一个神经网络相同的损失和优化方法。

```
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

print('Train...')
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_data=(x_test, y_test))
score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)
print('Test score:', score)
print('Test accuracy:', acc)
```

我们可以看到额外的层增加了我们的网络，不是吗？我们可以看到我们的精度提高了。随着准确性的提高，我们还可以看到我们在 CPU 上的训练时间增加到了 30 分钟以上。

我们之前提到过在分类器中使用预训练的单词嵌入可以改善结果——Keras 允许我们使用这些结果，而且非常容易。在我们关于单词嵌入的章节中([第 12 章](020dd724-0a11-4ba3-9a7c-050df9dfa1e4.xhtml)、 *Word2Vec* 、 *Doc2Vec* 、*和 Gensim* )，我们讨论了 GloVe 单词嵌入——如果你还有下载的话，我们可以马上开始。我们将遵循这个例子中的代码[19]。

```
BASE_DIR = ''" # you would have to paste the actual directory of where your GloVe file is over here.
GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')
MAX_SEQUENCE_LENGTH = 1000
MAX_NUM_WORDS = 20000
EMBEDDING_DIM = 100
```

我们将使用前面的变量/参数来帮助加载我们的单词嵌入。我们的第一步是从文件中访问这些嵌入并对它们进行索引。

```
print('Indexing word vectors.')

embeddings_index = {}
with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

print('Found %s word vectors.' % len(embeddings_index))
```

通过嵌入文件的一个简单循环就可以完成这个设置。我们现在建立一个矩阵，它将帮助我们实际使用嵌入。

```
print('Preparing embedding matrix.')

# prepare embedding matrix
num_words = min(MAX_NUM_WORDS, len(word_index) + 1)
embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))
for word, i in word_index.items():
    if i >= MAX_NUM_WORDS:
        continue
    embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix[i] = embedding_vector
```

我们现在准备在我们的模型中使用我们的嵌入——确保我们将训练参数设置为 false 是很重要的，所以我们现在使用单词 vectors。

```
embedding_layer = Embedding(num_words,
                            EMBEDDING_DIM,
                            weights=[embedding_matrix],
                            input_length=MAX_SEQUENCE_LENGTH,
                            trainable=False)
```

随着嵌入层的建立，我们的模型几乎完成了——我们现在遵循一个与我们之前使用的模式非常相似的模式。

```
print('Training model.')

# train a 1D convnet with global maxpooling
sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')
embedded_sequences = embedding_layer(sequence_input)
x = Conv1D(128, 5, activation='relu')(embedded_sequences)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = MaxPooling1D(5)(x)
x = Conv1D(128, 5, activation='relu')(x)
x = GlobalMaxPooling1D()(x)
x = Dense(128, activation='relu')(x)
preds = Dense(len(labels_index), activation='softmax')(x)
```

在这个例子中，我们以稍微不同的方式堆叠我们的层，用`x`变量*保存每一层的*。`preds`变量是我们的最后一层，包含了所有之前的层。我们用`Model`类建立了我们的模型，现在我们可以开始了！

```
model = Model(sequence_input, preds)
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop',
              metrics=['acc'])

model.fit(x_train, y_train,
          batch_size=128,
          epochs=10,
          validation_data=(x_val, y_val))
```

请注意，我们使用了不同的方法来计算损失；我们鼓励读者尝试不同的损失和优化自己获得一种感觉。我们已经看到了基本 LSTM、卷积神经网络和使用预训练单词嵌入的卷积神经网络的例子。我们还可以看到每个网络的性能都在逐步提高。当我们没有太多的训练数据时，嵌入对我们特别有用——我们以前读过单词嵌入是如何学习上下文的；我们使用相同的上下文为我们的网络注入一些额外的预测能力。

一般来说，卷积模型比顺序模型性能更好，使用单词嵌入的模型性能更好。这有道理；单词嵌入为模型添加了更多的上下文，并且从计算的角度更好地描述了每个单词，我们已经在[第 12 章](020dd724-0a11-4ba3-9a7c-050df9dfa1e4.xhtml)、 *Word2Vec* 、 *Doc2Vec* 、*和 Gensim* 中对此进行了讨论。至于何时使用哪个模型，如果我们可以访问在与我们要分类的上下文相似的数据集上训练的词嵌入，并且如果我们有足够强大的计算机来训练神经网络，我们还不如使用卷积网络来完成我们的分类任务。像任何机器学习任务一样，训练另一个更简单的模型也是值得的，例如支持向量机或朴素贝叶斯分类器。在我们检查了性能和精度之后，我们可以选择在最终的流水线中使用哪个模型。

Keras 为我们提供了易用性、灵活性和随意构建神经网络的能力。发表在 arXiv 上的深度学习论文通常会链接到带有 Keras 代码示例的 GitHub 知识库，阅读完本章和上一章后，您应该能够轻松理解这些神经网络是如何构建的。



# 空间分类

虽然 Keras 在独立的文本分类任务中工作得特别好，但有时将 Keras 与 spaCy 结合使用可能是有用的，spaCy 在文本分析中工作得非常好。在[第 3 章](be6f4ff3-1217-474a-8923-5eeeed17bfa1.xhtml)、 *spaCy 的语言模型*、[第 5 章](e7fc28c4-94a9-41aa-8836-ad4c9a55e880.xhtml)、*词性标注及其应用*、[第 6 章](6aa68517-4c79-4d41-8cc2-04b73b85e6f0.xhtml)、 *NER 标注及其应用*和[第 7 章](bdd3d124-b0be-4848-8a54-a8913ab8214e.xhtml) *、依存解析*中，我们已经看到 spaCy 如何很好地处理文本数据，当涉及到深度学习时也不例外——其面向文本的方法使其易于使用 spaCy 进行文本分类有两种方式——一种是使用自己的神经网络库`thinc`，另一种是使用 Keras。我们将要解释的两个例子都来自 spaCy 的文档，强烈建议您查看原始例子！

我们将要探索的第一个例子可以在空间例子页面上找到，标题是`deep_learning_keras.py` [ [20](https://github.com/explosion/spaCy/blob/master/examples/deep_learning_keras.py) 。在这个例子中，我们使用 LSTM 来完成情感分类任务。这将是一个经过 Keras 训练的模型。这个模型被训练来对句子进行分类，然后对分数进行汇总，这样我们就可以对文档进行分类。使用纯 Keras 或 Tensorflow 来执行这种层次聚合更加困难，因此这是一个观察 spaCy 威力的好例子。

```
import plac
import random
import pathlib
import cytoolz
import numpy
from keras.models import Sequential, model_from_json
from keras.layers import LSTM, Dense, Embedding, Bidirectional
from keras.layers import TimeDistributed
from keras.optimizers import Adam
import thinc.extra.datasets
from spacy.compat import pickle
import spacy
```

我们应该能够识别这些导入中的大部分，之前已经将它们与 Keras 或 spaCy 一起使用过。

```
class SentimentAnalyser(object):
    @classmethod
    def load(cls, path, nlp, max_length=100):
        with (path / 'config.json').open() as file_:
            model = model_from_json(file_.read())
        with (path / 'model').open('rb') as file_:
            lstm_weights = pickle.load(file_)
        embeddings = get_embeddings(nlp.vocab)
        model.set_weights([embeddings] + lstm_weights)
        return cls(model, max_length=max_length)

        def __init__(self, model, max_length=100):
            self._model = model
            self.max_length = max_length

        def __call__(self, doc):
            X = get_features([doc], self.max_length)
            y = self._model.predict(X)
            self.set_sentiment(doc, y)
```

前几行只是建立了我们的类，并指示如何加载我们的模型和嵌入权重。然后，我们初始化模型、最大长度，并设置预测指令。`load`方法返回加载的模型，我们在 evaluate 方法中使用它来设置管道。我们用模型和最大长度初始化这个类。`call`方法获得特征和预测。我们在下一段代码后继续解释，这是`pipe`方法。注意，它不是一个新的代码文件，而是一个`SentimentAnalyser`类的方法；请务必查看这个链接[21]以获得完整的代码！

```
        def pipe(self, docs, batch_size=1000, n_threads=2):
            for minibatch in cytoolz.partition_all(batch_size, docs):
                minibatch = list(minibatch)
                sentences = []
                for doc in minibatch:
                    sentences.extend(doc.sents)
                Xs = get_features(sentences, self.max_length)
                ys = self._model.predict(Xs)
                for sent, label in zip(sentences, ys):
                    sent.doc.sentiment += label - 0.5
                for doc in minibatch:
                    yield doc

        def set_sentiment(self, doc, y):
            doc.sentiment = float(y[0])
```

pipe 方法实际上是在将我们的数据集分成若干批之后执行预测的。我们可以看到`ys = self._model.predict(Xs)`线，计算的是情绪值。它还为文档分配一个情感值。现在我们已经完成了`SentimentAnalyser`类的编写，我们将开始编写有助于我们训练的方法。

```
        def get_labelled_sentences(docs, doc_labels):
            labels = []
            sentences = []
            for doc, y in zip(docs, doc_labels):
            for sent in doc.sents:
                sentences.append(sent)
                labels.append(y)
            return sentences, numpy.asarray(labels, dtype='int32')

        def get_features(docs, max_length):
            docs = list(docs)
            Xs = numpy.zeros((len(docs), max_length), dtype='int32')
            for i, doc in enumerate(docs):
                j = 0
                for token in doc:
                    vector_id = token.vocab.vectors.find(key=token.orth)
                    if vector_id >= 0:
                        Xs[i, j] = vector_id
                    else:
                        Xs[i, j] = 0
                    j += 1
                    if j >= max_length:
                        break
            return Xs
```

获取带标签的句子的方法相当简单，它返回句子和适当的标签。`get_features`方法需要多加注意:你可以注意到它是我们为每个文档构造特征向量的地方。

```
        def train(train_texts, train_labels, dev_texts, dev_labels,
                  lstm_shape, lstm_settings, lstm_optimizer, 
                  batch_size=100, nb_epoch=5, by_sentence=True):
            nlp = spacy.load('en_vectors_web_lg')
            nlp.add_pipe(nlp.create_pipe('sentencizer'))
            embeddings = get_embeddings(nlp.vocab)
            model = compile_lstm(embeddings, lstm_shape, lstm_settings)
            train_docs = list(nlp.pipe(train_texts))
            dev_docs = list(nlp.pipe(dev_texts))
            if by_sentence:
                train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)
                dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)

            train_X = get_features(train_docs, lstm_shape['max_length'])
            dev_X = get_features(dev_docs, lstm_shape['max_length'])
            model.fit(train_X, train_labels, validation_data=(dev_X, dev_labels),
                      nb_epoch=nb_epoch, batch_size=batch_size)
            return model
```

很容易猜到训练方法是我们所有繁重工作发生的地方——这里需要注意的一些重要行是涉及 spaCy 管道的行，我们在那里添加了一个*句子分析器*。管道设置之后的代码行涉及编译 LSTM(我们将在下面查看我们的模型)，加载我们的单词嵌入，然后从我们的文档中接收我们的特征，这样我们就可以继续进行训练。

```
        def compile_lstm(embeddings, shape, settings):
            model = Sequential()
            model.add(
                Embedding(
                    embeddings.shape[0],
                    embeddings.shape[1],
                    input_length=shape['max_length'],
                    trainable=False,
                    weights=[embeddings],
                    mask_zero=True
                )
            )
            model.add(TimeDistributed(Dense(shape['nr_hidden'], 
                                            use_bias=False)))
            model.add(Bidirectional(LSTM(shape['nr_hidden'],
                                    recurrent_dropout=settings['dropout'],
                                    dropout=settings['dropout'])))
            model.add(Dense(shape['nr_class'], activation='sigmoid'))
            model.compile(optimizer=Adam(lr=settings['lr']), 
                          loss='binary_crossentropy',metrics=['accuracy'])
            return model
```

这部分代码对我们来说应该更熟悉——正如我们在上一节中所做的那样，我们设置了每一层并将它们堆叠起来。我们可以使用任何我们想要的 Keras 模型来实现这一点，在这种情况下，使用了双向 LSTM。

```
     def get_embeddings(vocab):
         return vocab.vectors.data

     def evaluate(model_dir, texts, labels, max_length=100):
         def create_pipeline(nlp):
             '''
             This could be a lambda, but named functions are easier 
             to read in Python.
             '''
             return [nlp.tagger, nlp.parser, 
                     SentimentAnalyser.load(model_dir, nlp, 
                     max_length=max_length)]

         nlp = spacy.load('en')
         nlp.pipeline = create_pipeline(nlp)

         correct = 0
         i = 0
         for doc in nlp.pipe(texts, batch_size=1000, n_threads=4):
             correct += bool(doc.sentiment >= 0.5) == bool(labels[i])
             i += 1
         return float(correct) / i
```

evaluate 方法返回我们的模型执行得如何的分数；代码相当简单，只需用文档的标签检查指定的情感分数。

```
        def read_data(data_dir, limit=0):
            examples = []
            for subdir, label in (('pos', 1), ('neg', 0)):
                for filename in (data_dir / subdir).iterdir():
                    with filename.open() as file_:
                        text = file_.read()
                    examples.append((text, label))
            random.shuffle(examples)
            if limit >= 1:
                examples = examples[:limit]
            return zip(*examples) # Unzips into two lists
```

我们使用 IMDB 情感分析数据集；这个方法是访问这些数据的接口。

```
@plac.annotations(
    train_dir=("Location of training file or directory"),
    dev_dir=("Location of development file or directory"),
    model_dir=("Location of output model directory",),
    is_runtime=("Demonstrate run-time usage", "flag", "r", bool),
    nr_hidden=("Number of hidden units", "option", "H", int),
    max_length=("Maximum sentence length", "option", "L", int),
    dropout=("Dropout", "option", "d", float),
    learn_rate=("Learn rate", "option", "e", float),
    nb_epoch=("Number of training epochs", "option", "i", int),
    batch_size=("Size of minibatches for training LSTM", "option", "b", int),
    nr_examples=("Limit to N examples", "option", "n", int)
)
```

前面的注释设置了我们的选项，这些选项为模型设置了各种模型目录、运行时和参数。现在让我们继续讨论主函数。

```
def main(model_dir=None, train_dir=None, dev_dir=None,
         is_runtime=False,
         nr_hidden=64, max_length=100, # Shape
         dropout=0.5, learn_rate=0.001, # General NN config
         nb_epoch=5, batch_size=100, nr_examples=-1):  # Training params
    if model_dir is not None:
        model_dir = pathlib.Path(model_dir)
    if train_dir is None or dev_dir is None:
        imdb_data = thinc.extra.datasets.imdb()
    if is_runtime:
        if dev_dir is None:
            dev_texts, dev_labels = zip(*imdb_data[1])
        else:
            dev_texts, dev_labels = read_data(dev_dir)
        acc = evaluate(model_dir, dev_texts, dev_labels, 
                       max_length=max_length)
        print(acc)
    else:
        if train_dir is None:
            train_texts, train_labels = zip(*imdb_data[0])
        else:
            print("Read data")
            train_texts, train_labels = read_data(train_dir, 
                                                  limit=nr_examples)
        if dev_dir is None:
            dev_texts, dev_labels = zip(*imdb_data[1])
        else:
            dev_texts, dev_labels = read_data(dev_dir, imdb_data, 
                                              limit=nr_examples)
        train_labels = numpy.asarray(train_labels, dtype='int32')
        dev_labels = numpy.asarray(dev_labels, dtype='int32')
        lstm = train(train_texts, train_labels, dev_texts, dev_labels,
                     {'nr_hidden': nr_hidden, 'max_length': max_length, 
                      'nr_class': 1},
                     {'dropout': dropout, 'lr': learn_rate},
                     {},
                     nb_epoch=nb_epoch, batch_size=batch_size)
        weights = lstm.get_weights()
        if model_dir is not None:
            with (model_dir / 'model').open('wb') as file_:
                pickle.dump(weights[1:], file_)
            with (model_dir / 'config.json').open('wb') as file_:
                file_.write(lstm.to_json())
if __name__ == '__main__':
    plac.call(main) 
```

不要让主函数的大小吓到你——你可以注意到前几行设置了`model`文件夹，并将加载数据集。然后，我们检查是否希望打印运行时信息，在这种情况下，我们运行 evaluate 方法。如果没有，并且训练没有完成，我们继续训练我们的模型。`lstm.train()`方法训练模型，然后如果`model`文件夹不是未定义的，我们保存我们的模型。

在您自己的生产管道中运行、保存和使用模型是以这种方式使用 Keras 和 spaCy 的巨大动机。这里关键的一点是，我们正在为每个文档更新`sentiment`属性。我们如何决定使用它取决于我们自己。spaCy 实现的主要卖点之一是它不会删除或截断输入——作者认为这样做会对结果产生负面影响，因为用户倾向于在文档的最后一句话中总结他们的评论，并且可以从这句话中推断出很多观点。

现在我们有了训练好的模型，我们该如何使用它呢？我们的模型现在向我们的文档添加了一个属性，这就是`doc.sentiment`属性。这个值捕获了文档的情感。用户可以通过稍后加载保存的模型并通过管道运行任何文档来验证这一点，方法与我们在[第 5 章](e7fc28c4-94a9-41aa-8836-ad4c9a55e880.xhtml)、*词性标注及其应用*、[第 6 章](6aa68517-4c79-4d41-8cc2-04b73b85e6f0.xhtml)、 *NER 标注及其应用*和[第 7 章](bdd3d124-b0be-4848-8a54-a8913ab8214e.xhtml) *、依存解析:*中所做的相同

```
doc = nlp(document)
```

这里，`nlp`是我们刚刚训练的加载模型的管道对象，文档是我们希望分析的任何 unicode 文本。doc 对象现在包含了关于情感的信息。

我们还可以基于文档属于特定类别的概率来训练更传统的分类器。培训执行起来非常简单——`update`方法是管道的一部分，它是实际培训的内容。文档中的示例代码可以在[这里找到](https://spacy.io/usage/training#section-textcat)【21】，GitHub 上的代码可以在[这里找到](https://github.com/explosion/spacy/blob/master/examples/training/train_textcat.py)【22】。我们将带领读者浏览代码，并高度鼓励用户运行代码，看看它给管道增加了什么。请注意，这个文件是要一次性运行的，我们只有*分割的*代码，以便我们可以解释它。测试代码时，运行可以在这里找到的文件【22】。

```
import plac
import random
from pathlib import Path
import thinc.extra.datasets

import spacy
from spacy.util import minibatch, compounding  
```

这些导入是我们习惯看到的，但是我们这里没有 Keras，因为我们将使用内置的`thinc`库。

```
@plac.annotations(
    model=("Model name. Defaults to blank 'en' model.", "option", "m", 
           str),
    output_dir=("Optional output directory", "option", "o", Path),
    n_texts=("Number of texts to train from", "option", "t", int),
    n_iter=("Number of training iterations", "option", "n", int))
def main(model=None, output_dir=None, n_iter=20, n_texts=2000):
    if model is not None:
        nlp = spacy.load(model)  # load existing spaCy model
        print("Loaded model '%s'" % model)
    else:
        nlp = spacy.blank('en')  # create blank Language class
        print("Created blank 'en' model")
```

我们已经在打印过程中设置了注释，并加载了模型。如果我们没有通过一个模型，我们可以初始化一个空模型。

```
    if 'textcat' not in nlp.pipe_names:
        textcat = nlp.create_pipe('textcat')
        nlp.add_pipe(textcat, last=True)
    # otherwise, get it, so we can add labels to it
    else:
        textcat = nlp.get_pipe('textcat')

    # add label to text classifier
    textcat.add_label('POSITIVE')
```

如果文本分类器标签不存在，我们现在将它添加到我们的管道中——如果它确实存在，我们将获取它并向它添加一个样本标签。

```
    print("Loading IMDB data...")
    (train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)
    print("Using {} examples ({} training, {} evaluation)"
    .format(n_texts, len(train_texts), len(dev_texts)))
    train_data = list(zip(train_texts,
                          [{'cats': cats} for cats in train_cats]))
```

我们现在正在处理数据集——我们已经加载了数据集，然后存储了训练数据。

```
        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 
                       'textcat']
```

在我们开始任何训练之前，我们首先禁用管道的所有其他部分。

```
        with nlp.disable_pipes(*other_pipes): 
            optimizer = nlp.begin_training()
            print("Training the model...")
            print('{:^5}t{:^5}t{:^5}t{:^5}'.format('LOSS', 'P', 'R', 'F'))
            for i in range(n_iter):
                losses = {}
                # batch up the examples using spaCy's minibatch
                batches = minibatch(train_data, size=compounding(4., 32., 
                                    1.001))
                for batch in batches:
                    texts, annotations = zip(*batch)
                    nlp.update(texts, annotations, sgd=optimizer, drop=0.2,
                               losses=losses)
```

我们将使用批处理来训练我们的数据，类似于前面的例子。`nlp.update`方法是所有代码的核心，它使用训练信息和注释来执行训练。

```
            with textcat.model.use_params(optimizer.averages):
                # evaluate on the dev data split off in load_data()
scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)
                print('{0:.3f}t{1:.3f}t{2:.3f}t{3:.3f}'  
                # print a simple table
                      .format(losses['textcat'], scores['textcat_p'],
                      scores['textcat_r'], scores['textcat_f']))

    if output_dir is not None:
        output_dir = Path(output_dir)
        if not output_dir.exists():
            output_dir.mkdir()
        nlp.to_disk(output_dir)
        print("Saved model to", output_dir)

        # test the saved model
        print("Loading from", output_dir)
        nlp2 = spacy.load(output_dir)
        doc2 = nlp2(test_text)
        print(test_text, doc2.cats)
```

然后，我们用 evaluate 方法测试我们的模型，该方法计算精度、召回率和 f 值。主要功能的最后一部分是将训练好的模型保存在输出目录中(如果指定),并测试保存的模型。

```
def load_data(limit=0, split=0.8):
    """Load data from the IMDB dataset."""
    # Partition off part of the train data for evaluation
    train_data, _ = thinc.extra.datasets.imdb()
    random.shuffle(train_data)
    train_data = train_data[-limit:]
    texts, labels = zip(*train_data)
    cats = [{'POSITIVE': bool(y)} for y in labels]
    split = int(len(train_data) * split)
    return (texts[:split], cats[:split]), (texts[split:], cats[split:])

def evaluate(tokenizer, textcat, texts, cats):
    docs = (tokenizer(text) for text in texts)
    tp = 1e-8  # True positives
    fp = 1e-8  # False positives
    fn = 1e-8  # False negatives
    tn = 1e-8  # True negatives
    for i, doc in enumerate(textcat.pipe(docs)):
        gold = cats[i]
        for label, score in doc.cats.items():
            if label not in gold:
                continue
            if score >= 0.5 and gold[label] >= 0.5:
                tp += 1.
            elif score >= 0.5 and gold[label] < 0.5:
                fp += 1.
            elif score < 0.5 and gold[label] < 0.5:
                tn += 1
            elif score < 0.5 and gold[label] >= 0.5:
                fn += 1
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f_score = 2 * (precision * recall) / (precision + recall)
    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}

if __name__ == '__main__':
    plac.call(main)
```

我们之前在 main 函数中遇到过这些方法；一个是加载数据集，另一个是评估我们训练好的模型的性能。我们使用与`thinc`捆绑在一起的数据集，并适当地混合和分割返回数据。evaluate 函数只是计算真阴性、真阳性、假阴性和假阳性，以创建召回率、精确度和 f 值的度量。

```
test_text = "This movie disappointed me severely"
doc = nlp(test_text)
print(test_text, doc.cats)
```

`doc.cats`参数给出了分类的结果——在这里，它是负面情绪，并且被正确地分类为负面情绪。

这将是我们的最后一步——用一个例句来测试我们的模型。在这里，我们还可以看到使用 spaCy 进行深度学习的主要优势之一——它无缝地适合我们的管道，分类或情感分数最终成为文档的另一个属性。这与我们使用 Keras 进行深度学习的方式截然不同，在 Keras 中，我们的目的是生成文本或输出概率向量——这只是一种向量输入、向量输出的方法。当然，可以利用这些信息作为我们的文本分析管道的一部分，但是 spaCy 在幕后进行培训并学习文档属性的方式使得将这些信息作为任何文本分析管道的一部分变得非常容易。



# 摘要

在前一章中，我们向读者介绍了文本深度学习，在这一章中，我们看到了如何在我们自己的应用程序中利用深度学习的力量，无论我们使用 Keras 还是 spaCy。当设计智能文本系统时，知道如何分配情感分数或对我们的文档进行分类给了我们巨大的推动，并且有了预训练的模型，我们不必每次希望进行这样的分类时都执行繁重的计算。现在，我们有能力构建一个强大而多样的文本分析管道！

在下一章，我们将讨论两个流行的文本分析问题——情感分析和构建我们自己的聊天机器人——以及我们可以采取什么方法来解决这些问题。



# 参考

[1] Keras 序贯模型:
[https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/)

[2] Keras CNN LSTM: 
[https://github.com/keras-team/keras/blob/master/examples/imdb_cnn_lstm.py](https://github.com/keras-team/keras/blob/master/examples/imdb_cnn_lstm.py)

[3]预训练单词嵌入:
[https://blog . keras . io/using-Pre-trained-Word-embedding-in-a-keras-model . html](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html)

[4]CNTK:
[https://github.com/Microsoft/CNTK](https://github.com/Microsoft/CNTK)

[5]苹果 CoreML:
[https://developer.apple.com/documentation/coreml](https://developer.apple.com/documentation/coreml)

[6] Keras 文本处理:
[https://keras.io/preprocessing/text/](https://keras.io/preprocessing/text/)

[7] Keras 车型:
[https://keras.io/models/about-keras-models/](https://keras.io/models/about-keras-models/)

[8] Keras Layers: 
[https://keras.io/layers/about-keras-layers/](https://keras.io/layers/about-keras-layers/)

[9] Keras 核心层:
[https://keras.io/layers/core/](https://keras.io/layers/core/)

[10] Keras Datasets: 
[https://keras.io/datasets/](https://keras.io/datasets/)

[11] Keras LSTM: 
[https://keras.io/layers/recurrent/#lstm](https://keras.io/layers/recurrent/#lstm)

[12]克拉斯卷积层数:
[https://keras.io/layers/convolutional/](https://keras.io/layers/convolutional/)

[13] Keras 举例目录:
[https://github.com/keras-team/keras/tree/master/examples](https://github.com/keras-team/keras/tree/master/examples)

[14]激活功能:
[https://en.wikipedia.org/wiki/Activation_function](https://en.wikipedia.org/wiki/Activation_function)

[15] Keras 激活功能:
[https://keras.io/activations/](https://keras.io/activations/)

[16]汇集层:
[http://cs231n.github.io/convolutional-networks/#pool](http://cs231n.github.io/convolutional-networks/#pool)

[17]顺序示例:
[https://github . com/keras-team/keras/blob/master/examples/IMDB _ bidirectional _ lstm . py](https://github.com/keras-team/keras/blob/master/examples/imdb_bidirectional_lstm.py)

[18]卷积示例:
[https://github . com/keras-team/keras/blob/master/examples/IMDB _ CNN . py](https://github.com/keras-team/keras/blob/master/examples/imdb_cnn.py)

[19]带嵌入的卷积:
[https://github . com/keras-team/keras/blob/master/examples/pre trained _ word _ embeddings . py](https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)

[20]深度学习 Keras:
[https://github . com/explosion/spaCy/blob/master/examples/Deep _ Learning _ Keras . py](https://github.com/explosion/spaCy/blob/master/examples/deep_learning_keras.py)

[21]文本分类模型空间:
[https://spacy.io/usage/training#section-textcat](https://spacy.io/usage/training#section-textcat)

[22]文字分类代码:
[https://github . com/explosion/spacy/blob/master/examples/training/train _ Text cat . py](https://github.com/explosion/spacy/blob/master/examples/training/train_textcat.py)