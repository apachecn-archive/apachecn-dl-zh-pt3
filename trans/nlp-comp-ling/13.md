

# 十三、文本深度学习

到目前为止，我们已经探索了机器学习在各种背景下对文本的使用——主题建模、聚类、分类、文本摘要，甚至我们的帖子标签和 NER 标签都是使用机器学习进行训练的。在这一章中，我们将开始探索机器学习最前沿的形式之一——**深度学习**。深度学习是 ML 的一种形式，其中我们使用生物启发的结构来生成算法和架构，以对文本执行各种任务。这些任务包括文本生成、分类和词嵌入。在这一章中，我们将讨论深度学习的一些基础，以及如何实现我们自己的文本深度学习模型。以下是我们将在本章中涉及的主题:

*   深度学习
*   文本深度学习
*   文本生成



# 深度学习

在本书中，我们使用了机器学习技术，主题建模，聚类和分类算法，以及我们所说的**浅层学习**——词嵌入。词嵌入是我们对神经网络和它们可以学习的语义信息的第一次了解。

神经网络可以理解为一种计算系统或机器学习算法，其架构模糊地受到大脑中生物神经元的启发。我们在这里含糊地说，是因为我们对人脑缺乏彻底的了解——通过神经连接和大脑的结构肯定对神经网络的一些基本构建模块有影响，如**感知器** [ [1](https://en.wikipedia.org/wiki/Perceptron) 和 s **单层神经网络** [ [2](https://en.wikipedia.org/wiki/Feedforward_neural_network) 。

神经网络通常由许多执行数学运算并通过连接相互作用的节点组成。这个模型在某种意义上类似于大脑，因为节点倾向于代表神经元以及这些神经元之间的连接。不同的层可以执行不同种类的操作，通常有一个输入层、多个隐藏层和一个输出层。

![](Images/877610f2-4683-4e13-8df2-e0ec24224d2e.png)

图 13.1 神经网络的常见结构示例[4]

反过来，神经网络研究启发了认知研究，人们对使用神经网络来理解人类大脑也有相当大的兴趣。神经网络可以用于我们之前进行的大多数机器学习任务，例如分类、聚类，以及我们在上一章中看到的创建单词和文档的矢量表示。

在文本分析领域之外，神经网络取得了相当大的成功。在图像分类、计算机视觉、语音识别和医疗诊断中，最先进的结果通常是通过神经网络实现的。我们之前提到过，我们使用神经网络来生成词嵌入——在学习或训练完成后，我们使用隐藏层中存储的值作为我们的嵌入。

尽管这一章的标题是深度学习，但我们已经广泛地讨论了神经网络，但深度学习只是指具有多层的神经网络的另一种方式。由于当前大多数神经网络倾向于在其架构中使用多层，因此我们可以将这些技术称为深度学习技术。当然也有例外，比如在 Word2Vec 中，我们只从一层中选取权重。

神经网络和深度学习架构的用途非常广泛，即使我们可能对神经网络没有完整的数学理解，但对于实用的自然语言处理来说，它仍然是一个非常好的选择，这也是我们在本章中试图带领读者了解的内容。



# 文本深度学习(及更多)

当我们使用词嵌入时，我们已经直接意识到了神经网络的力量。这是神经网络的一个方面——使用架构本身的部分来获得有用的信息，但神经网络远不限于此。当我们开始使用更深的网络时，使用权重来提取有用的信息是不谨慎的——在这些情况下；我们对神经网络的自然输出更感兴趣。我们可以训练神经网络执行与文本分析相关的多项任务——事实上，对于其中一些任务，神经网络的引入已经完全改变了我们处理任务的方式。

这里一个流行的例子是**语言翻译**，特别是谷歌的**神经翻译**模型。从 2016 年 9 月开始，谷歌使用统计和基于规则的方法和模型来进行语言翻译，但随着谷歌大脑研究团队的出现，他们很快转向使用神经网络，以及一种现在被称为
**零镜头翻译**[5](https://ai.googleblog.com/2016/11/zero-shot-translation-with-googles.html)的技术。这意味着，如果谷歌翻译算法打算从马来西亚语翻译成阿拉伯语，例如，它将首先翻译成英语作为一个中间步骤。根据其当前的神经网络设置，模型接受一个输入句子，其输出是一个翻译的句子——当然，它不只是抛出第一个输出，通常有一个评分机制，在那里验证语法正确性。我们现在有了一个更干净的尝试翻译的方法，而不是将一个句子分成多个部分，执行基于规则的翻译并重新排列句子。神经模型也往往比统计模型小，即使它们可能需要更多的数据或时间来执行初始训练。当这些模型优于现有模型时，Google 会发布更多的语言——就在最近，Google 发布了许多印度语言的新模型。

尽管机器翻译取得了进步，但它仍然是一项艰巨的任务——尽管我们可以期待或多或少语法准确的句子，但它更意味着向用户提供一个关于输入句子在另一种语言中的含义的大致想法。像其他深度学习领域一样，人们可以期待机器翻译的结果只会变得更好。

词嵌入是神经网络用于文本的另一种非常流行的用途——考虑到单词向量和文档向量在许多 NLP 任务中的使用方式，这意味着词嵌入在许多涉及文本的机器学习算法中有一席之地。事实上，用词嵌入替换所有以前的向量意味着我们现在在所有的算法或应用中都有一些神经网络！凭借其捕捉上下文的能力，它可以在聚类或分类等任务中提供很大帮助。

此外，说到聚类或分类等技术，我们也可以训练神经网络来执行这些任务。事实上，更复杂的文本分析任务，比如构建聊天机器人，需要一路执行文本分类。我们在文本中称为**情感分析**的任务本质上是一个分类任务，我们将文档分类为具有正面或负面情感(或者多种情感，如果这是我们的工作的话)。我们可以使用更复杂的神经网络来执行这一任务，如**卷积神经网络** ( **CNN** ) [ [6](https://en.wikipedia.org/wiki/Convolutional_neural_network) ]或**循环神经网络**(**RNN**)[[7](https://en.wikipedia.org/wiki/Recurrent_neural_network)]，但即使是普通的单层神经网络也能做得很好。

我们在训练我们自己的词性标注者或 NER 标注者时看到了这一点——在引擎盖下发生的是一个神经网络，它被训练来识别单词的不同类别(T1)——这些类别是不同的词性或命名实体。所以，*从技术上来说*，我们已经在我们所有的应用中使用了深度学习的元素，只是使用了 spaCy 训练过的标签！

我们不会讨论神经网络的数学细节，因为这超出了本书的范围，但是当讨论不同种类的神经网络以及我们将如何使用它们时，我们将尝试讨论架构，更重要的是，讨论该特定方法的超参数和最佳实践。提醒一下:超参数是机器学习算法的参数，在开始算法之前设置。

当处理香草神经网络甚至卷积神经网络时，我们的输入空间和输出空间是固定的——我们决定输入是什么。它可能是一幅图像，也可能是一句话，但它基本上是一个矢量输入，产生一个矢量输出。在自然语言处理中，这个向量输出可以是，例如，文档属于某个类别的概率。循环神经网络在这方面是不同的，因为它的架构(信息是)——通过允许序列作为输入，我们可以做的不仅仅是预测类。循环神经网络对文本特别有用，因为它们将输入数据理解为序列，并允许我们捕捉句子中单词的上下文。

神经网络如何处理文本的一个想法是为文本主体生成一个概率语言模型。这可以理解为是一种我们计算下一个单词(或字符)的概率的技术！)在基于先前输入的序列中。换句话说，他们试图根据上下文来计算单词的概率。事实上，即使在神经网络被定期用于自然语言处理之前，这也是一种流行的方法——我们以前使用过 n 元语法，它或多或少基于相同的原理工作。基于一个语料库或一组文本，它试图了解两个单词在特定的上下文中相邻出现的概率——也就是周围的单词。这就是我们如何开始考虑 *new_york* 作为我们词汇中的新成员，它意味着这两个词有很高的概率会紧挨着出现，而这个概率是通过基本的条件概率和链概率规则计算出来的。

当使用神经网络时，我们可以认为，通过学习单词或字符出现的几率或概率，我们正在使用序列生成器，或者神经网络现在是一个生成模型。自然语言处理背景下的生成模型可能会特别有趣——如果我们可以教会神经网络什么样的句子以高概率出现，我们也可以尝试让这个神经网络输出模拟它被训练的文本的序列。

正是这种相同的思维让我们创建词嵌入——单词*蓝色*出现在句子*墙漆好了*之后的几率与单词*红色*出现的几率相似，我们的嵌入学习用相似的语义对*蓝色*和*红色*进行编码。这种语义理解通过共享表征的实验得到了进一步的探索。共享表示的思想是，共享相同语义的不同类型的输入可以映射到相同的向量空间——例如，英文单词 *dog* 和中文单词 *dog* 将映射到共享的中英向量空间中非常相似的向量。但是神经网络的力量更加令人印象深刻——训练一个网络也可以将图像映射到相同的空间是可能的！图像字幕也是由这种神经网络很好地执行的任务。

使用强化学习(我们的模型通过一个奖惩系统从自己的错误中学习)，神经网络也能够在围棋比赛中击败人类，这曾被认为是人工智能系统很难击败的比赛。

第一个自然语言处理任务之一是文本摘要——处理这类问题的传统方法是根据哪些句子提供了最多的信息来对句子进行排序，并选择其中的一个子集。我们在自己的文本摘要尝试中使用了这样的算法。然而，通过深度学习，我们现在有能力生成文本，就像更多的*人类*文本摘要尝试一样，我们将不只是选择重要的句子，而是从概率模型中创建摘要。这个过程也通常被称为**自然语言生成** ( **NLG** )。

事实上，当我们之前讨论神经网络在语言翻译中的作用时，正是通过这种生成模型，它用另一种语言再现了句子。作为我们将神经网络用于文本的第一个例子，我们将尝试在各种上下文中生成文本。



# 生成文本

在我们涉及深度学习和自然语言处理的讨论中，我们广泛地谈到了如何在文本生成中使用它来获得非常令人信服的结果——我们现在要亲自动手做一点文本生成。

我们将使用的神经网络架构是一个循环神经网络，特别是一个 **LSTM** [ [9](https://en.wikipedia.org/wiki/Long_short-term_memory) 。LSTM 代表**长短期记忆**，它很独特，因为它的架构允许它捕捉句子中单词的短期和长期上下文。深度学习研究人员 Colah 的非常受欢迎的博客文章*了解 LSTM 网络* [ [11](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) ]是进一步了解 LSTMs 的一个好方法。

这与 Andrej Karpathy 的热门博客文章[[10](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)],*《神经网络不合理的有效性》*中使用的架构相同，尽管 Karpathy 用 Lua 编写了他的神经网络代码——我们将使用 Keras，它具有高度的抽象性，是一个完美的选择。

用于深度学习的 Python 生态系统现在肯定正在蓬勃发展——根据您的用例，我们可以通过多种方式构建深度学习系统。对我们来说，我们想要高层次的抽象，并且能够轻松地使用文本来训练我们的机器。截至目前，在 2018 年，选择深度学习框架不是一件容易的事情，但我们将继续使用 Keras 来完成我们的深度学习任务，但在此之前，我们会简单讨论一下还有哪些其他工具。

1.  **tensor flow**([https://www.tensorflow.org/](https://www.tensorflow.org/)):tensor flow 是 Google 发布的神经网络库，也恰好是他们的人工智能团队 Google Brains 使用的同一框架。当然，它不同于用于生产的确切框架，但 TensorFlow 仍然得到了很好的维护，仍然是一个活跃的社区，并拥有强大的 GPU 支持。GPU 支持很重要，因为它允许我们比普通 CPU 更快地执行数学运算。由于其基于图形的计算模型，它最终成为构建神经网络的天然选择。它提供了一个高层次的控制和选择，根据你想要的低层次的运作，通常是一个受欢迎的选择，现在在研究和行业。
2.  **Theano**([http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)):可以说是第一个彻底的深度学习框架之一，由深度学习的先驱之一 Yoshia Bengio 在 MILA 创建。专注于使用符号图作为神经网络的构建模块，其 API 是相当低的级别，如果有效使用，可以产生一些非常强大的深度学习系统。它不再被维护，但仍然值得一试，即使只是为了历史！这些库，`Lasagne` [ [12](https://github.com/Lasagne/Lasagne) ]和`Blocks` [ [13](https://github.com/mila-udem/blocks) ]允许你从更高的抽象层使用这个 ano。
3.  **Caffe**([http://caffe.berkeleyvision.org/](http://caffe.berkeleyvision.org/))&**Caffe 2**([https://caffe2.ai/](https://caffe2.ai/)):Caffe 是第一个专门的深度学习框架之一，由加州大学伯克利分校开发。它既快速又模块化，虽然使用起来有点笨拙，因为它不是用原生 Python 编写的，并且需要您管理`.prototxt`文件才能在您的应用中使用网络。这些`.protoxt`文件使用预先描述的格式描述神经网络，你可以在这里找到【14】。这只是给我们的时间跨度编码神经网络增加了一层额外的复杂性，而且我们更愿意使用更多的抽象库。
4.  PyTorch(【https://pytorch.org/】T2):这个街区的新成员，同时也是一个发展迅速的库，py Torch 大致基于 Lua 的 Torch 库。**脸书人工智能研究**团队( **FAIR** )已经认可了 PyTorch，并且拥有基于动态计算图的低级和高级 API 的健康组合，它绝对值得一试。
5.  喀拉斯(【https://keras.io/】T2):喀拉斯将会是我们选择的图书馆——我们在这里并不孤单。凭借其高度的抽象和干净的 API，它仍然是原型开发的最佳深度学习框架，可以使用 Theano 或 TensorFlow 作为构建网络的后端。这是非常容易的想法——>执行，因为我们将看到在我们的文本生成的例子。它有一个庞大而活跃的社区，随着 TensorFlow 宣布它们将与 Keras 一起发布，这意味着在可预见的未来它将继续被使用。

我们邀请读者看看其他的深度学习框架——取决于用例；不同的框架可能更适合您！当然，我们将尝试的技术将保持不变，所以除了语法变化之外，我们可以期待相同的文本生成逻辑和过程。

我们之前提到过，在我们的例子中，我们将使用循环神经网络。循环神经网络比其他神经网络做得更好，因为它能够记住上下文，因为网络中的每一层都是用来自前一层的信息构建的——这一额外的上下文使它能够更好地执行，并赋予它名称*递归*。

我们将使用 RNN 的一种特殊变体，称为 LSTM，或长短期记忆——顾名思义，它有能力拥有可以持续很长一段时间的短期记忆。每当输入之间存在显著的时间延迟时，LSTMs 往往表现良好——考虑到语言的性质，其中在句子中稍后出现的单词受到句子上下文的影响，该属性开始变得更加重要。我们之前提到过，它是独一无二的，因为它可以理解周围单词的上下文，同时记住之前的单词。

对于 LSTM 和 RNN 背后的数学或直觉的更详细的解释，下面的博客帖子会非常有用(我们在本章前面已经遇到过这些博客帖子)。

*   了解 LSTM 网络[ [11](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
*   循环神经网络的不合理有效性[ [10](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

像往常一样，我们从导入开始——在开始之前，请确保使用 pip 或 conda 安装 Keras 和 tensorflow！

我们将用作参考的代码来自 Jupyter 笔记本[ [15](https://github.com/kirit93/Personal/blob/master/text_generation_keras/text_generation.ipynb) ]，尽管会有一些差异。

```py
import keras
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import ModelCheckpoint
from keras.utils import np_utils
import numpy as np
```

这里，我们使用 Keras 的序列模型，我们可以添加一个 LSTM 结构。下一步是处理数据组织。根据我们想要生成的数据类型，我们可以使用任何文本源作为输入。这就是我们可以发挥创造力的地方——我们希望我们的 RNN 像 J.K .罗琳、莎士比亚，甚至像你自己一样写作吗——如果你有足够多的写作范例的话！

当使用 Keras 生成文本时，我们需要生成书中所有不同字符的映射(我们的 LSTM 是字符级模型)。注意——`source_data.txt`这是*你选择的*个人数据集。在下面的示例代码中，所有其他变量都取决于您选择的数据集，但是无论您选择哪个文本文件，代码都会运行良好。

```py
filename    = 'data/source_data.txt'
data        = open(filename).read()
data        = data.lower()
# Find all the unique characters
chars       = sorted(list(set(data)))
char_to_int = dict((c, i) for i, c in enumerate(chars))
ix_to_char  = dict((i, c) for i, c in enumerate(chars))
vocab_size  = len(chars)
```

我们的两个字典将帮助我们向模型传递字符和生成文本。如果我们使用`print(chars)`、`vocab_size`和`char_to_int`，一个标准的数据源会给出类似这样的结果。

这是独特字符的列表:

```py
['n', ' ', '!', '&', "'", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
```

这是一些独特的字符:

```py
51
```

字符到整数的映射如下:

```py
{'n': 0, ' ': 1, '!': 2, '&': 3, "'": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '?': 22, '[': 23, ']': 24, 'a': 25, 'b': 26, 'c': 27, 'd': 28, 'e': 29, 'f': 30, 'g': 31, 'h': 32, 'i': 33, 'j': 34, 'k': 35, 'l': 36, 'm': 37, 'n': 38, 'o': 39, 'p': 40, 'q': 41, 'r': 42, 's': 43, 't': 44, 'u': 45, 'v': 46, 'w': 47, 'x': 48, 'y': 49, 'z': 50}
```

我们的 RNN 接受字符序列作为输入，并输出类似的序列。现在让我们将数据源分解成这样的序列。

```py
seq_length = 100
list_X = [ ]
list_Y = [ ]
for i in range(0, len(chars) - seq_length, 1):
    seq_in = raw_text[i:i + seq_length]
    seq_out = raw_text[i + seq_length]
    list_X.append([char_to_int[char] for char in seq_in])
    list_Y.append(char_to_int[seq_out])
n_patterns = len(list_X)
```

我们还需要做更多的工作来为我们的模型准备好我们的输入:

```py
X = np.reshape(list_X, (n_patterns, seq_length, 1)) 
# Encode output as one-hot vector
Y = np_utils.to_categorical(list_Y)
```

我们这样做是因为我们想要一次预测一个字符，这意味着我们想要一次性编码，这就是`np_utils.to_categorical`函数所做的。例如，当我们想用索引 37 对字母`m`进行编码时，它看起来像这样:

```py
[ 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 
  0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 
  1\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0.]
```

现在让我们定义我们的神经网络模型。

```py
model = Sequential()
model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1], activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam')
```

在这种情况下，我们定义了一个一层的 LSTM(我们用`Dense`创建)，0.2 的下降，SoftMax 激活，和 ADAM 优化器。

当神经网络仅在一个数据集上表现良好时，Dropout 是用于控制*过拟合*的值。激活方法决定了我们在网络中用什么值*激活*一个神经元，优化器被用来减少我们在神经网络中来回导航时的误差。

事实上，选择这些超参数最终是一个实践和微调的问题，尽管我们将在下一章简要介绍如何为您的特定文本处理任务选择合适的参数。目前，在理解其背后的直觉的同时，将它视为一个黑箱就足够了。注意，这里使用的超参数是使用 Keras 生成文本的标准参数。

训练我们的模型很容易——像 scikit-learn 一样，我们运行 fit 函数来完成这个任务。

```py
filepath="weights-improvement-{epoch:02d}-{loss:.4f}.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')
callbacks_list = [checkpoint]
# fit the model
model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)
```

`fit`函数将运行输入批处理 n_epochs 次，每当有改进时，它将把权重保存到一个文件中。这是通过回调处理的。

您应该在跑完 fit 后完成训练——请记住，根据所用数据集的大小，这可能需要几个小时甚至几天。

另一种选择是简单地加载已经预训练的模型的重量:

```py
filename = "weights.hdf5"
model.load_weights(filename)
model.compile(loss='categorical_crossentropy', optimizer='adam')
```

所以现在，用我们加载的权重或训练好的模型，我们可以一个字符一个字符地生成文本了！

```py
start   = np.random.randint(0, len(X) - 1)
pattern = np.ravel(X[start]).tolist()
```

我们希望随机开始文本生成，所以我们使用`numpy`在我们的范围内查找这个字符。

```py
output = []
for i in range(250):
    x           = np.reshape(pattern, (1, len(pattern), 1))
    x           = x / float(vocab_size)
    prediction  = model.predict(x, verbose = 0)
    index       = np.argmax(prediction)
    result      = index
    output.append(result)
    pattern.append(index)
    pattern = pattern[1 : len(pattern)]

print (""", ''.join([ix_to_char[value] for value in output]), """)
```

这里发生了什么？基于我们的输入`x`，我们为下一个字符选择最大概率(使用`argmax`，这是一个返回最大值的 indie 的方法)，然后将该索引转换为一个字符，并将其附加到我们的输出列表中。基于我们希望在输出中看到多少次迭代，我们运行了多少次循环。

在 LSTM 的例子中，我们刚刚看到，我们没有训练一个大规模的网络-通过在顶部堆叠更多的层，我们可以开始看到甚至更好的结果。在我们的例子中，我们已经看到，在几个时期之后，我们的模型开始表现得好得多。事实上，Andrej Karpathy 的博客很好地证明了这一点，并提供了各种各样的输入，从莎士比亚到 Linux 代码库！

进一步删减输入数据会给我们带来更好的结果，同时增加历元的数量。当然，添加更多的层或增加纪元的数量会增加我们的训练时间——如果我们的任务只是试验 RNNs，而不是构建可扩展或生产模型，Keras 做得非常好。



# 摘要

我们亲眼看到了深度学习令人难以置信的力量——我们可以成功地训练一个神经网络来生成与人类生成的文本非常相似的文本，至少在语法和某种程度上，在语法和拼写上是如此。通过更多的微调和一点点人工监督，我们可以看到如何用这种技术创建非常逼真的聊天机器人。

虽然这种文本分析对我们来说似乎不是特别*有用*，但神经网络在更实际的文本分析任务中有很多用途，例如在文本分类或文本聚类中。我们将在下一章探讨这类任务——特别是使用 Keras 和 spaCy 的文本分类。

在进入下一章之前，我们向读者展示以下链接；它们是讨论使用深度学习处理文本生成时的有效策略的博客帖子。

1.  NLP 最佳实践[ [16](http://ruder.io/deep-learning-nlp-best-practices/index.html#bestpractices) ]
2.  深度学习和表示[ [17](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)
3.  神经网络的不合理有效性[ [10](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
4.  2017 年最佳 NLP 和 DL[[18](https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/)]



# 参考

[1]感知器:
[https://en.wikipedia.org/wiki/Perceptron](https://en.wikipedia.org/wiki/Perceptron)

[2]前馈神经网络:
[https://en.wikipedia.org/wiki/Feedforward_neural_network](https://en.wikipedia.org/wiki/Feedforward_neural_network)

[3]生物启发计算:
[https://en.wikipedia.org/wiki/Bio-inspired_computing](https://en.wikipedia.org/wiki/Bio-inspired_computing)

[4] By Glosser.ca - Own work，衍生文件:人工神经网络. svg，CC By-SA 3.0:
[https://commons.wikimedia.org/w/index.php?curid=24913461](https://commons.wikimedia.org/w/index.php?curid=24913461)

【5】使用谷歌多语言神经机器翻译系统的零拍翻译:
[https://research . Google blog . com/2016/11/Zero-Shot-Translation-with-Google . html](https://ai.googleblog.com/2016/11/zero-shot-translation-with-googles.html)

[6]卷积神经网络:
[https://en.wikipedia.org/wiki/Convolutional_neural_network](https://en.wikipedia.org/wiki/Convolutional_neural_network)

[7]循环神经网络:
[https://en.wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network)

[8]强化学习:
[https://en.wikipedia.org/wiki/Reinforcement_learning](https://en.wikipedia.org/wiki/Reinforcement_learning)

【9】https://en.wikipedia.org/wiki/Long_short-term_memory:

[10]RNNs 的不合理有效性:
[http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)

[11]认识 http://colah.github.io/posts/2015-08-Understanding-LSTMs/

【12】千层面:
[https://github.com/Lasagne/Lasagne](https://github.com/Lasagne/Lasagne)

【十三】街区:
[https://github.com/mila-udem/blocks](https://github.com/mila-udem/blocks)

[14]Caffe:
[http://Caffe . berkeleyvision . org/tutorial/net _ layer _ blob . html](http://caffe.berkeleyvision.org/tutorial/net_layer_blob.html)

[15]文字生成:
[https://github . com/kirit 93/Personal/blob/master/Text _ Generation _ keras/Text _ Generation . ipynb](https://github.com/kirit93/Personal/blob/master/text_generation_keras/text_generation.ipynb)

[16]NLP 最佳实践的深度学习:
[http://ruder.io/deep-learning-nlp-best-practices/index.html](http://ruder.io/deep-learning-nlp-best-practices/index.html)

[17]深度学习、NLP 和表象:
[http://colah . github . io/posts/2014-07-NLP-RNNs-表象/](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)

[18]深度学习 2017:
[https://tryo labs . com/blog/2017/12/12/Deep-Learning-for-NLP-advances-and-trends-in-2017/](https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/)