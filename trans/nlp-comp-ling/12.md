<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# Word2Vec、Doc2Vec 和 Gensim

我们之前在整本书中多次谈到向量——它们用于理解并以数学形式表示我们的文本数据，我们使用的所有机器学习方法的基础都依赖于这些表示。我们将更进一步，*使用*机器学习技术来生成单词的矢量表示，更好地概括单词的意思。这种技术通常被称为**单词嵌入**，Word2Vec 和 Doc2Vec 是其中两种流行的变体。

*   Word2Vec
*   Doc2Vec
*   其他单词嵌入

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# Word2Vec

可以说是机器学习在文本分析中最重要的应用，Word2Vec 算法是一个迷人且非常有用的工具。顾名思义，它根据我们使用的语料库创建单词的向量表示。但是 Word2Vec 的神奇之处在于它如何设法捕获向量中单词的语义表示。论文*[[1](https://arxiv.org/pdf/1301.3781.pdf)]【米科洛夫等人，2013】，*单词和短语的分布式表示及其组合性* [ [2](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) ]【米科洛夫等人，2013】，*连续空间单词表示的语言规则* [ [3](http://www.aclweb.org/anthology/N13-1090) ]【米科洛夫等人，2013】为 Word2Vec 和 describe 奠定了基础*

 *我们提到过这些单词向量有助于表示单词的语义——这到底是什么意思？首先，这意味着我们可以对这些单词使用向量推理——最著名的例子之一来自 Mikolov 的论文，在那里我们看到如果我们使用单词 vectors 并执行(在这里，我们使用 V(单词)来表示单词的向量表示) **V** ( **国王** ) - **V** ( **男人** ) + **V** ( **女人**)，得到的向量最接近 **V 很容易看出为什么这是显著的——我们对这些单词的直观理解反映在单词的学习矢量表示中！**

这使我们能够在我们的文本分析管道中添加更多的功能——拥有向量的直观语义表示(通过扩展，文档——但我们将在稍后讨论)将会不止一次地派上用场。

寻找词对关系就是这样一种有趣的用法——如果我们定义两个词之间的关系，如法国:巴黎，使用适当的向量差，我们可以识别其他类似的关系——意大利:罗马，日本:东京是两个这样的例子，它们是使用 Word2Vec 找到的。我们可以像对待任何其他向量一样继续使用这些向量——通过添加两个向量，我们可以尝试获得我们认为是两个单词的*加法*的结果。比如 **V(越南)+ V(首都)**最接近 **V(河内)**的矢量表示。

这项技术究竟是如何导致对单词的这种理解的？Word2Vec 通过理解上下文来工作——特别是，哪些词会出现在某些词中？我们选择一个滑动窗口大小，并基于该窗口大小，尝试识别基于周围单词观察输出单词的条件概率。例如，如果句子是*，文本数据的个人性质总是增加一点额外的**动机**，这也可能意味着我们知道数据的性质，以及期望什么样的结果。*，我们的目标词是粗体字“动机”,我们尝试并计算出找到单词*动机*的几率。如果上下文是*,总是在窗口的左侧添加一个额外的*,而*也可能意味着在右侧添加*。当然，这只是一个说明性的例子——确切的训练程序要求我们选择窗口大小和维数以及其他细节。

进行 Word2Vec 训练的方法主要有两种，分别是**连续单词袋**模式( **CBOW** )和**跳克**模式。这些模型的基础架构在最初的研究论文中有描述，但是这两种方法都涉及到理解我们之前讨论过的上下文。Mikolov 和其他人写的论文提供了训练过程的进一步细节，由于代码是公开的，这意味着我们实际上知道引擎盖下发生了什么！

克里斯·麦考密克的博客文章[ [4](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) ， *Word2Vec 教程-跳格模型*，解释了跳格 Word2Vec 模型背后的一些数学直觉，阿德里安·科尔耶的文章[ [5](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/) ，*单词向量的惊人力量*讲述了我们可以用 word2vec 做的一些事情。如果你想更深入地了解 Word2Vec 的数学细节，这些链接会很有用，这是我们在本章中不讨论的话题。参考资料页面[ [6](http://mccormickml.com/2016/04/27/word2vec-resources/) ]包含了 Word2Vec 的理论和代码资源，如果您希望查找原始资料或其他实现细节，它也很有用。

虽然 Word2Vec 仍然是最流行的单词向量实现，但这不是第一次尝试，当然也不是最后一次——我们将在本章的最后一节讨论其他一些单词嵌入技术。现在，让我们开始自己使用这些单词向量。

Gensim 再次为我们提供帮助，可以说是该算法最可靠的开源实现，我们将探索如何使用它。* *<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 将 Word2Vec 与 Gensim 配合使用

虽然谷歌发布的原始 C 代码[ [7](https://code.google.com/archive/p/word2vec/) ]做了令人印象深刻的工作，但 Gensims 的实现是一个开源实现比原始实现更*高效的例子。*

Gensim 实现是在 2013 年发布原始算法时编写的——拉迪姆·řehůřek 的博客文章记录了在 Gensim 中实现相同功能时遇到的一些想法和问题，如果你想知道用 Python 编写 word2vec 的过程，这是值得一读的。涉及 Word2Vec 的交互式网络教程[ [9](https://rare-technologies.com/word2vec-tutorial/) ]非常有趣，它展示了我们之前谈到的一些 Word2Vec 的例子。如果您对在线运行 Gensim Word2Vec 代码感兴趣，它值得一看，也可以作为在 Gensim 中使用 Word2Vec 的快速教程。

我们现在将开始实际训练我们自己的 Word2Vec 模型。第一步，像我们使用的所有其他 Gensim 模型一样，包括导入适当的模型。

```
from gensim.models import word2vec
```

在这一点上，浏览一下`word2vec`类和`KeyedVector`类的文档是很重要的，我们都会经常用到它们。在文档页面中，我们列出了`word2vec.Word2Vec`类的参数。

1.  `sg`:定义训练算法。默认情况下(sg=0)，使用 CBOW。否则(sg=1)，使用 skip-gram。
2.  `size`:这是特征向量的维数。
3.  `window`:这是一个句子中当前单词和预测单词之间的最大距离。
4.  `alpha`:这是初始学习率(随着训练的进行，将线性下降到`min_alpha`)。
5.  `seed`:用于随机数发生器。每个单词的初始向量被播种以单词+ `str(seed)`的连接的散列。请注意，对于完全确定性可再现的运行，您还必须将模型限制为单个工作线程，以消除操作系统线程调度的排序抖动。(在 Python 3 中，解释器启动之间的再现性也需要使用 PYTHONHASHSEED 环境变量来控制散列随机化。)
6.  `min_count`:忽略所有总频率低于此的词。
7.  `max_vocab_size`:词汇构建时限制 RAM 如果有比这更多的独特的单词，然后修剪不常用的。每 1000 万个字类型需要 1 GB 左右的 RAM。设置为`None`表示无限制(默认)。
8.  `sample`:这是配置哪些高频词被随机下采样的阈值；默认值为 1e-3，有用范围为(0，1e-5)。
9.  使用这么多工作线程来训练模型(多核机器训练更快)。
10.  `hs`:如果为 1，将使用分层 softmax 进行模型训练。如果设置为 0(默认值)，并且负值为非零值，将使用负采样。
11.  `negative`:如果>为 0，将使用负采样，负采样的 int 指定需要抽取多少个*噪声字*(通常在 5-20 之间)。默认值为 5。如果设置为 0，则不使用负采样。
12.  `cbow_mean`:如果为 0，使用上下文词向量的和。如果为 1(默认值)，则使用平均值。仅在使用 CBOW 时适用。
13.  `hashfxn`:这是用于随机初始化权重的散列函数，以提高训练的可重复性。默认的是 Python 基本的内置散列函数。
14.  `iter`:这是对语料库的迭代次数。默认值为 5。

15.  `trim_rule`:词汇修整规则指定某些单词是否应该保留在词汇中，被修整掉，或者使用缺省值进行处理(如果字数超过< `min_count`则丢弃)。这可以是 None ( `min_count`将被使用)，或者是一个接受参数(`word`、`count`和`min_count`)并返回`utils.RULE_DISCARD`、`utils.RULE_KEEP`或`utils.RULE_DEFAULT`的 callable。请注意，如果给出了规则，它仅用于在`build_vocab()`期间删减词汇，并不作为模型的一部分存储。
16.  `sorted_vocab`:如果为 1(默认)，则在分配单词索引之前，按频率降序对词汇进行排序。
17.  `batch_words`:这是传递给工作线程(以及 cython 例程)的批量示例的目标大小(以文字表示)。默认值为 10000。(如果单个文本超过 10000 个单词，将通过更大的批量，但标准 cython 代码会截断到最大值)。

我们不会在我们的例子中使用或探索所有这些参数，但是它们仍然很重要——你的模型的微调在很大程度上依赖于此。当训练我们的模型时，我们可以使用我们自己的语料库或更通用的语料库——因为我们不希望就特定的主题或领域进行训练，我们将使用 Text8 语料库[ [10](http://mattmahoney.net/dc/textdata.html) ]，其中包含从维基百科中提取的文本数据。一定要先下载数据——我们通过在*实验程序*部分找到链接`text8.zip`来完成。

我们将或多或少地遵循附在本章末尾的 Jupyter 笔记本，它也可以在[这里](https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis/word2vec.ipynb)【13】找到。

```
sentences = word2vec.Text8Corpus('text8')
model = word2vec.Word2Vec(sentences, size=200, hs=1)
```

我们的模型将使用分级 **softmax** 进行训练，并将拥有 200 个特征。这意味着它具有分层输出，并在其最终层中使用 softmax 函数。softmax 函数是逻辑函数的一般化，该函数*将任意实值的 K 维向量 *z* 挤压成 K 维实值向量，其中每个条目都在范围(0，1)内，并且所有条目加起来都是 1。在这一点上，我们不需要理解数学基础，但是如果感兴趣，链接[1]到[3]会更详细地介绍这一点。*

打印我们的模型告诉我们:

```
print(model)
 Word2Vec(vocab=71290, size=200, alpha=0.025)
```

现在我们有了训练好的模型，让我们试试著名的*国王-男人+女人*的例子:

```
model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)[0]
```

这里，我们加上`king`和`woman`(它们是正参数)，减去`man`(它是负参数)，只选择元组中的第一个值。

```
(u'queen')
```

瞧啊。正如我们所料，`queen`是我们搜索与`woman`和`king`最相似，但与人类距离较远的词时最接近的词向量。请注意，由于这是一个概率训练过程，您可能会得到一个不同的单词，但仍然与单词的上下文相关。例如，像王座或帝国这样的词可能会出现。

我们也可以使用`most_similar_cosmul`方法 Gensim 文档[ [11](https://radimrehurek.com/gensim/models/word2vec.html) 描述这与传统的相似度函数略有不同，而是使用了奥马尔·利维和约夫·戈德堡在他们的论文[ [12](http://www.aclweb.org/anthology/W14-1618) ] *中描述的稀疏和显式单词表示中的语言规则*。积极的词仍然对相似性有积极的贡献，消极的词有消极的贡献，但是对支配计算的大距离的敏感度较低。考虑这个例子:

```
model.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])

[(u'queen', 0.8473771810531616),
 (u'matilda', 0.8126628994941711),
 (u'throne', 0.8048466444015503),
 (u'prince', 0.8044915795326233),
 (u'empress', 0.803791880607605),
 (u'consort', 0.8026778697967529),
 (u'dowager', 0.7984940409660339),
 (u'princess', 0.7976254224777222),
 (u'heir', 0.7949869632720947),
 (u'monarch', 0.7940317392349243)]
```

如果我们希望查找一个单词的矢量表示，我们需要做的就是:

```
model.wv['computer']
model.save("text8_model")
```

我们不会在这里显示输出，但是我们可以期望看到一个 200 维的数组，这是我们指定的大小。

如果我们希望将模型保存到磁盘上并再次使用，我们可以使用保存和加载功能来实现。这特别有用——我们可以保存和重新训练模型，或者进一步训练适应某个领域的模型。

```
model.save("text8_model")
model = word2vec.Word2Vec.load("text8_model")
```

Gensim 的神奇之处在于，它不仅仅赋予我们训练模型的能力——就像我们迄今为止看到的那样，它是 API，这意味着我们不必太担心数学工作，而是可以专注于利用这些词向量的全部潜力。让我们看看 Word2Vec 模型提供的其他一些漂亮的功能:

使用单词向量，我们可以确定列表中哪个单词离其他单词最远。Gensim 通过`doesnt_match`方法实现了这一功能，我们在这里举例说明:

```
model.wv.doesnt_match("breakfast cereal dinner lunch".split())

'cereal'
```

不出所料，与列表中其他词不匹配的一个词被挑了出来——在这里，它是麦片。我们还可以使用该模型来了解语料库中的单词有多相似或不同:

```
model.wv.similarity('woman', 'man')

0.6416034158543054 model.wv.similarity('woman', 'cereal')

0.04408454181286298 model.wv.distance('man', 'woman')

0.35839658414569464
```

在这种情况下，结果是不言自明的，正如预期的那样，单词`woman`和`cereal`不相似。在这里，`distance`仅仅是 1 -相似性。

我们可以使用`train`方法继续训练我们的 Word2Vec 模型——只需记住显式传递一个 epochs 参数，因为这是一种建议的方法，可以避免围绕模型自身进行多次训练的常见错误。Gensim 笔记本教程[ [14](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/online_w2v_tutorial.ipynb) ]带你了解如何使用 Word2Vec 进行在线培训。简而言之，它需要执行以下任务——构建新的词汇表，然后再次运行 train 功能。

一旦我们完成了模型的训练，我们建议你开始只使用模型的关键向量。到目前为止，您可能已经注意到，我们一直在使用键控向量(它只是一个用于存储向量的`Gensim`类)来执行我们的大多数任务—`model.wv`代表了这一点。为了释放一些 RAM 空间，我们可以运行以下命令:

```
word_vectors = model.wv
del model
```

我们现在可以执行我们在使用矢量这个词之前所做的所有任务。请记住，这不仅适用于 Word2Vec，甚至适用于所有的单词嵌入。

为了评估我们的模型做得有多好，我们可以在安装 Gensim 时加载的数据集上测试它。

```
model.wv.evaluate_word_pairs(os.path.join(module_path, 'test_data','wordsim353.tsv'))

((0.6230957719715976, 3.90029813472169e-39), SpearmanrResult(correlation=0.645315618985209, pvalue=1.0038208415351643e-42), 0.56657223796034)
```

在这里，为了确保找到我们的文件，我们必须指定模块路径——这是文件所在的文件夹`gensim/test`的路径。我们还可以通过运行下面的代码来测试我们的模型查找单词对和关系。

```
model.wv.accuracy(os.path.join(module_path, 'test_data', 'questions-words.txt'))
```

到目前为止，在我们的例子中，我们使用了一个我们自己训练的模型——有时这可能是一个相当耗时的练习，知道如何加载预训练的向量模型是很方便的。例如，Gensim 允许一个简单的界面来加载原始的 Google news 训练的 Word2Vec 模型(您可以从链接[9]下载这个文件)。

```
from gensim.models import KeyedVectors
# load the google word2vec model

filename = 'GoogleNews-vectors-negative300.bin'
model = KeyedVectors.load_word2vec_format(filename, binary=True)
```

我们的模型现在使用 300 维的词向量模型，我们可以再次运行我们之前运行的所有代码示例——结果不会有太大不同，但我们可以期待一个更复杂的模型。

Gensim 还允许类似的接口使用其他单词嵌入来下载模型——我们将在最后一节讨论这一点。我们现在可以训练模型，加载模型，并使用这些单词嵌入来进行实验！

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# Doc2Vec

我们知道文档的向量表示有多重要——例如，在各种聚类或分类任务中，我们必须将文档表示为向量。事实上，在本书的大部分内容中，我们已经研究了使用向量表示法的技术，或者研究了使用这些向量表示法的技术——主题建模、TF-IDF 和单词包是我们之前研究过的一些表示法。

在 Word2Vec 的基础上，kind 研究人员还实现了文档或段落的向量表示，通常称为 Doc2Vec。这意味着我们现在可以使用 Word2Vec 的语义理解能力来描述文档，并且可以在我们希望训练它的任何维度上使用它！

以前对文档使用 word2vec 信息的方法只是简单地对文档的单词向量进行平均，但这并不能提供足够细致的理解。为了实现文档向量，Mikilov 和 Le 简单地添加了另一个向量作为训练过程的一部分——他们称之为段落 id。类似于 word2vec，有两种初级训练方法——**分布式内存**版段落向量( **PV-DM** )和**单词**版段落向量( **PV-DBOW** )。它们是用于训练 Word2Vec 的 **CBOW** 和 **Skip Gram** 模型的变体，我们可以将其理解为通过添加标签或 ID 将上下文的概念扩展到段落。Mikolov 和 Le 的论文[[15](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)]*句子和文档的分布式表示*详细描述了该算法，如果您花力气阅读 Word2Vec 论文，这绝对值得一试！

为了更容易理解 Doc2Vec 的内部工作原理，对 Doc2Vec [ [16](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e) ]的简单介绍也有帮助。这篇博文向我们介绍了我们之前讨论过的训练方法，即 SkipGram 和 CBOW。

像往常一样，我们对理论不太感兴趣，而对这些算法的实际应用更感兴趣——所以让我们直接使用 Gensim for Doc2Vec！

Gensim 的 Doc2Vec 实现的一个主要区别是，它不期望一个简单的语料库作为输入——该算法期望标记或标签，我们也期望提供这作为我们输入的一部分。Gensim 帮助我们做到这一点。

```
gensim.models.doc2vec.LabeledSentence
```

或者，我们可以这样使用:

```
gensim.models.doc2vec.TaggedDocument

sentence = LabeledSentence(words=[u'some', u'words', u'here'], labels=[u'SENT_1'])
```

如果出现任何错误，也请尝试以下操作:

```
sentence = TaggedDocument(words=[u'some', u'words', u'here'], tags=[u'SENT_1'])
```

这里，`sentence`是我们的输入将会是什么样子的一个例子。对于我们的示例，我们将使用李新闻语料库，或多或少遵循李教程[[17](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb)]——我们熟悉这个语料库，之前在我们的主题建模练习中使用过它。需要注意的是，与 Word2Vec 类似，语料库的种类越多、规模越大，我们就可以更好地预期我们的训练结果。我们像以前一样加载语料库:

```
test_data_dir = '{}'.format(os.sep).join([gensim.__path__[0], 
                'test', 'test_data'])
lee_train_file = test_data_dir + os.sep + 'lee_background.cor'
lee_test_file = test_data_dir + os.sep + 'lee.cor'
```

为了构建我们的语料库，我们将使用`TaggedDocument`类。

```
def read_corpus(file_name, tokens_only=False):
    with smart_open.smart_open(file_name, encoding="iso-8859-1") as f:
        for i, line in enumerate(f):
            if tokens_only:
                yield gensim.utils.simple_preprocess(line)
            else:
                # For training data, add tags
                yield gensim.models.doc2vec.TaggedDocument(
                gensim.utils.simple_preprocess(line), [i])
```

在这里，我们只是添加文档编号作为我们的标签-如果我们有关于我们的数据的更进一步、更有用的信息，我们欢迎添加这些信息。在我们定义的读取 Lee 语料库的函数中，我们添加了一个只读取标记的参数——这是为了测试的目的。

```
train_corpus = list(read_corpus(lee_train_file))
test_corpus = list(read_corpus(lee_test_file, tokens_only=True))
```

从这里开始，Gensim 的简单 API 保持不变，为了定义和训练我们的模型，我们运行:

```
model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=100)
```

同样，我们将列出来自`Doc2Vec`类的所有参数。原始文件可以在[这里](https://radimrehurek.com/gensim/models/doc2vec.html)【18】找到。

1.  `dm`:定义训练算法。默认情况下(dm=1)，使用分布式内存(PV-DM)。否则，使用分布式单词包(PV-DBOW)。
2.  `size`:这是特征向量的维数。
3.  `window`:这是预测单词和用于文档内预测的上下文单词之间的最大距离。
4.  `alpha`:这是初始学习率(随着训练的进行，将线性下降到`min_alpha`)。
5.  `seed`:用于随机数发生器。请注意，对于完全确定性可再现的运行，您还必须将模型限制为单个工作线程，以消除操作系统线程调度的排序抖动。(在 Python 3 中，解释器启动之间的再现性也需要使用 PYTHONHASHSEED 环境变量来控制散列随机化。)
6.  `min_count`:忽略所有总频率低于此的词。
7.  `max_vocab_size`:词汇构建时限制 RAM 如果有比这更多的独特的单词，然后修剪不常用的。每 1000 万个字类型需要 1 GB 左右的 RAM。设置为“无”表示没有限制(默认)。
8.  `sample`:用于配置哪些高频词被随机下采样的阈值。
9.  `default`:这是 1e-3，值 1e-5(或更低)可能也有用，设置为 0.0 以禁用下采样。
10.  使用这么多工作线程来训练模型(多核机器训练更快)。
11.  `iter`:在语料库上的迭代次数。从 Word2Vec 继承的默认值是 5，但是 10 或 20 的值在已发布的*段落向量*实验中很常见。
12.  `hs`:如果为 1，将使用分层 softmax 进行模型训练。如果设置为 0(默认值)，并且负值为非零值，将使用负采样。
13.  `negative`:如果>为 0，将使用负采样，负采样的 int 指定应该抽取多少个干扰词(通常在 5-20 之间)。默认值为 5。如果设置为 0，则不使用负采样。

14.  `dm_mean`:如果为 0(默认)，使用上下文词向量的和。如果为 1，则使用平均值。仅在`dm`用于非串联模式时适用。
15.  `dm_concat`:如果 1，使用上下文向量的串联而不是求和/平均；默认值为 0(关闭)。注意串联会产生一个更大的模型，因为输入不再是一个(采样的或算术组合的)单词向量的大小，而是标签和上下文中所有单词串在一起的大小。
16.  `dm_tag_count`:这是使用`dm_concat`模式时，每个文档的预期恒定文档标签数；默认值为 1。
17.  `dbow_words`:如果设置为 1，则与 DBOW 文档向量训练同时训练单词向量(以跳格方式);默认值为 0(仅加快文档向量的训练)。
18.  `trim_rule`:词汇修整规则指定某些单词是否应该保留在词汇中，被修整掉，或者使用缺省值进行处理(如果字数超过< `min_count`则丢弃)。这可以是`None`(将使用`min_count`)，或者是接受参数(`word`、`count`和`min_count`)并返回`util.RULE_DISCARD`、`util.RULE_KEEP`或`util.RULE_DEFAULT`的 callable。注意，如果给定了规则，该规则只在`build_vocab()`期间使用，并不作为模型的一部分存储。

在我们的情况下，我们有一个相当小的语料库，所以我们决定 50 个维度，忽略低信息词的最小计数为 2，并且我们的训练算法迭代 100 次。

```
model.build_vocab(train_corpus)
model.train(train_corpus, total_examples=model.corpus_count, 
            epochs=model.epochs)
```

我们走吧！这样，我们就训练了 Doc2Vec 模型。这只是为了说明*如何着手建立语料库和训练模型——评估、评价和微调我们的模型是一个更加微妙的过程，也取决于我们的用例——我们可以通过尝试评估问答对或语义对来了解 Word2Vec 是如何做到这一点的。关于评估 Doc2Vec 的更详细的例子，以及代码例子，关于使用 IMDB 训练向量的 Gensim 笔记本[ [19](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb) 值得一看。*

在关于 Doc2Vec 的论文中，作者建议使用 PV-DBOW 训练方法以及 PV-DM 方法来训练模型。我们可以通过以下方式实现这一点:

```
from gensim.models import Doc2Vec

models = [
    # PV-DBOW
    Doc2Vec(dm=0, dbow_words=1, vector_size=200, window=8, min_count=10, epochs=50),

    # PV-DM w/average
    Doc2Vec(dm=1, dm_mean=1, vector_size=200, window=8, min_count=10, epochs=50),
]
```

然后，在开始培训之前，我们建立词汇。这里只需注意:`documents`是任何标记的文档，并且是一个占位符变量，我们可以使用`train_corpus`或者提供我们选择的不同文档。

```
models[0].build_vocab(documents)
models[1].reset_from(models[0])

for model in models:
    model.train(documents, total_examples=model.corpus_count, 
                epochs=model.epochs)
```

这给我们留下了两个训练好的模型，我们可以根据自己的意愿对它们进行评估。这个类在这里帮助了我们。

在运行这段代码之前，您可能需要运行`pip install testfixtures`。

```
from gensim.test.test_doc2vec import ConcatenatedDoc2Vec
new_model = ConcatenatedDoc2Vec((models[0], models[1]))
```

至于我们可以用 Doc2Vec 模型做什么，推断向量和搜索相似向量是更明显的应用。我们可以从[链接](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb) [17]或 Jupyter 笔记本的 Lee 数据集中看到这一点:

```
inferred_vector = model.infer_vector(train_corpus[0].words)
sims = model.docvecs.most_similar([inferred_vector])
print(sims)

[(0, 0.9216967225074768),
 (48, 0.822547435760498),
 (255, 0.7833435535430908),
 (40, 0.7805585861206055),
 (8, 0.7584196925163269),
 (33, 0.7528027892112732),
 (272, 0.7409536838531494),
 (9, 0.7000102400779724),
 (264, 0.6848353743553162),
 (10, 0.6837587356567383)]
```

请注意，在实际情况下，我们不会在训练集上测试最相似的向量，这只是为了说明如何使用这些方法。

我们可以看到，在与文档 0 最相似的文档列表中，ID 0 首先出现——这是显而易见的(还有冗余信息)。然而，当我们检查第 48 或 255 个文档时，事情变得有趣了。让我们看看文档 0 包含什么:

“今天强风将巨大的森林大火推向悉尼西南 goulburn 附近的 hill top 镇，数百人被迫撤离新南威尔士州南部高地的家园。新的大火迫使 hume 高速公路于下午 7 点左右关闭。随着风暴单元向东移动穿过蓝山，天气明显恶化，当局不得不决定疏散山顶外围街道上的居民 在新南威尔士州南部高地，估计居民已经离开家园前往附近的米塔贡。新南威尔士州农村消防部门表示，导致大火燃烧成手指形状的天气条件现在已经缓解，山顶及其周围的消防单位对保护所有财产持乐观态度，因为新南威尔士州新年前夜的大火不止燃烧。消防队员已被召集到古尔本南部的新火灾现场 虽然现阶段没有多少细节，但消防当局表示，它已经关闭了休姆高速公路的两个方向，同时悉尼西部的新火灾不再威胁克兰布鲁克地区的财产。在伊拉瓦拉悉尼猎人谷和北海岸的一些地方已经下雨，但气象局克莱尔·理查兹表示，雨水对缓解瀑布造成的全州仍在燃烧的数百起火灾几乎没有任何作用 她说，这些地区相当孤立，通常降雨量不到 5 毫米，在一些地方不到 5 毫米根本不重要，所以就降雨而言没有太大缓解，事实上，他们可能更多地阻碍了消防员的努力，因为这些雷暴伴随着阵风。”

简单浏览一下可以告诉我们它包含了关于火灾和消防员反应的信息。至于 48 号文件:

“今天早上，新南威尔士州数千名消防队员仍坚守在地面上，他们正在评估悉尼周围的火灾范围，在该州南海岸，消防队员正在与从悉尼西南的坎贝尔镇到皇家国家公园的火灾带进行战斗。数百人已经从悉尼南部和西南部的小村庄疏散。当局估计，在大悉尼地区，超过 14 所房屋被烧毁 悉尼北部的霍克斯伯里地区被毁，杰维斯湾的财产被毁。新南威尔士农村消防队的约翰·温特表示，消防队员主要关注的是从坎贝尔镇到海岸的火灾带，这将是一个非常困难的地区。我们预计皇家国家公园可能会在上午晚些时候受到火灾的影响。他说，从人口风险和对财产的威胁来看，这是肯定的 乐队将是我们最关心的地区在行动中看来最严重的火灾危险可能已经过去了虽然强风预计今天会让消防队员忙碌起来在过去的两天里大火已经燃烧了超过公顷昨天风速高达每小时公里在十几个地区煽动大火包括昆比恩康纳山万尼亚萨红山和黑山今天预计会有强风但消防当局有信心 他说:“他们有足够的资源来控制任何进一步的火灾。今明两天，该法案将实施全面禁火令。紧急服务部长特德·昆兰对消防队员的努力表示赞赏。所有人为了社区的利益，牺牲了自己的圣诞节，表现得非常出色。”

我们可以非常清楚地看到，上下文已经被 Doc2Vec 完美地捕捉到了！我们刚刚搜索了最相似的文档——想象一下，如果与文档的聚类和分类结合使用，Doc2Vec 可以带来多大的威力。我们敦促读者使用 Doc2Vec 代替 TF-IDF 或主题模型作为表示，重试第十章、*中的一些问题，对文本进行聚类和分类*。

我们现在有向量化的能力(通过语义理解！)我们的文字和文件。虽然 word2vec 和 doc2vec 仍然是最流行的矢量化算法，但它们并不是唯一的算法——让我们在下一节探索如何使用一些替代算法。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 其他单词嵌入

对于我们的矢量化任务，有大量的单词嵌入可供选择——这些方法的原始实现分散在不同的语言中，托管网站、二进制文件和存储库——但幸运的是，Gensim 再次伸出了援手，为大多数(如果不是全部)其他单词嵌入提供了实现或记录良好的包装器。

Gensim 有`WordRank`、`VarEmbed`和`FastText`的包装器，以及**庞加莱嵌入**和`FastText`的本地实现。Gensim 也有一个简洁的脚本来使用**手套嵌入**，这在比较不同种类的嵌入时很方便。

Gensim 的`KeyedVectors`类意味着我们有一个基类来使用我们所有的单词嵌入。文档页面[ [21](https://radimrehurek.com/gensim/models/keyedvectors.html) ]涵盖了您需要知道的大部分信息(尽管我们已经在 Word2Vec 的例子中使用了这些向量)。

我们需要知道的是，在我们完成模型训练后，更谨慎的做法是这样运行:

```
word_vectors = model.wv
```

此外，继续使用`word_vectors`完成我们所有的其他任务——最相似的单词，最不相似的单词，以及运行单词嵌入测试。值得看一看`KeyedVectors.py` [ [22](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py) 文件的代码，看看在引擎盖下发生了什么！

一旦我们知道了如何使用单词向量，我们就可以看看如何使用 Python 来启动和运行其他单词嵌入。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 手套

GloVe 是一种单词向量表示方法，其中在来自语料库的聚集的全局单词-单词共现统计上执行训练。这意味着像 Word2Vec 一样，它使用上下文来理解和创建单词表示。手套方法是由斯坦福大学自然语言处理实验室开发的——你可以在他们的网站上找到更多关于他们项目的信息。描述该方法的研究论文名为 *GloVe:单词表示的全局向量* [ [23](https://nlp.stanford.edu/pubs/glove.pdf) ，非常值得一读，因为它在描述他们自己的方法之前描述了 LSA 和 Word2Vec 的一些缺点。

GloVe 有多种实现方式，甚至在 Python 中也有多种实现方式——但是我们将只坚持使用这些向量的*,而不是训练它们。当然，如果人们希望训练他们自己的手套向量，这可以通过`glove_python` [ [24](https://github.com/maciejkula/glove-python) 或仅仅`glove` [ [25](https://github.com/JonathanRaiman/glove) 来完成。你也可以在[这里](https://github.com/stanfordnlp/GloVe)【26】看一下原来的斯坦福代码。*

像往常一样，我们将使用 Gensim 来加载这些向量。我们的第一步是下载(或训练)我们的手套向量。一旦我们保存了它们，我们就把 GloVe vector 格式转换成 Word2Vec 格式，这样我们就可以在 Gensim API 中继续使用它们。记得从[链接](https://nlp.stanford.edu/projects/glove/)【22】下载手套输入文件。

```
from gensim.scripts.glove2word2vec import glove2word2vec

glove_input_file = 'glove.6B.100d.txt'
word2vec_output_file = 'glove.6B.100d.txt.word2vec'
glove2word2vec(glove_input_file, word2vec_output_file)
```

这里，我们已经加载了 glove vectors 并将其转换为 word2vec 格式，并进一步保存到磁盘。我们加载它的方式与加载任何保存的矢量文件的方式相同。

```
from gensim.models import KeyedVectors
filename = 'glove.6B.100d.txt.word2vec'
model = KeyedVectors.load_word2vec_format(filename, binary=False)
```

我们的模型现在应该与我们的 word2vec 模型工作方式相同——尽管如果我们按照 GloVe 论文描述的结果来看会稍微好一点。让我们举一个例子:

```
model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)

[(u'queen', 0.7698540687561035)]
```

就像时钟一样，我们得到了预期的结果！

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# FastText

FastText 是脸书人工智能研究所开发的一种矢量表示技术。顾名思义，这是一种执行相同任务的快速有效的方法——由于其训练方法的性质，它最终也会学习形态学细节。FastText 是独一无二的，因为它可以为未知单词或词汇表以外的单词导出单词向量——这是因为通过考虑单词的形态特征，它可以*为未知单词创建*单词向量。

这在形态结构很重要的语言中变得特别有趣——土耳其语和芬兰语就是两个这样的例子。这也意味着，利用有限的词汇，仍然可以进行足够智能的单词嵌入。以英语为例，这意味着它能够理解像*迷人的*或*奇怪的*这样的词中的 *ly* 代表什么。我们可以进一步引申说，根据 FastText，*嵌入(奇怪)-嵌入(奇怪)~=嵌入(迷人)-嵌入(迷人)*。

我们看到 FastText 在实践中或多或少地捕捉到了这一点，并通过使用 Word2Vec 或 GloVe 等词的字符级分析做到了这一点。我们通过测量向量在语义任务和句法任务中的表现来测试单词嵌入的性能。由于形态学指的是单词的结构或语法，FastText 往往在这类任务中表现更好，Word2Vec 在语义任务中表现更好。

描述该方法的原始论文名为*用子词信息丰富词向量*，可以在 arxiv [ [27](https://arxiv.org/pdf/1607.04606.pdf) 上找到。脸书的实现可以在他们的 GitHub repo [ [28](https://github.com/facebookresearch/fastText) ]中找到。我们将使用 Gensim 来使用 FastText，它包含本机实现和包装器。博客文章[ [29](https://rare-technologies.com/fasttext-and-gensim-word-embeddings/) ]涵盖了我们之前讨论过的 FastText 和 Word2Vec 之间的一些比较，而笔记本[ [30](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb) ]有相同的代码示例。这篇博文是 Gensim 官方博客的一部分，并使用 Gensim 作为通用界面进行比较。

训练类似于我们处理过的其他 gensim 模型。为了使用和训练本机 Gensim 实现[ [31](https://radimrehurek.com/gensim/models/fasttext.html#module-gensim.models.fasttext) ]，我们可以运行以下代码，其中`data`是您希望训练模型的文本数据的占位符变量。

```
from gensim.models.fasttext import FastText

ft_model = FastText(size=100)
ft_model.build_vocab(data)

ft_model.train(data, total_examples=ft_model.corpus_count, 
               epochs=ft_model.iter)
```

我们也可以通过一个**包装器** [ [32](https://radimrehurek.com/gensim/models/wrappers/fasttext.html) ]来使用原始的 C++代码，尽管这需要我们首先下载代码。

```
from gensim.models.wrappers.fasttext import FastText

# Set FastText home to the path to the FastText executable
ft_home = '/home/bhargav/Gensim/fastText/fasttext'

# train the model
model_wrapper = FastText.train(ft_home, train_file)
```

使用从 FastText 生成的矢量与我们之前讨论的所有单词矢量操作类似，因此我们将不解释如何使用它们——为了简单说明，下面的 Jupyter 笔记本帮助——笔记本 1 [ [33](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb) ]，笔记本 2 [ [30](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb) 。

用 FastText 尝试的一个有趣的练习是看它如何评估词汇表中没有的单词。考虑这个例子:

```
print('dog' in ft_model.wv.vocab)
print('dogs' in ft_model.wv.vocab)

True
False
```

但是，尽管狗不在训练词汇表中，我们仍然可以为`dog`和`dogs`生成单词向量！对这些向量的快速观察也告诉我们，它们与我们预期的非常相似。我们可以进一步证实这一点:

```
print('dog' in model)
print('dogs' in model)

True
True
```

我们让用户使用 FastText 来测试 Gensim 提供的另一种方法。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# WordRank

WordRank，顾名思义，试图将嵌入作为一个排名问题来解决。其背后的思想仍然类似于 GloVe，其中我们使用单词的全局共现来生成单词嵌入。代码可以从 bitbucket [ [34](https://bitbucket.org/shihaoji/wordrank) ]下载，也可以使用 GitHub [ [35](https://github.com/shihaoji/wordrank) 。描述该方法的原始论文名为 *WordRank:通过鲁棒排序*学习单词嵌入，也可以在 arxiv [ [36](https://arxiv.org/pdf/1506.02761.pdf) 上找到。

同样，我们将使用 Gensim 的包装器来访问和使用 WordRank。在这里，数据是一个变量，它保存了您的个人 Gensim 安装的路径，后跟数据。还记得我们在 Lee 语料库中是如何做的吗——我们使用了`gensim.__path__[0]`。

```
from gensim.models.wrappers import Wordrank

wordrank_path = 'wordrank' # path to Wordrank directory
out_dir = 'model' # name of output directory to save data to
data = '../../gensim/test/test_data/lee.cor' # sample corpus

model = Wordrank.train(wordrank_path, data, out_dir, iter=21, 
                       dump_period=10)
```

在这个特殊的例子中，我们使用同一个 Lee 语料库来运行我们的训练和测试。

我们需要注意两个参数，`dump_period`和`iter`，当它转储嵌入文件并开始下一次迭代时，这两个参数需要同步。例如，如果你想要 20 次迭代后的结果，我们设置`iter=21`，并且`dump_period`可以是任何没有余数的倍数——对于 20，可以是 2、4、5 或 10。

Gensim 文档[ [37](https://radimrehurek.com/gensim/models/wrappers/wordrank.html) ]可以在[这里](https://radimrehurek.com/gensim/models/wrappers/wordrank.html)找到，也可以看看基础教程[ [38](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/WordRank_wrapper_quickstart.ipynb) 。

一些警告——窗口大小为 15 可获得最佳结果，100 个时期比 500 个时期更好，因为训练时间可能会很长。同样，与其他嵌入一样，我们在所有单词向量中使用包含相同方法的`KeyedVectors`类。为了对 FastText、word2vec 和 WordRank 进行比较，博客文章[ [39](https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-word2vecs-canute/) 和 Jupyter notebook [ [40](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb) ]将带你一探究竟。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 瓦尔梅德

Varembed 是我们将讨论的第四种单词嵌入方法，和 FastText 一样，它利用形态学信息来生成单词向量。描述该方法的原始论文题为*概率神经单词嵌入的形态学先验*，可以在 arxiv [ [41](https://arxiv.org/pdf/1608.01056.pdf) 上找到。

类似于我们的手套向量，我们不能用新单词更新我们的模型，并且需要训练新的模型。关于训练我们自己的模型的信息可以在包含代码的原始[ [42](https://github.com/rguthrie3/MorphologicalPriorsForWordEmbeddings) 中找到。

Gensim 附带了在 Lee 数据集上训练的 Varembed 单词嵌入，因此我们将利用这一点来说明如何设置模型。你可以找到 Varembed [ [43](https://radimrehurek.com/gensim/models/wrappers/varembed.html) ]的文档。在这里，Varembed 是一个变量，它保存您的个人 Gensim 安装和测试数据的路径。还记得我们在 Lee 语料库中是如何做的吗——我们使用了`gensim.__path__[0]`。

```
from gensim.models.wrappers import varembed

varembed_vectors = '../../gensim/test/test_data/varembed_leecorpus_vectors.pkl'

model = varembed.VarEmbed.load_varembed_format(vectors=varembed_vectors)
```

我们之前提到了 Varembed 如何使用形态信息——我们可以通过添加这些信息来相应地调整我们的向量。同样，Gensim 附带了这些形态学信息。

```
morfessors = '../../gensim/test/test_data/varembed_leecorpus_morfessor.bin'
model = varembed.VarEmbed.load_varembed_format(vectors=varembed_vectors, morfessor_model=morfessors)
```

一旦我们的模型被加载，我们使用它的方法类似于我们的其他单词嵌入。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 庞加莱

我们要看的最后一个单词嵌入技术是庞加莱嵌入，也是由脸书人工智能研究所的优秀人员开发的。总的想法是使用单词的图形表示来更好地理解单词之间的关系并生成单词嵌入。庞加莱嵌入也可以使用这种图形表示来捕获分层信息——在题为*庞加莱嵌入用于学习分层表示*的原始论文[ [44](https://arxiv.org/pdf/1705.08039.pdf) 中，通过使用 **WordNet** 名词层次来学习这种分层信息。这些信息是在双曲空间中计算的，而不是在传统的欧几里得空间中计算的——这使我们能够更好地捕捉等级的概念。

Gensim 的笔记本目录包含训练这些嵌入所需的数据。我们可以使用以下内容来访问它:

```
import os

poincare_directory = os.path.join(os.getcwd(), 'docs', 'notebooks', 
                                  'poincare')
data_directory = os.path.join(poincare_directory, 'data')
wordnet_mammal_file = os.path.join(data_directory, 
                                   'wordnet_mammal_hypernyms.tsv')
```

为了使用这些数据来训练我们的模型，我们运行:

```
from gensim.models.poincare import PoincareModel, PoincareKeyedVectors, PoincareRelations
relations = PoincareRelations(file_path=wordnet_mammal_file, delimiter='t')
model = PoincareModel(train_data=relations, size=2, burn_in=0)
                      model.train(epochs=1, print_every=500)
```

我们也可以使用我们自己的关系来训练我们的模型。在这种情况下，每个*关系*只是一对节点。Gensim 也有预先训练好的模型，我们可以使用:

```
models_directory = os.path.join(poincare_directory, 'models')
test_model_path = os.path.join(models_directory, 
                  'gensim_model_batch_size_10_burn_in_0_epochs_50_neg_20_dim_50')
                  model = PoincareModel.load(test_model_path)
```

我们可以对我们的庞加莱模型使用标准的单词嵌入方法，也可以使用与图形相关的信息，比如`closest_child`、`closest_parent`和`norm`。

有关该型号的更多信息，请参考以下内容:

文献[45]:[https://radimrehurek.com/gensim/models/poincare.html](https://radimrehurek.com/gensim/models/poincare.html)

评估[46]:[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Poincare % 20 evaluation . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Poincare%20Evaluation.ipynb)

training[47]:[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Poincare % 20 tutorial . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Poincare%20Tutorial.ipynb)

博文[48]:[https://rare-technologies . com/implementing-poincare-embeddings/](https://rare-technologies.com/implementing-poincare-embeddings/)

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 摘要

在这一章中，我们探讨了文本分析的主要创新之一，单词嵌入或单词向量。单词向量是独一无二的，它不仅是我们表示文档和单词的一种方式，也提供了一种新的看待单词的方式。Word2Vec 的成功导致了各种单词嵌入方法的爆发，每种方法都有自己的特点、优点和缺点。我们不仅了解了流行的 Word2Vec 和 Doc2Vec 实现，还了解了其他五种单词嵌入方法——所有这些方法在 Gensim 生态系统中都得到了很好的支持，使它们易于使用。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 参考

[1] *向量空间中单词表征的有效估计*【米科洛夫等 2013】:
[https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf)

[2] *单词和短语的分布式表示及其组合性*【米科洛夫等人 2013】:
[https://papers . nips . cc/paper/5021-单词和短语的分布式表示及其组合性. pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)

[3] *连续空间词表征的语言规律*【米科洛夫等 2013】:
[http://www.aclweb.org/anthology/N13-1090](http://www.aclweb.org/anthology/N13-1090)

[4] Word2Vec 教程-skip-gram 模型:
[http://mccormickml . com/2016/04/19/word 2 vec-Tutorial-The-Skip-Gram-Model/](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
【5】词向量的惊人力量:
[https://blog . acolyer . org/2016/04/21/The-Amazing-power-of-word-vectors/](https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/)

[6] Word2Vec 资源:
[http://mccormickml.com/2016/04/27/word2vec-resources/](http://mccormickml.com/2016/04/27/word2vec-resources/)

[7]原 C 字 2Vec 代码:
[https://code.google.com/archive/p/word2vec/](https://code.google.com/archive/p/word2vec/)

[8] *用 Word2Vec 和 Gensim 进行深度学习:*
[https://rare-technologies . com/Deep-Learning-with-word 2 vec-and-Gensim/](https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/)

[9]互动 Word2Vec 教程:
[https://rare-technologies.com/word2vec-tutorial/](https://rare-technologies.com/word2vec-tutorial/)

[10] text8 数据文件:
[http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)

[11] Word2Vec 型号:
[https://radimrehurek.com/gensim/models/word2vec.html](https://radimrehurek.com/gensim/models/word2vec.html)

[12] *稀疏词表征和显性词表征的语言规律:*
[【http://www.aclweb.org/anthology/W14-1618】](http://www.aclweb.org/anthology/W14-1618)

[13] Word2Vec/Doc2Vec 笔记本:
[https://github . com/bhargavader/personal/blob/master/notebooks/text _ analysis/word 2 vec . ipynb](https://github.com/bhargavvader/personal/blob/master/notebooks/text_analysis/word2vec.ipynb)

[14]Online word 2 vec:
[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Online _ w2v _ tutorial . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/online_w2v_tutorial.ipynb)

[15] *分发陈述的句子和文件*:
[https://cs.stanford.edu/~quocle/paragraph_vector.pdf](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)

[16]温柔介绍 doc 2 vec:
[https://medium . com/scale about/A-gentle-introduction-to-doc 2 vec-db 3 e 8 c 0 CCE 5 e](https://medium.com/scaleabout/a-gentle-introduction-to-doc2vec-db3e8c0cce5e)

[17] Doc2Vec Lee 教程:
[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc 2 vec-Lee . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb)

[18]doc 2 vec Gensim:
[https://radimrehurek.com/gensim/models/doc2vec.html](https://radimrehurek.com/gensim/models/doc2vec.html)

[19]doc 2 vec IMDB:
[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc 2 vec-IMDB . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb)

[20]keyed vectors:
[https://radimrehurek.com/gensim/models/keyedvectors.html](https://radimrehurek.com/gensim/models/keyedvectors.html)

[21] KeyedVectors 文件:
[https://github . com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyed vectors . py](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/models/keyedvectors.py)

[22]手套:
[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)

[23] GloVe:单词表示的全局向量:
[https://nlp.stanford.edu/pubs/glove.pdf](https://nlp.stanford.edu/pubs/glove.pdf)

[24]手套蟒:
[https://github.com/maciejkula/glove-python](https://github.com/maciejkula/glove-python)

[25]手套:
[https://github.com/JonathanRaiman/glove](https://github.com/JonathanRaiman/glove)

[26]斯坦福德手套:
[https://github.com/stanfordnlp/GloVe](https://github.com/stanfordnlp/GloVe)

[27] *用子词信息丰富词向量:*
[https://arxiv.org/pdf/1607.04606.pdf](https://arxiv.org/pdf/1607.04606.pdf)

[28] fastText: 
[https://github.com/facebookresearch/fastText](https://github.com/facebookresearch/fastText)

[29] *FastText 和 gensim 单词嵌入:*
[https://rare-technologies . com/fast text-and-gensim-word-embedding/](https://rare-technologies.com/fasttext-and-gensim-word-embeddings/)

[30] FastText 对比笔记本:
[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word 2 vec _ fast text _ comparison . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb)

[31] Gensim fastText: 
[https://radimrehurek.com/gensim/models/fasttext.html#module-gensim.models.fasttext](https://radimrehurek.com/gensim/models/fasttext.html#module-gensim.models.fasttext)

[32] fastText wrapper: 
[https://radimrehurek.com/gensim/models/wrappers/fasttext.html](https://radimrehurek.com/gensim/models/wrappers/fasttext.html)

[33] FastText Gensim 笔记本:
[https://github . com/RaRe-Technologies/Gensim/blob/develop/docs/notebooks/fast text _ tutorial . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/FastText_Tutorial.ipynb)

[34] WordRank: 
[https://bitbucket.org/shihaoji/wordrank](https://bitbucket.org/shihaoji/wordrank)

[35]word rank GitHub:
[https://github.com/shihaoji/wordrank](https://github.com/shihaoji/wordrank)

[36] *WordRank:通过鲁棒排序学习单词嵌入:*
[https://arxiv.org/pdf/1506.02761.pdf](https://arxiv.org/pdf/1506.02761.pdf)

[37]word rank Gensim:
[https://radimrehurek . com/Gensim/models/wrappers/word rank . html](https://radimrehurek.com/gensim/models/wrappers/wordrank.html)

[38] WordRank 教程:
[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word rank _ wrapper _ quick start . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/WordRank_wrapper_quickstart.ipynb)

[39]word rank blog-post:
[https://rare-technologies . com/word rank-embedding-crowned-is-most-similar-to-king-not-word 2 vecs-Canute/](https://rare-technologies.com/wordrank-embedding-crowned-is-most-similar-to-king-not-word2vecs-canute/)

[40] WordRank Jupyter 笔记本:
[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/word rank _ comparisons . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Wordrank_comparisons.ipynb)

[41] *概率神经词嵌入的形态学先验:*
[https://arxiv.org/pdf/1608.01056.pdf](https://arxiv.org/pdf/1608.01056.pdf)

[42]GitHub page var embed:
[https://GitHub . com/rguthrie 3/morphologicalpriorsforworddings](https://github.com/rguthrie3/MorphologicalPriorsForWordEmbeddings)

[43]var embed:
[https://radimrehurek . com/gensim/models/wrappers/var embed . html](https://radimrehurek.com/gensim/models/wrappers/varembed.html)

[44]庞加莱嵌入:
[https://arxiv.org/pdf/1705.08039.pdf](https://arxiv.org/pdf/1705.08039.pdf)

[45]文献:
[https://radimrehurek.com/gensim/models/poincare.html](https://radimrehurek.com/gensim/models/poincare.html)

[46]评测:
[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Poincare % 20 Evaluation . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Poincare%20Evaluation.ipynb)

[47]培训:
[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Poincare % 20 tutorial . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Poincare%20Tutorial.ipynb)

[48]博文:
[https://rare-technologies . com/implementing-poincare-embeddings/](https://rare-technologies.com/implementing-poincare-embeddings/)*