

# 四、spaCy的语言模型

而我们在[第一章](26004756-ab10-461a-b1ed-b254daa537ea.xhtml)、*中介绍了文本分析什么是文本分析？*，我们没有讨论构建文本分析管道背后的任何技术细节。在这一章中，我们将向您介绍 spaCy 的语言模型——这些将作为文本分析的第一步，也是我们管道中的第一个构件。在这一章中，我们将向读者介绍 spaCy，以及我们如何使用 spaCy 来帮助我们完成文本分析任务，并谈论它的一些更强大的功能，如**词性**-标记和**命名实体识别**-标记。最后，我们将通过一个例子来说明如何使用自然语言处理 Python 库 spaCy 快速有效地预处理数据。

我们将在本章中讨论以下主题:

*   宽大的
*   装置
*   标记文本
*   摘要
*   参考



# 宽大的

讨论了文本分析的一些基础知识之后，让我们首先进入我们将学习使用的第一个 Python 包- **spaCy** [ [1](https://spacy.io/) 。

spaCy 将自己描述为自然语言处理的工业力量，它肯定会尽最大努力来实现这一承诺。spaCy 专注于完成工作，而不是一种更学术的方法，它只提供了一种词性标注算法和一种命名实体识别器(每种语言)。这也意味着软件包不会因为不必要的特性而臃肿。

我们之前提到了学术方法——这是什么意思？自然语言处理和机器学习中的大量开源包通常由研究人员和学术界工作人员创建或维护。虽然它们最终会*工作*——但这些项目的目的并不是提供算法的最新实现。 **NLTK** [ [2](http://www.nltk.org/) ]就是这样一个例子，在这里，图书馆的主要焦点是给学生和研究人员提供一个可以玩的工具包。另一方面，spaCy 可以非常令人满意地用于生产代码中——这意味着您可以期望它在真实世界的数据上执行，并且如果有足够的远见，它也可以是可伸缩的。

spaCy 的创建者和维护者 Matt Honnibal 的博客文章[ [3](https://explosion.ai/blog/dead-code-should-be-buried) ]更详细地介绍了开源 NLP 库面临的问题，以及 spaCy 的哲学。问题的关键仍然是在一些库中缺乏监管和维护(例如**模式** [ [4](https://www.clips.uantwerpen.be/pattern) ，最近才尝试迁移到 Python 3)，在 NLTK 的情况下，过时的技术或者仅仅作为**包装器** [ [5](https://en.wikipedia.org/wiki/Wrapper_function) ]工具，提供允许您使用其他位置标记器或解析器的绑定。

然而，尽管如此，探索 NLTK 所提供的东西仍然是值得的——它仍然是研究传统 NLP 技术的一个相当方便的工具，并提供了各种语料库(如 brown corpus [ [6](https://en.wikipedia.org/wiki/Brown_Corpus) ])。这个[链接](http://www.nltk.org/book/ch02.html)【7】**是 NLTK 书的一部分，是开始探索这些语料库的一种方式。我们不会深入研究 NLTK 的内部工作原理，充分利用您的 NLP 项目不需要具备 NLTK 的先决知识。**

 **在本书中，我们将使用 spaCy (v2.0)进行文本预处理和计算语言学。以下是 spaCy 的特性:

1.  非破坏性标记化
2.  支持 21 种以上的自然语言
3.  5 种语言的 6 种统计模型
4.  预先训练的单词向量
5.  简单的深度学习集成
6.  词性标注
7.  命名实体识别
8.  标记依存句法分析
9.  句法驱动的句子分割
10.  语法和 NER 的内置可视化工具
11.  方便的字符串到哈希映射
12.  导出到 numpy 数据数组
13.  高效的二进制序列化
14.  简单的模型打包和部署
15.  最先进的速度
16.  稳健、经过严格评估的精确度

下面是一张在他们的[网站](https://spacy.io)上提到的具有空间特征的表格:

![](Images/ee0337b3-c21f-4b4e-bd97-285c568880d3.png)

图 3.1 事实与数据页面的功能比较** **

# 装置

让我们开始设置和安装 spaCy。spaCy 与 64 位 CPython [ [8](https://en.wikipedia.org/wiki/CPython) ] 2.6+∕3.3+兼容，可以在 Unix/Linux、macOS/OS X 和 Windows 上运行。CPython 是用 C 语言编写的 Python 的参考实现——我们不需要知道它背后的细节，如果你有一个稳定的 Python 安装在运行，很可能你的 CPython 模块也很好。最新的 spaCy 版本可以通过 **Pip** [ [9](https://pypi.org/) ](仅限源码包)和 **Conda** [ [10](https://conda.io/docs/) ]获得。Pip 和 conda 是两个 Python 包发行商。安装需要一个工作的构建环境。我们将使用 Python 3，尽管这些例子对于 Python 2 也是有效的。

Pip 仍然是最直接的选择，但是对于安装了 anaconda 的用户来说，他们将使用 conda。

```py
pip install -U spacy
```

当使用`pip`时，通常建议您在`virtualenv`工具中安装软件包，以避免修改系统状态。

因为我们将在整本书中下载大量的 Python 包，所以准确理解 Python 中的虚拟环境是如何工作的是有意义的——这篇[文章](http://docs.python-guide.org/en/latest/dev/virtualenvs/) [11]是学习相同内容的好资源。

```py
virtualenv env source env/bin/activate pip install spacy
```

希望到目前为止，您应该已经启动并运行了 spaCy

```py
import spacy
```

到您的 Python 中，终端应该让您验证 spaCy 安装。



# 解决纷争

现在，在安装过程中可能会出现一些问题；这可能是因为 CPython 安装的复杂性。如果您运行的是 Mac 系统，您可能需要运行以下命令:

```py
xcode-select -install
```

这将安装 Mac 命令行开发工具。

大多数可能出现的常见安装问题都在 Stack Overflow 和 spaCy GitHub 页面中有很好的记录。

以下两个链接有助于故障排除:

*   无法 Pip 安装(Mac) [ [12](https://github.com/explosion/spaCy/issues/269)
*   为空间(窗口)[ [13](https://stackoverflow.com/questions/43370851/failed-building-wheel-for-spacy) 构建轮子失败

一般来说，如果您使用的虚拟环境具有正确的 Xcode(适用于 Mac 用户)和 Python 依赖项，应该不会有无法解决的安装问题。

当我们在讨论 spaCy 时，重要的是要知道有哪些其他工具可用于类似的任务，以及 spaCy 与这些工具相比如何——spaCy 页面上的事实和数字[[14](https://spacy.io/usage/facts-figures)]页面进入 spaCy 性能背后的数字。

让我们继续我们空间的第一个用法——语言模型。



# 语言模型

spaCy 最有趣的特性之一是它的语言模型。语言模型是一个统计模型，它让我们执行我们想要的自然语言处理任务，比如词性标注和 NER 标注。这些语言模型没有打包在 spaCy 中，而是需要下载——我们将在本章的后面详细讨论*如何*下载这些模型。

不同的语言有不同的模型来执行这些任务，同样的语言也有不同的模型——这些模型之间的差异主要是统计上的，您可以根据您的用例使用不同的模型。不同的模型只是在不同的数据集上进行训练。它仍然是相同的底层算法。他们模型的空间文档让我们对他们如何工作有了更多的了解。

截至目前，已有适用于英语、德语、法语、西班牙语、葡萄牙语、意大利语和荷兰语的型号，并且这一数字有望增长。有关模型的更多信息，比如命名约定或版本控制，您可以访问模型概述页面[ [16](https://spacy.io/models/) 。在简要介绍如何创建我们自己的管道和模型之前，我们将更多地关注这些模型的使用。



# 安装语言模型

从 v1.7.0 开始，spaCy 的模型可以作为 Python 包安装。这意味着它们是应用的一个组件，就像任何其他模块一样。可以从下载 URL 或本地目录手动或通过 pip 安装模型。

下载和使用这些模型最简单的方法是使用 spaCy 的`download`命令。

```py
# out-of-the-box: download best-matching default model spacy download en # english model spacy download de # german model spacy download es # spanish model spacy download fr # french model spacy download xx # multi-language model # download best-matching version of specific model for your spaCy installation spacy download en_core_web_sm # download exact model version (doesn't create shortcut link) spacy download en_core_web_sm-2.0.0 --direct
```

`download`命令的作用是使用 pip 来安装模型，将其放在您的`site-packages`文件夹中，并创建一个快捷链接，允许您稍后轻松加载它。

例如，如果我们想要使用英语语言模型，我们首先在终端中运行以下命令:

```py
pip install spacy spacy download en
```

接下来，我们在 Python shell 中运行以下命令:

```py
import spacy nlp = spacy.load('en')
```

我们现在已经加载了英语语言模型，我们可以使用它通过管道处理我们的文本，如下所示:

```py
doc = nlp(u'This is a sentence.')
```

在 Python 3 中，字符串默认是 Unicode 的，然而，在 Python 2 中，我们需要用`u'`将字符串括起来。我们将在下一节讨论更多关于`doc`对象的性质，以及管道中到底发生了什么。

也可以通过`pip`下载模型——要使用`pip`直接下载模型，只需将`pip install`指向归档文件的 URL 或本地路径。要找到模型的直接链接，请前往模型发布[ [17](https://github.com/explosion/spacy-models/releases) ]并找到存档链接。

其中有些型号可能相当大，全英文型号超过 1 GB。

```py
# with external URL pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-1.2.0/en_core_web_md-1.2.0.tar.gz # with local file pip install /Users/you/en_core_web_md-1.2.0.tar.gz
```

默认情况下，这会将模型安装到您的`site-packages`目录中。然后，您可以使用`spacy.load()`通过它的包名来加载它，创建一个快捷链接来为它指定一个自定义名称，或者将其作为一个模块显式导入。

一旦我们通过 pip 或 spaCy 的下载器下载了一个模型，我们就可以调用`load()`方法，如下所示:

```py
import en_core_web_md

nlp = en_core_web_md.load()

doc = nlp(u'This is a sentence.')
```

spaCy 的模型使用页面[[15](https://spacy.io/usage/models)**详细介绍了如何使用自定义快捷链接手动下载模型，以及其他可能有用的信息——我们将在本书中进一步讨论这些主题(在[第 5 章](e7fc28c4-94a9-41aa-8836-ad4c9a55e880.xhtml)、*位置标记及其应用*、[第 6 章](6aa68517-4c79-4d41-8cc2-04b73b85e6f0.xhtml)、 *NER 标记及其应用*、和[第 7 章](bdd3d124-b0be-4848-8a54-a8913ab8214e.xhtml)， *依存解析*，关于词性标注，NER 标注，以及依存解析)，但是有必要快速浏览一下我们如何组织这些模型。**

**

# 安装——如何安装，为什么安装？

您选择如何加载您的模型是个人偏好和您正在工作的项目类型的问题。例如，对于较大的代码库，通常推荐本地导入，因为这将使模型与您现有的构建过程、持续集成工作流和测试框架的集成更加容易。您还可以像项目中使用的任何其他库或模块一样，将模型添加到您的`requirements.txt`文件中。请注意，需求文件是大多数 Python 项目中的标准特性。文档页面[ [18](https://pip.readthedocs.io/en/1.1/requirements.html) 对此有更详细的解释。它还会阻止你试图加载一个没有安装的模型，因为你的代码会立即引发一个`ImportError`错误，而不是在稍后调用`spacy.load()`时失败。

除了我们之前提到的语言，spaCy 还开始了意大利语、葡萄牙语、荷兰语、瑞典语、芬兰语、挪威语、丹麦语、匈牙利语、波兰语、希伯来语、孟加拉语、印地语、印度尼西亚语、泰语、中文(普通话)和日语的标记化工作。同样，由于 spaCy 是开源的，您可以为正在进行的工作做出贡献。

既然我们确切地知道了如何在我们的系统上获得模型，让我们开始询问更多关于这些模型的问题——它是如何执行位置标记或 NER 标记的？当我们通过管道传递 Unicode (Unicode 是一致编码的行业标准)时，会返回哪种对象？我们如何使用该对象进行预处理？我们将在下一节尝试回答这些问题，同时讨论 spaCy 必须提供的关于其模型的其他可能性，例如训练我们自己的模型或向 spaCy 添加新语言。



# 语言模型的基本预处理

在[第 1 章](26004756-ab10-461a-b1ed-b254daa537ea.xhtml)、*什么是文本分析？*，我们提到了预处理的重要性——毕竟，垃圾进来，垃圾出去，对吗？但是我们没有深入了解*如何*清理我们的脏数据。幸运的是，在自然语言处理中，这是一个经过充分研究的问题，当我们想要清理时，有许多不同的预处理技术、管道和想法可供我们使用。

从技术上来说，我们不需要一个专门帮助我们进行预处理的包——用 Python 进行简单的字符串操作就可以做到，尽管需要付出更多的努力。我们将使用 spaCy 来帮助我们进行预处理，尽管在理论上，例如，甚至可以使用 NLTK。那么，为什么还要费心使用 spaCy 呢？这是因为除了基本的预处理，它还能在一个处理步骤中实现更多的功能——这一点我们将在本章中很快看到。

特别是，我们将使用 spaCy 的语言模型来帮助我们进行预处理。在我们进入具体的预处理步骤之前，让我们先了解一下运行以下代码时会发生什么:

```py
doc = nlp(u'This is a sentence.')
```

当您在 Unicode 文本上调用`nlp`时，spaCy 首先将文本标记化以产生一个 Doc 对象。Doc 然后在几个不同的步骤中被处理，我们也称之为我们的*流水线*。

![](Images/0b0806f5-8116-48df-82a5-b2320e5424df.png)

图 3.2 默认管道



# 标记文本

你可以看到这个管道的第一步是标记化，这到底是什么？

标记化的任务是将文本分割成有意义的片段，称为标记。这些段可以是单词、标点、数字或其他特殊字符，它们是句子的组成部分。在 spaCy 中，标记器的输入是一个 Unicode 文本，输出是一个`Doc`对象[ [19](https://spacy.io/api/doc) ]。

不同的语言会有不同的标记化规则。让我们看一个英语中标记化如何工作的例子。让我们去公园吧。，这很简单，可以用适当的数字索引分解如下:

| Zero | 1 | Two | 3 | four | 5 | six |
| 让 | 我们 | 去 | 到 | 这 | 公园 | 。 |

这看起来非常像我们刚刚运行`text.split(' ')`时的结果——什么时候标记化需要更多的努力？

如果上一句是*我们去公园吧。*相反，分词器必须足够聪明，将*的 Let*拆分成*的 Let* 和*的*。这意味着要遵循一些特殊的规则。spaCy 的英语语言标记器在拆分句子后检查以下内容:

子字符串是否匹配标记化器异常规则？比如，*不要*不包含空格，而应该拆分成两个记号，*要*和*不要*，而*英国*应该始终保持一个记号。

前缀、后缀或中缀可以拆分吗？例如，标点符号，如逗号、句号、连字符或引号。

与管道的其他部分不同，我们不需要统计模型来执行标记化。全局和特定于语言的标记器数据通过`spacy/lang` [ [20](https://github.com/explosion/spaCy/tree/develop/spacy/lang) 文件夹中的语言数据提供，该文件夹只是一个包含特定于模型的数据的目录。记号赋予器异常定义了特殊情况，比如英语中的“不要”，需要拆分成两个记号:`{ORTH: "do"}` 和`{ORTH: "n't", LEMMA: "not"}`。前缀、后缀和中缀主要定义标点规则——例如，何时拆分句点(在句末)，以及何时保留包含句点的标记不变(缩写如 *N.Y.* )。这里，ORTH 指的是文本内容，LEMMA 指的是没有屈折后缀的单词。

![](Images/be082290-f6e9-42f0-8130-0136f4535fbc.png)

图 3.3 句子“我们去纽约吧！”的空间标记化示例

我们可以向记号赋予器添加我们自己的特例，也可以定制 spaCy 的记号赋予器类。如果我们构造自己的记号赋予器，我们可以像这样简单地添加它:

```py
nlp = spacy.load('en')
```

创建我们自己的标记器的细节在 spaCy 文档的语言特征[ [21](https://spacy.io/usage/linguistic-features#section-tokenization) 部分，尽管我们将在[第 5 章](e7fc28c4-94a9-41aa-8836-ad4c9a55e880.xhtml)、*词性标注及其应用*、[第 6 章](6aa68517-4c79-4d41-8cc2-04b73b85e6f0.xhtml)、 *NER 标注及其应用*和[第 7 章](bdd3d124-b0be-4848-8a54-a8913ab8214e.xhtml) *、* *依存解析*中详细介绍训练和创建我们自己的模型

因此，一旦我们将句子传递到 nlp 管道，第一步就是标记化——一旦完成，我们现在就要处理由标记组成的 Doc 对象——这是我们之前描述的句子的基本部分。一旦我们在文档中有了令牌，管道中的其他组件就会处理每个令牌。



# 词性(POS)-标注

我们之前描述的默认管道的第二个组件是**张量器**。

tensorizer 将 doc 的内部表示编码为一个浮点数组。这是一个必要的步骤，因为 spaCy 的模型是神经网络模型，只有*讲*张量——每个 Doc 对象都应该被张量化。我们作为用户不需要关心这个。在这一步之后，我们开始我们的第一个注释——词性标注。

在第一章中，我们简要地提到了词性标注，即用适当的词性来标记句子中的每个标记，如名词、动词等等。spaCy 使用统计模型来执行其词性标注。为了从令牌中获取注释，我们只需查找令牌上的`pos_`属性。

考虑这个例子:

```py
doc = nlp(u'John and I went to the park'')

for token in doc:
 print((token.text, token.pos_))
```

这将为我们提供以下输出:

```py
(u'John', u'PROPN')
(u'and', u'CCONJ')
(u'I', u'PRON')
(u'went', u'VERB')
(u'to', u'ADP')
(u'the', u'DET')
(u'park', u'NOUN')
(u'.', u'PUNCT')
('John', 'PROPN')
('and', 'CCONJ')
('I', 'PRON')
('went', 'VERB')
('to', 'ADP')
('the', 'DET')
('park', 'NOUN')
('.', 'PUNCT')
```

我们将在[第 4 章](ad7d7774-1bef-4a55-ae7c-0d77097a43b7.xhtml)、*Gensim–向量化文本和转换以及 n-grams* 中更详细地介绍词性标注并训练我们自己的词性标注员。到目前为止，知道什么是词性标注就足够了，例如，如果我们希望删除特定的词性，我们可以使用它来清理我们的文本。

管道的下一部分是解析器，它执行依赖解析。解析是指对符号串进行任何种类的分析以理解符号之间的关系，而依存解析是指理解这些符号之间的依存关系。例如，在英语中，这可以用于描述单个标记之间的关系，如主语或宾语。spaCy 有一个丰富的 API 来导航解析树。由于解析并不真正用于预处理，我们将跳过细节，把它留到下一章。

![](Images/dec40ac2-4e74-4da8-a612-204ff064551c.png)

图 3.4 依存解析示例



# 命名实体识别

现在我们有了管道的最后一部分，在这里我们执行命名实体识别。命名实体是一个被赋予名称的*现实世界对象*,例如，一个人、一个国家、一种产品或一个组织。spaCy 可以通过请求模型进行预测来识别文档中各种类型的命名实体。我们必须记住，由于模型是统计的，并且取决于它们被训练的例子，它们并不总是完美地工作，并且以后可能需要一些调整，这取决于你的用例——我们有一章专门用来更好地理解命名实体识别以及如何训练我们自己的模型。

命名实体可作为文档的`ents`属性:

```py
doc = nlp(u'Microsoft has offices all over Europe.') for ent in doc.ents:
 print(ent.text, ent.start_char, ent.end_char, ent.label_)

(u'Microsoft', 0, 9, u'ORG')
(u'Europe', 31, 37, u'LOC')
```

spaCy 具有以下内置实体类型:

*   人物，包括虚构的人物
*   国籍、宗教或政治团体
*   建筑、机场、高速公路、桥梁等等
*   `ORG`:公司、代理处、机构等等
*   国家、城市和州
*   非 GPE 地区、山脉和水体
*   物品、交通工具、食物等等(不是服务)
*   命名的飓风、战役、战争、体育赛事等等
*   书籍、歌曲等的名称
*   `LAW`:被命名为法律的文件
*   `LANGUAGE`:任何命名语言



# 基于规则的匹配

*   `ORTH`:令牌的精确逐字文本
*   `LOWER, UPPER`:令牌的小写和大写形式
*   `IS_ALPHA`:令牌文本由字母数字字符组成
*   `IS_ASCII`:令牌文本由 ASCII 字符组成
*   `IS_DIGIT`:令牌文本由数字组成
*   `IS_LOWER`、`IS_UPPER`、`IS_TITLE`:令牌文本有小写、大写和标题
*   `IS_PUNCT`、`IS_SPACE`、`IS_STOP`:令牌是标点、空白和停用词
*   `LIKE_NUM`、`LIKE_URL`、`LIKE_EMAIL`:令牌文本类似于数字、URL 和电子邮件
*   `POS`、`TAG`:令牌的简单扩展 POS 标签
*   `DEP`、`LEMMA`、`SHAPE`:令牌的依赖标签、引理和形状

SpaCy 的默认管道也执行基于规则的匹配。这进一步注释了包含更多信息的标记，在预处理过程中很有价值。以下是可用的令牌属性:

与之前的管道组件一样，我们可以添加自己的规则。但是现在，这些信息足够我们在预处理中使用了。

现在我们知道了当我们通过管道传递文本时，*spaCy 如何处理我们的文本，我们可以讨论常见的预处理技术。*



# 预处理

预处理文本的奇妙之处在于它几乎是直观的——我们去掉了任何我们认为不会在最终输出中使用的信息，保留了我们认为重要的信息。在这里，我们的信息是文字——有些文字并不总能提供有用的见解。在文本挖掘和自然语言处理社区，这些词被称为**停用词** [ [22](https://en.wikipedia.org/wiki/Stop_words) ]。

停用词是在我们对其运行任何文本挖掘或 NLP 算法之前从我们的文本中过滤掉的词。同样，我们想提醒大家注意的是，这并不是在所有情况下都是如此——如果我们想要找到文体上的相似之处或者理解作者是如何使用停用词的，我们显然需要停用词！

每种语言都没有通用的停用词列表，这在很大程度上取决于用例以及我们期望看到什么样的结果。通常，它是语言中最常见的词的列表，如 *of，the，want，to，*和*有*。

使用 spaCy，停用词很容易识别——每个标记都有一个`IS_STOP`属性，让我们知道这个词是否是停用词。每种语言的停用词列表可以在`spacy/lang` [ [20](https://github.com/explosion/spaCy/tree/develop/spacy/lang) 文件夹中找到。

我们也可以将自己的停用词添加到停用词列表中。例如:

```py
my_stop_words = [u'say', u'be', u'said', u'says', u'saying', 'field']
for stopword in my_stop_words:
 lexeme = nlp.vocab[stopword]
 lexeme.is_stop = True
```

我们也可以用这个来添加单词:

```py
from spacy.lang.en.stop_words import STOP_WORDS

print(STOP_WORDS) # <- Spacy's default stop words

STOP_WORDS.add("your_additional_stop_word_here")
```

当清理我们的文本时，我们可以简单地选择不向我们的语料库添加停用词。

你可能已经注意到，在前面的例子中，`say`、`saying`和`says`这些词几乎都向我们提供了相同的信息——除了语法上的差异，只看到这些词的一种表示不会影响我们的结果。

有两种流行的技术可以实现这一点，词干化和词汇化。词干提取通常包括截掉单词的末尾，遵循一些基本规则。例如，单词`say`、`saying`和`says`都将变成`say`。词干分析是没有上下文的，例如，不依赖于词性来做出决定。另一方面，词汇化通过形态分析来寻找词根。

斯坦福大学的 NLP 书籍[ [23](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html) ] 中的这一页提供了一个很好的解释，描述了两者及其差异。就我们而言，我们不需要担心我们从哪里得到我们的词根，只需要担心我们正在得到它们。在 spaCy 中，单词的词汇化形式通过`.lemma_` 属性访问。

现在，根据我们所知道的，我们可以做一些基本的预处理。我们来清理一下这句话:`the horse galloped down the field and past the 2 rivers.`。我们希望去掉停用词、数字，并将字符串转换成一个列表，以便以后使用。

```py
doc = nlp(u'the horse galloped down the field and past the river.')
sentence = []
for w in doc:
 # if it's not a stop word or punctuation mark, add it to our article!
 if w.text != 'n' and not w.is_stop and not w.is_punct and not w.like_num:
 # we add the lematized version of the word
 sentence.append(w.lemma_)
print(sentence)
```

通过使用`.is_stop`、`is_punct`和`w.like_num`属性，我们可以删除句子中我们不需要的部分。请务必注意，我们在句子后面附加了通过`w.lemma_`访问的单词的词汇化形式。

这是预处理后我们的输出:

```py
[u'horse', u'gallop', u'past', u'river']
```

我们可以根据我们的用例进一步删除或不删除单词。在我们的例子中，数字被认为是不重要的信息，但在某些情况下，它可能是重要的。例如，我们可能想要从一个句子中删除所有动词——在这种情况下，我们可以简单地检查特定标记的 POS 标签。

请注意，我们之前已经在停用词中添加了*字段*——正因为如此，我们在最后一句中没有字段。

spaCy 的管道以这样一种方式注释文本，我们可以非常容易地使用该信息来处理我们的文本。方便的是，我们可以在以后的文本处理中进一步使用这些信息，而不仅仅是在预处理中。通过 spaCy 管道运行我们的任何 NLP 任务都是有意义的，无论是自定义的还是其他方式，只需要我们将获得的大量信息和注释，几乎只需要五行代码。



# 摘要

spaCy 为我们提供了一种非常简单的方法来注释您的文本数据，通过语言模型，我们可以用大量信息来注释您的文本数据——不仅仅是标记和它是否是停用词，还包括词性、命名实体标签等等——我们还可以自己训练这些注释模型，为语言模型和处理管道提供大量功能！下载模型和使用虚拟环境也是这个过程的一个重要部分。我们现在将继续以机器可以理解我们的方式使用我们清理的数据——通过向量，以及我们需要什么样的 Python 库来实现同样的目的。



# 参考

[1]spaCy:
[https://spaCy . io](https://spacy.io)

[2]NLTK:
[http://www.nltk.org](http://www.nltk.org)

【3】死码应葬:
[https://explosion.ai/blog/dead-code-should-be-buried](https://explosion.ai/blog/dead-code-should-be-buried)

[4]图案:
[https://www.clips.uantwerpen.be/pattern](https://www.clips.uantwerpen.be/pattern)

[5]包装函数:
[https://en.wikipedia.org/wiki/Wrapper_function](https://en.wikipedia.org/wiki/Wrapper_function)

[6]布朗文集:
[https://en.wikipedia.org/wiki/Brown_Corpus](https://en.wikipedia.org/wiki/Brown_Corpus)

[7] NLTK 文集:
[http://www.nltk.org/book/ch02.html](http://www.nltk.org/book/ch02.html)

[8]CPython:
[https://en.wikipedia.org/wiki/CPython](https://en.wikipedia.org/wiki/CPython)

[9]pipi
https://pipi . python . org/pipi 的缩写形式

[10]康达:
[https://conda.io/docs/](https://conda.io/docs/)

[11]虚拟环境:
[http://docs.python-guide.org/en/latest/dev/virtualenvs/](http://docs.python-guide.org/en/latest/dev/virtualenvs/)

[12]不能 Pip 安装:
[https://github.com/explosion/spaCy/issues/269](https://github.com/explosion/spaCy/issues/269)

[13]为空间构建车轮失败:
[https://stack overflow . com/questions/43370851/Failed-building-wheel-for-spacy](https://stackoverflow.com/questions/43370851/failed-building-wheel-for-spacy)

[14]空间事实与数字:
[https://alpha.spacy.io/usage/facts-figures](https://alpha.spacy.io/usage/facts-figures)

[15]空间语言模型:
[https://alpha.spacy.io/usage/models#languages](https://alpha.spacy.io/usage/models#languages)

[16] spaCy 车型概述:
[https://alpha.spacy.io/models/](https://alpha.spacy.io/models/)

[17] spaCy 车型发布:
[https://github.com/explosion/spacy-models/releases](https://github.com/explosion/spacy-models/releases)

[18]需求文件:
[https://pip.readthedocs.io/en/1.1/requirements.html](https://pip.readthedocs.io/en/1.1/requirements.html)

[19]空间 Doc 对象:
[https://spacy.io/api/doc](https://spacy.io/api/doc)

[20]空间语言目录:
[https://github.com/explosion/spaCy/tree/develop/spacy/lang](https://github.com/explosion/spaCy/tree/develop/spacy/lang)

[21] spaCy 语言特征:
[https://alpha . spaCy . io/usage/language-features # section-tokenization](https://alpha.spacy.io/usage/linguistic-features#section-tokenization)

[22]停字:
[https://en.wikipedia.org/wiki/Stop_words](https://en.wikipedia.org/wiki/Stop_words)

[23]词汇化和词干化:
[https://NLP . Stanford . edu/IR-book/html/html edition/Stemming-and-lemma tization-1 . html](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)****