

# 十、文本聚类和分类

在上一章中，我们学习了主题模型，以及它们如何帮助我们组织和更好地理解我们的文档及其子结构。我们现在将继续我们的下一组机器学习算法，并针对两个特定的任务-聚类和分类。我们将了解这两个任务的直观推理是什么，以及如何使用流行的 Python 机器学习库 scikit-learn 来执行这些任务:

*   聚类文本
*   文本分类



# 聚类文本

到目前为止，我们看了分析文本，以更好地理解文本或语料库由什么组成。当我们试图进行词性标注或 NER 标注时，我们感兴趣的是知道什么样的单词出现在我们的文档中，当我们进行主题建模时，我们想知道隐藏在我们文本中的潜在主题。当然，我们可以使用我们的主题模型来尝试对文章进行聚类，但这不是它的目的；如果我们也尝试这样做，我们会傻傻地期待很好的结果。请记住，由于主题建模的目的是发现语料库中隐藏的主题，而不是将文档分组在一起，因此我们的方法并不适合这项任务。例如，在我们执行主题建模之后，一个文档可以由 30%的主题 1、30%的主题 2 和 40%的主题 3 组成。在这种情况下，我们不能使用这些信息进行聚类。

现在，让我们开始探索如何使用机器学习方法来完成本质上更加量化的任务:**聚类**和**分类**。聚类是一种流行的机器学习任务，经典聚类任务中使用的技术也可以用于文本。顾名思义，聚类是将数据点分组在一起或聚集在同一组中的任务，其中同一组中的点比其他组中的点彼此更相似。在我们的上下文中，数据点可以被认为是文档，或者在某些情况下，是单词。聚类是一个无监督的学习问题。在我们开始将我们的数据点分配给它们之前，我们并不知道这些集群或组(尽管我们可能知道我们可能会发现什么)。

分类是一项类似的任务，是通过包含已知类别成员的观察值(或实例)的训练数据集来识别新观察值属于哪一组类别(子群体)的问题。一个例子是将给定的电子邮件分配到*垃圾邮件*或*非垃圾邮件*类别，或者将报纸文章分配到预定的类别或组。

一个著名的聚类或分类任务的例子可以是 Iris flower 数据集[ [1](https://en.wikipedia.org/wiki/Iris_flower_data_set) ]，其中我们试图根据花瓣长度找出一朵花属于哪一类。用于这些目的的另一个流行的数据集是 MNIST 数据集[ [2](http://yann.lecun.com/exdb/mnist/) ]，它包含手写数字，这些数字应该被分类到它应该代表的数字之下。

文本聚类遵循标准聚类问题遵循的大部分原则，但我们必须记住一件事:文本分析中的大量维度。例如，在 Iris 数据集中，只有四个特征用于识别我们的类别或聚类。然而，在文本的情况下，当设置我们的问题时，我们必须处理整个词汇量。当然，我们会尽最大努力，使用我们之前讨论过的 SVD、LDA 和 LSI 等技术来降低维数。

虽然我们之前主要使用 Gensim 来执行我们的定量任务，并为计算语言学使用 spaCy，但我们将继续使用更传统的机器学习库 scikit-learn。事实上，我们已经在本书的前面介绍了 scikit-learn，但是从本章开始，我们可以期待更多地使用它。

当我们执行聚类和分类任务时，您可能经常会遇到 Word2Vec 和 Doc2Vec，这是将单词和文档表示为向量的两种方式。我们必须记住，它只是单词和文档的另一种向量表示，尽管其方式比我们迄今探索的方式更复杂。我们将在[第 12 章](020dd724-0a11-4ba3-9a7c-050df9dfa1e4.xhtml)、 *Word2Vec、Doc2Vec 和 Gensim* 中详细探讨 Word2Vec 和 Doc2Vec，并使用它们重温聚类和分类，但现在，理解它们作为我们可以为我们的聚类或分类算法提供更多精选信息的方式就足够了。



# 开始聚类

像我们之前应用的所有其他文本分析算法一样，最重要的步骤仍然是预处理步骤——去掉我们的停用词和词条。

一旦我们完成了这一步，下一步就是将我们的文档转换成我们最熟悉的向量表示。

因为我们正在处理 scikit-learn 的聚类和分类实现，所以让我们使用 scikit-learn 进行预处理。我们还应该利用这个机会来决定我们打算在实验中使用哪个数据集。虽然有许多可靠的选择，但我们将坚持使用流行的 20 个新闻组[ [3](http://qwone.com/~jason/20Newsgroups/) ]数据集。由于数据集与 scikit-learn 捆绑在一起，加载和使用它也变得很容易。

您可以按照 Jupyter 笔记本[ [4](https://github.com/bhargavvader/personal/blob/master/notebooks/clustering_classing.ipynb) ]上的聚类和分类了解全部细节；我们将使用那里的代码片段来解释这个过程。

为了开始访问我们的数据集，我们运行:

```py
from sklearn.datasets import fetch_20newsgroups 

categories = [ 
    'alt.atheism', 
    'talk.religion.misc', 
    'comp.graphics', 
    'sci.space', 
] 

dataset = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42) 

labels = dataset.target 
true_k = np.unique(labels).shape[0] 
data = dataset.data
```

让我们简单看一下到目前为止我们做了什么。`import`语句允许我们轻松地访问 20NG 数据集，出于示例的考虑，我们决定只选取 4 个类别。下面的[ [3](http://qwone.com/~jason/20Newsgroups/) 会给你完整的类别列表。我们通过选择所有子集来创建数据集，同时也对数据集进行洗牌，但使用随机状态集。像往常一样，我们现在必须将我们的文本数据转换为机器学习算法可以理解的形式——向量。

我们将使用 scikit-learn 的内置`TfidfVectorizer`来简化我们的工作:

```py
from sklearn.feature_extraction.text import TfidfVectorizer 

vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words='english', use_idf=True) 

X = vectorizer.fit_transform(data)
```

`X`对象现在是我们的输入向量，它包含数据集的 TF-IDF 表示。我们必须记住，当我们进行 TF-IDF 转换时，我们仍然在处理相当高维的数据。为了更好地理解数据的本质，将数据可视化是很有用的。我们可以通过对数据集进行主成分分析[ [5](https://en.wikipedia.org/wiki/Principal_component_analysis) ]来将维数减少到 2。PCA 是一种从数据集中寻找不相关(数学上，这些被称为*线性不相关*)成分的算法。通过从高维数据集中识别这些不相关的组件，我们有效地执行了降维。注意，我们*只是*这样做是为了可视化的目的；对于聚类问题，我们将尝试其他降维技术:

```py
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline

newsgroups_train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'sci.space'])
pipeline = Pipeline([ 
    ('vect', CountVectorizer()), 
    ('tfidf', TfidfTransformer()), 
])
X_visualise = pipeline.fit_transform(newsgroups_train.data).todense()

pca = PCA(n_components=2).fit(X_visualise)
data2D = pca.transform(X_visualise)
plt.scatter(data2D[:,0], data2D[:,1], c=newsgroups_train.target)
```

让我们简单讨论一下这段代码。我们再次加载数据，但是只有两个类别(我们想要可视化的类别)。我们对此运行了计数矢量器和 TF-IDF 转换，并拟合了 PCA 模型，其中我们只需要两个关键组件。通过绘制该图，我们可以了解数据集中的聚类是如何分离的:

![](Images/52ed701f-aeb1-494d-bb93-62e143e332e3.png)

图 10.1:可视化我们的数据集

值得注意的是，此处的轴仅代表 PCA 发现的两种成分。

现在让我们回到最初的向量`X`，并设置它为集群做好准备。在讨论主题模型时，我们讨论了它们如何作为一种降维技术。让我们使用**奇异值分解** ( **SVD** )和**潜在语义分析** ( **LSA** / **LSI** )(我们在[第八章](02e3238f-db9c-4d6e-ba96-70ba9db11dc0.xhtml)、*主题模型*主题建模中遇到过这些方法)来对这个例子进行降维。

**Note:** We have to re-normalize after we run our SVD on the dataset.

```py
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer

n_components = 5
svd = TruncatedSVD(n_components)
normalizer = Normalizer(copy=False)
lsa = make_pipeline(svd, normalizer)
X = lsa.fit_transform(X)
```

最后一个`X`是我们将要使用的输入。它已经被清洗，TF-IDF 改造，并进一步缩小了它的尺寸。现在已经准备好运行集群技术了！



# k 均值

K-means [ [6](https://en.wikipedia.org/wiki/K-means_clustering) ]是用于聚类的经典机器学习算法。直观上很容易理解。基于用户决定的预定数量的聚类，它尝试创建聚类。这是通过减少点与该点所分配到的相应质心的距离来实现的。这是一个迭代算法，并不断做这个过程，直到质心和指定的点不变。尽管我们没有必要继续下去，但还是有必要花时间来研究算法背后的理论。

将 K-means 与 scikit-learn 结合使用非常容易，scikit-learn 提供了两种实现[ [7](http://scikit-learn.org/stable/modules/clustering.html#k-means) ]，我们可以在小批量中使用，也可以不使用。在我们的代码中，我们允许用户在使用哪个选项之间切换:

```py
minibatch = True

if minibatch: 
    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1, 
                         init_size=1000, batch_size=1000) 
else: 
    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, 
                n_init=1) 
km.fit(X)
```

瞧啊。我们现在有一个*f*T2it 模型，它有四个不同的集群。与其将此形象化，不如让我们试着找出每个集群的热门词汇:

```py
original_space_centroids = svd.inverse_transform(km.cluster_centers_)  
order_centroids = original_space_centroids.argsort()[:, ::-1]
```

由于我们的 LSI 转换，前面的代码是必要的。

```py
terms = vectorizer.get_feature_names() 

for i in range(true_k): 
    print("Cluster %d:" % i) 
    for ind in order_centroids[i, :10]: 
        print(' %s' % terms[ind])

Cluster 0:
 graphics
 space
 image
 com
 university
 nasa
 images
 ac
 programposting
Cluster 1:
 god
 people
 com
 jesus
 don
 say
 believe
 think
 bible
 just
Cluster 2:
 space
 henry
 toronto
 nasa
 access
 com
 digex
 pat
 gov
 alaska
Cluster 3:
 sgi
 livesey
 keith
 solntze
 wpd
 jon
 com
 caltech
 morality
 moral
```

**Note:** You might see different results, as machine learning algorithms do not produce the exact same results each time.

我们可以看到四个集群中的每一个都代表了我们最初选择的四个类别——我们的集群结果很好！我们可以进一步使用我们的 fit 模型来预测新文档属于哪个聚类；只需记住对新文档也运行相同的预处理步骤。这很简单:

```py
km.predict(X_test)
```

我们在这里做了什么？我们加载数据集，选择四个类别，运行预处理步骤，可视化我们的数据，训练 K-means 模型，并打印每个聚类的热门词，看看它们是否有意义——它们做得很好。因为我们知道有四个类别，所以我们选择我们的 K-均值聚类有四个聚类，即， *K=4* 。

我们可以自由地对预处理进行更多的尝试，并且可以通过不同的步骤得到不同的结果。现在让我们探索另一种形式的集群。



# 分层聚类

在我们深入研究分层集群之前，浏览一下关于集群的 scikit-learn 文档会非常方便。我们必须记住，在 scikit-learn 中使用不同的模型非常容易，并且聚类过程中的几乎所有其他步骤始终保持不变。

我们将使用 Ward 的算法/方法[ [9](https://en.wikipedia.org/wiki/Ward's_method) ]来尝试层次聚类。该算法基于减少每个聚类内的方差的思想，并使用距离度量来做到这一点。Ward 的方法是各种层次聚类算法中最早使用的方法之一，这些算法基于构建聚类并将它们排列成层次结构。在我们的例子中，我们将使用树状图[ [10](https://en.wikipedia.org/wiki/Dendrogram#Clustering_example) ]来表示我们的层次聚类。

要为此方法设置数据集，我们必须首先创建一个具有成对距离的矩阵。我们可以用 scikit 很容易地做到这一点——像这样学习:

```py
from sklearn.metrics.pairwise import cosine_similarity
dist = 1 - cosine_similarity(X)
```

现在我们已经准备好了距离矩阵，我们将使用 SciPy 的`ward`和`dendrogram`函数:

```py
from scipy.cluster.hierarchy import ward, dendrogram 

linkage_matrix = ward(dist)  
fig, ax = plt.subplots(figsize=(10, 15)) # set size 
ax = dendrogram(linkage_matrix, orientation="right")
```

就是这样！SciPy 为我们做了所有艰苦的工作，并向我们展示了这张漂亮的图表。树状图让我们了解了文档可以排列在哪些簇中。x 轴表示文档的名称或索引，但是现在看不到了，因为文档太多了。y 轴表示每个聚类层次之间的距离:

![](Images/73fdd0e2-f528-4d2f-a4c2-3ffe02f4e2e4.png)

图 10.2 在 SciPy 中使用 Ward 算法进行文本聚类生成的树状图示例

我们可以看到，在这种特殊情况下，这可能不是聚类的最佳方法，主要是因为文档的数量。更难看出文档之间的关系和集群所代表的内容。不过，对于较小的语料库来说，尝试这样做可能会非常方便！

以下教程(参考资料部分的链接)也说明了我们尝试过的方法，但是是在不同的数据集上:

*   布兰多姆玫瑰集群[ [11](http://brandonrose.org/clustering)
*   使用文本[[12](https://de.dariah.eu/tatom/working_with_text.html)]

我们想再次强调，在将我们的语料库提供给聚类算法之前，要使用不同的维度缩减和向量表示。Word2vec 和 Doc2Vec 都提供了非常有趣的方法来做到这一点，Gensim 为此提供了现成的实现！https://towards data science . com/automatic-topic-clustering-using-doc 2 vec-E1 CEA 88449 c[13]上关于用 Word2Vec 进行聚类的博文也试图解释这一点。

我们现在将继续对文本文档进行分类，这是机器学习算法在文本中的另一种流行用法。



# 文本分类

在上一节中，我们讨论了聚类，这是一种无监督的学习算法。另一方面，分类是一种监督学习算法。有监督和无监督是什么意思？在我们之前的例子中，我们有*标签*或者真值。这是关于文档实际属于哪个类或标签的信息。但是你也会注意到我们从未使用过这些信息。当我们训练我们的模型时，我们从不使用标签。这种学习称为无监督学习，聚类是无监督学习任务的一个常见示例。

在分类问题中，我们知道我们想要将文档或数据点分配到的类别，并且我们使用该信息来训练我们的模型。事实上，我们很快就会看到，除了我们将关注我们的标签，以及我们将使用不同的机器或模型进行训练之外，我们的聚类和分类方法几乎没有任何变化。

就像我们在整本书中一直强调的那样，在我们开始将文本输入任何机器学习管道之前，确保我们的文本被清理和矢量化是很重要的。我们的步骤将保持不变，虽然我们有自由改变周围的东西一点，直到我们得到的准确性或性能，我们正在寻找。

我们将使用朴素贝叶斯分类器[ [14](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) ]和支持向量机[ [15](https://en.wikipedia.org/wiki/Support_vector_machine) ]分类器来帮助我们完成分类任务。虽然这些机器的数学本质超出了我们的范围，但是它们的 scikit-learn 文档(NB [ [16](http://scikit-learn.org/stable/modules/naive_bayes.html) 、SVM [ [17](http://scikit-learn.org/dev/modules/svm.html) )提供了一些直观的解读:

![](Images/b10b0806-2f37-41da-a9a3-e79395fb4dca.jpg)

图 10.3:使用核变换输入空间- SVM。来源:Quora，支持向量机(SVM)通俗地说是什么意思？[19]

支持向量机通过使用内核[ [18](https://en.wikipedia.org/wiki/Kernel_method) ]对输入空间进行*变换*来工作，以便我们可以最好地绘制一条线(或者在更高维度中，如文本-平面的情况)来区分类别。核是帮助我们转换维度空间的数学函数。

简而言之，**朴素贝叶斯分类器**通过应用贝叶斯定理和*朴素*假设每对特征之间的独立性来工作；我们可以预测文档可能属于哪个类别。人们必须注意到，独立性通常是假定的。当这种情况不成立时，称为*幼稚*。使用标签计算文档是否属于某个类别的先验概率。本质上，我们试图找出哪些单词预测哪个类。代码本身非常简单:唯一的区别是我们也使用标签来训练我们的机器。这是代码片段看起来的样子，但是你可能想要再次参考笔记本以防你偶然发现任何错误。不要忘记在训练模型之前转换数据，如果是稀疏数组，请运行`X = X.toarray()`:

```py
from sklearn.naive_bayes import GaussianNB 
gnb = GaussianNB() 
gnb.fit(X, labels) 

from sklearn.svm import SVC 
svm = SVC() 
svm.fit(X, labels)
```

我们做到了！

模型`gnb`和`svm`可以使用它们的`predict()`方法将未知文档分类。

例如，使用朴素贝叶斯:

```py
gnb.predict(X_test)
```

会给我们一个包含所有预测类的数组。我们的数据集中有四个类，这是我们看到的结果:

```py
array([0, 3, 3, ..., 3, 3, 3])
```

与 SVM 类似，我们运行:

```py
svm.predict(X_test)
```

我们的结果是:

```py
array([0, 3, 3, ..., 3, 3, 3])
```

虽然聚类往往也是一个更具解释性的过程，但在分类过程中，我们往往希望提高我们预测正确类别的准确性或成功率。`GridSearchCV` [21]是一个 scikit-learn 函数，它允许我们为分类器对象选择最佳参数，并且我们可以使用`classificaiton_report`对象来检查分类器的性能。

要了解如何做到这一点，链接到的 scikit-learn 文档页面([http://scikit-learn . org/stable/modules/generated/sk learn . metrics . classification _ report . html](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html))为我们提供了一个简单的示例:

```py
from sklearn import svm, datasets
from sklearn.model_selection import GridSearchCV
iris = datasets.load_iris()
parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}
svc = svm.SVC()
clf = GridSearchCV(svc, parameters)
clf.fit(iris.data, iris.target)
```

在前面的例子中，我们正在对 SVM 进行网格搜索，并在`linear`和`rbf`内核以及两个不同的 c 值之间进行选择

http://scikit-learn . org/stable/auto _ examples/text/document _ classification _ 20 news groups . html[20]上的代码向我们展示了选择 sci kit-learn 的多个分类器的过程，您会注意到这种方法与您目前看到的代码非常相似。这个链接值得一游，看看 scikit-learn 还能提供什么，以及如何比较这些分类器的结果。您可以看到下面的图像来说明这一点，其中分类器在相对性能和时间方面进行了相互比较。

![](Images/bce30001-f966-4ff4-904b-48872d075eee.png)

图 10.4 不同分类器在 20NG 数据集上的性能。请注意，这些是我们自己没有研究过的分类器；链接 21 描述了生成该图像的代码和分类器。

对于那些好奇使用更强大的机器学习工具的人来说，[http://nadbordrozd . github . io/blog/2016/05/20/text-class ification-with-word2vec/](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)[22]上的[博文](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)告诉我们如何使用 word 2 vec 对文档进行分类。我们将在 Word2Vec 和 Doc2Vec 一章中详细讨论这个过程。



# 摘要

这就是总结！你现在可以自己构建基本的分类器——将电子邮件分类为垃圾邮件和非垃圾邮件的经典问题现在你可以自己复制了。我们已经看到了各种聚类算法，如 k-means 和层次聚类算法。我们讨论了什么是监督和非监督学习算法，并看到了如何使用 scikit-learn 运行这两种算法的示例。

您还可以使用我们拥有的聚类和主题建模工具，以各种方式探索您的文本数据。让我们在下一章尝试更进一步——建立一个基本的信息检索机器，它可以搜索相似的文档。



# 参考

[1]鸢尾花数据集:
[https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)

[2]http://yann.lecun.com/exdb/mnist/数字数据集:

[3] 20 NG 数据集:
[http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)

[4]聚类与分类笔记本:
[https://github . com/bhargavader/personal/blob/master/notebooks/Clustering _ classing . ipynb](https://github.com/bhargavvader/personal/blob/master/notebooks/clustering_classing.ipynb)

[5]主成分分析:
[https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)

[6] K-Means 聚类:
[https://en.wikipedia.org/wiki/K-means_clustering](https://en.wikipedia.org/wiki/K-means_clustering)

[7]scikit-learn k-means:
[http://scikit-learn . org/stable/modules/clustering . html # k-means](http://scikit-learn.org/stable/modules/clustering.html#k-means)

[8] scikit-learn 聚类:
[http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html)

[9]沃德的方法:
[https://en.wikipedia.org/wiki/Ward's_method](https://en.wikipedia.org/wiki/Ward's_method)

[10]树状图:
[https://en.wikipedia.org/wiki/Dendrogram#Clustering_example](https://en.wikipedia.org/wiki/Dendrogram#Clustering_example)

[11]文献聚类:
[http://brandonrose.org/clustering](http://brandonrose.org/clustering)

【12】与文字打交道:
[https://de.dariah.eu/tatom/working_with_text.html](https://de.dariah.eu/tatom/working_with_text.html)

[13]使用 Doc2Vec 的自动主题聚类:
[https://towards data science . com/Automatic-Topic-Clustering-using-doc 2 vec-E1 CEA 88449 c](https://towardsdatascience.com/automatic-topic-clustering-using-doc2vec-e1cea88449c)

[14]朴素贝叶斯:
[https://en.wikipedia.org/wiki/Naive_Bayes_classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)

[15]支持向量机:
[https://en.wikipedia.org/wiki/Support_vector_machine](https://en.wikipedia.org/wiki/Support_vector_machine)

[16]朴素贝叶斯 sci kit-Learn:
[http://scikit-learn.org/stable/modules/naive_bayes.html](http://scikit-learn.org/stable/modules/naive_bayes.html)

[17]支持向量机 Scikit-Learn:
[http://scikit-learn.org/dev/modules/svm.html](http://scikit-learn.org/dev/modules/svm.html)

【18】仁法:
[https://en.wikipedia.org/wiki/Kernel_method](https://en.wikipedia.org/wiki/Kernel_method)

[19]通俗地说，SVM 是什么意思？:
[https://www . quora . com/What-does-support-vector-machine-SVM-mean-in-laymans-terms](https://www.quora.com/What-does-support-vector-machine-SVM-mean-in-laymans-terms)

[20]文本文档的分类 scikit-learn:
[http://scikit-learn . org/stable/auto _ examples/text/document _ class ification _ 20 news groups . html](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html)

[21]GridSearchCV:
[http://scikit-learn . org/stable/modules/generated/sk learn . model _ selection。GridSearchCV.html](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)

[22]用 Word2Vec 进行文本分类:
[http://nadbordrozd . github . io/blog/2016/05/20/Text-Classification-with-word 2 vec/](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/)