

# 六、NER 标注及其应用

在前一章中我们看到了如何使用 spaCy 的语言管道——词性标注，这是一个非常强大的工具，现在我们将探索另一个有趣的用法，NER 标注。我们将从语言学和文本分析的角度讨论这到底是什么，以及它的用法的详细例子，以及如何用 spaCy 训练我们自己的 NER 标记者。以下是我们将在本章中涉及的主题:

*   什么是 NER 标签？
*   Python 中的 NER 标记
*   训练你的 NER 标记者
*   NER-标记示例和可视化



# 什么是 NER 标签？

我们通过扩展首字母缩略词开始了我们关于词性标注的章节，我们在这里也将做同样的事情。**代表**命名实体识别**，与词性标注一起，是自然语言处理的支柱之一。**

 **让我们从理解什么是命名实体开始。命名实体是具有适当名称的真实世界对象，例如法国、唐纳德·特朗普和推特。在这些例子中，法兰西是一个国家，将被标识为 **GPE** ( **地缘政治实体**)，唐纳德川普是(一个人)的**，Twitter 是一家公司，因此被标识为**组织** ( **组织**)。在 David Nadeau 和 Satoshi Sekine(纽约大学)的一项名为*命名实体识别和分类调查*的研究中，我们得到了一个更严格的定义:**

*在“命名实体”的表述中，“命名”一词旨在将实体的可能集合限制为仅那些一个或多个刚性指示符代表所指对象的实体。当一个指示物在每一个可能的世界中指示相同的事物时，它就是严格的。*

我们由此要理解的是，它指的是一个*特定的*物体或人。例如，如果我们给这个句子加上 NER 标签，*埃马纽埃尔·马克龙是法国现任总统。我们会把埃马纽埃尔·马克龙认作一个人，把法兰西认作一个国家——但不会把*总统*认作一个国家，因为它可以指代许多对象，比如不同国家的总统，甚至是一个组织的总统。*

存在多少不同类别的命名实体？同样，就像词类一样，这真的取决于我们。我们可以选择对我们的实体含糊其辞，只识别少数几个，或者有一个真正细粒度的类别集。我们必须记住，大多数现代 NER 标记者，类似于后标记者，都是经过统计训练的模型，其中类的数量等于我们希望它们达到的数量，并且根据问题的不同，这可能会不断变化。

但是话虽如此，还是有一些类别我们会经常看到，就像我们之前讨论过的那些。这些将是一个人(`PER`)、位置(`LOC`)、组织(`ORG`)和其他各种实体(`MISC`)。

![](Images/2c6385d6-d8e5-4ac4-8d6d-2f2442d21f6c.png)

图 6.1 spaCy 的轻量级维基百科训练标记器，只有基本的实体类型

你可能想知道为什么在前面的部分我们使用了特定的缩写(`PER`、`LOC`、`ORG`和`MISC`)。这是因为，就像我们在前一章探讨的词性标注一样，当我们执行 NER 标注时，我们将主要讨论空间。这些首字母缩写词也会出现在其他标签和方案中。

除了这些显而易见的实体类型之外，在执行这项任务时，我们还需要识别哪些类别呢？时间表达式和数字表达式经常出现在这种情况下。但是，如果我们非常严格地坚持我们的命名实体或刚性指示器的定义，这可能会使我们感到困惑。例如，想想 2016 年。它指定了一个特定的年份，我们可以认为它是一个命名的实体。但是如果我们有这样一句话——

我喜欢在七月去海滩。

在这里，任何月份都可以代替七月，没有上下文，很难称之为**严格的指示符**，或者是指某个*特定的*月份。但是，考虑一下这句话:

去年七月，我喜欢去海滩。

突然，单词 July 现在指的是一个特定的月份，是一个严格的指示符，应该被视为命名实体类型。然而，在这种情况下可能很难识别上下文，我们可能会有一个不正确的标签。正是在这种情况下，我们必须决定，当我们执行文本分析任务时，坚持严格的定义并不总是明智的，在这种情况下稍微灵活一点，可以导致更好的执行，更实用的机器。

![](Images/1225d2c5-0232-4de8-ab47-c7c6221bf92c.png)

图 6.2 spaCy 提供的实体类型的完整列表

BBN 科技公司发布了一份用于回答问题的实体和子实体的列表，可以在注释子类型[ [2](https://catalog.ldc.upenn.edu/docs/LDC2005T33/BBN-Types-Subtypes.html) ]找到。spaCy 为其命名实体分类提供了 18 个不同的类别，我们将在本章的剩余部分使用这些类别。

所以，我们又一次解决了*什么*的问题。*为什么*我们现在应该对 NER 标签感兴趣？像往常一样，简单地识别文本中的命名实体通常不是我们任务的最终结果，但它最终成为进一步任务的重要构件。实体链接是一项任务，我们使用实体识别，然后尝试导出它们之间的关系。想想这句话:

罗马是意大利的首都。

任何 NER 标记者都会认出罗马是一个地方(GPE)，就像意大利一样。为了能够得出罗马是一个城市的结论，这是与意大利这个国家联系在一起的，而不是美国 R&B 艺术家所说的罗马，我们称之为**命名实体消歧** ( **内德**)的那种任务。

这在生物医学研究中也有很大的价值，在生物医学研究中，科学家试图识别基因和基因产物。通过分析和确定其他组织与收入之间的联系，企业可以使用它来帮助确定哪些组织是最重要的。尽管这两个例子都是特定于领域的；不要指望一个受过医学期刊数据训练的标记者能在财务文档上表现出色！这是 NER 标注和词性标注的一个区别；虽然 POS 或多或少会出现在不同种类的文献中，但命名实体可能会因上下文而完全不同。这导致即使训练有素的模型也变得脆弱；这意味着在不同的域中使用时,*很容易破坏*。

在分析文学和写作风格时，NER 标记可以再次派上用场，这一点我们可以在范·戴尔和[ [公司的研究*命名实体识别和解决文学研究*)中看到。NER 标记在科学中最流行的用法仍然是在医学和生物学领域，这也可以通过专门致力于从医学文档中提取实体的竞赛的存在而明显看出。](https://pure.uva.nl/ws/files/2676433/168352_2014_VanDalenOskam_07_Namescape.pdf)

我们有足够的动力去建造一个 NER 标靶；那么，下一个问题，我们到底要怎么做？

就像我们到目前为止试图解决的大多数问题一样(也像我们将在本书中继续解决的问题一样！)，答案是统计建模。类似于我们使用带注释的数据集并提取相关特征的标注器，我们将做同样的事情，但是使用带实体注释的数据集。值得注意的是，在这种情况下，当我们谈论相关特征时，我们讨论的是可以用来预测我们希望识别的未知物体的类别的可能信息。在我们的 NER 标记上下文中，单词的词性标记以及周围单词的词性标记可以用作预测符！

这也是为什么在我们的管道中，我们在 NER 标记之前执行位置标记；虽然，在 spaCy 的情况下，它是一个预训练的统计模型，这并不重要。可用于预测单词是否是命名实体的其他可能特征是单词的前缀或后缀(例如，-ion)，它是否包含特殊符号，或者它是否是大写的。

一旦我们准备好了我们的功能，我们就可以使用大量的机器学习算法来训练我们的模型——**CRF**s(**条件随机场**，在约翰·拉弗蒂和他的公司的*条件随机场:用于分割和标记序列数据的概率模型*。[[4](https://repository.upenn.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1162&context=cis_papers)]中描述)通常是 NER 标记的流行选择，深度学习方法也是如此，类似于我们讨论的词性标记。

当然，就像在自然语言处理中执行的大多数任务一样，我们也可以尝试更多基于规则的方法。Epaminondas Kapetanios and co .【[5](https://books.google.fr/books?id=YXv6AQAAQBAJ&source=gbs_navlinks_s)】的《自然语言处理:语义方面》一书的第 13.2.1 节列出并引用了多种这样的方法。一个这样的示例规则是:

识别匹配的称呼字典，然后是匹配的姓氏字典，并将整个区域标记为候选人。

这项技术要求我们拥有存储称呼的字典，以及姓氏的字典。这使得这种方法非常不理想；字典可能会变得很大，占用空间，如果不定期更新，可能会变得多余，并且可能使我们的方法更特定于领域，或者更重要的是，特定于数据。

可以看出为什么我们喜欢在整本书中坚持使用统计模型，这无疑有助于统计方法的表现远远超过基于规则的方法。

我们对 NER 标记的内容、原因和方式有所了解，是时候开始破解并学习用 Python 自己构建模型，以及如何训练这些 NER 标记者了。** **

# Python 中的 NER 标记

我们的 NER 标记方法将反映我们的词性标记方法；毕竟，它们是非常相似的任务，两者都可以比作分类的机器学习任务，在分类中，我们将一个未知对象分配给它所属的概率最高的类。

我们完成这项任务的另一个相似之处是，我们将使用 spaCy 来进行 NER 标记。同样，这并不意味着空间是执行 NER 标记的唯一方式；有两种流行的替代方案，一种是 NLTK，另一种是斯坦福 NER 标记器。

在我们开始解释之前，有必要简要了解一下术语*组块*。它是在句子的词性标注完成后，将句子分解成组成部分的过程。这些组成部分的例子有名词短语或动词短语。例如，考虑下面的句子:

棕色的小狗对着黑猫吠叫。

在这种情况下，我们可以很容易地识别这两个名词短语:小棕色狗和黑猫。当我们做 NER 标记时，这些组块可以派上用场，我们将在[第 7 章](bdd3d124-b0be-4848-8a54-a8913ab8214e.xhtml)、*依存解析*中更详细地探讨这些主题。事实上，组块也被称为浅层解析。

那么，在 NER 标记过程中，它与我们有什么关系呢？如果你还记得，当我们引用 NER 标签的例子时，我们说过*唐纳德·川普*将被标记为一个人；不仅仅是*唐纳德*，或者*川普*，而是整个短语。当我们标记时，这种关于一组单词作为名词短语的知识可以帮助我们做出决定。

在我们在网上找到的大多数标签中，我们可以找到标签系统，如 IOB 标签系统。这只是我们在 NER 标记时进一步识别或表示标记的一种方式。这就是 **IOB** 的简单意思:

**B-{组块类型}** -对于开始组块中的单词

**I-{组块类型}** -用于组块内的单词

**O** -在任何块之外

spaCy 也使用这样的系统；它将 L 和 U 相加，因为我们在 spaCy 中处理令牌，所以它被认为是一个 **BILOU** 系统。

![](Images/343beabf-9017-4f0b-8cc2-42c207ce9709.png)

图 6.3 spaCy 自己的 NER 标签 BILOU 系统

尽管我们将在很大程度上使用 spaCy，但是让我们简单地讨论一下 NLTK: NLTK 使用这些块作为树状系统的一部分来进行标记，尽管它也有一个遵循 IOB 系统的 tagger。下面是一些代码片段，解释了如何使用这两者，以及如何在它们之间转换:

```py
from nltk.chunk import conlltags2tree, tree2conlltags 
from nltk import pos_tag 
from nltk import word_tokenize
from nltk.chunk import ne_chunk
```

我们的导入，这些模型在 NLTK 中的 CoNLL(来自 CoNLL conference)语料库上进行训练。因为我们已经完成了标记化、词性标注和分块，所以对于基于树的标注，我们需要做的就是使用`conlltags2tree`方法来查看我们的标签。

```py
sentence = "Clement and Mathieu are working at Apple." 
ne_tree = ne_chunk(pos_tag(word_tokenize(sentence))) 

iob_tagged = tree2conlltags(ne_tree)
print(iob_tagged) 

[('Clement', 'NNP', u'B-PERSON'), ('and', 'CC', u'O'), ('Mathieu', 'NNP', u'B-PERSON'), ('are', 'VBP', u'O'), ('working', 'VBG', u'O'), ('at', 'IN', u'O'), ('Apple', 'NNP', u'B-ORGANIZATION'), ('.', '.', u'O')]
```

请注意，我们首先对句子进行了标记，然后对其进行了词性标记，并在将其传递给基于树的标记器之前对其进行了分块。我们的输出是用词性和命名实体类适当标记的每个单词。

```py
ne_tree = conlltags2tree(iob_tagged)
print(ne_tree) 

(S 
  (PERSON Clement/NNP) 
  and/CC 
  (PERSON Mathieu/NNP) 
  are/VBP 
  working/VBG 
  at/IN 
  (ORGANIZATION Apple/NNP) 
  ./.)
```

另一个受欢迎的标记器是斯坦福命名实体识别器，分别是 T2、NER、T4 和 T5。我们之前提到过 **CRF** s ( **条件随机字段**)以及它们是如何作为一种机器学习构造，经常用于训练与基于文本的问题相关的分类器；斯坦福的 tagger 使用了同样的算法。虽然它是用 Java 编写的，要使用它，您必须下载 JAR 文件(您可以在网站上找到这些文件)，但 NLTK 为我们提供了一个 Python 接口来访问标记器。

下载完 JAR 文件后，我们必须从 NLTK 链接到它们。JAR 文件是由 Java 代码创建的 Java 文件——在我们的例子中，我们可以将它们理解为通过 Python 加载的库。

```py
from nltk.tag import StanfordNERTagger
st = StanfordNERTagger('/usr/share/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz', '/usr/share/stanford-ner/stanford-ner.jar', encoding='utf-8')
```

请注意你必须参考的网址。因为我们的例子只涉及英语，所以我们只加载英语类。

让我们像使用其他 NLTK 标记一样简单地使用它:

```py
st.tag('Baptiste Capdeville is studying at Columbia University in NY'.split())

[('Baptiste', 'PERSON'), ('Capdeville', 'PERSON'), ('is', 'O'), ('studying', 'O'), 
('at', 'O'), ('Columbia', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('in', 'O'), ('NY', 'LOCATION')] 
```

我们再次看到，类似于我们的 POS-tagging 示例，NLTK 由于其提供的简单 API 而具有吸引力，但是这仍然不是我们想要在任何生产级软件中使用的。在我们开始使用 spaCy 进行 NER 标记之前，我们鼓励读者浏览以下关于 NLTK 及其功能的链接:

1.  测试 NLTK 和斯坦福 NER 标签的准确性[ [7](https://pythonprogramming.net/testing-stanford-ner-taggers-for-accuracy/?completed=/named-entity-recognition-stanford-ner-tagger/)
2.  如何在 Python NLTK 等编程语言中使用斯坦福命名实体识别器(NER)[[8](https://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages)
3.  第 7 章(NLTK 书)——从文本中提取信息[ [9](http://www.nltk.org/book/ch07.html)
4.  Python (PyNER) [ [10](http://erickpeirson.github.io/pythia/python/2015/05/01/named-entity-recognition-on-large-collections.html) ]大型集合上的命名实体识别



# NER-用空格标记

当我们谈论词性标注时，我们已经讨论了 spaCy 令人难以置信的强大和简单性——当使用 spaCy 进行 NER 标注时，我们将引用相同的原因。实际上，如果你遵循了前一章的词性标注，我们已经完成了词性标注；由于 NER 标记是 spaCy 自然管道的一部分，简单地用管道处理一个文档意味着除了被标记和词性标记之外，它已经被 NER 标记了(更不用说依赖解析了！).

设置我们的模型包括我们之前看到的相同步骤。

```py
import spacy
nlp = spacy.load('en')
```

现在让我们决定一些我们想要 NER 标签的句子。

```py
sent_0 = nlp(u'Donald Trump visited at the government headquarters in France today.') 

sent_1 = nlp(u'Emmanuel Jean-Michel Frédéric Macron is a French politician serving as President of France and ex officio Co-Prince of Andorra since 14 May 2017.') 

sent_2 = nlp(u"He studied philosophy at Paris Nanterre University, completed a Master's of Public Affairs at Sciences Po, and graduated from the École nationale d'administration (ÉNA) in 2004.") 

sent_3 = nlp(u'He worked at the Inspectorate General of Finances, and later became an investment banker at Rothschild & Cie Banque.')
```

这个`sent_0`句子很简单，它将说明我们如何期望一个基本句子被 spaCy NER 标记。

当 spaCy 处理文档时，命名实体存储在一个`Doc`类的`ents`属性中。我们仍然可以通过令牌访问实体，令牌存储在`ent_type`中。以下是说明两者用法的示例:

```py
for token in sent_0:
    print(token.text, token.ent_type_)

(u'Donald', u'PERSON')
(u'Trump', u'PERSON')
(u'visited', u'')
(u'at', u'')
(u'the', u'')
(u'government', u'')
(u'headquarters', u'')
(u'in', u'')
(u'France', u'GPE')
(u'today', u'DATE')
(u'.', u'')
```

对于那些没有被识别为命名实体的单词，返回一个空字符串。对于那些被标识为命名实体的实体，将返回适当的标记。在我们的例子中，我们只有三个实体，`Donald Trump`、`France`和`today`，它们分别被正确地标识为`PERSON`、`GPE`和`DATE`。因为政府总部没有提到某个特定的实体，所以它没有被确定为一个命名的实体。我们可能会争辩说，因为提到了`France`，人们可能会认为政府总部也应该被标记，但这是一个灰色区域，我们可以在这里给我们的标记者一张通行证。

记住，spaCy 希望我们访问`doc.ents` streamable 对象中的实体。这一片`Doc`级被称为`Span`级 [11](https://spacy.io/api/span) 。

```py
for ent in sent_0.ents:
    print(ent.text, ent.label_)

(u'Donald Trump', u'PERSON')
(u'France', u'GPE')
(u'today', u'DATE')
```

您可以看到，span 只拾取了实体，并打印了三个实体。请注意，`Donald Trump`是一个实体，当只是打印出令牌时，它并没有被固有地捕获。

让我们试试下一个句子，它更长并且包含一个法语名字，这可能会使我们的英语标签混乱。

```py
for token in sent_1:
    print(token.text, token.ent_type_)

(u'Emmanuel', u'PERSON')
(u'Jean', u'PERSON')
(u'-', u'PERSON')
(u'Michel', u'PERSON')
(u'Frxe9dxe9ric', u'')
(u'Macron', u'')
(u'is', u'')
(u'a', u'')
(u'French', u'NORP')
(u'politician', u'')
(u'serving', u'')
(u'as', u'')
(u'President', u'')
(u'of', u'')
(u'France', u'GPE')
(u'and', u'')
(u'ex', u'')
(u'officio', u'')
(u'Co', u'PERSON')
(u'-', u'PERSON')
(u'Prince', u'PERSON')
(u'of', u'')
(u'Andorra', u'')
(u'since', u'')
(u'14', u'DATE')
(u'May', u'DATE')
(u'2017', u'DATE')
(u'.', u'')
```

在这里，我们注意到几个特点。在这里，`é`字符上的重音符号摆脱了 Unicode，所以`Macron`不会作为实体的一部分被捕获。我们将在本章后面看到，如果不是因为这个例子中的重音符号或 Unicode 的阅读方式，`Macron`将会作为实体的一部分被包含进来。我们可以看到`Co-Prince of Andorra`也没有被认为是可能的最佳方式。

让我们看看同一个例子，但是只打印实体:

```py
for ent in sent_1.ents:
    print(ent.text, ent.label_)

(u'Emmanuel Jean-Michel', u'PERSON')
(u'French', u'NORP')
(u'France', u'GPE')
(u'Co-Prince', u'PERSON')
(u'14 May 2017', u'DATE')
```

我们非常清楚地看到这里的错误。也就是说，在运行下一个示例之前，让我们删除出现的重音符号:

```py
for token in sent_2:
    print(token.text, token.ent_type_)

(u'He', u'')
(u'studied', u'')
(u'philosophy', u'')
(u'at', u'')
(u'Paris', u'ORG')
(u'Nanterre', u'ORG')
(u'University', u'ORG')
(u',', u'')
(u'completed', u'')
(u'a', u'')
(u'Masters', u'ORG')
(u'of', u'ORG')
(u'Public', u'ORG')
(u'Affairs', u'ORG')
(u'at', u'')
(u'Sciences', u'')
(u'Po', u'')
(u',', u'')
(u'and', u'')
(u'graduated', u'')
(u'from', u'')
(u'the', u'ORG')
(u'Ecole', u'ORG')
(u'Nationale', u'ORG')
(u'Administration', u'ORG')
(u'(', u'')
(u'ENA', u'ORG')
(u')', u'')
(u'in', u'')
(u'2004', u'DATE')
(u'.', u'')
```

我们在这个例子中看不到错误——让我们只检查跨度，看看它是否提取了所有的短语。

```py
(u'Paris Nanterre University', u'ORG')
(u'Masters of Public Affairs', u'ORG')
(u'the Ecole Nationale Administration', u'ORG')
(u'ENA', u'ORG')
(u'2004', u'DATE')
```

瞧啊。我们看到，一旦我们去除了讨厌的口音，它是平滑的。

```py
for token in sent_3:
    print(token.text, token.ent_type_)

(u'He', u'')
(u'worked', u'')
(u'at', u'')
(u'the', u'ORG')
(u'Inspectorate', u'ORG')
(u'General', u'ORG')
(u'of', u'ORG')
(u'Finances', u'ORG')
(u',', u'')
(u'and', u'')
(u'later', u'')
(u'became', u'')
(u'an', u'')
(u'investment', u'')
(u'banker', u'')
(u'at', u'')
(u'Rothschild', u'ORG')
(u'&', u'ORG')
(u'Cie', u'ORG')
(u'Banque', u'ORG')
(u'.', u'')

for ent in sent_3.ents:
    print(ent.text, ent.label_)

(u'the Inspectorate General of Finances', u'ORG')
(u'Rothschild & Cie Banque', u'ORG')
```

我们开始了——我们已经看到了 spaCy 在各种环境中是如何工作的，以及可能会混淆它的东西。总的来说，它工作得很好，我们鼓励读者尝试一些他们的例子。

就像 spaCy 模型的张贴者一样，我们也被鼓励训练 spaCy 自己的 NER 模型。



# 训练我们自己的 NER 标签员

在前一章的词性标注中，我们详细讨论了用于标注的统计模型的训练过程。NER 标记的想法保持不变——我们选择我们认为能够指示命名实体标记的特征，将这些特征插入到机器学习模型中，向其输入带注释的数据，并让机器从提供的示例中学习。

如果您需要复习 spaCy 模型中的培训过程，我们建议您重新阅读本书的[第 5 章](e7fc28c4-94a9-41aa-8836-ad4c9a55e880.xhtml)、*词性标注及其应用*中的*培训我们自己的词性标注者*部分。

我们现在将检查存在于`spaCy examples`文件夹中的两个代码文件:一个训练一个空白模型来执行 NER 标记，另一个向现有模型添加一个新实体。

以下代码出现在`train_ner.py`文件[ [12](https://github.com/explosion/spacy/blob/master/examples/training/train_ner.py) ]中:

```py
import plac
import random
from pathlib import Path
import spacy

# training data
TRAIN_DATA = [ 
    ('Who is Shaka Khan?', { 
        'entities': [(7, 17, 'PERSON')] 
    }), 
    ('I like London and Berlin.', { 
        'entities': [(7, 13, 'LOC'), (18, 24, 'LOC')] 
    })
]
```

我们已经设置了基本导入和训练示例。一个友好的提醒，这些例子太少了，任何严肃的训练都不会发生，这仅仅是一个代表性的例子。

```py
@plac.annotations( 
    model=("Model name. Defaults to blank 'en' model.", "option", "m", str), 
    output_dir=("Optional output directory", "option", "o", Path), 
    n_iter=("Number of training iterations", "option", "n", int)) 
def main(model=None, output_dir=None, n_iter=100): 
    """Load the model, set up the pipeline and train the 
       entity recognizer.""" 
    if model is not None: 
        nlp = spacy.load(model)  # load existing spaCy model 
        print("Loaded model '%s'" % model) 
    else: 
        nlp = spacy.blank('en')  # create blank Language class 
        print("Created blank 'en' model") 
```

我们已经为模型的保存位置以及迭代次数设置了注释。我们的模型已经加载，现在我们已经创建了一个空白模型。

```py
    # create the built-in pipeline components and add them to the pipeline# nlp.create_pipe works for built-ins that are registered with spaCy
        if 'ner' not in nlp.pipe_names: 
            ner = nlp.create_pipe('ner') 
            nlp.add_pipe(ner, last=True) 
    # otherwise, get it so we can add labels
        else: 
            ner = nlp.get_pipe('ner') 
    # add labels 
        for _, annotations in TRAIN_DATA: 
            for ent in annotations.get('entities'): 
                 ner.add_label(ent[2])
    # get names of other pipes to disable them during training other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] 
        with nlp.disable_pipes(*other_pipes):  # only train NER 
            optimizer = nlp.begin_training() 
            for itn in range(n_iter): 
                random.shuffle(TRAIN_DATA) 
                losses = {} 
                for text, annotations in TRAIN_DATA: 
                    nlp.update( 
                        [text], # batch of texts 
                        [annotations], # batch of annotations 
                        drop=0.5, # dropout-make it harder to memorise data
                        sgd=optimizer, # callable to update weights 
                        losses=losses) 
                print(losses) 
```

我们在这里注意到，它遵循的训练原则与贴标签者完全相同。我们首先向管道添加`ner`标签，然后禁用管道的所有其他组件，这样我们只训练/更新 NER 标签。训练本身很简单，`nlp.update()`方法为我们抽象了一切，让 spaCy 处理实际的机器学习和繁重的工作。

```py
    # test the trained model
    for text, _ in TRAIN_DATA:
        doc = nlp(text)
        print('Entities', [(ent.text, ent.label_) for ent in doc.ents])
        print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])
    # save model to output directory 
        if output_dir is not None: 
            output_dir = Path(output_dir) 
            if not output_dir.exists(): 
                output_dir.mkdir() 
        nlp.to_disk(output_dir) 
        print("Saved model to", output_dir)
    # test the saved model 
        print("Loading from", output_dir) 
        nlp2 = spacy.load(output_dir) 
        for text, _ in TRAIN_DATA: 
            doc = nlp2(text) 
            print('Entities', [(ent.text, ent.label_) for ent in doc.ents]) 
            print('Tokens', [(t.text, t.ent_type_, t.ent_iob) for t in doc])

if __name__ == '__main__': 
    plac.call(main) 
```

在我们的训练完成后不久，我们测试我们的模型，然后将它保存到指定的目录中。如果我们运行该文件没有任何错误，我们应该会得到以下输出:

```py
Entities [('Shaka Khan', 'PERSON')]
Tokens [('Who', '', 2), ('is', '', 2), ('Shaka', 'PERSON', 3),
('Khan', 'PERSON', 1), ('?', '', 2)]
Entities [('London', 'LOC'), ('Berlin', 'LOC')]
Tokens [('I', '', 2), ('like', '', 2), ('London', 'LOC', 3),
('and', '', 2), ('Berlin', 'LOC', 3), ('.', '', 2)]
```

现在让我们看看向模型中添加一个新类。这里的原理保持不变；我们加载模型，禁用不需要更新的管道，添加新标签，然后遍历示例并更新它们。同样，就像以前的例子一样，不要期望训练过的模型能创造奇迹——我们没有足够的训练样本。

实际的训练是通过循环示例并调用`nlp.entity.update()`来完成的。`update()`方法遍历输入的单词。每说一个词，它都会做一个预测。然后，它查阅`GoldParse`实例上提供的注释，看看它是否正确。如果它错了，它会调整它的权重，这样下次正确的动作会得到更高的分数。

```py
import plac 
import random 
from pathlib import Path 
import spacy

# new entity label 
LABEL = 'ANIMAL' 

TRAIN_DATA = [ 
    ("Horses are too tall and they pretend to care about your feelings", { 
        'entities': [(0, 6, 'ANIMAL')] 
    }), 

    ("Do they bite?", { 
        'entities': [] 
    }), 

    ("horses are too tall and they pretend to care about your feelings", { 
        'entities': [(0, 6, 'ANIMAL')] 
    }), 

    ("horses pretend to care about your feelings", { 
        'entities': [(0, 6, 'ANIMAL')] 
    }), 

    ("they pretend to care about your feelings, those horses", { 
        'entities': [(48, 54, 'ANIMAL')] 
    }), 

    ("horses?", { 
        'entities': [(0, 6, 'ANIMAL')] 
    }) 
]
```

我们已经设置了导入和训练示例。

如果您正在使用一个现有的模型，请确保混入 spaCy 之前正确识别的其他实体类型的示例。否则，你的模型可能会学习新的类型，但是*会忘记*它以前知道的。

这篇博文链接解释了这个忘记旧功能的错误，[https://explosion . ai/blog/伪彩排-灾难性-遗忘](https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting)。

```py
@plac.annotations( 
    model=("Model name. Defaults to blank 'en' model.", "option", "m", str), 
    new_model_name=("New model name for model meta.", "option", "nm", str), 
    output_dir=("Optional output directory", "option", "o", Path), 
    n_iter=("Number of training iterations", "option", "n", int)) 
def main(model=None, new_model_name='animal', output_dir=None, n_iter=20): 
    """Set up the pipeline and entity recognizer, and train 
       the new entity.""" 
    if model is not None: 
        nlp = spacy.load(model)  # load existing spaCy model 
        print("Loaded model '%s'" % model) 
    else: 
        nlp = spacy.blank('en')  # create blank Language class 
        print("Created blank 'en' model") 
    # Add entity recognizer to model if it's not in the pipeline
    # nlp.create_pipe works for built-ins that are registered with spaCy 
    if 'ner' not in nlp.pipe_names: 
        ner = nlp.create_pipe('ner') 
        nlp.add_pipe(ner) 
    # otherwise, get it, so we can add labels to it    else: 
        ner = nlp.get_pipe('ner') 
```

前面的步骤与前面的示例相似。仔细注意下一行——这是我们添加标签的地方。

```py
        ner.add_label(LABEL)   # add new entity label to entity recognizer 
        if model is None: 
            optimizer = nlp.begin_training() 
        else: 
        # Note that 'begin_training' initializes the models, so it'll
        # zero out existing entity types. 
        optimizer = nlp.entity.create_optimizer()
# get names of other pipes to disable them during training        other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner'] 
        with nlp.disable_pipes(*other_pipes):  # only train NER 
        for itn in range(n_iter):
```

```py
            random.shuffle(TRAIN_DATA) 
            losses = {} 
            for text, annotations in TRAIN_DATA: 
                nlp.update([text], [annotations], sgd=optimizer, 
                           drop=0.35, losses=losses) 
            print(losses) 
```

我们用训练前一个模型的方式训练我们的模型；毕竟训练过程还是老样子。

```py
    # test the trained model 
    test_text = 'Do you like horses?' 
    doc = nlp(test_text) 
    print("Entities in '%s'" % test_text) 
    for ent in doc.ents: 
        print(ent.label_, ent.text) 

    # save model to output directory 
    if output_dir is not None: 
        output_dir = Path(output_dir) 
        if not output_dir.exists(): 
            output_dir.mkdir() 
        nlp.meta['name'] = new_model_name  # rename model 
        nlp.to_disk(output_dir) 
        print("Saved model to", output_dir)
# test the saved model 
        print("Loading from", output_dir) 
        nlp2 = spacy.load(output_dir) 
        doc2 = nlp2(test_text) 
        for ent in doc2.ents: 
            print(ent.label_, ent.text) 

if __name__ == '__main__': 
    plac.call(main)
```

代码的其余部分保持不变；关键的区别在于训练数据，增加了新的类，并且考虑到我们也需要增加旧的例子。

值得看看 spaCy 的 *NER 语言特征*页面[[16](https://spacy.io/usage/training#section-ner)——他们也提供了关于如何设置实体注释的有用建议。

spaCy 为我们提供了一种简单的方法来训练我们的模型，尽管现有的模型也做得很好。我们不应该忘记隐藏在引擎盖下的东西——接受特征并做出预测的统计模型。甚至 NLTK 也为我们提供了训练他们模型的能力。有许多教程解释了如何构建自己的分类器，或者如何更新 NLTK 分类器。虽然理解训练 NER 分类器背后的概念是有趣的，但它与我们的直接原因无关。我们提供了这些教程的列表，以防读者感兴趣:

1.  使用 Python [ [13](https://nlpforhackers.io/named-entity-extraction/) ]构建自己的命名实体识别器的完整指南
2.  Python 中命名实体识别简介[ [14](https://www.depends-on-the-definition.com/introduction-named-entity-recognition-python/) ]

3.  使用 Python [ [15](http://www.albertauyeung.com/post/python-sequence-labelling-with-crf/) ]中的 CRF 执行序列标记



# NER-标记示例和可视化

spaCy 最令人印象深刻的产品之一是其可视化套件和 API，尤其是`displaCy` [ [17](https://explosion.ai/demos/displacy-ent) 。我们在前一章可视化词性标签时讨论了这一点。虽然它在可视化依赖解析方面给人印象最深(我们将在下一章看到)，但它在实体方面也做得不错。

![](Images/039a1c31-139e-49a5-af04-7e8e32a32e87.png)

图 6.4 埃隆·马斯克关于 https://www.wired.com 的一篇文章的新闻摘录示例

我们可以在上面的例子中看到，spaCy 很好地捕捉到了实体。事实上，即使是埃隆马斯克的页面也被标记为一个组织，这可以被认为是一个组织。这可能是特斯拉之前的背景，也可能是它之后的官方页面，我们无法确定。我们确实又犯了一个有趣的错误，Twitter 是一个地缘政治实体。再说一次，如果我们考虑到脸书和推特已经大到足以成为一个国家，我们可以让它继续下滑。但玩笑归玩笑，处理这样的词并不总是容易的，除非语料库是在类似领域训练的。让我们来看看我们之前用 NER 标记的句子:

*伊曼纽尔·让-米歇尔·弗雷德里克·马克龙(法语发音:[ɛmanɥɛlmakʁɔ̃]；出生于 1977 年 12 月 21 日)是一名法国政治家，自 2017 年 5 月 14 日起担任法国总统和安道尔当然联合亲王。*

进入政界之前，他是一名高级公务员和投资银行家。马克龙在巴黎楠泰尔大学学习哲学，在巴黎政治学院获得公共事务硕士学位，并于 2004 年从国立行政学院(éNA)毕业。他在财政检查总署工作，后来成为罗斯柴尔德&银行的投资银行家。

![](Images/b1481638-ddbf-45a1-83ea-c45d29f663ff.png)

图 6.5 可视化来自法国总统埃马纽埃尔·马克龙的维基页面的实体的示例

我们可以在这里看到 Macron 的全名被捕获-口音没有摆脱 web-app！

除了简洁的可视化之外，我们还可以将 ner 用于简单的、更可能是无意义的任务，比如在一个句子中交换两个 ner。

```py
words, indices = [], [] 
for i, w in enumerate(nlp(u'Tom went to London before going to Paris.')): 
    words.append(w.text_with_ws), indices.append(i) if w.ent_type_ == "GPE" else words.append(w.text_with_ws)
```

```py
words[indices[0]], words[indices[1]] = words[indices[1]], words[indices[0]] 
print(''.join(words)) 

Tom went to Paris before going to London.
```

在五个句子中，我们把伦敦换成了巴黎——这可能不是迄今为止我们见过的最有用的句子操作，但它很好地说明了空间感的轻松。



# 摘要

我们再一次看到了 spaCy 在处理计算语言学任务上的出色表现，也看到了 NER 标记是多么有用。虽然是一项用于文本分析的任务，但模型本身是一项统计任务——理解这一点有助于为构建我们自己的模型(如果我们愿意)或更新 spaCy 使用的现有模型设置环境。

在下一章，我们将看到 spaCy 如何处理计算语言学的最后一节——依存句法分析。



# 参考

[1]命名实体识别与分类综述:
[https://nlp.cs.nyu.edu/sekine/papers/li07.pdf](https://nlp.cs.nyu.edu/sekine/papers/li07.pdf)

[2]标注子类型:
[https://catalog . LDC . upenn . edu/docs/LDC 2005 t33/BBN-类型-子类型. html](https://catalog.ldc.upenn.edu/docs/LDC2005T33/BBN-Types-Subtypes.html)

[3]文学研究的命名实体识别与解析:
[https://pure . UVA . nl/ws/files/2676433/168352 _ 2014 _ VanDalenOskam _ 07 _ name scape . pdf](https://pure.uva.nl/ws/files/2676433/168352_2014_VanDalenOskam_07_Namescape.pdf)

[4]条件随机场:分割和标记序列数据的概率模型:
[https://repository.upenn.edu/cgi/viewcontent.cgi?referer=&httpsredir = 1&article = 1162&context = cis _ papers](https://repository.upenn.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1162&context=cis_papers)

[5]自然语言处理:语义方面:
[https://books.google.fr/books?id=YXv6AQAAQBAJ&source = GBS _ nav links _ s](https://books.google.fr/books?id=YXv6AQAAQBAJ&source=gbs_navlinks_s)

【6】斯坦福 NER:
[https://nlp.stanford.edu/software/CRF-NER.shtml](https://nlp.stanford.edu/software/CRF-NER.shtml)

[7]测试 NLTK 和斯坦福 NER Taggers 的准确性:
[https://python programming . net/Testing-Stanford-ner-Taggers-for-Accuracy/？completed =/named-entity-recognition-Stanford-ner-tagger/](https://pythonprogramming.net/testing-stanford-ner-taggers-for-accuracy/?completed=/named-entity-recognition-stanford-ner-tagger/)

[8]如何在 Python NLTK 等编程语言中使用斯坦福命名实体识别器(NER):
[http://textminingonline . com/How-to-Use-Stanford-Named-Entity-Recognizer-ner-in-Python-NLTK-and-Other-Programming-Languages](http://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages)

[9]第七章(NLTK 书)——从文本中提取信息:
[http://www.nltk.org/book/ch07.html](http://www.nltk.org/book/ch07.html)

[10]Python(PyNER)大型集合上的命名实体识别:
[http://erickpeirson . github . io/pythia/Python/2015/05/01/Named-Entity-Recognition-On-Large-Collections . html](http://erickpeirson.github.io/pythia/python/2015/05/01/named-entity-recognition-on-large-collections.html)

[11]跨度:
[https://spacy.io/api/span](https://spacy.io/api/span)

[12]train _ ner . py:
[https://github . com/explosion/spacy/blob/master/examples/training/train _ ner . py](https://github.com/explosion/spacy/blob/master/examples/training/train_ner.py)

[13]用 Python 构建自己的命名实体识别器的完整指南:
[https://nlpforhackers.io/named-entity-extraction/](https://nlpforhackers.io/named-entity-extraction/)

[14]Python 中命名实体识别简介:
[https://www . depends-on-the-definition . com/Introduction-Named-Entity-Recognition-Python/](https://www.depends-on-the-definition.com/introduction-named-entity-recognition-python/)

[15]在 Python 中使用 CRF 进行序列标记:
[http://www . albertau yeung . com/post/Python-Sequence-labeling-with-CRF/](http://www.albertauyeung.com/post/python-sequence-labelling-with-crf/)

[16]https://spacy.io/usage/training#section-ner spaCy·NER:

[17]显示:
[https://explosion.ai/demos/displacy](https://explosion.ai/demos/displacy)

[18]生物创意:
[http://www.biocreative.org/](http://www.biocreative.org/)**