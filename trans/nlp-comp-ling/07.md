

# 依存句法分析

我们在[第五章](e7fc28c4-94a9-41aa-8836-ad4c9a55e880.xhtml)、*词性标注及其应用*和[第六章](6aa68517-4c79-4d41-8cc2-04b73b85e6f0.xhtml)、 *NER 标注及其应用、*中看到了 spaCy 的语言管道如何执行各种复杂的计算语言学算法，比如词性标注和 NER 标注。然而，这并不是所有的 spaCy packs，在这一章中，我们将探索依赖解析的强大功能，以及如何在各种上下文和应用程序中使用它。在继续使用 spaCy 以及训练我们自己的依赖解析器之前，我们将先看一下依赖解析的理论。以下是我们将在本章中涉及的主题:

*   依存句法分析
*   用 Python 进行依赖解析
*   训练我们的依存解析器
*   摘要
*   参考



# 依存句法分析

解析仍然是我们能够在文本中进行的最重要的过程之一。然而，它并不局限于自然语言，也有计算机语言的历史，这种思想也可以扩展到任何符合某种形式语法规则的数据结构。

这意味着为了能够进行任何类型的解析，我们需要两样东西——一个解析器和一个语法。但是等等，*到底*在解析什么？

我们可以理解它是一种分析句子或分解句子来理解句子结构的方法。我们分解句子以理解其底层结构的方式构成了解析的关键，并且我们可以尝试用许多不同的方式来解释句子的结构。

我们在这里提到句子是因为它与我们相关，因为自然语言，但是解析是一种可以在任何具有正式语法的语句上执行的活动。比如我们来看这个简单的算术语句: *((7 + 3) * (5 - 2))*

![](Images/550b8fe8-5180-4eb8-83c9-dd8b13c4403b.png)

图 7.1 一个解析简单数学语句的例子

我们如何打破这种局面？这四个数字是我们的四个主要组成部分，其他符号代表这些数字之间的动作。根据标准的 BODMAS 算术规则，我们将首先完成括号中的动作。然后，我们描述数学符号(+、-、*)如何与树的叶子相关联——这里的叶子是树最底部的节点，是数字 7、3、5 和 2。下图解释了我们将如何解析这样的语句。

既然我们知道了解析的概念是什么，我们就可以关注它与我们的关系了。即使在自然语言处理领域，术语解析也可能有两种不同的含义——传统的句子解析是指理解一个单词的句子的含义，而在计算语言学的上下文中，它也可以指通过算法产生解析树(与我们前面看到的树没有什么不同)的形式分析。

在本章的讨论中，无论何时我们提到解析，我们都会提到传统的句子解析。在传统的句子分析领域，有许多思想流派，其中最流行的有两种——依存分析和短语结构分析。我们将在文本分析中大量使用依赖解析，但是理解这两种解析是值得的。

一点历史依赖解析是一种相当新的解析方法，法国语言学家 Lucien Tesnière [ [1](http://www.home.uni-osnabrueck.de/bschwisc/archives/tesniere.pdf) ]被认为是引入了这一思想流派。**选区解析**另一方面，已经存在了很长时间，亚里士多德关于**术语逻辑** [ [2](https://plato.stanford.edu/archives/win2016/entries/logic-ancient/) 的思想据说类似于我们理解选区的方式。它被正式归功于诺姆·乔姆斯基，他被认为是语言学之父。

顾名思义，依存句法分析是指通过句子中单词之间的依存关系来理解句子的结构。依存的概念是句子中的单词通过有向链接相互连接。另一方面，短语结构解析将句子分解成短语或单独的成分，也可以称为选区解析。因此，虽然经过依存关系解析的句子会给我们关于句子中单词之间关系的信息，但是使用成分关系解析的句子会帮助我们理解如何对句子进行分组。

![](Images/b2bd0657-7341-4f85-8aca-4d98ea4314c3.png)

图 7.2 说明了成分分析和依存分析之间的区别。鸣谢:维基百科编辑 Tjo3ya [ [4](https://en.wikipedia.org/wiki/Phrase_structure_grammar#/media/File:Thistreeisillustratingtherelation(PSG).png) ]

我们可以从使用短语或成分解析的句子中提取什么样的信息？这种解析依赖于将句子分成短语，特别是分成主语(通常是名词短语 ( **NP** )和谓语(**动词短语** ( **VP** ))。从图中可以看出，单词之间的关系涉及多个链接。事实上，在这个例子中，我们看到了一个几乎递归的结构。句子的单词也被称为树叶，这里的每个短语都是节点。这对于找出一个句子中存在什么样的短语以及子短语是很有用的。由于这导致我们识别主语和宾语，我们有了一些关于单词上下文的语义信息，这些信息可能是以前不知道的。例如，考虑句子:*狮子吃了斑马。*

我们之前已经讨论了如何将单词表示为向量([第三章](be6f4ff3-1217-474a-8923-5eeeed17bfa1.xhtml)， *spaCy 的语言模型*)，其中一种表示是单词包表示。在这种情况下，我们只会意识到单词的存在(假设停用词被删除)*狮子*、*吃*和*斑马*。虽然狮子很可能真的吃了斑马，但我们不能*真的*确定，除非我们知道句子的顺序和结构——对句子的短语分析将为我们提供主语(狮子)和宾语(斑马),这将允许我们确认我们的直觉，即狮子确实吃了斑马。

同样，由于这不是一本语言学书籍，我们将不会进一步关注现有的语法种类，或者甚至是正在使用的解析技术(还有很多！维基百科上关于短语结构语法的文章[ [5](https://en.wikipedia.org/wiki/Phrase_structure_grammar) ]有一个有用的总结)，而是关于如何实际执行解析，以及如何解释和使用结果。

依存句法分析关注句子中单词之间的关系或依存关系。也就是说，在这样的解析过程中，可以表示多种依赖关系；最流行的是语义依赖、形态依赖、韵律依赖和句法依赖(Joakim Nivre 的文章[ [6](http://stp.lingfil.uu.se/~nivre/docs/05133.pdf) ]总结了其中一些依赖背后的理论，维基百科上关于依赖语法的页面也是如此[ [7](https://en.wikipedia.org/wiki/Dependency_grammar) ])。

在这一章中，我们将集中讨论一种特殊的依赖分析——句法依赖分析。这部分是因为依赖性分析中的大部分工作都涉及到句法依赖性分析，部分也是因为 spaCy 的分析算法是一个句法依赖性分析器。这种解析，顾名思义，就是给一个句子分配一个句法结构，在我们的例子中，这将是一棵树。

让我们总结一下这两种解析方法的一些区别。选区分析将一个句子分解成子短语，其中非末端节点是短语的类型，末端节点(叶子)是句子中的单词，边缘是未标记的。我们会用它们来理解句子中的短语，以及主语和宾语。

依存分析根据关系连接单词，树中的每个顶点代表一个单词。有子词和父节点，每条边都有标注，说明词与词之间的关系。

选区分析器和依存分析器在句子之间的第一次分解或拆分上也有所不同；选区分析器将一个句子分解成一个主语和一个宾语，通常是一个名词短语和一个动词短语。另一方面，依存解析器认为动词是句子的中心，所有的依存关系都是围绕它建立的。

我们已经谈了很多关于这些依赖性的问题，但是它们到底是什么呢？spaCy 使用*清除样式* [ [8](http://www.mathcs.emory.edu/~choi/doc/cu-2012-choi.pdf) 来标记它的依赖项。我们再次强调，理解语言依赖和它们的含义超出了本书的范围，我们鼓励读者使用前面提到的链接和研究文章来更新他们关于依赖的知识。话虽如此，让我们看看这个简单的例子:

狗比猫跑得快。

如果我们依存解析这个句子并可视化它(再次使用总是有用的`displaCy` [ [9](https://explosion.ai/demos/displacy) ])，这就是我们看到的。

![](Images/b612b19c-6cf0-4004-ba93-bac1c80f2080.png)

图 7.3 可视化依存解析

在这个例子中，词根是*是*，是句子的主要动词。狗是名词短语，标记为 **nsubj** ，表示句子的名词性主语。 **Acomp** 的意思是形容词性的补语，意思是修饰形容词或者给形容词增加意义的从句或者短语。*比*这个词是我们的介词；而 **pobj** 代表介词的宾语，这里是*猫*。

为了更快地汇总 spaCy 分配的标签，注释页面[ [10](https://spacy.io/api/annotation#dependency-parsing) ]非常有用。

我们现在对什么是依存解析有了一个概念，并了解了为什么它们在我们的文本分析任务中非常有用。但是这些短语或依赖关系的信息在哪里会派上用场呢？

像大多数 NLP 任务一样，完成一项任务可以极大地帮助其他任务。在这种情况下，用短语规则解析句子可以帮助我们进行 NER 标记。我们记得在前一章中，名词组块通常被标记为一个完整的实体，这些组块通常在解析后被识别。解析器的另一个主要用途是在机器翻译中，语义和句法信息非常重要。由于我们在执行解析时正在构建树，所以我们可以转换这种树，并将其表示为知识图，其中包含关于单词以及它们如何相互关联的信息。使用这样的知识图作为中间步骤，我们可以尝试执行语言不可知的翻译。

这种句子的知识图表示在构建聊天机器人或系统时也很有帮助，我们必须理解需要执行的任务——在这种情况下，识别动作非常重要。解析还可以帮助验证句子的语法正确性。

但是让我们超越语法正确性，尝试解决另一个问题:歧义。像大多数语言一样，英语并不总是直截了当的，一个逗号就能改变一个句子的意思。考虑下面两句话:

我看见一个拿着望远镜的女孩。

我看见一个女孩，拿着望远镜。

虽然两个句子看起来意思相同，但第二个句子中的逗号完全改变了这一点。第一句暗示了主语，我看到一个有望远镜的女孩。另一方面，第二句话暗示受试者看到一个女孩*使用*望远镜。spaCy 的依赖解析器是如何处理这个的？

![](Images/89d58a9a-155f-4ad5-a00d-dc1bff4a94be.png)

图 7.4 不带逗号的可视化

不出所料，没有逗号，依存关系将*一个女孩*、*与*和*望远镜*联系起来，暗示这个女孩拥有望远镜。

当我们再次想象它时，但这次句子中用了一个逗号:

![](Images/1366bac8-6e97-4da4-8d46-06fcd8184a63.png)

图 7.5 用逗号可视化

我们马上就能看到单词*与*以及*一个望远镜*是如何与词根动词 *saw* 联系在一起的。这是因为看的动作是用望远镜完成的。

在这里，依存关系帮助解决两个非常相似的句子之间的歧义。

很明显，依存句法分析有着丰富的应用。构造这样的解析器一直是自然语言处理中的一个问题，也是我们不打算解释或解决的一个问题，因为它们值得有自己的章节和理论基础。旧的方法大量使用基于规则的解析技术，这些技术依赖于所使用的语法。现在，我们已经像词性标注和 NER 标注一样，转向了统计方法来进行语法分析，在这种方法中，我们使用一种概率度量来标记我们的短语和依存关系，这种度量根据历史训练数据和一些基本规则来告诉我们句子最有可能的语法分析方式。我们已经在[第 5 章](e7fc28c4-94a9-41aa-8836-ad4c9a55e880.xhtml)、*词性标注及其应用*和[第 6 章](6aa68517-4c79-4d41-8cc2-04b73b85e6f0.xhtml)、 *NER 标注及其应用*中看到了训练这种模型的两个实例，所以现在你应该对这个过程很熟悉了。

像往常一样，Python 编程语言为我们提供了丰富的工具和库来执行依赖解析——让我们转到下一节讨论这个问题。



# Python 中的依赖解析

很容易在[第 4 章](ad7d7774-1bef-4a55-ae7c-0d77097a43b7.xhtml)、 *Gensim -矢量化文本和转换以及 n-grams* 、[第 5 章](e7fc28c4-94a9-41aa-8836-ad4c9a55e880.xhtml)、*词性标注及其应用*和[第 6 章](6aa68517-4c79-4d41-8cc2-04b73b85e6f0.xhtml)、*NER-标注及其应用*中发现这种趋势——所有这些都选择 spaCy 作为首选实现，不仅仅是因为其准确性和速度，还因为它自然地适合我们的文本分析管道。我们仍然讨论了可用于执行该任务的其他 Python 库，我们将对依赖解析做同样的事情。

像往常一样，我们将从 NLTK 开始，它提供了关于解析方法的大多数选项，但是与前面的情况不同，它不是一个如此直观的 API，并且我们被迫传递自己的语法来获得有效的结果。我们的目的不是在运行计算语言学算法之前学习语法，这也是我们总是更喜欢 spaCy 作为行业强度代码的另一个原因。

然而，我们要做的是演示如何使用用 NLTK 包装的**斯坦福依赖解析器**。

第一步是从*斯坦福依赖解析器*页面【 [11](https://nlp.stanford.edu/software/nndep.shtml) 】下载必要的 JAR 文件(仅仅为了历史值，也值得您查看一下斯坦福的另一个*统计解析器* [ [12](https://nlp.stanford.edu/software/lex-parser.shtml) )。).

```
from nltk.parse.stanford import StanfordDependencyParser 
path_to_jar = 'path_to/stanford-parser-full-2014-08-27/stanford-parser.jar' 
path_to_models_jar = 'path_to/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar' 
dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)
```

前面几行代码演示了如何将 Stanford JAR 文件加载到我们的 Python NLTK 接口中，与前面的 POS-tagging 和 NER-tagging 示例一样，它链接到您机器上的 JAR 文件。确保发布文件目录的路径。

```
result = dependency_parser.raw_parse('I shot an elephant in my sleep')
dep = result._next_()
list(dep.triples())
```

如果您打印该列表，这是我们期望的输出:

```
[((u'shot', u'VBD'), u'nsubj', (u'I', u'PRP')), 
((u'shot', u'VBD'), u'dobj', (u'elephant', u'NN')), ((u'elephant', u'NN'), u'det', (u'an', u'DT')), 
((u'shot', u'VBD'), u'prep', (u'in', u'IN')), 
((u'in', u'IN'), u'pobj', (u'sleep', u'NN')), 
((u'sleep', u'NN'), u'poss', (u'my', u'PRP$'))]
```

我们可以看到，动词`shot`扎根于树的根部。

这是我们将使用 NLTK 演示的范围，但是如果读者希望定义一种语法并使用更多的学术统计或基于规则的解析技术，下面的链接将对此进行说明:

1.  NLTK 依存语法[ [13](http://www.nltk.org/howto/dependency.html) ]

2.  NLTK 书第八章:分析句子结构[ [14](http://www.nltk.org/book/ch08.html) ]

3.  在 Windows 和 Linux 上使用 Python 中的 NLTK 配置斯坦福解析器和斯坦福 NER 标记器[ [15](https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a) ]

我们现在将直接跳到 spaCy 的依赖解析 API。



# 带空间的依存句法分析

如果你已经阅读了这本书的每一章，直到这一章，你就已经多次完成了对数据的依赖解析；文本在管道中的每次运行都已经注释了文档中句子中的单词，以及它们与句子中其他单词的依赖关系。让我们再次设置我们的模型，类似于我们在前面章节中所做的。

```
import spacy
nlp = spacy.load('en')
```

现在我们的管道已经准备好了，我们可以开始分析我们的句子了。

spaCy 的管道解析部分既进行短语解析又进行依存解析——这意味着我们可以获得关于句子中的名词和动词块的信息，以及关于单词之间的依存关系的信息。

短语分析也可以被称为组块，因为我们得到的*个组块*是句子的一部分，也就是短语。这些组块存储在每个句子的`noun_chunks`属性中。

让我们用三个简单的句子来说明这一点:

```
sent_0 = nlp(u'Myriam saw Clement with a telescope.') 
sent_1 = nlp(u'Self-driving cars shift insurance liability 
             toward manufacturers.') 
sent_2 = nlp(u'I shot the elephant in my pyjamas.') 

for chunk in sent_0.noun_chunks: 
    print(chunk.text, chunk.root.text, chunk.root.dep_, 
          chunk.root.head.text) 

(u'Myriam', u'Myriam', u'nsubj', u'saw') 
(u'Clement', u'Clement', u'dobj', u'saw') 
(u'a telescope', u'telescope', u'pobj', u'with') 
```

我们可以看到，现在我们有了块、根文本(我们可以在`a telescope`块中看到，它的根是`telescope`)、依赖类型和头。不出所料，由于动词是`saw`，它是`Myriam`和`Clement`的中心，其中`Myriam`是主语，`Clement`是宾语。

下一句话更好地概括了组块的概念。

```
for chunk in sent_1.noun_chunks:
    print(chunk.text, chunk.root.text, chunk.root.dep_, 
          chunk.root.head.text)

(u'Self-driving cars', u'cars', u'nsubj', u'shift')
(u'insurance liability', u'liability', u'dobj', u'shift')
(u'manufacturers', u'manufacturers', u'pobj', u'toward')
```

我们有三个名词短语，其中`Self-driving cars`和`insurance liability`让我们更清楚地知道什么是名词短语——这里`Self-driving`和`insurance`修饰词根名词`car`和`liability`。`manufacturers`是句子的最后一个名词，是动词`toward`的宾语。

我们的最后一个例子更简单:

```
for chunk in sent_2.noun_chunks:
    print(chunk.text, chunk.root.text, chunk.root.dep_, 
          chunk.root.head.text)

(u'I', u'I', u'nsubj', u'shot')
(u'the elephant', u'elephant', u'dobj', u'shot')
(u'my pyjamas', u'pyjamas', u'pobj', u'in')
```

单词`the`和`my`标识了大象和睡衣，是我们名词短语的一部分。

现在让我们再来看看我们的句子，但是用单个的单词代替短语。记下我们在前面的例子中是如何访问块的，以及我们将如何在接下来的例子中访问令牌。

```
for token in sent_0:
    print(token.text, token.dep_, token.head.text, token.head.pos_, 
          [child for child in token.children])

(u'Myriam', u'nsubj', u'saw', u'VERB', [])
(u'saw', u'ROOT', u'saw', u'VERB', [Myriam, Clement, with, .])
(u'Clement', u'dobj', u'saw', u'VERB', [])
(u'with', u'prep', u'saw', u'VERB', [telescope])
(u'a', u'det', u'telescope', u'NOUN', [])
(u'telescope', u'pobj', u'with', u'ADP', [a])
(u'.', u'punct', u'saw', u'VERB', [])
```

输出类似于名词块示例，增加了一个包含节点子节点(如果有)的列表。在前面的例子中，我们可以立即看到单词`saw`(根动词)是头节点，有四个子节点依赖于它，这在列表中是可见的。

依赖关系与我们之前在名词块的例子中观察到的一样。

```
for token in sent_1:
    print(token.text, token.dep_, token.head.text, token.head.pos_, 
          [child for child in token.children])

(u'Autonomous', u'amod', u'cars', u'NOUN', [])
(u'cars', u'nsubj', u'shift', u'VERB', [Autonomous])
(u'shift', u'ROOT', u'shift', u'VERB', [cars, liability, .])
(u'insurance', u'compound', u'liability', u'NOUN', [])
(u'liability', u'dobj', u'shift', u'VERB', [insurance, toward])
(u'toward', u'prep', u'liability', u'NOUN', [manufacturers])
(u'manufacturers', u'pobj', u'toward', u'ADP', [])
(u'.', u'punct', u'shift', u'VERB', [])
```

随着动词越来越多，我们的解析看起来更加有趣——我们可以看到动词`shift`是如何与句子中的各种单词相联系的。我们现在给用户一个练习——使用上面提供的信息，为句子画出你自己的依存关系图，并使用 displaCy 验证这一点。

```
for token in sent_2:
    print(token.text, token.dep_, token.head.text, token.head.pos_, 
          [child for child in token.children])

(u'I', u'nsubj', u'shot', u'VERB', [])
(u'shot', u'ROOT', u'shot', u'VERB', [I, elephant, .])
(u'the', u'det', u'elephant', u'NOUN', [])
(u'elephant', u'dobj', u'shot', u'VERB', [the, in])
(u'in', u'prep', u'elephant', u'NOUN', [pyjamas])
(u'my', u'poss', u'pyjamas', u'NOUN', [])
(u'pyjamas', u'pobj', u'in', u'ADP', [my])
(u'.', u'punct', u'shot', u'VERB', [])
```

我们的最后一个例子很简单，没有任何异常。

现在，让我们看看导航该树的其他方法。我们已经说过每个句子只有一个中心，有时我们会想识别这个。一种方法是从下往上迭代，即迭代可能的主语，而不是可能的动词。

例如，遍历主题将如下所示:

```
from spacy.symbols import nsubj, VERB

verbs = set()
for possible_subject in sent_1:
    if possible_subject.dep == nsubj and possible_subject.head.pos == VERB: 
        verbs.add(possible_subject.head)
```

我们已经遍历了所有的单词，并检查了有名词性主语(`nsubj`)的情况，以及单词的中心是动词的情况。当我们打印`verbs`时，对句子 1 运行这个命令会得到以下结果:

```
{shift}
```

这是我们期望看到的！

也可以直接搜索动词，但这需要两倍的迭代次数。

`doc`变量是一个占位符变量，您需要传递自己的文档。

```
verbs = []
for possible_verb in doc:
    if possible_verb.pos == VERB:
        for possible_subject in possible_verb.children:
            if possible_subject.dep == nsubj:
                verbs.append(possible_verb)
                break
```

虽然这给出了相同的结果，但是注意有两个 for 循环。

空间还为我们提供了一些有用的属性，比如`lefts`、`rights`、`n_rights`和`n_lefts`。这为我们提供了关于树中特定令牌左边是什么，右边是什么，以及二者的数量的信息。

让我们来看看这个例子，用句法中心来寻找短语。

```
root = [token for token in sent_1 if token.head == token][0]
subject = list(root.lefts)[0]
for descendant in subject.subtree:
    assert subject is descendant or subject.is_ancestor(descendant)
    print(descendant.text, descendant.dep_, descendant.n_lefts, 
          descendant.n_rights, [ancestor.text for ancestor in 
          descendant.ancestors])
```

我们通过检查头是令牌本身来找到根。主题应该在这棵树的左边，所以我们对此进行检查。然后，我们遍历主题并打印适当的后代和其他叶子的数量。让我们来看看运行上述代码后我们的一个句子的输出:

```
(u'Autonomous', u'amod', 0, 0, [u'cars', u'shift'])
(u'cars', u'nsubj', 1, 0, [u'shift'])
```

spaCy 关于依赖解析的部分[ [16](https://spacy.io/usage/linguistic-features#section-dependency-parse) ]有这些例子和更多的例子(尽管解释较少)，我们强烈建议您访问这个页面。注释页[ [10](https://spacy.io/api/annotation#dependency-parsing) 进一步推荐阅读。

我们如何在一个更现实的例子中使用它的一个例子是，例如，识别常用的形容词来描述一本书中的一个角色。

`book`变量是一个占位符变量，您需要传递自己的文档。

```
adjectives = [] 
for sent in book.sents:  
    for word in sent:  
        if 'Character' in word.string:  
            for child in word.children:  
                if child.pos_ == 'ADJ': 
                adjectives.append(child.string.strip()) 
Counter(adjectives).most_common(10) 
```

代码本身非常简单，但却能有效地完成工作。我们迭代我们的书籍句子，在句子中寻找我们的角色，寻找该角色的孩子，并检查孩子是否是一个形容词。它是一个孩子意味着这个单词可能已经被标记为一个依赖项，由孩子来描述词根(这里，*字符*取决于它是谁)。通过检查最常见的形容词，我们可以对书中的人物做一个小小的分析。

现在让我们继续训练我们自己的解析器吧！



# 训练我们的依存解析器

同样，如果你已经阅读了[第 4 章](ad7d7774-1bef-4a55-ae7c-0d77097a43b7.xhtml)、 *Gensim -向量化文本和转换以及 n-grams* 、[第 5 章](e7fc28c4-94a9-41aa-8836-ad4c9a55e880.xhtml)、*词性标注及其应用*和[第 6 章](6aa68517-4c79-4d41-8cc2-04b73b85e6f0.xhtml)、 *NER 标注及其应用*，那么你会对 spaCy 中训练我们自己的模型背后的理论感到很舒服。我们建议您回去阅读第 4 章 Gensim 中的*矢量变换部分和第 5 章的*训练我们自己的标注者*部分，以更新您对机器学习，特别是空间环境中训练的确切含义的想法。*

同样，spaCy 的优势是我们不需要关心正在使用的算法，或者哪些特性是选择依赖解析的最佳选择——这通常是机器学习研究中最困难的部分。我们知道已经选择了一个最佳的学习算法，我们所要关心的是通过适当的训练示例和设置 API，以便我们适当地更新我们的模型。这就是我们在接下来的两个代码示例中要做的事情。

两者中的第一个告诉我们如何从空白模型开始更新依赖解析器，源代码可以在`train_parser.py`文件[ [17](https://github.com/explosion/spacy/blob/master/examples/training/train_parser.py) 中找到。

```
from __future__ import unicode_literals, print_function

import plac
import random
from pathlib import Path
import spacy
```

像往常一样，我们先从导入开始，然后再讨论我们的训练数据。

```
# training data
TRAIN_DATA = [ 
    ("They trade mortgage-backed securities.", { 
        'heads': [1, 1, 4, 4, 5, 1, 1], 
        'deps': ['nsubj', 'ROOT', 'compound', 'punct', 'nmod', 'dobj', 
                 'punct'] 
    }), 
    ("I like London and Berlin.", { 
        'heads': [1, 1, 1, 2, 2, 1], 
        'deps': ['nsubj', 'ROOT', 'dobj', 'cc', 'conj', 'punct'] 
    })
]
```

我们需要在训练数据中给出 heads 和 dependency 标签的例子。快速浏览一下我们的训练数据就可以证实这一点；在这两个例子中，动词是索引为 0 的单词，依赖关系相当简单。

```
@plac.annotations( 
    model=("Model name. Defaults to blank 'en' model.", "option", "m", 
           str), 
    output_dir=("Optional output directory", "option", "o", Path), 
    n_iter=("Number of training iterations", "option", "n", int)) 
def main(model=None, output_dir=None, n_iter=10): 
    """Load the model, set up the pipeline and train the parser.""" 
    if model is not None: 
        nlp = spacy.load(model)  # load existing spaCy model 
        print("Loaded model '%s'" % model) 
    else: 
        nlp = spacy.blank('en')  # create blank Language class 
        print("Created blank 'en' model") 
```

这一步同样类似于我们的其他培训示例，我们加载一个空白模型。

```
    # add the parser to the pipeline if it doesn't exist    
    # nlp.create_pipe works for built-ins that are registered with spaCy 
    if 'parser' not in nlp.pipe_names: 
        parser = nlp.create_pipe('parser') 
        nlp.add_pipe(parser, first=True) 
    # otherwise, get it, so we can add labels to it 
    else: 
        parser = nlp.get_pipe('parser') 
```

这里的注释相当简单明了；如果不存在解析器，我们在管道中添加一个解析器，如果存在，我们添加标签。

```
    # add labels to the parser 
    for _, annotations in TRAIN_DATA: 
        for dep in annotations.get('deps', []): 
            parser.add_label(dep)
    # get names of other pipes to disable them during training 
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'parser'] 
    with nlp.disable_pipes(*other_pipes):  # only train parser 
        optimizer = nlp.begin_training() 
        for itn in range(n_iter): 
            random.shuffle(TRAIN_DATA) 
            losses = {} 
            for text, annotations in TRAIN_DATA: 
                nlp.update([text], [annotations], sgd=optimizer, 
                           losses=losses) 
            print(losses) 
```

我们遵循前一章的训练示例的相同过程，其中我们添加标签，禁用管道的其他部分，以便我们只训练解析器。

```
    # test the trained model 
    test_text = "I like securities." 
    doc = nlp(test_text) 
    print('Dependencies', [(t.text, t.dep_, t.head.text) for t in doc])
    # save model to output directory 
    if output_dir is not None: 
        output_dir = Path(output_dir) 
        if not output_dir.exists(): 
            output_dir.mkdir() 
        nlp.to_disk(output_dir) 
        print("Saved model to", output_dir)
        # test the saved model 
        print("Loading from", output_dir) 
        nlp2 = spacy.load(output_dir) 
        doc = nlp2(test_text) 
        print('Dependencies', [(t.text, t.dep_, t.head.text) for t in doc]) 
```

最后的步骤包括训练我们的模型，并将其保存到适当的目录中。

```
if __name__ == '__main__': 
    plac.call(main) 
```

在运行主文件时，我们应该会看到以下输出:

```
[ 
    ('I', 'nsubj', 'like'), 
    ('like', 'ROOT', 'like'), 
    ('securities', 'dobj', 'like'), 
    ('.', 'punct', 'like') 
]
```

虽然前面的训练示例相当普通，它遵循了与 POS 和 NER 标签完全相同的风格，但我们可以用解析做更多有趣的事情；例如，添加我们自己的自定义语义。

这是什么意思？我们现在可以训练我们的解析器理解单词之间新的语义关系或依赖性。spaCy 文档页面提供了以下示例来说明这一点:

![](Images/087df735-a74c-4ca0-8a82-9769e694a62d.png)

图 7.6 具有附加依赖性“质量”的依赖性解析

这特别有趣，因为我们可以对我们自己的依赖关系建模，这对我们的特定用例是有用的；尽管我们必须记住，它可能不总是导致*正确的*依存解析，但是它在封装单词之间的关系方面仍然是有用的。

完成这个训练的代码可以在`train_intent_parser.py` [ [18](https://github.com/explosion/spacy/blob/master/examples/training/train_intent_parser.py) 文件中找到。

根据文件中的注释，在本例中，我们将为一个常见的*聊天意图*构建一个消息解析器:查找本地企业。我们的消息语义将有以下类型的关系:`ROOT`、`PLACE`、`QUALITY`、`ATTRIBUTE`、`TIME`和`LOCATION`。

```
"show me the best hotel in berlin"

('show', 'ROOT', 'show')
('best', 'QUALITY', 'hotel') --> hotel with QUALITY best
('hotel', 'PLACE', 'show') --> show PLACE hotel
('berlin', 'LOCATION', 'hotel') --> hotel with LOCATION berlin

```

现在让我们从代码开始。

```
from __future__ import unicode_literals, print_function 

import plac 
import random 
import spacy 
from pathlib import Path

# training data: texts, heads and dependency labels
# for no relation, we simply chose an arbitrary dependency label, e.g. '-'
TRAIN_DATA = [ 
    ("find a cafe with great wifi", { 
        'heads': [0, 2, 0, 5, 5, 2],  # index of token head 
        'deps': ['ROOT', '-', 'PLACE', '-', 'QUALITY', 'ATTRIBUTE'] 
    }), 
    ("find a hotel near the beach", { 
        'heads': [0, 2, 0, 5, 5, 2], 
        'deps': ['ROOT', '-', 'PLACE', 'QUALITY', '-', 'ATTRIBUTE'] 
    }), 
    ("find me the closest gym that's open late", { 
        'heads': [0, 0, 4, 4, 0, 6, 4, 6, 6], 
        'deps': ['ROOT', '-', '-', 'QUALITY', 'PLACE', '-', '-', 
                 'ATTRIBUTE', 'TIME'] 
    }), 
    ("show me the cheapest store that sells flowers", { 
        'heads': [0, 0, 4, 4, 0, 4, 4, 4],  # attach "flowers" to store! 
        'deps': ['ROOT', '-', '-', 'QUALITY', 'PLACE', '-', '-', 'PRODUCT'] 
    }), 
    ("find a nice restaurant in london", { 
        'heads': [0, 3, 3, 0, 3, 3], 
        'deps': ['ROOT', '-', 'QUALITY', 'PLACE', '-', 'LOCATION'] 
    }), 
    ("show me the coolest hostel in berlin", { 
        'heads': [0, 0, 4, 4, 0, 4, 4], 
        'deps': ['ROOT', '-', '-', 'QUALITY', 'PLACE', '-', 'LOCATION'] 
    }), 
    ("find a good italian restaurant near work", { 
        'heads': [0, 4, 4, 4, 0, 4, 5], 
        'deps': ['ROOT', '-', 'QUALITY', 'ATTRIBUTE', 'PLACE', 'ATTRIBUTE', 
                 'LOCATION'] 
    }) 
]
```

我们有必要仔细研究一下培训示例。就像评论里提到的，`ROOT`、`PLACE`、`QUALITY`、`ATTRIBUTE`、`TIME`、`LOCATION`是我们新的依赖。我们的例子说明了这一点，例子中的一些品质是`coolest`、`good`、`great`和`closest`。像`near`、`open`这样的词被标记为属性，以区别于品质。地点、时间和位置都是非常明确的依赖关系。在构建语义信息图时，这种信息非常有用。

```
@plac.annotations( 
    model=("Model name. Defaults to blank 'en' model.", "option", "m", 
           str), 
    output_dir=("Optional output directory", "option", "o", Path), 
    n_iter=("Number of training iterations", "option", "n", int)) 
def main(model=None, output_dir=None, n_iter=5): 
    """Load the model, set up the pipeline and train the parser.""" 
    if model is not None: 
        nlp = spacy.load(model)  # load existing spaCy model 
        print("Loaded model '%s'" % model) 
    else: 
        nlp = spacy.blank('en')  # create blank Language class 
        print("Created blank 'en' model")
    # We'll use the built-in dependency parser class, but we want to create
    # a fresh instance - just in case. 
    if 'parser' in nlp.pipe_names: 
        nlp.remove_pipe('parser') 
    parser = nlp.create_pipe('parser') 
    nlp.add_pipe(parser, first=True) 

    for text, annotations in TRAIN_DATA: 
        for dep in annotations.get('deps', []): 
            parser.add_label(dep) 
```

训练示例仍然是唯一真正的变化；我们可以看到，这一步反映了前面的培训示例。

```
    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'parser'] 
    with nlp.disable_pipes(*other_pipes):  # only train parser 
        optimizer = nlp.begin_training() 
        for itn in range(n_iter): 
            random.shuffle(TRAIN_DATA) 
            losses = {} 
            for text, annotations in TRAIN_DATA: 
                nlp.update([text], [annotations], sgd=optimizer, 
                           losses=losses) 
            print(losses)
    # test the trained model 
    test_model(nlp)
    # save model to output directory 
    if output_dir is not None: 
        output_dir = Path(output_dir) 
        if not output_dir.exists(): 
            output_dir.mkdir() 
        nlp.to_disk(output_dir) 
        print("Saved model to", output_dir)
    # test the saved model 
        print("Loading from", output_dir) 
        nlp2 = spacy.load(output_dir) 
        test_model(nlp2) 

def test_model(nlp): 
    texts = ["find a hotel with good wifi", 
             "find me the cheapest gym near work", 
             "show me the best hotel in berlin"] 
    docs = nlp.pipe(texts) 
    for doc in docs: 
        print(doc.text) 
        print([(t.text, t.dep_, t.head.text) for t in doc 
              if t.dep_ != '-']) 

if __name__ == '__main__': 
    plac.call(main) 
```

其余步骤照此进行；让我们看看运行主模块时的结果。

```
find a hotel with good wifi 
 [ 
      ('find', 'ROOT', 'find'), 
      ('hotel', 'PLACE', 'find'), 
      ('good', 'QUALITY', 'wifi'), 
      ('wifi', 'ATTRIBUTE', 'hotel') 
    ] 
    find me the cheapest gym near work 
    [ 
      ('find', 'ROOT', 'find'), 
      ('cheapest', 'QUALITY', 'gym'), 
      ('gym', 'PLACE', 'find') 
      ('work', 'LOCATION', 'near') 
    ] 
    show me the best hotel in berlin 
    [ 
      ('show', 'ROOT', 'show'), 
      ('best', 'QUALITY', 'hotel'), 
      ('hotel', 'PLACE', 'show'), 
      ('berlin', 'LOCATION', 'hotel') 
    ]
```

瞧，这就是我们期望看到的！

这个例子说明了在创建我们的定制模型时 spaCy 的真正力量；我们不仅可以用特定领域的数据重新训练我们的模型以更好地满足我们的规范，还可以训练全新的依赖关系。再加上一个非常容易使用的训练 API，很明显，它在所有实际的 NLP 应用程序中处于领先地位。

当考虑空间和依赖解析时，一些有用的链接也可能是相关的:

1.  具有空间[ [19](https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy) ]的依赖树
2.  用 500 行 Python [ [20](https://explosion.ai/blog/parsing-english-in-python) ]解析英语



# 摘要

这就把我们带到了空间和依赖解析这一章的结尾。前四章已经说明了空间的许多力量，以及我们如何利用这些力量。特别是依存句法分析，对我们来说仍然非常重要，因为寻找句子中单词之间的语义或句法关系可以有许多用途，无论是简单地识别特定单词最常用的形容词或副词，还是映射自定义关系。

在接下来的章节中，我们将从基于计算语言学的算法转移到基于信息检索的算法来进行我们的文本分析。特别是，这将是主题模型以及聚类和分类算法。



# 参考

[1]结构语法介绍:
[http://www . home . uni-osnabrueck . de/bschwisc/archives/tesniere . pdf](http://www.home.uni-osnabrueck.de/bschwisc/archives/tesniere.pdf)

[2]术语逻辑:
[https://Plato . Stanford . edu/archives/win 2016/entries/Logic-ancient/](https://plato.stanford.edu/archives/win2016/entries/logic-ancient/)

[3]https://en.wikipedia.org/wiki/Noam_Chomsky:

[4]图片链接:
[https://en . Wikipedia . org/wiki/Phrase _ structure _ grammar #/media/File:thisreeisillstratingtherelation(PSG)。巴布亚新几内亚](https://en.wikipedia.org/wiki/Phrase_structure_grammar#/media/File:Thistreeisillustratingtherelation(PSG).png)

[5]短语结构语法:
[https://en.wikipedia.org/wiki/Phrase_structure_grammar](https://en.wikipedia.org/wiki/Phrase_structure_grammar)

[6]依存语法与依存句法分析:
[http://stp.lingfil.uu.se/~nivre/docs/05133.pdf](http://stp.lingfil.uu.se/~nivre/docs/05133.pdf)

[7]依存语法:
[https://en.wikipedia.org/wiki/Dependency_grammar](https://en.wikipedia.org/wiki/Dependency_grammar)

[8]清体:
[http://www.mathcs.emory.edu/~choi/doc/cu-2012-choi.pdf](http://www.mathcs.emory.edu/~choi/doc/cu-2012-choi.pdf)

[9]displaCy:
[https://explosion.ai/demos/displacy](https://explosion.ai/demos/displacy)

[10]空间注释页:
[https://spacy.io/api/annotation#dependency-parsing](https://spacy.io/api/annotation#dependency-parsing)

[11]斯坦福依存解析器:
[https://nlp.stanford.edu/software/nndep.shtml](https://nlp.stanford.edu/software/nndep.shtml)

[12]斯坦福统计解析器:
[https://nlp.stanford.edu/software/lex-parser.shtml](https://nlp.stanford.edu/software/lex-parser.shtml)

[13] NLTK 依存文法:
[http://www.nltk.org/howto/dependency.html](http://www.nltk.org/howto/dependency.html)

[14]分析句子结构:
[http://www.nltk.org/book/ch08.html](http://www.nltk.org/book/ch08.html)

[15]在 Windows 和 Linux 上用 Python 中的 NLTK 配置斯坦福解析器和斯坦福 NER Tagger:
[https://blog . manash . me/Configuring-Stanford-Parser-and-Stanford-ner-Tagger-with-NLTK-in-Python-on-Windows-f 685483 c 374 a](https://blog.manash.me/configuring-stanford-parser-and-stanford-ner-tagger-with-nltk-in-python-on-windows-f685483c374a)

[16] spaCy 依存解析:
[https://spaCy . io/usage/language-features # section-dependency-parse](https://spacy.io/usage/linguistic-features#section-dependency-parse)

[17] spaCy train 解析器:
[https://spaCy . io/usage/language-features # section-dependency-parse](https://spacy.io/usage/linguistic-features#section-dependency-parse)

[18]spaCy train intent parser:
[https://github . com/explosion/spaCy/blob/master/examples/training/train _ intent _ parser . py](https://github.com/explosion/spacy/blob/master/examples/training/train_intent_parser.py)

【19】如何用 spaCy 得到依赖树？:
[https://stack overflow . com/questions/36610179/how-to-get-dependency-tree-with-spacy](https://stackoverflow.com/questions/36610179/how-to-get-the-dependency-tree-with-spacy)

[20]解析 500 行 Python 中的英语:
[https://explosion.ai/blog/parsing-english-in-python](https://explosion.ai/blog/parsing-english-in-python)