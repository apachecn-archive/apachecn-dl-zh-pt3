<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 相似性查询和摘要

一旦我们开始用向量表示法来表示文本文档，就有可能开始寻找文档之间的相似性或距离，这正是我们将在本章中学习的内容。我们现在知道各种不同的向量表示，从标准的词袋或 TF-IDF 到文本文档的主题模型表示。我们还将了解 Gensim 中实现的一个非常有用的功能，以及如何使用它——摘要和关键词提取。以下是我们将从本章中学到的内容的总结:

*   相似性度量
*   相似性查询
*   文本摘要

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 相似性度量

相似性度量[ [1](https://en.wikipedia.org/wiki/Metric_(mathematics)) ]是一种数学构造，在自然语言处理中特别有用，尤其是在信息检索中。让我们首先试着理解什么是度量。我们可以将度量理解为一个函数，它定义了一个集合中每一对元素之间的距离，或向量。很明显这对我们很有用——我们可以根据距离来比较两个文档的相似程度。distance 函数返回的低值意味着这两个文档相似，而高值意味着它们完全不同。

虽然我们在示例中提到了文档，但是从技术上来说，我们可以比较一个集合中的任意两个元素——这也意味着我们可以比较由主题模型创建的两组主题。我们可以在文档的 TF-IDF 表示和 LSI 或 LDA 表示之间进行检查。

我们大多数人都已经知道一种距离或相似性度量——欧几里德度量。这是我们在高中数学中遇到的第一个距离度量，我们可能会看到它被用来计算二维空间(XY)中两点之间的距离。虽然我们不会深入度量的数学细节，但了解距离度量的四个特征是值得的。

d(x，y) >= 0
这必须是非负的。

d(x，y) = 0 <=> x = y
这里，如果 x 和 y 相同，那么距离一定为零。

d(x，y) = d(y，x)
这必须是对称的。

d(x，z) <= d(x,y) + d(y,z)
这必须服从三角形不等式定律。

![](Images/a3961f7b-e498-41a7-bb84-b11ebcd54c77.png)

图 11.1 函数成为度量的四个数学前提

Gensim(和 scikit-learn，以及大多数其他机器学习或科学计算包)认识到距离度量的重要性，并将它们作为包的一部分来实现，这意味着在文档或主题的上下文中使用它们很容易。

现在让我们来讨论我们如何实际使用这些——我们将遵循我为 Gensim 写的这个教程，你可以在链接[ [2](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/distance_metrics.ipynb) 中的[这里](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/distance_metrics.ipynb)找到它。请注意，Gensim 笔记本没有 TfIdf 型号，本章添加这些型号是为了进一步帮助说明距离。

让我们首先提醒自己我们将要计算的确切距离——文档的两个向量表示。让我们建立我们的语料库和我们将要比较的文档。我们以前在第九章、*高级主题建模*中使用过这个版本的语料库来说明文档词主题的例子。

```
texts = [['bank','river','shore','water'], 
        ['river','water','flow','fast','tree'], 
        ['bank','water','fall','flow'], 
        ['bank','bank','water','rain','river'], 
        ['river','water','mud','tree'], 
        ['money','transaction','bank','finance'], 
        ['bank','borrow','money'],  
        ['bank','finance'], 
        ['finance','money','sell','bank'], 
        ['borrow','sell'], 
        ['bank','loan','sell']] 

dictionary = Dictionary(texts) 
corpus = [dictionary.doc2bow(text) for text in texts]
```

为下面的语料库创建 TF-IDF 和 LDA 模型将帮助我们说明我们的距离度量。

```
from gensim.models import ldamodel
from gensim.models import TfidfModel

tfidf = TfidfModel(corpus) 
model = ldamodel.LdaModel(corpus, id2word=dictionary, num_topics=2) 
```

注意，现在 TF-IDF 的表示将具有与词汇表大小一样多的特征，而 LDA 模型表示将具有与主题数量一样多的特征。我们稍后将使用这两个模型来比较距离。

现在，我们的话题是什么样的？

```
model.show_topics()

[(0, u'0.164*"bank" + 0.142*"water" + 0.108*"river" + 0.076*"flow" + 0.067*"borrow" + 0.063*"sell" + 0.060*"tree" + 0.048*"money" + 0.046*"fast" + 0.044*"rain"'), 
(1, u'0.196*"bank" + 0.120*"finance" + 0.100*"money" + 0.082*"sell" + 0.067*"river" + 0.065*"water" + 0.056*"transaction" + 0.049*"loan" + 0.046*"tree" + 0.040*"mud"')]
```

让我们用三个文档来比较一下——一个是关于河岸的文档，一个是关于金融银行的文档，还有一个是两者都有上下文的文档(可能是河岸上的金融银行？).

```
doc_water = ['river', 'water', 'shore']
doc_finance = ['finance', 'money', 'sell']
doc_bank = ['finance', 'bank', 'tree', 'water']
```

一旦我们有了我们的文档，我们很快就把它们转换成单词包、TF-IDF 和 LdaModel 表示。

```
bow_water = model.id2word.doc2bow(doc_water)
bow_finance = model.id2word.doc2bow(doc_finance)
bow_bank = model.id2word.doc2bow(doc_bank)

lda_bow_water = model[bow_water]
lda_bow_finance = model[bow_finance]
lda_bow_bank = model[bow_bank]

tfidf_bow_water = tfidf[bow_water]
tfidf_bow_finance = tfidf[bow_finance]
tfidf_bow_bank = tfidf[bow_bank]
```

让我们看看`lda_bow_water`，看看它是什么样子的:

```
[(0, 0.8225102558524345), (1, 0.17748974414756546)]
```

这是有道理的——该文档包含与河岸有关的单词，其 topic_0 的比例为 82%。`lda_bow_finance`变量应该大致相反——我们来测试一下:

```
[(0, 0.14753674420005805), (1, 0.852463255799942)]
```

瞧，正如我们所料——这两个文档的 LDA 表示完全不同，这一点我们甚至在构造文档时就能看出来。这意味着它们的距离也相当大，因为它们不是相似的文档。

让我们快速浏览一下`lda_bow_bank`:

```
[(0, 0.44153395450870797), (1, 0.558466045491292)]
```

就主题而言，这是一份平衡的文件(正如所料)。

让我们导入我们将使用的距离函数——海灵格度量 [ [3](https://en.wikipedia.org/wiki/Hellinger_distance) ]、**库尔巴克-莱布勒散度**函数[ [4](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) ]和**雅克卡指数** [ [5](https://en.wikipedia.org/wiki/Jaccard_index) 。Hellinger 和 KL-Divergence 是两个距离度量，帮助我们识别两个概率分布的相似性或差异性。链接 3、4 和 5 解释了这些指标背后的数学基础；我们应该记住的是，在决定比较两个文档并尝试两种方法时，没有一个完美的度量标准可供选择，这就是为什么我们包含了两种方法的示例。Jaccard 指数是一个更传统的指标，主要用于比较两组数据。

```
from gensim.matutils import kullback_leibler, jaccard, hellinger
```

让我们找出文档之间的距离:

```
hellinger(lda_bow_water, lda_bow_finance)
0.5125119977875359

hellinger(lda_bow_finance, lda_bow_bank)
0.2340730527221049

hellinger(lda_bow_bank, lda_bow_water)
0.28728176544255285
```

解释这些结果非常简单——我们发现文档返回的最大 Hellinger 距离与金融和水有关——它们没有太多共同点，所以这是一个好结果。世行文件包含了金融和水资源两方面的内容，与水资源和金融文件的距离相同，但似乎与水资源文件的距离更远(0.287 比 0.234)。这些是范围从 0 到 1 的相对值，其中 0 表示没有距离，0.5 可以直观地理解为在之间的*，1 表示它们相等。这也是有道理的——当我们看`lda_bow_bank`时，它更倾向于金融而不是水。*

**Try this:** A small exercise for the reader would be to identify why the bank document tend toward the finance topic – is the word bank in the document associated with finance or water? Doing a document word coloring would be a way to identify this!

如我们所见，使用这些距离度量，我们可以确定某些文档有多远或多近。在小语料库和小文档中，它的有用性可能不那么明显，但随着我们的继续，这变得非常有价值。我们可以用 KL 函数和 Jaccard 函数做同样的实验。一个微妙的点要记住；在最严格的意义上，Kullback-Leibler 函数不是度量。这是因为它不是对称的。这意味着例如`kullback_leibler(lda_bow_finance, lda_bow_bank)`不等于`kullback_leibler(lda_bow_bank, lda_bow_finance)`。

让我们来说明这一点；我们已经计算了水和财务文件之间的海灵格距离。通过交换金融和水资源文档来计算距离应该返回与之前相同的值，因为 Hellinger 度量是一个数学距离度量。

```
hellinger(lda_bow_finance, lda_bow_water)
0.5125119977875359
```

正如我们所料，我们得到了与之前相同的值，这证实了我们已经知道的关于海灵格的东西——它是一个对称的距离函数。让我们用 KL 函数做同样的练习。

```
kullback_leibler(lda_bow_water, lda_bow_bank)
0.30823547

kullback_leibler(lda_bow_bank, lda_bow_water)
0.36547804
```

这两种价值观相差不是很远，但它们并不相同；这意味着虽然 KL 函数可以给我们一个直觉，告诉我们两个概率分布有多远或多近，但它不是一个严格的数学距离度量。然而，这并没有降低它的有用性——接近 0 的值仍然被认为是相似的，接近 1 的值是不相似的。

我们的最后一个距离函数是流行的 Jaccard 度量。与其他距离函数不同，Jaccard 方法也适用于单词包。

```
jaccard(bow_water, bow_bank)
0.8571428571428572

jaccard(doc_water, doc_bank)
0.8333333333333334

jaccard(['word'], ['word'])
0.0
```

前面三个例子展示了两种不同的输入法。

在第一种情况下，我们向`jaccard`呈现已经是单词包格式的文档向量。距离可以被定义为 1 减去向量的并集的交集的大小。我们可以看到(人工检查也是如此),距离可能很长——事实也确实如此。最后两个例子说明了`jaccard`接受偶数列表(即文档)作为输入的能力。在最后一种情况下，因为它们是相同的向量，返回值是`0`——这意味着距离是 0，它们非常相似。

我们也可以使用这些距离函数来发现主题本身的远近。虽然用更大的语料库和更大的词汇表来尝试这样做更有用，但我们还是会尝试一下。首先，我们必须对我们显示主题的方式做出适当的改变，以便我们可以将它传递到我们的距离函数中。

```
def make_topics_bow(topic):
    # takes the string returned by model.show_topics()
    # split on strings to get topics and the probabilities
    topic = topic.split('+')
    # list to store topic bows
    topic_bow = []
    for word in topic:
        # split probability and word
        prob, word = word.split('*')
        # get rid of spaces
        word = word.replace(" ","")
        # convert to word_type
        word = model.id2word.doc2bow([word])[0][0]
        topic_bow.append((word, float(prob)))
    return topic_bow
```

在将`model.show_topics()`的结果传递给这些函数时，我们可以创建适当的表示。

```
topic_water, topic_finance = model.show_topics()
finance_distribution = make_topics_bow(topic_finance[1])
water_distribution = make_topics_bow(topic_water[1])
```

例如，让我们看看`finance_distribution`会是什么样子。

```
[(3, 0.196),
 (12, 0.12),
 (10, 0.1),
 (14, 0.082),
 (2, 0.067),
 (0, 0.065),
 (11, 0.056),
 (15, 0.049),
 (5, 0.046),
 (9, 0.04)]
```

这基本上映射了词的 ID 和它在主题中的比例。

现在让我们运行以下代码:

```
hellinger(water_distribution, finance_distribution)
0.36453028040240248
```

小语料库和主题中单词库的重叠意味着距离似乎没有我们预期的那么大——一个有趣的实验是用更大的语料库生成更多的主题，并根据它们的相似程度对主题对进行排名——这将更好地说明我们一直在使用的距离度量。

这很大程度上总结了在文档和主题的上下文中使用距离函数——但我们应该记住，我们可以在主题分布的任何两个向量表示之间进行比较，所以这是一个有用的函数。

关于使用距离度量的更多细节，建议您运行 Jupyter 笔记本[ [2](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/distance_metrics.ipynb) ]来说明这些示例。

我们现在可以继续进行查询，并将这些距离度量用于更复杂的目的！

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 相似性查询

既然我们有能力在两个文档之间进行比较，我们就有可能设置我们的算法来为输入查询提取最相似的文档——只需索引每个文档，然后搜索语料库和查询之间返回的最低距离值，并返回具有最低距离值的文档——这些将是最相似的。然而，幸运的是，Gensim 有内置的结构来完成这个文档相似性任务！

我们将使用相似性模块来构建这个结构。

```
from gensim import similarities
```

我们之前提到过创建一个索引——我们可以通过相似性模块更快地完成这项工作。正如 Gensim 文档中提到的`Similarity`类——`Similarity`类将索引分成几个更小的子索引(**碎片**)，它们是基于磁盘的。如果您的整个索引适合内存(数十万个文档占 1 GB 内存)，您也可以直接使用`MatrixSimilarity`或`SparseMatrixSimilarity`类。这些方法更简单，但是伸缩性不好(它们将整个索引保存在 RAM 中，没有分片)。

因为我们有一个小的语料库，我们可以使用`MatrixSimilarity`类来创建我们的索引。

```
index = similarities.MatrixSimilarity(model[corpus])
```

我们根据语料库的 LDA 转换所产生的相似性创建了我们的索引。我们可以使用 TF-IDF 甚至单词包来创建相同的索引，但是在使用主题时，我们可以期待更好的性能。我们还应该记住，我们的查询应该与我们创建索引的表示在同一个输入空间中。

既然我们已经创建了索引，我们就可以通过查询在语料库中找到最相似的文档。我们用同一个`lda_bow_finance`文档，找出哪些文章最相似。

```
sims = index[lda_bow_finance]
```

Sims 现在包含了类似的文档；让我们更好地看看里面有什么。

```
print(list(enumerate(sims)))

[(0, 0.36124918),
 (1, 0.27387184),
 (2, 0.30807066),
 (3, 0.30388257),
 (4, 0.33108047),
 (5, 0.99913883),
 (6, 0.8764254),
 (7, 0.9970802),
 (8, 0.99956596),
 (9, 0.5114244),
 (10, 0.9995375)]
```

我们走吧！我们现在有了一个列表，其中包含每个文档和相应的相似性值。请记住，这些值是使用余弦相似度生成的 Gensim 没有插入我们自己的相似度指标的功能，因此在此之前，我们必须坚持使用余弦相似度-或创建我们自己的索引方法。

让我们看看哪些文档实际上被提取了，并根据它们的相似程度对它们进行排序。

```
sims = sorted(enumerate(sims), key=lambda item: -item[1])

for doc_id, similarity in sims:
    print(texts[doc_id], similarity)

['finance', 'money', 'sell', 'bank'] 0.99956596
['bank', 'loan', 'sell'] 0.9995375
['money', 'transaction', 'bank', 'finance'] 0.99913883
['bank', 'finance'] 0.9970802
['bank', 'borrow', 'money'] 0.8764254
['borrow', 'sell'] 0.5114244
['bank', 'river', 'shore', 'water'] 0.36124918
['river', 'water', 'mud', 'tree'] 0.33108047
['bank', 'water', 'fall', 'flow'] 0.30807066
['bank', 'bank', 'water', 'rain', 'river'] 0.30388257
['river', 'water', 'flow', 'fast', 'tree'] 0.27387184
```

很漂亮，是吧？通过简单的排序`sims`,我们得到了每个文档相似性的有序列表——然后我们打印原始文档。我们的查询是金融相关文档的 LDA 表示，相似性查询返回所有金融相关文档中最相似的，而与树和河流相关的文档最不相似——正如我们所料。

Gensim 网站上的教程[ [6](https://radimrehurek.com/topic_modeling_tutorial/3%20-%20Indexing%20and%20Retrieval.html) ]进行了类似的实验，但在维基百科语料库上——这是一个关于如何在更大的语料库上进行相似性查询的有用演示，如果你正在处理一个非常大的语料库，这是值得一查的。

一个与 Gensim 相关的项目， **simserver** [ [7](https://pypi.org/project/simserver/) ]包含更多专用的相似性查询功能，但是这个项目不再作为开源项目维护——也就是说，教程[ [8](https://radimrehurek.com/gensim/simserver.html) ]可能仍然是相关的，GitHub 源代码[ [9](https://github.com/RaRe-Technologies/gensim-simserver) ]可能会启发你的相似性查询工作。

上两节我们看到了什么？我们现在可以有效地比较两个概率分布，这意味着我们可以比较主题和文档。这意味着我们离创建自己的搜索引擎又近了一步——有了相似性模块，繁重的工作已经为我们完成了，我们有了一个现成的 API 来进行基本的查询！

在示例中，我们使用 LDA 模型进行距离计算并生成相似性指数。然而，我们可以使用文档的任何向量表示来生成它——由我们来决定哪一个对我们的用例最有效。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 总结文本

通常在文本分析中，对大量文本进行总结是很有用的——在深入分析之前对文本进行简单的浏览，或者识别文本中的关键词。它通常也是最终的游戏——它本身就是一个文本分析任务。我们不会致力于构建我们自己的文本摘要管道，而是专注于使用 Gensim 为我们提供的内置摘要 API。

重要的是要记住，Gensim 中包含的算法不会创建自己的句子，而是从我们运行算法的文本中提取关键句子。这个摘要器基于 **TextRank** 算法，来自 Mihalcea 等人的一篇文章，名为 *TextRank* [ [10](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf) ]。该算法后来由 Barrios 等人在另一篇文章*中通过引入一个 **BM25 排序函数** [ [12](https://en.wikipedia.org/wiki/Okapi_BM25) ]对自动摘要的 TextRank 的相似性函数* [ [11](https://arxiv.org/pdf/1602.03606.pdf) ]进行了改进。

必须指出的是，与迄今为止讨论的所有其他算法不同，自 Gensim 版本 3.4.0 [ [13](https://pypi.org/project/gensim/) ]起，Gensim 中的文本摘要模块仅适用于英语——它不允许您对文本进行预处理或添加自己的停用词。

为了说明摘要模块，我们将使用《哈利·波特与魔法石》电影中的一个故事。

```
from gensim.summarization import summarize
```

我们现在可以简单地使用摘要模块来创建摘要文本。

```
print(summarize(text))
```

记住复制你想要总结的文本，并将其存储在`text`变量中。在我们的例子中，我们将使用以下文本。

11 岁的哈利·波特一直过着平凡的生活，经常被乖戾冷酷的叔叔婶婶弗农和佩妮·德思礼虐待，被宠坏的儿子达力欺负。

海格解释了哈利隐藏的过去，他是詹姆和莉莉·波特的儿子，詹姆和分别是巫师和女巫，他们是如何被历史上最邪恶最强大的黑巫师伏地魔杀害的，这导致了一岁的哈利被送去和他的叔叔婶婶住在一起。

在那里，哈利还与另一个一年级新生德拉科·马尔福为敌，马尔福对赫敏有偏见，因为她是麻瓜的女儿，麻瓜是巫师们使用的一个术语，用来描述没有魔法能力的普通人。

他最终和罗恩、赫敏一起进了格兰芬多，而德拉科被分到了斯莱特林，就像他之前的家人一样。霍格沃茨开始上课时，尽管没有任何经验，哈利还是发现了自己骑扫帚飞行的天赋，并被招进了他家的魁地奇(一种竞争性的巫师运动，在空中进行)队担任找球手，据说这是最困难的角色。

当学校的校长阿不思·邓布利多被骗出霍格沃茨时，哈利、赫敏和罗恩害怕盗窃即将发生，他们自己从活板门下来。

多事之秋在最后的盛宴中结束，在此期间，格兰芬多赢得了学院杯。哈利回到女贞路过暑假，忘记告诉他们未成年的男女巫师禁止使用魔法，因此期待着假期的一些乐趣和安宁。

快速浏览一下，我们会发现这几乎涵盖了这本书的重要部分(对维基百科原文的进一步检查会对此有所帮助)。当然，这不是一个完美的故事总结——还需要一些微调。

如果我们只想从段落中挑选出最上面的句子，并作为列表返回，我们可以使用`split`选项，它返回一个字符串列表，而不是单个字符串。

我们还可以通过`ratio`参数或`word_count`参数来调整汇总器输出多少文本。使用`ratio`参数，您可以指定原始文本中句子的多少部分应该作为输出返回。默认值为 20%。

现在，让我们运行以下代码:

```
print (summarize(text, word_count=50))
```

我们得到这个:

```
He winds up in Gryffindor instead with Ron and Hermione while Draco is sorted into Slytherin, like his whole family before him. As classes begin at Hogwarts, Harry discovers his innate talent for flying on broomsticks despite no prior experience and is recruited into his House's Quidditch (a competitive wizards' sport played in the air) team as a Seeker, which is said to be the most difficult role.
```

这里所做的是什么被认为是排名第一的句子，这是被选择的，因为 50 个单词的限制只有一个句子可以晋级-我们在这里看到，如果句子很长，它并不总是非常短的摘要的最佳算法。

对读者来说，这是一个有趣的实验——试着在 IMDB 上对《哈利波特与魔法石》电影的剧情概要使用相同的摘要技术，并比较结果！

如前所述，这个模块还支持关键字提取。关键字提取与摘要生成(即句子提取)的工作方式相同，即算法试图找到重要的或似乎代表整个文本的单词。关键词不总是单个单词；在多词关键字的情况下，它们通常都是名词。

```
from gensim.summarization import keywords
print(keywords(text))

harry wizard wizarding wizards school hagridhermione year named powerful dark slytherin burns burning life constantly hogwarts magical final son quirrell magic like corridor cloak grubby report owl earlier railway voldemort powers power london 
```

```
desires come comes hidden dog standing stand protect protective events eventful despite explains houses house ron gryffindor instead game source requires unique skills possessed ordinary master

```

快速浏览一下单词，我们会发现它们确实是大纲中的关键词。

供您参考，关键字模块中涉及的其他参数如下:

*   `text (str)`:输入文本
*   `ratio (float, optional)`:如果没有选择`words`选项，则按照规定的比例减少句子数量，否则忽略该比例
*   `words (int, optional)`:返回的字数
*   `split (bool, optional)`:如果为真，是否拆分关键字
*   `scores (bool, optional)`:关键字是否得分
*   `pos_filter (tuple, optional)`:词性过滤器
*   `lemmatize (bool, optional)`:如果为真——对单词进行词条整理
*   `deacc (bool, optional)`:如果为真–移除重音

Gensim 教程中的一段摘录告诉了我们该算法的复杂性和时间:

![](Images/c66df3e1-6283-450c-b57f-22da6a233028.png)

图 11.2 描述运行时间对语料库大小的图

在前面的图中，我们看到了运行时间以及数据集的大小。为了创建不同大小的数据集，我们简单地采用了文本的前缀；换句话说，我们取书的前 n 个字符。该算法在时间上似乎是二次的，因此在将大型数据集插入汇总器之前需要小心。运行时间不同的一个原因是使用的数据结构。该算法使用图形表示数据，其中顶点(节点)是句子，然后在顶点之间构建加权边，表示句子之间的相互关系。这意味着每段文本将有不同的图形，从而使运行时间不同。在最坏的情况下，这种数据结构的大小是二次的(最坏的情况是当每个顶点都有一条边连接到其他每个顶点)。

Gensim 有另一种提取关键词的方法:Montemurro 和 Zanette 的基于熵的关键词提取算法。论文-*对书面语言中编码的语义信息进行量化* [ [14](https://arxiv.org/abs/0907.1558) 描述了算法，该算法使用每个单词在块中分布的熵来挑选关键词。

```
from gensim.summarization import mz_keywords

mz_keywords(text, scores=True, weighted=False, threshold=1.0)

[(u'had', 0.002358350743193241),
 (u'from', 0.002039753203785301),
 (u'hagrid', 0.002039753203785301),
 (u'hermione', 0.002039753203785301),
 (u'into', 0.002039753203785301),
 (u'hogwarts', 0.0017206396372542237),
 (u'an', 0.001400618744466898),
 (u'first', 0.001400618744466898),
 (u'ron', 0.001400618744466898),
 (u'slytherin', 0.001400618744466898),
 (u'trapdoor', 0.001400618744466898),
 (u'is', 0.00111564319627375),
 (u'dark', 0.0010787207994767374),
 (u'instead', 0.0010787207994767374),
 (u'snape', 0.0010787207994767374),
 (u'wizard', 0.0010787207994767374)]
```

**MZ 关键词**提取算法往往对较大的语料库表现更好。算法的复杂度是 *O(Nw)* ，其中 *N* 是文档中的字数，w 是唯一字的个数。该算法的参数如下:

*   `text (str)`:用于汇总的文档。
*   `blocksize (int, optional)`:分析中使用的块的大小。
*   `scores (bool, optional)`:是否用关键字返回分数。
*   `split (bool, optional)`:是否以列表形式返回结果。
*   `weighted (bool, optional)`:是否按词频加权评分。False 可用于较短的文本，并允许自动设定阈值。
*   `threshold (float or 'auto', optional)`:返回关键字的最低分，`auto`计算阈值为*n _ blocks/(n _ blocks+1.0)+1e-8*，使用`auto`搭配`weighted=False`。

有了文本相似性和文本摘要，我们现在可以构建更复杂的文本分析管道。在接下来的几章中，我们将致力于更高级的文本机器学习技术，如深度学习。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 摘要

在本章中，我们看到了如何使用基本的数学和信息检索方法来帮助识别两个文本文档的相似性或不相似性。我们还看到了如何将这些方法扩展到任何概率分布，例如主题模型本身——这可能特别方便，尤其是当我们处理的主题数量超过我们肉眼可以分析的数量时。摘要也是我们现在接触到的另一个有用的工具——因为它的工作原理是哪些关键词在一篇文章中提供了最多的信息，我们可以使用这些关键词的知识来进一步帮助我们建立自然语言处理管道。

我们现在将转向更高级的主题，涉及神经网络和文本数据的深度学习。其中包括 Word2Vec 和 Doc2Vec 等方法，以及浅层和深层神经网络。我们将在文本中探索 Python 包、理论以及这些深度学习方法的应用。

<title>Unknown</title>  <link href="../stylesheet.css" rel="stylesheet" type="text/css"> <link href="../page_styles1.css" rel="stylesheet" type="text/css">

# 参考

[1]相似度度量:
[https://en . Wikipedia . org/wiki/Metric _(数学)](https://en.wikipedia.org/wiki/Metric_(mathematics))

[2]距离度量 ipynb:
[https://github . com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Distance _ Metrics . ipynb](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/distance_metrics.ipynb)

[3]海灵格距离:
[https://en.wikipedia.org/wiki/Hellinger_distance](https://en.wikipedia.org/wiki/Hellinger_distance)

[4]KL-divergence:
[https://en . Wikipedia . org/wiki/kull back % E2 % 80% 93 lei bler _ divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)

[5]贾卡:[https://en . Wikipedia . org/wiki/jacard _ index](https://en.wikipedia.org/wiki/Jaccard_index)

[6] Gensim 维基百科举例:
[https://radimrehurek . com/topic _ modeling _ tutorial/3% 20-% 20 indexing % 20 和%20Retrieval.html](https://radimrehurek.com/topic_modeling_tutorial/3%20-%20Indexing%20and%20Retrieval.html)

[7]sim server:
[https://pypi.org/project/simserver/#description](https://pypi.org/project/simserver/#description)

[8]文档相似度服务器:
[https://radimrehurek.com/gensim/simserver.html](https://radimrehurek.com/gensim/simserver.html)

[9]Gensim SimServer GitHub:
[https://github.com/RaRe-Technologies/gensim-simserver](https://github.com/RaRe-Technologies/gensim-simserver)

[10]text rank:
[http://web . eecs . umich . edu/~ mihalcea/papers/mihalcea . em NLP 04 . pdf](http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)

[11]用于自动摘要的 TextRank 的相似度函数的变体:
[https://arxiv.org/pdf/1602.03606.pdf](https://arxiv.org/pdf/1602.03606.pdf)

[12]BM25:
[https://en.wikipedia.org/wiki/Okapi_BM25](https://en.wikipedia.org/wiki/Okapi_BM25)

[13]根西姆:
[https://pypi.org/project/gensim/#description](https://pypi.org/project/gensim/#description)

[14]走向量化的书面语言中编码的语义信息:
[https://arxiv.org/abs/0907.1558](https://arxiv.org/abs/0907.1558)