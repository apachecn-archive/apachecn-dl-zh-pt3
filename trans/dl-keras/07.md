

# 其他深度学习模型

到目前为止，大多数讨论都集中在不同的分类模型上。使用对象特征和它们的标签来训练这些模型，以预测迄今为止未见过的对象的标签。这些模型也有一个相当简单的架构，我们到目前为止看到的所有模型都有一个由 Keras sequential API 建模的线性管道。

在这一章中，我们将关注更复杂的架构，其中的管道不一定是线性的。Keras 提供了功能 API 来处理这些类型的架构。在本章中，我们将学习如何使用函数式 API 来定义我们的网络。注意，函数式 API 也可以用来构建线性架构。

分类网络最简单的扩展是回归网络。监督机器学习的两大子类是分类和回归。网络现在预测的不是一个类别，而是一个连续的值。当我们讨论无状态和有状态 rnn 时，您看到了一个回归网络的例子。许多回归问题可以用分类模型很容易地解决。在本章中，我们将看到一个这样的网络预测大气中苯的例子。

另一类模型处理从未标记的数据中学习数据的结构。这些被称为**无监督**(或者更准确地说，自我监督)模型。它们类似于分类模型，但是标签隐含在数据中。我们已经看到了这种模型的例子；例如，CBOW 和 skip-gram word2vec 模型是自监督模型。自动编码器是这种模型的另一个例子。我们将学习自动编码器，并描述一个构建句子的紧凑向量表示的例子。

然后，我们将看看如何将我们到目前为止看到的网络组成更大的计算图。这些图形通常是为了实现一些自定义目标而构建的，这些自定义目标是顺序模型无法单独实现的，并且可能有多个输入和输出以及到外部组件的连接。我们将看到一个组成这样一个问答网络的例子。

然后我们绕道看一下 Keras 后端 API，以及我们如何使用这个 API 来构建定制组件以扩展 Keras 的功能。

回到无标签数据的模型，另一类不需要标签的模型是生成模型。这些模型使用一组现有对象进行训练，并试图学习这些对象来自的分布。一旦学习了分布，我们就可以从这个分布中抽取看起来像原始训练数据的样本。我们已经看到了一个这样的例子，我们训练了一个角色 RNN 模型来生成类似于前一章中的*爱丽丝梦游仙境*的文本。这个想法已经被讨论过了，所以我们不会在这里讨论生成模型的这个特殊方面。然而，我们将研究如何利用训练有素的网络学习数据分布的想法，使用在 ImageNet 数据上预先训练的 VGG-16 网络来创建有趣的视觉效果。

总而言之，我们将在本章中学习以下主题:

*   Keras 功能 API
*   回归网络
*   用于无监督学习的自动编码器
*   用函数式 API 构建复杂网络
*   定制 Keras
*   生成网络

让我们开始吧。



# Keras 功能 API

Keras functional API 将每一层定义为一个函数，并提供操作符将这些函数组合成一个更大的计算图。函数是某种具有单个输入和单个输出的变换。例如，函数 *y = f(x)* 定义了一个输入为 *x* 输出为 *y* 的函数 *f* 。让我们考虑一下 Keras 的简单序列模型(更多信息请参考:[https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/)):

```

from keras.models import Sequential
from keras.layers.core import dense, Activation

model = Sequential([
   dense(32, input_dim=784),
   Activation("sigmoid"),
   dense(10),
   Activation("softmax"),
])

model.compile(loss="categorical_crossentropy", optimizer="adam")

```

如您所见，顺序模型将网络表示为线性管道或层列表。我们也可以将网络表示为下列嵌套函数的组合。这里 *x* 是形状*(无，784)* 的输入张量 *y* 是*(无，10)* 的输出张量。此处*无*指尚未确定的批量:

![](assets/func-api-1.png)

其中:

![](assets/func-api-2.png)

可以使用 Keras functional API 重新定义网络，如下所示。请注意预测变量是如何由我们之前在方程中定义的相同函数组合而成的:

```

from keras.layers import Input
from keras.layers.core import dense
from keras.models import Model
from keras.layers.core import Activation

inputs = Input(shape=(784,))

x = dense(32)(inputs)
x = Activation("sigmoid")(x)
x = dense(10)(x)
predictions = Activation("softmax")(x)

model = Model(inputs=inputs, outputs=predictions)

model.compile(loss="categorical_crossentropy", optimizer="adam")

```

由于模型是功能层的组合，因此模型也是功能。因此，您可以通过对适当形状的输入张量进行调用，将训练好的模型视为另一层。因此，如果您已经构建了一个做一些有用的事情(如图像分类)的模型，您可以使用 Keras 的`TimeDistributed`包装器轻松地将其扩展到处理一系列图像:

```

sequence_predictions = TimeDistributed(model)(input_sequences)

```

功能 API 可用于定义任何可使用顺序 API 定义的网络。此外，以下类型的网络只能使用功能 API 来定义:

*   具有多个输入和输出的模型
*   由多个子模型组成的模型
*   使用共享层的模型

具有多个输入和输出的模型是通过分别组合输入和输出来定义的，如前面的示例所示，然后在`Model`构造函数的输入和输出参数中传递一个输入函数数组和一个输出函数数组:

```

model = Model(inputs=[input1, input2], outputs=[output1, output2])

```

具有多个输入和输出的模型通常也由多个子网组成，这些子网的计算结果被合并到最终结果中。merge 函数提供了多种方式来合并中间结果，例如向量加法、点积和连接。我们将在本章后面的问答例子中看到合并的例子。

函数式 API 的另一个好用途是使用共享层的模型。共享层被定义一次，并在需要共享其权重的每个管道中被引用。

在这一章中，我们几乎只使用函数式 API，所以你会看到很多它的用法的例子。Keras 网站上有更多函数式 API 的使用示例。



# 回归网络

监督学习的两个主要技术是分类和回归。在这两种情况下，模型都使用数据进行训练，以预测已知的标签。在分类的情况下，这些标签是离散值，例如文本类型或图像类别。在回归的情况下，这些标签是连续值，如股票价格或人类智商(IQ)。

我们看到的大多数例子都显示了深度学习模型被用来执行分类。在这一节中，我们将看看如何使用这样的模型来执行回归。

回想一下，分类模型具有密集层，其末端具有非线性激活，其输出维度对应于模型可以预测的类别数量。因此，ImageNet 图像分类模型的末尾有一个密集(1，000)层，对应于它可以预测的 1，000 个 ImageNet 类。同样，情绪分析模型在末尾有一个密集层，对应积极或消极的情绪。

回归模型在末尾也有一个密集层，但只有一个输出，即输出维度为 1，没有非线性激活。因此，密集层只返回前一层激活的总和。此外，所使用的损失函数通常是**均方误差** ( **MSE** )，但也可以使用其他一些目标(在 https://keras.io/losses/[的 Keras 目标页面上列出)。](https://keras.io/losses/)



# Keras 回归示例—预测空气中的苯含量

在本例中，我们将预测大气中苯的浓度，给定一些其他变量，如大气中一氧化碳、一氧化二氮等的浓度以及温度和相对湿度。我们将使用的数据集是来自 https://archive.ics.uci.edu/ml/datasets/Air+Quality UCI 机器学习库的空气质量数据集。该数据集包含来自五个金属氧化物化学传感器阵列的 9，358 个小时平均读数实例。传感器阵列位于意大利的一个城市，记录时间是从 2004 年 3 月到 2005 年 2 月。

像往常一样，首先我们导入所有必需的库:

```

from keras.layers import Input
from keras.layers.core import dense
from keras.models import Model
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd

```

数据集以 CSV 文件的形式提供。我们将输入数据加载到 Pandas(更多信息请参考:[http://pandas.pydata.org/](http://pandas.pydata.org/))数据框中。Pandas 是一个流行的围绕数据框架构建的数据分析库，数据框架是从 R 语言借用的概念。我们在这里使用熊猫来读取数据集有两个原因。首先，数据集包含由于某种原因无法记录的空字段。第二，数据集使用逗号作为小数点，这在一些欧洲国家很常见。Pandas 具有处理这两种情况的内置支持，以及一些其他便利，我们很快就会看到:

```

DATA_DIR = "../data"
AIRQUALITY_FILE = os.path.join(DATA_DIR, "AirQualityUCI.csv")

aqdf = pd.read_csv(AIRQUALITY_FILE, sep=";", decimal=",", header=0)

# remove first and last 2 cols 
del aqdf["Date"]
del aqdf["Time"]
del aqdf["Unnamed: 15"]
del aqdf["Unnamed: 16"]

# fill NaNs in each column with the mean value
aqdf = aqdf.fillna(aqdf.mean())

Xorig = aqdf.as_matrix()

```

前面的示例删除了包含观察日期和时间的前两列，以及看似虚假的后两列。接下来，我们用该列的平均值替换空字段。最后，我们将数据帧导出为矩阵，供下游使用。

需要注意的一点是，每列数据都有不同的刻度，因为它们测量的数量不同。例如，氧化锡的浓度在 1000 范围内，而非甲烷碳氢化合物的浓度在 100 范围内。在许多情况下，我们的要素是同质的，因此缩放不是问题，但在这种情况下，缩放数据通常是一种好的做法。此处的缩放包括从每列中减去该列的平均值，然后除以其标准偏差:

![](assets/zscore.png)

为此，我们使用由`scikit-learn`库提供的`StandardScaler`类，如下所示。我们存储平均值和标准偏差，因为我们以后在报告结果或根据新数据进行预测时会用到它们。我们的目标变量是输入数据集中的第四列，因此我们将这个缩放后的数据分成输入变量`X`和目标变量`y`:

```

scaler = StandardScaler()
Xscaled = scaler.fit_transform(Xorig)
# store these off for predictions with unseen data
Xmeans = scaler.mean_
Xstds = scaler.scale_

y = Xscaled[:, 3]
X = np.delete(Xscaled, 3, axis=1)

```

然后，我们将数据分成前 70%用于训练，后 30%用于测试。这为我们提供了 6，549 条训练记录和 2，808 条测试记录:

```

train_size = int(0.7 * X.shape[0])
Xtrain, Xtest, ytrain, ytest = X[0:train_size], X[train_size:], 
    y[0:train_size], y[train_size:]

```

接下来，我们定义我们的网络。这是一个简单的两层密集网络，采用 12 个特征的向量作为输入，并输出缩放预测。隐藏的致密层有八个神经元。我们用称为 *glorot uniform* 的特定初始化方案来初始化两个密集层的权重矩阵。关于初始化方案的完整列表，请参考这里的 Keras 初始化:【https://keras.io/initializers/】T4。使用的损失函数是均方误差(`mse`)，优化器是`adam`:

```

readings = Input(shape=(12,))
x = dense(8, activation="relu", kernel_initializer="glorot_uniform")(readings)
benzene = dense(1, kernel_initializer="glorot_uniform")(x)

model = Model(inputs=[readings], outputs=[benzene])
model.compile(loss="mse", optimizer="adam")

```

我们针对 20 个时期和 10:

```

NUM_EPOCHS = 20
BATCH_SIZE = 10

history = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,
    validation_split=0.2)

```

这导致模型在训练集上的均方误差为 0.0003(约 2% RMSE)，在验证集上的均方误差为 0.0016(约 4% RMSE)，如训练步骤的日志所示:

![](assets/ss-7-1.png)

我们还查看了最初记录的一些苯浓度值，并将它们与我们的模型预测的值进行了比较。实际值和预测值都从其缩放后的 *z* 值重新缩放为实际值:

```

ytest_ = model.predict(Xtest).flatten()
for i in range(10):
    label = (ytest[i] * Xstds[3]) + Xmeans[3]
    prediction = (ytest_[i] * Xstds[3]) + Xmeans[3]
    print("Benzene Conc. expected: {:.3f}, predicted: {:.3f}".format(label, prediction))

```

并排比较表明，预测值与实际值非常接近:

```

Benzene Conc. expected: 4.600, predicted: 5.254
Benzene Conc. expected: 5.500, predicted: 4.932
Benzene Conc. expected: 6.500, predicted: 5.664
Benzene Conc. expected: 10.300, predicted: 8.482
Benzene Conc. expected: 8.900, predicted: 6.705
Benzene Conc. expected: 14.000, predicted: 12.928
Benzene Conc. expected: 9.200, predicted: 7.128
Benzene Conc. expected: 8.200, predicted: 5.983
Benzene Conc. expected: 7.200, predicted: 6.256
Benzene Conc. expected: 5.500, predicted: 5.184

```

最后，我们将整个测试集的实际值与预测值绘制成图表。我们再次看到网络预测的值非常接近预期值:

```

plt.plot(np.arange(ytest.shape[0]), (ytest * Xstds[3]) / Xmeans[3], 
    color="b", label="actual")
plt.plot(np.arange(ytest_.shape[0]), (ytest_ * Xstds[3]) / Xmeans[3], 
    color="r", alpha=0.5, label="predicted")
plt.xlabel("time")
plt.ylabel("C6H6 concentrations")
plt.legend(loc="best")
plt.show()

```

前面示例的输出如下:

![](assets/regression-chart.png)

# 无监督学习—自动编码器

自动编码器是一类神经网络，它试图使用反向传播将输入重新创建为其目标。自动编码器由两部分组成，编码器和解码器。编码器将读取输入并将其压缩为紧凑的表示形式，解码器将读取该紧凑的表示形式并从中重新创建输入。换句话说，自动编码器试图通过最小化重构误差来学习恒等函数。

尽管 identity 函数看起来并不是一个非常有趣的学习函数，但是它的学习方式使它变得非常有趣。自动编码器中隐藏单元的数量通常少于输入(和输出)单元的数量。这迫使编码器学习解码器重构的输入的压缩表示。如果输入数据中存在输入特征之间的相关性形式的结构，那么自动编码器将发现其中一些相关性，并最终学习数据的低维表示，类似于使用**主成分分析** ( **PCA** )学习的数据。

一旦训练了自动编码器，我们通常会丢弃解码器组件，而使用编码器组件来生成输入的紧凑表示。或者，我们可以使用编码器作为特征检测器，生成输入的紧凑、语义丰富的表示，并通过将 softmax 分类器附加到隐藏层来构建分类器。

自动编码器的编码器和解码器组件可以使用密集网络、卷积网络或递归网络来实现，具体取决于所建模的数据类型。例如，对于用于构建**协同过滤** ( **CF** )模型的自动编码器，密集网络可能是一个不错的选择(有关更多信息，请参考 S. Sedhain 的文章:*AutoRec:auto encoders Meet Collaborative Filtering*、2015 年 ACM 第 24 届国际万维网会议论文集和 H. Cheng 的文章*推荐系统深度学习*、2016 年 ACM 第一届推荐系统深度学习研讨会论文集)类似地，卷积神经网络可能适用于本文中涵盖的用例:*参见:使用深度学习从面部移除眼镜*，作者 M. Runfeldt。递归网络是建立在文本数据基础上的自动编码器的良好选择，如 deep patient(有关更多信息，请参考文章: *Deep Patient:从电子健康记录中预测患者未来的无监督表示*，作者 R. Miotto，科学报告 6，2016)和 skip-thought vectors(有关更多信息，请参考文章: *Skip-Thought Vectors* ，作者 R. Kiros，神经信息处理系统进展，2015)。

自动编码器也可以通过连续堆叠将输入压缩为越来越小的表示的编码器，并以相反的顺序堆叠解码器来堆叠。堆叠自动编码器具有更强的表达能力，并且连续的表示层捕获输入的分层分组，类似于卷积神经网络中的卷积和汇集操作。

堆叠式自动编码器过去是一层一层地训练的。例如，在接下来显示的网络中，我们将首先训练层 *X* 来使用隐藏层 *H1* 重建层*X’*(忽略 *H2* )。然后，我们将训练层 *H1* 使用隐藏层 *H2* 重建层*H1*。最后，我们将所有的层按照所示的配置堆叠在一起，并对其进行微调，以从 *X* 重建*X’*。然而，如今有了更好的激活和正则化函数，完全训练这些网络是相当普遍的:

![](assets/stacked-autoencoder.png)

Keras 的博客文章，*在 Keras*(【https://blog.keras.io/building-autoencoders-in-keras.html】)中有很多构建自动编码器的例子，这些自动编码器使用完全连接的卷积神经网络来重建 MNIST 数字图像。它还对去噪和变分自动编码器进行了很好的讨论，我们不会在这里讨论。



# Keras 自动编码器示例—句子向量

在本例中，我们将构建并训练一个基于 LSTM 的自动编码器，为 Reuters-21578 语料库中的文档生成句子向量([https://archive . ics . UCI . edu/ml/datasets/Reuters-21578+Text+Categorization+Collection](https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection))。我们已经在[第 5 章](700e9954-f126-49b5-b4e4-fa7321296e85.xhtml)、*单词嵌入*中看到，如何使用单词嵌入创建向量来表示一个单词，这些向量表示该单词在它出现的其他单词的上下文中的含义。在这里，我们将看到如何为句子建立相似的向量。句子是一个单词序列，所以句子向量代表句子的意思。

构建句子向量最简单的方法是将单词向量相加，然后除以单词数。然而，这将句子视为一个单词包，并且没有考虑单词的顺序。因此，在这种情况下，句子*狗咬了人*和*人咬了狗*将被视为相同。LSTMs 设计用于处理序列输入，并考虑单词的顺序，从而为句子提供更好、更自然的表示。

首先，我们导入必要的库:

```

from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint
from keras.layers import Input
from keras.layers.core import RepeatVector
from keras.layers.recurrent import LSTM
from keras.layers.wrappers import Bidirectional
from keras.models import Model
from keras.preprocessing import sequence
from scipy.stats import describe
import collections
import matplotlib.pyplot as plt
import nltk
import numpy as np
import os

```

数据以一组 SGML 文件的形式提供。我们已经在[第 6 章](57a694a6-93f4-4eec-9fbf-e4eafd2d6824.xhtml)、*递归神经网络— RNN* 中对这些数据进行了解析，并将其合并到一个文本文件中，作为我们基于 GRU 的词性标注示例。我们将重用这些数据，首先将每个文本块转换成一个句子列表，每行一个句子:

```

sents = []
fsent = open(sent_filename, "rb")
for line in fsent:
    docid, sent_id, sent = line.strip().split("t")
    sents.append(sent)
fsent.close()

```

为了扩大词汇量，我们一个字一个字地再读一遍这个句子列表。每个单词在添加时都被规范化。规范化是用数字`9`替换任何看起来像数字的标记，并将其小写。结果就是词频表，`word_freqs`。我们还计算每个句子的句子长度，并通过用空格重新连接标记来创建已解析句子的列表，以便在后续步骤中更容易解析:

```

def is_number(n):
    temp = re.sub("[.,-/]", "", n)
    return temp.isdigit()

word_freqs = collections.Counter()
sent_lens = []
parsed_sentences = []
for sent in sentences:
    words = nltk.word_tokenize(sent)
    parsed_words = []
    for word in words:
        if is_number(word):
            word = "9"
        word_freqs[word.lower()] += 1
        parsed_words.append(word)
    sent_lens.append(len(words))
    parsed_sentences.append(" ".join(parsed_words))

```

这为我们提供了一些关于语料库的信息，有助于我们为我们的 LSTM 网络计算出好的常数值:

```

sent_lens = np.array(sent_lens)
print("number of sentences: {:d}".format(len(sent_lens)))
print("distribution of sentence lengths (number of words)")
print("min:{:d}, max:{:d}, mean:{:.3f}, med:{:.3f}".format(
np.min(sent_lens), np.max(sent_lens), np.mean(sent_lens),
np.median(sent_lens)))
print("vocab size (full): {:d}".format(len(word_freqs)))

```

这为我们提供了关于语料库的以下信息:

```

number of sentences: 131545
 distribution of sentence lengths (number of words)
 min: 1, max: 429, mean: 22.315, median: 21.000
 vocab size (full): 50751

```

基于这些信息，我们为我们的 LSTM 模型设置以下常数。我们选择我们的`VOCAB_SIZE`作为`5000`，也就是说，我们的词汇覆盖了最频繁的 5000 个单词，覆盖了语料库中使用的 93%以上的单词。剩余的单词被视为词汇 ( **OOV** )之外的**，并替换为令牌`UNK`。在预测时，任何模型没有见过的单词也将被赋予标记`UNK`。`SEQUENCE_LEN`被设置为大约是训练集中句子长度中位数的两倍，事实上，在我们的 1.31 亿个句子中，大约有 1.1 亿个句子比这个设置短。短于`SEQUENCE_LENGTH`的句子将被一个特殊的`PAD`字符填充，而长的句子将被截断以符合限制:**

```

VOCAB_SIZE = 5000
SEQUENCE_LEN = 50

```

由于我们的 LSTM 的输入将是数字，我们需要构建在单词和单词 id 之间来回转换的查找表。由于我们将词汇量限制为 5000，并且我们必须添加两个伪单词`PAD`和`UNK`，我们的查找表包含最频繁出现的 4998 个单词的条目，外加`PAD`和`UNK`:

```

word2id = {}
word2id["PAD"] = 0
word2id["UNK"] = 1
for v, (k, _) in enumerate(word_freqs.most_common(VOCAB_SIZE - 2)):
    word2id[k] = v + 2
id2word = {v:k for k, v in word2id.items()}

```

我们网络的输入是一个单词序列，每个单词用一个向量表示。简单地说，我们可以对每个单词使用一次性编码，但是这使得输入数据非常大。所以我们用 50 维手套嵌入对每个单词进行编码。嵌入生成一个形状为`(VOCAB_SIZE, EMBED_SIZE)`的矩阵，其中每一行代表我们词汇表中一个单词的手套嵌入。`PAD`和`UNK`行(分别为`0`和`1`)分别由零和随机统一值填充:

```

EMBED_SIZE = 50

def lookup_word2id(word):
    try:
        return word2id[word]
    except KeyError:
        return word2id["UNK"]

def load_glove_vectors(glove_file, word2id, embed_size):
    embedding = np.zeros((len(word2id), embed_size))
    fglove = open(glove_file, "rb")
    for line in fglove:
        cols = line.strip().split()
        word = cols[0]
        if embed_size == 0:
            embed_size = len(cols) - 1
        if word2id.has_key(word):
            vec = np.array([float(v) for v in cols[1:]])
        embedding[lookup_word2id(word)] = vec
    embedding[word2id["PAD"]] = np.zeros((embed_size))
    embedding[word2id["UNK"]] = np.random.uniform(-1, 1, embed_size)
    return embedding

embeddings = load_glove_vectors(os.path.join(
    DATA_DIR, "glove.6B.{:d}d.txt".format(EMBED_SIZE)), word2id, EMBED_SIZE)

```

我们的自动编码器模型采用一个手套字向量序列，并学习产生另一个与输入序列相似的序列。编码器 LSTM 将序列压缩成固定大小的上下文向量，解码器 LSTM 使用该向量来重建原始序列。此处显示了网络示意图:

![](assets/sent-thoughts.png)

因为输入相当大，所以我们将使用一个生成器来产生每一批输入。我们的生成器批量生成形状为`(BATCH_SIZE, SEQUENCE_LEN, EMBED_SIZE)`的张量。这里的`BATCH_SIZE`是`64`，由于我们使用的是 50 维手套向量，`EMBED_SIZE`是`50`。我们在每个时期的开始打乱句子，并返回 64 个句子的批次。每个句子被表示为手套单词向量的向量。如果词汇表中的单词没有相应的手套嵌入，则用零向量表示。我们构建了生成器的两个实例，一个用于训练数据，一个用于测试数据，分别由原始数据集的 70%和 30%组成:

```

BATCH_SIZE = 64

def sentence_generator(X, embeddings, batch_size):
    while True:
        # loop once per epoch
        num_recs = X.shape[0]
        indices = np.random.permutation(np.arange(num_recs))
        num_batches = num_recs // batch_size
        for bid in range(num_batches):
            sids = indices[bid * batch_size : (bid + 1) * batch_size]
            Xbatch = embeddings[X[sids, :]]
            yield Xbatch, Xbatch

train_size = 0.7
Xtrain, Xtest = train_test_split(sent_wids, train_size=train_size)
train_gen = sentence_generator(Xtrain, embeddings, BATCH_SIZE)
test_gen = sentence_generator(Xtest, embeddings, BATCH_SIZE)

```

现在我们准备定义自动编码器。如图所示，它由一个编码器 LSTM 和一个解码器 LSTM 组成。编码器 LSTM 读取代表一批句子的形状张量`(BATCH_SIZE, SEQUENCE_LEN, EMBED_SIZE)`。每个句子被表示为一个填充的固定长度的单词序列，大小为`SEQUENCE_LEN`。每个单词被表示为一个 300 维的手套向量。编码器 LSTM 的输出维数是一个超参数`LATENT_SIZE`，它是稍后将从训练好的自动编码器的编码器部分出来的句子向量的大小。维度的向量空间`LATENT_SIZE`代表编码句子含义的潜在空间。对于每个句子，LSTM 的输出是一个大小为(`LATENT_SIZE`)的向量，因此对于批处理，输出张量的形状是`(BATCH_SIZE, LATENT_SIZE)`。这现在被馈送到 RepeatVector 层，该层在整个序列中复制这一点。这一层的输出张量具有形状`(BATCH_SIZE, SEQUENCE_LEN, LATENT_SIZE)`。这个张量现在被馈入解码器 LSTM，其输出维数是`EMBED_SIZE`，因此输出张量具有形状`(BATCH_SIZE, SEQUENCE_LEN, EMBED_SIZE)`，即与输入张量形状相同。

我们用`SGD`优化器和`mse`损失函数编译这个模型。我们之所以用 MSE，是想在维度`LATENT_SIZE`的嵌入空间里重构一个意思相近的句子，也就是和原句接近的东西:

```

inputs = Input(shape=(SEQUENCE_LEN, EMBED_SIZE), name="input")
encoded = Bidirectional(LSTM(LATENT_SIZE), merge_mode="sum",
    name="encoder_lstm")(inputs)
decoded = RepeatVector(SEQUENCE_LEN, name="repeater")(encoded)
decoded = Bidirectional(LSTM(EMBED_SIZE, return_sequences=True),
    merge_mode="sum",
    name="decoder_lstm")(decoded)

autoencoder = Model(inputs, decoded)

autoencoder.compile(optimizer="sgd", loss="mse")

```

我们使用以下代码训练自动编码器 10 个时期。选择 10 个时期是因为 MSE 损失在此时间内收敛。我们还根据 MSE 损失保存目前检索到的最佳模型:

```

num_train_steps = len(Xtrain) // BATCH_SIZE
num_test_steps = len(Xtest) // BATCH_SIZE
checkpoint = ModelCheckpoint(filepath=os.path.join(DATA_DIR,
    "sent-thoughts-autoencoder.h5"), save_best_only=True)
history = autoencoder.fit_generator(train_gen,
    steps_per_epoch=num_train_steps,
    epochs=NUM_EPOCHS,
    validation_data=test_gen,
    validation_steps=num_test_steps,
    callbacks=[checkpoint])

```

培训结果如下所示。如您所见，训练 MSE 从 0.14 降低到 0.1，验证 MSE 从 0.12 降低到 0.1:

![](assets/ss-7-2.png)

或者，图形显示如下:

![](assets/autoencoder-lossfunc.png)

因为我们输入的是嵌入矩阵，所以输出也是单词嵌入矩阵。由于嵌入空间是连续的，而我们的词汇是离散的，所以不是每个输出嵌入都会对应一个单词。我们能做的最好的事情是找到一个最接近输出嵌入的单词，以便重构原始文本。这有点麻烦，所以我们将以不同的方式评估我们的 autoencoder。

由于自动编码器的目标是产生良好的潜在表示，我们将使用原始输入的编码器产生的潜在向量与自动编码器的输出进行比较。首先，我们将编码器组件提取到它自己的网络中:

```

encoder = Model(autoencoder.input, autoencoder.get_layer("encoder_lstm").output)

```

然后，我们在测试集上运行自动编码器，以返回预测的嵌入。然后，我们通过编码器发送输入嵌入和预测嵌入，以从每个生成句子向量，并使用*余弦*相似性来比较这两个向量。余弦相似度接近 1 表示高相似度，接近 0 表示低相似度。以下代码针对 500 个测试句子的随机子集运行，并产生从源嵌入生成的句子向量和由自动编码器生成的相应目标嵌入之间的余弦相似性的一些样本值:

```

def compute_cosine_similarity(x, y):
    return np.dot(x, y) / (np.linalg.norm(x, 2) * np.linalg.norm(y, 2))

k = 500
cosims = np.zeros((k))
i = 0
for bid in range(num_test_steps):
    xtest, ytest = test_gen.next()
    ytest_ = autoencoder.predict(xtest)
    Xvec = encoder.predict(xtest)
    Yvec = encoder.predict(ytest_)
    for rid in range(Xvec.shape[0]):
        if i >= k:
            break
        cosims[i] = compute_cosine_similarity(Xvec[rid], Yvec[rid])
        if i <= 10:
            print(cosims[i])
            i += 1
if i >= k:
    break

```

余弦相似度的前 10 个值如下所示。正如我们所见，这些向量似乎非常相似:

```

0.982818722725
0.970908224583
0.98131018877
0.974798440933
0.968060493469
0.976065933704
0.96712064743
0.949920475483
0.973583400249
0.980291545391
0.817819952965

```

测试集中前 500 个句子的句子向量的余弦相似度值的分布直方图如下所示。如前所述，它证实了从自动编码器的输入和输出生成的句子向量非常相似，表明得到的句子向量是句子的良好表示:

![](assets/autoencoder-cosims.png)

# 构建深层网络

我们广泛研究了这三个基本的深度学习网络——**全连接网络**(**【FCN】**)、CNN 和 RNN 模型。虽然其中每一种都有最适合它们的特定用例，但是您也可以通过将这些模型组合成类似 Lego 的构建块，并使用 Keras functional API 以新的有趣方式将它们粘合在一起，从而组成更大、更有用的模型。

这种模型倾向于某种程度上专用于它们被构建的任务，所以不可能对它们进行概括。然而，通常它们涉及从多个输入中学习或产生多个输出。一个例子可以是问题回答网络，其中网络学习预测给定故事和问题的答案。另一个例子可以是计算一对图像之间的相似性的连体网络，其中该网络被训练成使用一对图像作为输入来预测二元(相似/不相似)或分类(相似程度)标签。另一个例子可以是对象分类和定位网络，其中它学习预测图像类别以及图像在图片中的位置。前两个例子是具有多个输入的复合网络的例子，最后一个例子是具有多个输出的复合网络的例子。



# Keras 示例—用于回答问题的记忆网络

在这个例子中，我们将为问答建立一个记忆网络。记忆网络是一种专门的架构，除了其他可学习单元(通常是 rnn)之外，还包括一个记忆单元。每个输入更新存储器状态，并且通过使用存储器以及来自可学习单元的输出来计算最终输出。这种架构是在 2014 年通过论文提出的(有关更多信息，请参考 J. Weston，S. Chopra 和 A. Bordes，arXiv:1410.3916，2014 年撰写的:*存储器网络*)。一年后，另一篇论文(欲了解更多信息，请参考:*迈向 AI-完整问答:一组先决玩具任务*，作者 J. Weston，arXiv:1502.05698，2015)提出了合成数据集和一组标准的 20 个问答任务的想法，每个问题的难度都比前一个更高，并应用了各种深度学习网络来解决这些任务。其中，记忆网络在所有任务中取得了最好的结果。这个数据集后来通过 https://research.fb.com/projects/babi/脸书的 bAbI 项目()向公众开放。我们的记忆网络的实现与本文中描述的最相似(有关更多信息，请参考 S. Sukhbaatar，J. Weston 和 R. Fergus 著的*端到端记忆网络*，神经信息处理系统进展，2015 年)，因为所有训练都在单个网络中联合进行。它使用 bAbI 数据集来解决第一个问题回答任务。

首先，我们将导入必要的库:

```

from keras.layers import Input
from keras.layers.core import Activation, dense, Dropout, Permute
from keras.layers.embeddings import Embedding
from keras.layers.merge import add, concatenate, dot
from keras.layers.recurrent import LSTM
from keras.models import Model
from keras.preprocessing.sequence import pad_sequences
from keras.utils import np_utils
import collections
import itertools
import nltk
import numpy as np
import matplotlib.pyplot as plt
import os

```

第一个问题回答任务的 bAbI 数据由 10，000 个短句组成，每个短句用于训练集和测试集。一个故事由两到三个句子组成，后面跟着一个问题。每个故事的最后一句都附有问题和答案。下面的代码块将每个训练和测试文件解析为故事、问题和答案的三元组列表:

```

DATA_DIR = "../data"
TRAIN_FILE = os.path.join(DATA_DIR, "qa1_single-supporting-fact_train.txt")
TEST_FILE = os.path.join(DATA_DIR, "qa1_single-supporting-fact_test.txt")

def get_data(infile):
    stories, questions, answers = [], [], []
    story_text = []
    fin = open(TRAIN_FILE, "rb")
    for line in fin:
        line = line.decode("utf-8").strip()
        lno, text = line.split(" ", 1)
        if "t" in text:
            question, answer, _ = text.split("t")
            stories.append(story_text)
            questions.append(question)
            answers.append(answer)
            story_text = []
        else:
            story_text.append(text)
    fin.close()
    return stories, questions, answers

data_train = get_data(TRAIN_FILE)
data_test = get_data(TEST_FILE)

```

我们的下一步是浏览生成的列表中的文本，并建立我们的词汇。这对于我们来说应该很熟悉了，因为我们已经用过几次类似的习语了。与前一次不同，我们的词汇量相当小，只有 22 个独特的单词，所以我们不会有任何超出词汇量的单词:

```

def build_vocab(train_data, test_data):
    counter = collections.Counter()
    for stories, questions, answers in [train_data, test_data]:
        for story in stories:
            for sent in story:
                for word in nltk.word_tokenize(sent):
                    counter[word.lower()] += 1
                for question in questions:
                    for word in nltk.word_tokenize(question):
                         counter[word.lower()] += 1
                for answer in answers:
                    for word in nltk.word_tokenize(answer):
                         counter[word.lower()] += 1
    word2idx = {w:(i+1) for i, (w, _) in enumerate(counter.most_common())}
    word2idx["PAD"] = 0
idx2word = {v:k for k, v in word2idx.items()}
    return word2idx, idx2word

word2idx, idx2word = build_vocab(data_train, data_test)

vocab_size = len(word2idx)

```

记忆网络是基于 RNNs 的，故事和问题中的每个句子都被视为一个单词序列，所以我们需要找出我们的故事和问题的序列的最大长度。下面的代码块实现了这一点。我们发现一个故事的最大长度是 14 个单词，一个问题的最大长度是 4 个单词:

```

def get_maxlens(train_data, test_data):
    story_maxlen, question_maxlen = 0, 0
    for stories, questions, _ in [train_data, test_data]:
        for story in stories:
            story_len = 0
            for sent in story:
                swords = nltk.word_tokenize(sent)
                story_len += len(swords)
            if story_len > story_maxlen:
                story_maxlen = story_len
        for question in questions:
            question_len = len(nltk.word_tokenize(question))
            if question_len > question_maxlen:
                question_maxlen = question_len
    return story_maxlen, question_maxlen

story_maxlen, question_maxlen = get_maxlens(data_train, data_test)

```

如前所述，我们的 RNNs 的输入是一个单词 id 序列。因此，我们需要使用我们的词汇词典将(故事、问题和答案)三元组转换为一系列整数单词 id。下一个代码块会这样做，并对故事的结果序列进行零填充，并回答我们之前计算的最大序列长度。此时，我们拥有训练集和测试集中每个三元组的填充单词 ID 序列列表:

```

def vectorize(data, word2idx, story_maxlen, question_maxlen):
    Xs, Xq, Y = [], [], []
    stories, questions, answers = data
    for story, question, answer in zip(stories, questions, answers):
        xs = [[word2idx[w.lower()] for w in nltk.word_tokenize(s)] 
                   for s in story]
        xs = list(itertools.chain.from_iterable(xs))
        xq = [word2idx[w.lower()] for w in nltk.word_tokenize(question)]
        Xs.append(xs)
        Xq.append(xq)
        Y.append(word2idx[answer.lower()])
    return pad_sequences(Xs, maxlen=story_maxlen),
        pad_sequences(Xq, maxlen=question_maxlen),
        np_utils.to_categorical(Y, num_classes=len(word2idx))

Xstrain, Xqtrain, Ytrain = vectorize(data_train, word2idx, story_maxlen, question_maxlen)
Xstest, Xqtest, Ytest = vectorize(data_test, word2idx, story_maxlen, question_maxlen)

```

我们想要定义模型。该定义比我们之前看到的要长，因此在浏览定义时参考图表可能会更方便:

![](assets/memnet.png)

我们的模型有两个输入，问题和句子的单词 id 序列。其中的每一个都被传递到嵌入层，以将单词 id 转换为 64 维嵌入空间中的向量。此外，故事序列通过一个额外的嵌入来传递，该嵌入将其投影到大小为`max_question_length`的嵌入中。所有这些嵌入层从随机权重开始，并与网络的其余部分联合训练。

使用点积合并前两个嵌入(故事和问题)以形成网络的记忆。这些表示故事和问题中的单词在嵌入空间中是相同的或彼此接近的。存储器的输出与第二故事嵌入合并并求和以形成网络响应，该网络响应再次与问题的嵌入合并以形成响应序列。该响应序列通过 LSTM 发送，其上下文向量被发送到密集层以预测答案，答案可以是词汇表中的一个单词。

使用 RMSprop 优化器和分类交叉熵作为损失函数来训练该模型:

```

EMBEDDING_SIZE = 64
LATENT_SIZE = 32

# inputs
story_input = Input(shape=(story_maxlen,))
question_input = Input(shape=(question_maxlen,))

# story encoder memory
story_encoder = Embedding(input_dim=vocab_size,
output_dim=EMBEDDING_SIZE,
    input_length=story_maxlen)(story_input)
story_encoder = Dropout(0.3)(story_encoder)

# question encoder
question_encoder = Embedding(input_dim=vocab_size,
output_dim=EMBEDDING_SIZE,
    input_length=question_maxlen)(question_input)
question_encoder = Dropout(0.3)(question_encoder)

# match between story and question
match = dot([story_encoder, question_encoder], axes=[2, 2])

# encode story into vector space of question
story_encoder_c = Embedding(input_dim=vocab_size,
output_dim=question_maxlen,
    input_length=story_maxlen)(story_input)
story_encoder_c = Dropout(0.3)(story_encoder_c)

# combine match and story vectors
response = add([match, story_encoder_c])
response = Permute((2, 1))(response)

# combine response and question vectors
answer = concatenate([response, question_encoder], axis=-1)
answer = LSTM(LATENT_SIZE)(answer)
answer = Dropout(0.3)(answer)
answer = dense(vocab_size)(answer)
output = Activation("softmax")(answer)

model = Model(inputs=[story_input, question_input], outputs=output)
model.compile(optimizer="rmsprop", loss="categorical_crossentropy",
    metrics=["accuracy"])

```

我们以 32 的批量大小训练该网络 50 个时期，并在验证集上实现超过 81%的准确度:

```

BATCH_SIZE = 32
NUM_EPOCHS = 50
history = model.fit([Xstrain, Xqtrain], [Ytrain], batch_size=BATCH_SIZE, 
    epochs=NUM_EPOCHS,
    validation_data=([Xstest, Xqtest], [Ytest]))

```

以下是培训日志的记录:

![](assets/ss-7-3.png)

此图以图形方式显示了本次训练运行的训练和验证损失和准确度的变化:

![](assets/memnn-lossfunc-1.png)

我们针对测试集中的前 10 个故事运行模型，以验证预测的准确性:

```

ytest = np.argmax(Ytest, axis=1)
Ytest_ = model.predict([Xstest, Xqtest])
ytest_ = np.argmax(Ytest_, axis=1)

for i in range(NUM_DISPLAY):
    story = " ".join([idx2word[x] for x in Xstest[i].tolist() if x != 0])
    question = " ".join([idx2word[x] for x in Xqtest[i].tolist()])
    label = idx2word[ytest[i]]
    prediction = idx2word[ytest_[i]]
    print(story, question, label, prediction)

```

如你所见，这些预测大多是正确的:

![](assets/memnn-preds-1.png)

# 定制 Keras

正如将我们的基本构建模块组成更大的架构使我们能够构建有趣的深度学习模型一样，有时我们需要看看光谱的另一端。Keras 已经内置了很多功能，所以很有可能您可以用提供的组件构建所有的模型，而根本不需要定制。如果您确实需要定制，Keras 可以满足您的需求。

正如您所记得的，Keras 是一个高级 API，它委托 TensorFlow 或 Theano 后端来完成繁重的计算。您为定制构建的任何代码都将调用这些后端之一。为了保持您的代码在两个后端之间的可移植性，您的定制代码应该使用 Keras 后端 API([https://keras.io/backend/](https://keras.io/backend/))，它提供了一组函数，就像您选择的后端上的一个门面。根据所选的后端，对后端 facade 的调用将转换为适当的 TensorFlow 或 Theano 调用。可用功能的完整列表及其详细描述可以在 Keras 后端页面上找到。

除了可移植性之外，使用后端 API 还可以提高代码的可维护性，因为与同等的 TensorFlow 或 Theano 代码相比，Keras 代码通常更高级、更紧凑。在不太可能的情况下，您确实需要切换到直接使用后端，您的 Keras 组件可以直接在 TensorFlow(而不是 Theano though)代码中使用，如本文 Keras 博客([https://blog . Keras . io/Keras-as-a-simplified-interface-to-tensor flow-tutorial . html](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html))中所述。

自定义 Keras 通常意味着编写自己的自定义图层或自定义距离函数。在本节中，我们将演示如何构建一些简单的 Keras 层。在后续章节中，您将看到更多使用后端函数构建其他定制 Keras 组件的示例，比如目标(损失函数)。



# Keras 示例-使用 lambda 层

Keras 提供了一个λ层；它可以包装你选择的函数。例如，如果您想要构建一个按元素对其输入张量求平方的图层，您可以简单地说:

```

model.add(lambda(lambda x: x ** 2))

```

你也可以在 lambda 层中包装函数。例如，如果要构建一个自定义图层来计算两个输入张量之间的元素欧氏距离，则需要定义一个函数来计算值本身，以及一个从该函数返回输出形状的函数，如下所示:

```

def euclidean_distance(vecs):
    x, y = vecs
    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))

def euclidean_distance_output_shape(shapes):
    shape1, shape2 = shapes
    return (shape1[0], 1)

```

然后，您可以使用 lambda 层调用这些函数，如下所示:

```

lhs_input = Input(shape=(VECTOR_SIZE,))
lhs = dense(1024, kernel_initializer="glorot_uniform", activation="relu")(lhs_input)

rhs_input = Input(shape=(VECTOR_SIZE,))
rhs = dense(1024, kernel_initializer="glorot_uniform", activation="relu")(rhs_input)

sim = lambda(euclidean_distance, output_shape=euclidean_distance_output_shape)([lhs, rhs])

```



# Keras 示例-构建自定义规范化图层

虽然 lambda 层非常有用，但有时您需要更多的控制。作为一个例子，我们将看一下标准化层的代码，它实现了一种叫做**本地响应标准化**的技术。这种技术对局部输入区域上的输入进行归一化，但后来不再受欢迎，因为它不如其他正则化方法(如丢弃和批量归一化)以及更好的初始化方法有效。

构建定制层通常涉及到后端功能，因此它涉及到从张量的角度考虑代码。你会记得，与张量一起工作是一个两步的过程。首先，定义张量并将它们排列在计算图中，然后用实际数据运行图。所以在这个层次工作比在喀拉的其他地方工作更难。Keras 文档中有一些关于构建自定义图层的指南([https://keras.io/layers/writing-your-own-keras-layers/](https://keras.io/layers/writing-your-own-keras-layers/))，你一定要读一读。

让后端 API 中的代码开发变得更容易的方法之一是拥有一个小的测试工具，您可以运行它来验证您的代码正在做您想要它做的事情。下面是我从 Keras 源代码中改编的一个小工具，用于根据一些输入运行您的层并返回一个结果:

```

from keras.models import Sequential
from keras.layers.core import Dropout, Reshape

def test_layer(layer, x):
    layer_config = layer.get_config()
    layer_config["input_shape"] = x.shape
    layer = layer.__class__.from_config(layer_config)
    model = Sequential()
    model.add(layer)
    model.compile("rmsprop", "mse")
    x_ = np.expand_dims(x, axis=0)
    return model.predict(x_)[0]

```

这里是用 Keras 提供的`layer`对象进行的一些测试，以确保线束运行正常:

```

from keras.layers.core import Dropout, Reshape
from keras.layers.convolutional import ZeroPadding2D
import numpy as np

x = np.random.randn(10, 10)
layer = Dropout(0.5)
y = test_layer(layer, x)
assert(x.shape == y.shape)

x = np.random.randn(10, 10, 3)
layer = ZeroPadding2D(padding=(1,1))
y = test_layer(layer, x)
assert(x.shape[0] + 2 == y.shape[0])
assert(x.shape[1] + 2 == y.shape[1])

x = np.random.randn(10, 10)
layer = Reshape((5, 20))
y = test_layer(layer, x)
assert(y.shape == (5, 20))

```

在我们开始构建本地响应规范化层之前，我们需要花点时间来理解它到底是做什么的。这项技术最初用于 Caffe，Caffe 文档(【http://caffe.berkeleyvision.org/tutorial/layers/lrn.html】T4)将其描述为一种*侧向抑制*，通过对局部输入区域进行归一化来工作。在`ACROSS_CHANNEL`模式中，局部区域延伸到附近的通道，但没有空间范围。在`WITHIN_CHANNEL`模式中，局部区域在空间上延伸，但是在单独的通道中。我们将如下实现`WITHIN_CHANNEL`模型。`WITHIN_CHANNEL`模型中局部响应标准化的公式由下式给出:

![](assets/lrn.png)

自定义层的代码遵循标准结构。`__init__`方法用于设置应用特定参数，即与层相关的超参数。由于我们的层只进行正向计算，没有任何可学习的权重，所以我们在 build 方法中所做的只是设置输入形状并委托给超类的 build 方法，该方法负责任何必要的簿记工作。在涉及可学习权重的层中，此方法是您设置初始值的地方。

call 方法执行实际的计算。请注意，我们需要考虑维度排序。另一件要注意的事情是，批处理大小在设计时通常是未知的，所以您需要编写您的操作，以便批处理大小不会被显式调用。计算本身相当简单，并严格遵循公式。分母中的和也可以认为是行和列维度上的平均池，填充大小为 *(n，n)* ，步长为 *(1，1)* 。因为汇集的数据已经被平均了，我们不再需要将总和除以 *n* 。

该类的最后一部分是`get_output_shape_for`方法。由于图层对输入张量的每个元素进行归一化，因此输出大小与输入大小相同:

```

from keras import backend as K
from keras.engine.topology import Layer, InputSpec

class LocalResponseNormalization(Layer):

    def __init__(self, n=5, alpha=0.0005, beta=0.75, k=2, **kwargs):
        self.n = n
        self.alpha = alpha
        self.beta = beta
        self.k = k
        super(LocalResponseNormalization, self).__init__(**kwargs)

    def build(self, input_shape):
        self.shape = input_shape
        super(LocalResponseNormalization, self).build(input_shape)

    def call(self, x, mask=None):
        if K.image_dim_ordering == "th":
            _, f, r, c = self.shape
        else:
            _, r, c, f = self.shape
        squared = K.square(x)
        pooled = K.pool2d(squared, (n, n), strides=(1, 1),
            padding="same", pool_mode="avg")
        if K.image_dim_ordering == "th":
            summed = K.sum(pooled, axis=1, keepdims=True)
            averaged = self.alpha * K.repeat_elements(summed, f, axis=1)
        else:
            summed = K.sum(pooled, axis=3, keepdims=True)
            averaged = self.alpha * K.repeat_elements(summed, f, axis=3)
        denom = K.pow(self.k + averaged, self.beta)
        return x / denom

    def get_output_shape_for(self, input_shape):
        return input_shape

```

您可以在开发过程中使用我们在这里描述的测试工具来测试这一层。运行它比试图构建一个完整的网络更容易，或者更糟，等到您完全指定了层之后再运行它:

```

x = np.random.randn(225, 225, 3)
layer = LocalResponseNormalization()
y = test_layer(layer, x)
assert(x.shape == y.shape)

```

虽然在有经验的 Keras 开发人员中构建定制的 Keras 层似乎是相当常见的，但是在互联网上并没有太多的例子。这可能是因为自定义图层通常是为特定的狭义目的而构建的，可能并不广泛有用。这种可变性也意味着一个单独的例子不能展示您可以用 API 做什么的所有可能性。既然你对如何构建自定义 Keras 层有了很好的想法，你可能会发现看看 Keunwoo Choi 的`melspectogram`([https://keun woochoi . WordPress . com/2016/11/18/for-beginners-writing-a-Custom-Keras-Layer/](https://keunwoochoi.wordpress.com/2016/11/18/for-beginners-writing-a-custom-keras-layer/))和 Shashank Gupta 的`NodeEmbeddingLayer`([http://shashan G7 . github . io/2016/10/12/Custom-Layer-1](http://shashankg7.github.io/2016/10/12/Custom-Layer-In-Keras-Graph-Embedding-Case-Study.html)



# 生成模型

创成式模型是学习创建数据的模型，这些数据类似于它被训练的数据。我们看到了一个学习写散文的生成模型的例子，类似于[第六章](57a694a6-93f4-4eec-9fbf-e4eafd2d6824.xhtml)、*递归神经网络*中的*爱丽丝梦游仙境*。在该示例中，我们训练了一个模型来预测给定前 10 个字符的文本的第 11 个字符。然而，另一种类型的生成模型是**生成对抗模型** ( **GAN** )，这是最近出现的一类非常强大的模型——你可以在[第四章](a67ea944-b1a6-48a3-b8aa-4e698166c0eb.xhtml)、*生成对抗网络和 WaveNet* 中看到 GAN 的例子。生成模型的直觉是，它学习其训练数据的良好内部表示，因此能够在*预测*阶段生成类似的数据。

生成模型的另一个视角是概率性的。典型的分类或回归网络，也称为判别模型，学习一个将输入数据 *X* 映射到某个标签或输出 *y* 的函数，即这些模型学习条件概率 *P(y|X)* 。另一方面，生成模型同时学习联合概率和标签，即 *P(x，y)* 。该知识然后可以用于创建可能的新 *(X，y)* 样本。这使得生成模型能够解释输入数据的底层结构，即使没有标签。这在现实世界中是一个非常重要的优势，因为未标记的数据比标记的数据更丰富。

简单的生成模型，例如上面提到的例子，也可以扩展到音频，例如，学习生成和播放音乐的模型。WaveNet 论文中描述了一个有趣的例子(有关更多信息，请参考 A. van den Oord 于 2016 年发表的文章:*wave net:Raw Audio 的生成模型*)。)描述了使用 atrous 卷积层构建的网络，并在 gith hub(【https://github.com/basveeling/wavenet】)上提供了 Keras 实现。



# 举例来说——深度做梦

在这个例子中，我们将看一个稍微不同的生成网络。我们将看到如何采取预先训练的卷积网络，并使用它来生成图像中的新对象。被训练来辨别图像的网络对图像有足够的了解来生成它们。这是由谷歌的亚历山大·莫尔德温采夫(Alexander Mordvintsev)首先证明的，并在这篇谷歌研究博客文章中进行了描述([https://Research . Google blog . com/2015/06/inceptionism-going-deep-into-neural . html](https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html))。它最初被称为*无概念主义*，但是术语*深度做梦*变得更加流行来描述这项技术。

深度做梦采用反向传播的梯度激活，并将其添加到图像中，循环反复运行相同的过程。网络在这个过程中优化了损失函数，但是我们可以在输入图像(三个通道)中看到它是如何做到的，而不是在不容易可视化的高维隐藏层中。

这个基本策略有许多变化，每一种都会产生新的有趣的效果。一些变化是模糊、对总激活增加约束、衰减梯度、通过裁剪和缩放无限放大图像、通过随机移动图像增加抖动等等。在我们的示例中，我们将展示最简单的方法——我们将为预训练的 VGG-16 的每个汇集层优化所选层的激活均值的梯度，并观察对我们的输入图像的影响。

首先，像往常一样，我们将申报我们的进口:

```

from keras import backend as K
from keras.applications import vgg16
from keras.layers import Input
import matplotlib.pyplot as plt
import numpy as np
import os

```

接下来，我们将加载我们的输入图像。这个图像可能是你从关于深度学习的博客帖子中熟悉的。原图来自这里([https://www . Flickr . com/photos/bill Garrett-newagecrap/14984990912](https://www.flickr.com/photos/billgarrett-newagecrap/14984990912)):

```

DATA_DIR = "../data"
IMAGE_FILE = os.path.join(DATA_DIR, "cat.jpg")
img = plt.imread(IMAGE_FILE)
plt.imshow(img)

```

前面示例的输出如下:

![](assets/cat-orig.png)

接下来，我们定义一对函数来预处理图像和从四维表示中去处理图像，该四维表示适合于输入到预训练的 VGG-16 网络:

```

def preprocess(img):
    img4d = img.copy()
    img4d = img4d.astype("float64")
    if K.image_dim_ordering() == "th":
        # (H, W, C) -> (C, H, W)
        img4d = img4d.transpose((2, 0, 1))
        img4d = np.expand_dims(img4d, axis=0)
        img4d = vgg16.preprocess_input(img4d)
    return img4d

def deprocess(img4d):
    img = img4d.copy()
    if K.image_dim_ordering() == "th":
        # (B, C, H, W)
        img = img.reshape((img4d.shape[1], img4d.shape[2],         img4d.shape[3]))
        # (C, H, W) -> (H, W, C)
        img = img.transpose((1, 2, 0))
    else:
        # (B, H, W, C)
        img = img.reshape((img4d.shape[1], img4d.shape[2], img4d.shape[3]))
    img[:, :, 0] += 103.939
    img[:, :, 1] += 116.779
    img[:, :, 2] += 123.68
    # BGR -> RGB
    img = img[:, :, ::-1]
    img = np.clip(img, 0, 255).astype("uint8")
return img

```

这两个函数是互逆的，即把图像通过`preprocess`再通过`deprocess`会返回原始图像。

接下来，我们加载预先训练好的 VGG-16 网络。该网络已经过 ImageNet 数据的预训练，可从 Keras 发行版获得。你已经在第三章[、*中学习了如何使用预训练模型。我们选择其完全连接的层已经被移除的版本。除了省去我们自己删除它们的麻烦之外，这还允许我们传入任何形状的图像，因为我们需要在输入中指定图像的宽度和高度是因为这决定了完全连接的层中权重矩阵的大小。因为 CNN 变换本质上是局部的，所以图像的大小不会影响卷积层和池层的权重矩阵的大小。因此，对图像大小的唯一限制是，它必须在批处理中保持不变:*](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml)

```

img_copy = img.copy()
print("Original image shape:", img.shape)
p_img = preprocess(img_copy)
batch_shape = p_img.shape
dream = Input(batch_shape=batch_shape)
model = vgg16.VGG16(input_tensor=dream, weights="imagenet", include_top=False)

```

在下面的计算中，我们需要通过名称来引用 CNN 的层对象，所以让我们构建一个字典。我们还需要理解层命名约定，所以我们把它扔掉:

```

layer_dict = {layer.name : layer for layer in model.layers}
print(layer_dict)

```

前面示例的输出如下:

```

{'block1_conv1': <keras.layers.convolutional.Convolution2D at 0x11b847690>,
 'block1_conv2': <keras.layers.convolutional.Convolution2D at 0x11b847f90>,
 'block1_pool': <keras.layers.pooling.MaxPooling2D at 0x11c45db90>,
 'block2_conv1': <keras.layers.convolutional.Convolution2D at 0x11c45ddd0>,
 'block2_conv2': <keras.layers.convolutional.Convolution2D at 0x11b88f810>,
 'block2_pool': <keras.layers.pooling.MaxPooling2D at 0x11c2d2690>,
 'block3_conv1': <keras.layers.convolutional.Convolution2D at 0x11c47b890>,
 'block3_conv2': <keras.layers.convolutional.Convolution2D at 0x11c510290>,
 'block3_conv3': <keras.layers.convolutional.Convolution2D at 0x11c4afa10>,
 'block3_pool': <keras.layers.pooling.MaxPooling2D at 0x11c334a10>,
 'block4_conv1': <keras.layers.convolutional.Convolution2D at 0x11c345b10>,
 'block4_conv2': <keras.layers.convolutional.Convolution2D at 0x11c345950>,
 'block4_conv3': <keras.layers.convolutional.Convolution2D at 0x11d52c910>,
 'block4_pool': <keras.layers.pooling.MaxPooling2D at 0x11d550c90>,
 'block5_conv1': <keras.layers.convolutional.Convolution2D at 0x11d566c50>,
 'block5_conv2': <keras.layers.convolutional.Convolution2D at 0x11d5b1910>,
 'block5_conv3': <keras.layers.convolutional.Convolution2D at 0x11d5b1710>,
 'block5_pool': <keras.layers.pooling.MaxPooling2D at 0x11fd68e10>,
 'input_1': <keras.engine.topology.InputLayer at 0x11b847410>}

```

然后，我们计算五个汇集层中每一层的损失，并计算三个步骤的平均活化梯度。渐变被添加回图像，并且图像在每个步骤的每个汇集层显示:

```

num_pool_layers = 5
num_iters_per_layer = 3
step = 100

for i in range(num_pool_layers):
    # identify each pooling layer
    layer_name = "block{:d}_pool".format(i+1)
    # build loss function that maximizes the mean activation in layer
    layer_output = layer_dict[layer_name].output
    loss = K.mean(layer_output)
    # compute gradient of image wrt loss and normalize
    grads = K.gradients(loss, dream)[0]
    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)
    # define function to return loss and grad given input image
    f = K.function([dream], [loss, grads])
    img_value = p_img.copy()
    fig, axes = plt.subplots(1, num_iters_per_layer, figsize=(20, 10))
    for it in range(num_iters_per_layer):
        loss_value, grads_value = f([img_value])
        img_value += grads_value * step 
        axes[it].imshow(deprocess(img_value))
    plt.show()

```

生成的图像如下所示:

![](assets/cat-pool1.png)![](assets/cat-pool2.png)![](assets/cat-pool3.png)![](assets/cat-pool5.png)![](assets/cat-pool6.png)

正如你所看到的，深度做梦的过程放大了所选图层的渐变效果，产生了非常超现实的图像。后面的层反向传播梯度，导致更多的失真，反映了它们更大的感受野和它们识别更复杂特征的能力。

为了说服我们自己，一个经过训练的网络确实学习了它所训练的图像的各种类别的表示，让我们考虑一个完全随机的图像，如下所示，并通过预先训练的网络传递它:

```

img_noise = np.random.randint(100, 150, size=(227, 227, 3), dtype=np.uint8)
plt.imshow(img_noise)

```

前面示例的输出如下:

![](assets/random-noise.png)

将此图像通过前面的代码会在每一层产生非常特定的模式，如下所示，这表明网络正在尝试在随机数据中找到一种结构:

![](assets/noise-pool1.png)![](assets/noise-pool2.png)![](assets/noise-pool3.png)![](assets/noise-pool4.png)![](assets/noise-pool5.png)

我们可以用噪声图像作为输入重复实验，计算单个滤波器的损耗，而不是取所有滤波器的平均值。我们选择的过滤器用于 ImageNet 标签非洲象(`24`)。因此，我们用下面的代码替换前面代码中的损失值。因此，我们不是计算所有过滤器的平均值，而是将损失计算为代表非洲象类别的过滤器的输出:

```

loss = layer_output[:, :, :, 24]

```

在`block4_pool`输出中，我们得到看起来非常像大象鼻子的重复图像，如下所示:

![](assets/random-african-elephant.png)

# Keras 示例—风格转移

本文描述了深度做梦的一种扩展(有关更多信息，请参考:L. A. Gatys，A. S. Ecker 和 M. Bethge 在 2016 年 IEEE 计算机视觉和模式识别会议上发表的论文，使用卷积神经网络进行的*图像风格转移)，该扩展表明，经过训练的神经网络，如 VGG-16，可以学习内容和风格，并且这两者可以独立操作。因此，通过将一个对象(内容)的图像与一幅画的图像(样式)相结合，可以将它设计成看起来像一幅画的样式。*

像往常一样，让我们从导入我们的库开始:

```

from keras.applications import vgg16
from keras import backend as K
from scipy.misc import imresize
import matplotlib.pyplot as plt
import numpy as np
import os

```

我们的例子将展示用罗莎琳德·惠勒(【https://goo.gl/0VXC39】)的克洛德·莫内的*日本桥*的复制品来设计我们的猫的形象:

```

DATA_DIR = "../data"
CONTENT_IMAGE_FILE = os.path.join(DATA_DIR, "cat.jpg")
STYLE_IMAGE_FILE = os.path.join(DATA_DIR, "JapaneseBridgeMonetCopy.jpg")
RESIZED_WH = 400

content_img_value = imresize(plt.imread(CONTENT_IMAGE_FILE), (RESIZED_WH, RESIZED_WH))
style_img_value = imresize(plt.imread(STYLE_IMAGE_FILE), (RESIZED_WH, RESIZED_WH))

plt.subplot(121)
plt.title("content")
plt.imshow(content_img_value)

plt.subplot(122)
plt.title("style")
plt.imshow(style_img_value)

plt.show()

```

前面示例的输出如下:

![](assets/cat-style.png)

如前所述，我们声明我们的两个函数来来回转换 CNN 所期望的图像和四维张量:

```

def preprocess(img):
    img4d = img.copy()
    img4d = img4d.astype("float64")
    if K.image_dim_ordering() == "th":
        # (H, W, C) -> (C, H, W)
        img4d = img4d.transpose((2, 0, 1))
    img4d = np.expand_dims(img4d, axis=0)
    img4d = vgg16.preprocess_input(img4d)
    return img4d

def deprocess(img4d):
    img = img4d.copy()
    if K.image_dim_ordering() == "th":
        # (B, C, H, W)
        img = img.reshape((img4d.shape[1], img4d.shape[2], img4d.shape[3]))
        # (C, H, W) -> (H, W, C)
        img = img.transpose((1, 2, 0))
    else:
        # (B, H, W, C)
        img = img.reshape((img4d.shape[1], img4d.shape[2], img4d.shape[3]))
    img[:, :, 0] += 103.939
    img[:, :, 1] += 116.779
    img[:, :, 2] += 123.68
    # BGR -> RGB
    img = img[:, :, ::-1]
    img = np.clip(img, 0, 255).astype("uint8")
    return img

```

我们声明张量来保存内容图像和样式图像，并声明另一个张量来保存组合图像。内容和风格图像然后被连接成一个单一的输入张量。输入张量将被馈送到预训练的 VGG-16 网络:

```

content_img = K.variable(preprocess(content_img_value))
style_img = K.variable(preprocess(style_img_value))
if K.image_dim_ordering() == "th":
    comb_img = K.placeholder((1, 3, RESIZED_WH, RESIZED_WH))
else:
    comb_img = K.placeholder((1, RESIZED_WH, RESIZED_WH, 3))

# concatenate images into single input
input_tensor = K.concatenate([content_img, style_img, comb_img], axis=0)

```

我们实例化预训练的 VGG-16 网络的实例，用 ImageNet 数据进行预训练，并排除完全连接的层:

```

model = vgg16.VGG16(input_tensor=input_tensor, weights="imagenet", include_top=False)

```

如前所述，我们构建一个层字典，以将层名称映射到经过训练的 VGG-16 网络的输出层:

```

layer_dict = {layer.name : layer.output for layer in model.layers}

```

下一个模块定义了计算`content_loss`、`style_loss`和`variational_loss`的代码。最后，我们将损失定义为这三种损失的线性组合:

```

def content_loss(content, comb):
    return K.sum(K.square(comb - content))

def gram_matrix(x):
    if K.image_dim_ordering() == "th":
        features = K.batch_flatten(x)
    else:
        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))
    gram = K.dot(features, K.transpose(features))
    return gram

def style_loss_per_layer(style, comb):
    S = gram_matrix(style)
    C = gram_matrix(comb)
    channels = 3
    size = RESIZED_WH * RESIZED_WH
    return K.sum(K.square(S - C)) / (4 * (channels ** 2) * (size ** 2))

def style_loss():
    stl_loss = 0.0
    for i in range(NUM_LAYERS):
        layer_name = "block{:d}_conv1".format(i+1)
        layer_features = layer_dict[layer_name]
        style_features = layer_features[1, :, :, :]
        comb_features = layer_features[2, :, :, :]
        stl_loss += style_loss_per_layer(style_features, comb_features)
    return stl_loss / NUM_LAYERS

def variation_loss(comb):
    if K.image_dim_ordering() == "th":
        dx = K.square(comb[:, :, :RESIZED_WH-1, :RESIZED_WH-1] - 
                      comb[:, :, 1:, :RESIZED_WH-1])
        dy = K.square(comb[:, :, :RESIZED_WH-1, :RESIZED_WH-1] - 
                      comb[:, :, :RESIZED_WH-1, 1:])
    else:
        dx = K.square(comb[:, :RESIZED_WH-1, :RESIZED_WH-1, :] - 
                      comb[:, 1:, :RESIZED_WH-1, :])
        dy = K.square(comb[:, :RESIZED_WH-1, :RESIZED_WH-1, :] - 
                      comb[:, :RESIZED_WH-1, 1:, :])
     return K.sum(K.pow(dx + dy, 1.25))

CONTENT_WEIGHT = 0.1
STYLE_WEIGHT = 5.0
VAR_WEIGHT = 0.01
NUM_LAYERS = 5

c_loss = content_loss(content_img, comb_img)
s_loss = style_loss()
v_loss = variation_loss(comb_img)
loss = (CONTENT_WEIGHT * c_loss) + (STYLE_WEIGHT * s_loss) + (VAR_WEIGHT * v_loss)

```

这里，内容损失是从目标层提取的内容图像和组合图像的特征之间的均方根距离(也称为 **L2 距离**)。最小化它的效果是保持样式化的图像接近原始图像。

风格损失是基础图像表示和风格图像的格拉姆矩阵之间的 L2 距离。一个矩阵 *M* 的一个克矩阵是 *M* 乘以 *M* 的转置，即 *MT * M* 。这种损失衡量特征在内容图像表示和样式图像中一起出现的频率。这样做的一个实际含义是内容和样式矩阵必须是正方形的。

总变化损失测量相邻像素之间的差异。最小化它的效果是相邻的像素将是相似的，所以最终的图像是平滑的而不是跳动的。

我们计算梯度和损失函数，并反向运行我们的网络五次迭代:

```

grads = K.gradients(loss, comb_img)[0]
f = K.function([comb_img], [loss, grads])

NUM_ITERATIONS = 5
LEARNING_RATE = 0.001

content_img4d = preprocess(content_img_value)
for i in range(NUM_ITERATIONS):
    print("Epoch {:d}/{:d}".format(i+1, NUM_ITERATIONS))
    loss_value, grads_value = f([content_img4d])
    content_img4d += grads_value * LEARNING_RATE 
    plt.imshow(deprocess(content_img4d))
    plt.show()

```

最后两次迭代的输出如下所示。正如你所看到的，它在最终的图像中获得了印象主义的模糊性，甚至是画布的纹理:

![](assets/cat-style-epoch4.png)![](assets/cat-style-epoch5.png)

# 摘要

在这一章中，我们介绍了一些在前面章节中没有介绍的深度学习网络。我们首先简要介绍了 Keras functional API，它允许我们构建比我们迄今所见的顺序网络更复杂的网络。然后我们看了回归网络，它允许我们在一个连续的空间里做预测，并打开了我们可以解决的一系列全新的问题。然而，回归网络实际上是标准分类网络的非常简单的修改。我们关注的下一个领域是自动编码器，这是一种网络类型，允许我们进行无监督的学习，并利用我们现在都可以访问的大量未标记数据。我们还学会了如何将我们已经学过的像巨大的乐高积木一样的网络组合成更大更有趣的网络。然后，我们从使用较小的网络构建大型网络转移到学习如何使用 Keras 后端层定制网络中的各个层。最后，我们看了生成模型，另一类学习模仿输入的模型，并看了这种模型的一些新用途。

在下一章中，我们将把注意力转向另一种称为强化学习的学习方式，并通过在 Keras 中构建和训练一个网络来玩一个简单的计算机游戏来探索其概念。