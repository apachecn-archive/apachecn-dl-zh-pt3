

# 四、生成对抗网络与小波

在本章中，我们将讨论**生成对抗网络** ( **GANs** )和 WaveNets。深度学习之父之一 Yann LeCun 在 ML([https://www . quora . com/What-s-some-recent-and-potential-upcome-breakthrough-in-deep-learning](https://www.quora.com/What-are-some-recent-and-potentially-upcoming-breakthroughs-in-deep-learning))中将 GANs 定义为*过去 10 年中最有趣的想法。甘人能够学习如何复制看起来真实的合成数据。例如，计算机可以学习如何绘画和创建逼真的图像。这个想法最初是由 Ian Goodfellow 提出的(更多信息请参考: *NIPS 2016 教程:生成性对抗性网络*，作者 I. Goodfellow，2016)；他曾与蒙特利尔大学、谷歌大脑以及最近的 open ai([https://openai.com/](https://openai.com/))合作。WaveNet 是由谷歌 DeepMind 提出的一种深度生成网络，旨在教会计算机如何再现人类的声音和乐器，两者的质量都令人印象深刻。*

在本章中，我们将涵盖以下主题:

*   甘是什么？
*   深度卷积 GAN
*   氮化镓的应用



# 什么是甘？

甘的主要直觉可以很容易地被认为类似于*艺术伪造*，这是一个创造艺术作品(【https://en.wikipedia.org/wiki/Art】)的过程，这些作品被错误地归功于其他通常更著名的艺术家。GANs 同时训练两个神经网络，如下图所示。生成器 *G(Z)* 制作赝品，鉴别器 *D(Y)* 根据对真品和复制品的观察，判断复制品的逼真程度。 *D(Y)* 取一个输入， *Y，*(比如一张图片)并表示投票来判断输入的真实程度——一般来说，接近 0 的值表示*真实*，接近 1 的值表示*伪造*。 *G(Z)* 从随机噪声 *Z* 中获取输入，并训练自己欺骗 *D* 认为 *G(Z)* 产生的任何东西都是真实的。因此，训练鉴别器 *D(Y)* 的目标是最大化来自真实数据分布的每个图像的 *D(Y)* ，并且最小化不来自真实数据分布的每个图像的 *D(Y* )。于是， *G* 和 *D* 玩一个相反的游戏；因此得名*对抗训练*。注意，我们以交替的方式训练 *G* 和 *D* ，其中它们的每个目标都表示为通过梯度下降优化的损失函数。生成模型学习如何更成功地伪造，而鉴别模型学习如何更成功地识别伪造。鉴别器网络(通常是标准的卷积神经网络)试图对输入图像是真实的还是生成的进行分类。重要的新想法是通过鉴别器和发生器反向传播来调整发生器的参数，使得发生器可以学习如何在越来越多的情况下欺骗鉴别器。最后，生成器将学习如何生成与真实图像无法区分的伪造图像:

![](assets/B06258_04a_01.png)

当然，GANs 要求在两个人的博弈中找到均衡。为了有效地学习，要求如果一个玩家在一轮更新中成功地走下坡路，同样的更新也必须让另一个玩家走下坡路。想想吧！如果伪造者学会了如何在每一个场合愚弄法官，那么伪造者自己就没什么可学的了。有时，两个玩家最终会达到一个平衡，但这并不总是有保证的，两个玩家可以继续玩很长时间。下图提供了一个从双方学习的例子:

![](assets/B06258_04a_001.png)

# 一些 GAN 应用

我们已经看到，生成器学习如何伪造数据。这意味着它学习如何创建新的合成数据，这些数据是由网络创建的，看起来真实，就像是由人类创建的。在详细介绍一些 GAN 代码之前，我想分享一下最近的一篇论文的结果: *StackGAN:利用堆叠生成对抗网络*进行文本到照片级逼真图像合成，作者是、、、、、和迪米特里斯·梅塔克萨斯(代码可在线获得:[和](https://github.com/hanzhanggit/StackGAN))。
在这里，GAN 已经被用于从文本描述开始合成伪造的图像。结果令人印象深刻。第一列是测试集中的真实图像，其余的列包含由 StackGAN 的 Stage-I 和 Stage-II 从相同的文本描述生成的图像。更多例子可以在 YouTube 上找到([https://www.youtube.com/watch?v=SuRyL5vhCIM&feature = youtu . be](https://www.youtube.com/watch?v=SuRyL5vhCIM&feature=youtu.be)):

![](assets/B06258_04a_002.jpg)


现在让我们看看 GAN 如何学会*伪造*MNIST 数据集。在这种情况下，存在 GAN 和 ConvNets 的组合(关于更多信息，请参考:a .拉德福德，l .梅斯和 s .钦塔拉，arXiv: 1511.06434，2015，利用深度卷积生成对抗网络的无监督表示学习),用于生成器和鉴别器网络。一开始，生成器不会创建任何可以理解的东西，但经过几次迭代后，合成的伪造数字越来越清晰。在下图中，面板按增加的训练时段排序，您可以看到面板之间的质量在提高:

![](assets/B06258_04a_004.png)

下图显示了随着迭代次数的增加，伪造的手写数字:

![](assets/B06258_04a_005.png)

下图显示了在计算过程中伪造的手写数字。结果与原始结果几乎没有区别:

![](assets/B06258_04a_006.png)

GAN 最酷的用途之一是在生成器的向量 *Z* 中对人脸进行运算。换句话说，如果我们停留在合成伪造图像的空间，就有可能看到这样的东西:

*【微笑女人】-【中性女人】+【中性男人】=【微笑男人】*

或者像这样:

*【戴眼镜的男人】-【不戴眼镜的男人】+【不戴眼镜的女人】=【戴眼镜的女人】*

下一张图片摘自 a .拉德福德、l .梅斯和 s .钦塔拉的文章*深度卷积生成对抗网络的无监督表示学习*，arXiv: 1511.06434，2015 年 11 月:

![](assets/B06258_04a_007.png)



# 深度卷积生成对抗网络

论文中介绍了**深度卷积生成对抗网络**(**DCGAN**):*深度卷积生成对抗网络*的无监督表示学习，作者 a .拉德福德，l .梅斯，s .钦塔拉，arXiv: 1511.06434，2015。生成器使用一个 100 维的均匀分布空间， *Z，*然后通过一系列的面对面卷积运算将其投影到一个更小的空间。下图显示了一个示例:

![](assets/B06258_04a_008.png)

DCGAN 发生器可以用下面的 Keras 代码描述:它也由一个实现来描述，可在:[https://github.com/jacobgil/keras-dcgan](https://github.com/jacobgil/keras-dcgan)获得:

```

def generator_model():
    model = Sequential()
    model.add(Dense(input_dim=100, output_dim=1024))
    model.add(Activation('tanh'))
    model.add(Dense(128*7*7))
    model.add(BatchNormalization())
    model.add(Activation('tanh'))
    model.add(Reshape((128, 7, 7), input_shape=(128*7*7,)))
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(64, 5, 5, border_mode='same'))
    model.add(Activation('tanh'))
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(1, 5, 5, border_mode='same'))
    model.add(Activation('tanh'))
    return model

```

注意，代码使用 Keras 1.x 语法运行。然而，由于 Keras 的遗留接口，可以在 Keras 2.0 中运行它。在这种情况下，会报告一些警告，如下图所示:

![](assets/dcgan-compatibility.png)

现在让我们看看代码。第一密集层将 100 个维度的向量作为输入，并使用激活函数`tanh`产生 1024 个维度作为输出*。*我们假设输入是从 *[-1，1】*中的均匀分布中采样的。下一个密集层使用批量归一化在输出中产生 128 x 7 x 7 的数据(有关更多信息，请参考 S. Ioffe 和 C. Szegedy，arXiv: 1502.03167，2014 的*批量归一化:通过减少内部协变量偏移加速深度网络训练*)，这是一种通过将每个单元的输入归一化为零均值和单元方差来帮助稳定学习的技术*。*批量标准化已经被经验证明在许多情况下加速了训练，减少了不良初始化的问题，并且通常产生更准确的结果。还有一个`Reshape()`模块产生 127 x 7 x 7 (127 个通道，7 个宽度，7 个高度)`dim_ordering`到`tf`的数据，还有一个`UpSampling()`模块产生每一个到 2 x 2 正方形的重复。之后，我们有一个卷积层，在 5×5 卷积核上产生 64 个滤波器，激活`tanh` *，*后接一个新的`UpSampling()`和一个滤波器的最终卷积，在 5×5 卷积核上激活`tanh` *。*注意，这个 ConvNet 没有池操作。鉴别器可以用以下代码描述:

```

def discriminator_model():
    model = Sequential()
    model.add(Convolution2D(64, 5, 5, border_mode='same',
    input_shape=(1, 28, 28)))
    model.add(Activation('tanh'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Convolution2D(128, 5, 5))
    model.add(Activation('tanh'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(1024))
    model.add(Activation('tanh'))
    model.add(Dense(1))
    model.add(Activation('sigmoid'))
    return model

```

该代码采用形状为`(1, 28, 28)`的标准 MNIST 图像，并以`tanh`作为激活函数，应用与 64 个大小为 5×5 的滤波器的卷积。随后是大小为 2 x 2 的最大汇集运算和进一步的卷积最大汇集运算。最后两个阶段是密集的，最后一个阶段是对伪造的预测，它仅由一个具有`sigmoid`激活功能的神经元组成。对于选定数量的历元，通过使用`binary_crossentropy`作为损失函数，依次训练发生器和鉴别器。在每个时期，发生器做出许多预测(例如，它创建伪造的 MNIST 图像)，鉴别器在将预测与真实的 MNIST 图像混合后尝试学习。32 个纪元后，生成器学会伪造这组手写数字。没有人给机器编写过书写程序，但它已经学会了如何书写与人类书写的数字没有区别的数字。请注意，训练 GANs 可能非常困难，因为需要找到两个玩家之间的平衡。如果你对这个话题感兴趣，我建议你看看修行者(【https://github.com/soumith/ganhacks】)收集的一系列招数:

![](assets/Capture.png)

# 伪造 MNIST 的敌对势力

keras adversarial([https://github.com/bstriner/keras-adversarial](https://github.com/bstriner/keras-adversarial))是 Ben Striner([https://github.com/bstriner](https://github.com/bstriner)和[https://github . com/bstriner/keras-adversarial/blob/master/license . txt](https://github.com/bstriner/keras-adversarial/blob/master/LICENSE.txt))开发的用于构建 GANs 的开源 Python 包。由于 Keras 最近才迁移到 2.0，我建议下载最新的 Keras 对抗包:

```

git clone --depth=50 --branch=master https://github.com/bstriner/keras-adversarial.git

```

并安装`setup.py`:

```

python setup.py install

```

请注意，本期跟踪了与 Keras 2.0 的兼容性[https://github.com/bstriner/keras-adversarial/issues/11](https://github.com/bstriner/keras-adversarial/issues/11)。

如果发生器 *G* 和鉴别器 *D* 基于同一个模型 *M、*，那么它们可以组合成一个对抗模型；它使用相同的输入， *M* ，但是将 *G* 和 *D* 的目标和指标分开。该库具有以下 API 调用:

```

adversarial_model = AdversarialModel(base_model=M,
    player_params=[generator.trainable_weights, discriminator.trainable_weights],
    player_names=["generator", "discriminator"])

```

如果生成器 *G* 和鉴别器 *D* 基于两个不同的模型，那么可以使用这个 API 调用:

```

adversarial_model = AdversarialModel(player_models=[gan_g, gan_d],
    player_params=[generator.trainable_weights, discriminator.trainable_weights],
    player_names=["generator", "discriminator"])

```

让我们看一个 MNIST 计算的例子:

```

import matplotlib as mpl
# This line allows mpl to run with no DISPLAY defined
mpl.use('Agg')

```

让我们看看开源代码([https://github . com/bstriner/keras-adversarial/blob/master/examples/examples _ gan _ convolutional . py](https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_convolutional.py))。注意，代码使用了 Keras 1.x 的语法，但是由于`legacy.py`中包含的一组方便的实用函数，它也运行在 Keras 2.x 之上。`legacy.py`的代码在[附录](c0a1905f-57cc-401c-b485-6bf0854e43e9.xhtml)、*结论*中报告，可在[https://github . com/bstriner/keras-adversarial/blob/master/keras _ adversarial/legacy . py](https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py)中获得。

首先，开源示例导入了许多模块。除了 LeakyReLU 之外，我们以前都见过，leaky ReLU 是 ReLU 的一个特殊版本，当单位不活动时，它允许一个小的梯度。实验表明，LeakyReLU 可以在多种情况下提高 GANs 的性能(有关更多信息，请参考:B. Xu，N. Wang，T. Chen 和 M. Li，arXiv:1505.00853，2014)的*卷积网络中校正激活的经验评估*:

```

from keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU,
    Input, Activation, BatchNormalization
from keras.models import Sequential, Model
from keras.layers.convolutional import Convolution2D, UpSampling2D
from keras.optimizers import Adam
from keras.regularizers import l1, l1l2
from keras.datasets import mnist

import pandas as pd
import numpy as np

```

然后，导入 GANs 的特定模块:

```

from keras_adversarial import AdversarialModel, ImageGridCallback,
    simple_gan, gan_targets
from keras_adversarial import AdversarialOptimizerSimultaneous,
    normal_latent_sampling, AdversarialOptimizerAlternating
from image_utils import dim_ordering_fix, dim_ordering_input,
    dim_ordering_reshape, dim_ordering_unfix

```

多人游戏中的对抗模型训练。给定一个有 *n* 个目标和 *k* 个玩家的基础模型，创建一个有 *n*k* 个目标的模型，其中每个玩家优化该玩家目标的损失。此外，`simple_gan`用给定的`gan_targets`产生一个 GAN。注意，在库中，生成器和鉴别器的标签是相反的；直觉上，这是 GANs 的标准做法:

```

def gan_targets(n):
    """
    Standard training targets [generator_fake, generator_real, discriminator_fake,     
    discriminator_real] = [1, 0, 0, 1]
    :param n: number of samples
    :return: array of targets
    """
    generator_fake = np.ones((n, 1))
    generator_real = np.zeros((n, 1))
    discriminator_fake = np.zeros((n, 1))
    discriminator_real = np.ones((n, 1))
    return [generator_fake, generator_real, discriminator_fake, discriminator_real]

```

该示例以类似于我们之前看到的方式定义了生成器。然而，在这种情况下，我们使用函数语法——管道中的每个模块只是作为输入传递给下一个模块。所以，第一个模块是密集的，使用`glorot_normal`初始化。这种初始化使用由节点的输入和输出之和缩放的高斯噪声。所有其他模块都使用相同类型的初始化。`BatchNormlization`函数中的`mode=2`参数基于每批统计数据产生特征标准化。从实验上看，这产生了更好的结果:

```

def model_generator():
    nch = 256
    g_input = Input(shape=[100])
    H = Dense(nch * 14 * 14, init='glorot_normal')(g_input)
    H = BatchNormalization(mode=2)(H)
    H = Activation('relu')(H)
    H = dim_ordering_reshape(nch, 14)(H)
    H = UpSampling2D(size=(2, 2))(H)
    H = Convolution2D(int(nch / 2), 3, 3, border_mode='same', 
        init='glorot_uniform')(H)
    H = BatchNormalization(mode=2, axis=1)(H)
    H = Activation('relu')(H)
    H = Convolution2D(int(nch / 4), 3, 3, border_mode='same', 
        init='glorot_uniform')(H)
    H = BatchNormalization(mode=2, axis=1)(H)
    H = Activation('relu')(H)
    H = Convolution2D(1, 1, 1, border_mode='same', init='glorot_uniform')(H)
    g_V = Activation('sigmoid')(H)
    return Model(g_input, g_V)

```

鉴别器与本章前面定义的非常相似。唯一的主要区别是采用了`LeakyReLU`:

```

def model_discriminator(input_shape=(1, 28, 28), dropout_rate=0.5):
    d_input = dim_ordering_input(input_shape, name="input_x")
    nch = 512
    H = Convolution2D(int(nch / 2), 5, 5, subsample=(2, 2),
        border_mode='same', activation='relu')(d_input)
    H = LeakyReLU(0.2)(H)
    H = Dropout(dropout_rate)(H)
    H = Convolution2D(nch, 5, 5, subsample=(2, 2),
        border_mode='same', activation='relu')(H)
    H = LeakyReLU(0.2)(H)
    H = Dropout(dropout_rate)(H)
    H = Flatten()(H)
    H = Dense(int(nch / 2))(H)
    H = LeakyReLU(0.2)(H)
    H = Dropout(dropout_rate)(H)
    d_V = Dense(1, activation='sigmoid')(H)
    return Model(d_input, d_V)

```

然后，定义了两个用于加载和归一化 MNIST 数据的简单函数:

```

def mnist_process(x):
    x = x.astype(np.float32) / 255.0
    return x

def mnist_data():
    (xtrain, ytrain), (xtest, ytest) = mnist.load_data()
    return mnist_process(xtrain), mnist_process(xtest)

```

下一步，GAN 被定义为联合 GAN 模型中发生器和鉴别器的组合。注意，权重是用`normal_latent_sampling`初始化的，它从正态高斯分布中采样:

```

if __name__ == "__main__":
    # z in R^100
    latent_dim = 100
    # x in R^{28x28}
    input_shape = (1, 28, 28)
    # generator (z -> x)
    generator = model_generator()
    # discriminator (x -> y)
    discriminator = model_discriminator(input_shape=input_shape)
    # gan (x - > yfake, yreal), z generated on GPU
    gan = simple_gan(generator, discriminator, normal_latent_sampling((latent_dim,)))
    # print summary of models
    generator.summary()
    discriminator.summary()
    gan.summary()

```

在此之后，该示例创建我们的 GAN，并编译使用`Adam`优化器训练的模型，其中`binary_crossentropy`用作损失函数:

```

# build adversarial model
model = AdversarialModel(base_model=gan,
    player_params=[generator.trainable_weights, discriminator.trainable_weights],
    player_names=["generator", "discriminator"])
model.adversarial_compile(adversarial_optimizer=AdversarialOptimizerSimultaneous(),
    player_optimizers=[Adam(1e-4, decay=1e-4), Adam(1e-3, decay=1e-4)],
    loss='binary_crossentropy')

```

定义了用于创建看起来像真实图像的新图像的生成器。每个历元将在训练期间生成看起来像原始图像的新的伪造图像:

```

def generator_sampler():
    zsamples = np.random.normal(size=(10 * 10, latent_dim))
    gen = dim_ordering_unfix(generator.predict(zsamples))
    return gen.reshape((10, 10, 28, 28))

generator_cb = ImageGridCallback(
    "output/gan_convolutional/epoch-{:03d}.png",generator_sampler)
xtrain, xtest = mnist_data()
xtrain = dim_ordering_fix(xtrain.reshape((-1, 1, 28, 28)))
xtest = dim_ordering_fix(xtest.reshape((-1, 1, 28, 28)))
y = gan_targets(xtrain.shape[0])
ytest = gan_targets(xtest.shape[0])
history = model.fit(x=xtrain, y=y,
validation_data=(xtest, ytest), callbacks=[generator_cb], nb_epoch=100,
    batch_size=32)
df = pd.DataFrame(history.history)
df.to_csv("output/gan_convolutional/history.csv")
generator.save("output/gan_convolutional/generator.h5")
discriminator.save("output/gan_convolutional/discriminator.h5")

```

注意`dim_ordering_unfix`是支持`image_utils.py`中定义的不同图像排序的效用函数，如下:

```

def dim_ordering_fix(x):
    if K.image_dim_ordering() == 'th':
        return x
    else:
        return np.transpose(x, (0, 2, 3, 1))

```

现在让我们运行代码，看看发生器和鉴频器的损耗。在下面的屏幕截图中，我们看到了鉴别器和生成器的网络转储:

![](assets/keras_adversarial_MNIST.png)

以下屏幕截图显示了用于培训和验证的样本数量:

![](assets/B06258_04a_010.png)

经过 5-6 次迭代后，我们已经生成了可接受的人工图像，并且计算机已经学会了如何再现手写字符，如下图所示:

![](assets/B06258_04a_011.png)

# 用于伪造 CIFAR 的 Keras 对抗性 GANs

现在我们可以使用 GAN 方法来学习如何伪造 CIFAR-10 并创建看起来真实的合成图像。我们来看看开源代码([https://github . com/bstriner/keras-adversarial/blob/master/examples/examples _ gan _ cifar 10 . py](https://github.com/bstriner/keras-adversarial/blob/master/examples/example_gan_cifar10.py))。再次注意，它使用 Keras 1.x 的语法，但由于包含在`legacy.py`([https://github . com/bstriner/Keras-adversarial/blob/master/Keras _ adversarial/legacy . py](https://github.com/bstriner/keras-adversarial/blob/master/keras_adversarial/legacy.py))中的一组方便的实用函数，它也运行在 Keras 2.x 之上。首先，开源示例导入了许多包:

```

import matplotlib as mpl
# This line allows mpl to run with no DISPLAY defined
mpl.use('Agg')
import pandas as pd
import numpy as np
import os
from keras.layers import Dense, Reshape, Flatten, Dropout, LeakyReLU, 
    Activation, BatchNormalization, SpatialDropout2D
from keras.layers.convolutional import Convolution2D, UpSampling2D, 
    MaxPooling2D, AveragePooling2D
from keras.models import Sequential, Model
from keras.optimizers import Adam
from keras.callbacks import TensorBoard
from keras.regularizers import l1l2
from keras_adversarial import AdversarialModel, ImageGridCallback, 
    simple_gan, gan_targets
from keras_adversarial import AdversarialOptimizerSimultaneous, 
    normal_latent_sampling, fix_names
import keras.backend as K
from cifar10_utils import cifar10_data
from image_utils import dim_ordering_fix, dim_ordering_unfix, 
    dim_ordering_shape

```

接下来，它定义了一个生成器，该生成器使用卷积与`l1`和`l2`正则化、批量归一化和上采样的组合。注意，`axis=1`说要先归一化张量的维数，而`mode=0`说要采用基于特征的归一化。这个特殊的网络是许多微调实验的结果，但它本质上仍然是卷积 2D 和上采样操作的序列，它在开始时使用了一个`Dense`模块，在结束时使用了一个`sigmoid`。此外，每个卷积使用一个`LeakyReLU`激活函数和`BatchNormalization`:

```

def model_generator():
    model = Sequential()
    nch = 256
    reg = lambda: l1l2(l1=1e-7, l2=1e-7)
    h = 5
    model.add(Dense(input_dim=100, output_dim=nch * 4 * 4, W_regularizer=reg()))
    model.add(BatchNormalization(mode=0))
    model.add(Reshape(dim_ordering_shape((nch, 4, 4))))
    model.add(Convolution2D(nch/2, h, h, border_mode='same', W_regularizer=reg()))
    model.add(BatchNormalization(mode=0, axis=1))
    model.add(LeakyReLU(0.2))
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(nch / 2, h, h, border_mode='same', W_regularizer=reg()))
    model.add(BatchNormalization(mode=0, axis=1))
    model.add(LeakyReLU(0.2))
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(nch / 4, h, h, border_mode='same', W_regularizer=reg()))
    model.add(BatchNormalization(mode=0, axis=1))
    model.add(LeakyReLU(0.2))
    model.add(UpSampling2D(size=(2, 2)))
    model.add(Convolution2D(3, h, h, border_mode='same', W_regularizer=reg()))
    model.add(Activation('sigmoid'))
    return model

```

然后，定义一个鉴别器。同样，我们有一系列卷积 2D 操作，在这种情况下，我们采用`SpatialDropout2D`，它会丢弃整个 2D 特征图，而不是单个元素。出于类似的原因，我们也使用`MaxPooling2D`和`AveragePooling2D`:

```

def model_discriminator():
    nch = 256
    h = 5
    reg = lambda: l1l2(l1=1e-7, l2=1e-7)
    c1 = Convolution2D(nch / 4, h, h, border_mode='same', W_regularizer=reg(),
    input_shape=dim_ordering_shape((3, 32, 32)))
    c2 = Convolution2D(nch / 2, h, h, border_mode='same', W_regularizer=reg())
    c3 = Convolution2D(nch, h, h, border_mode='same', W_regularizer=reg())
    c4 = Convolution2D(1, h, h, border_mode='same', W_regularizer=reg())
    def m(dropout):
        model = Sequential()
        model.add(c1)
        model.add(SpatialDropout2D(dropout))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(LeakyReLU(0.2))
        model.add(c2)
        model.add(SpatialDropout2D(dropout))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(LeakyReLU(0.2))
        model.add(c3)
        model.add(SpatialDropout2D(dropout))
        model.add(MaxPooling2D(pool_size=(2, 2)))
        model.add(LeakyReLU(0.2))
        model.add(c4)
        model.add(AveragePooling2D(pool_size=(4, 4), border_mode='valid'))
        model.add(Flatten())
        model.add(Activation('sigmoid'))
        return model
    return m

```

现在可以生成适当的 GANs。以下函数接受多个输入，包括生成器、鉴别器、潜在维度数和 GAN 目标:

```

def example_gan(adversarial_optimizer, path, opt_g, opt_d, nb_epoch, generator,
        discriminator, latent_dim, targets=gan_targets, loss='binary_crossentropy'):
    csvpath = os.path.join(path, "history.csv")
    if os.path.exists(csvpath):
        print("Already exists: {}".format(csvpath))
    return

```

然后为鉴别器创建两个 gan，一个有漏失，另一个没有漏失:

```

print("Training: {}".format(csvpath))
# gan (x - > yfake, yreal), z is gaussian generated on GPU
# can also experiment with uniform_latent_sampling
d_g = discriminator(0)
d_d = discriminator(0.5)
generator.summary()
d_d.summary()
gan_g = simple_gan(generator, d_g, None)
gan_d = simple_gan(generator, d_d, None)
x = gan_g.inputs[1]
z = normal_latent_sampling((latent_dim,))(x)
# eliminate z from inputs
gan_g = Model([x], fix_names(gan_g([z, x]), gan_g.output_names))
gan_d = Model([x], fix_names(gan_d([z, x]), gan_d.output_names))

```

这两个 GANs 现在被组合成一个具有独立权重的对抗模型，然后该模型被编译:

```

# build adversarial model
model = AdversarialModel(player_models=[gan_g, gan_d],
    player_params=[generator.trainable_weights, d_d.trainable_weights],
    player_names=["generator", "discriminator"])
model.adversarial_compile(adversarial_optimizer=adversarial_optimizer,
    player_optimizers=[opt_g, opt_d], loss=loss)

```

接下来，对样本图像进行简单的回调，并在定义了方法`ImageGridCallback`的文件上打印:

```

# create callback to generate images
zsamples = np.random.normal(size=(10 * 10, latent_dim))
def generator_sampler():
    xpred = dim_ordering_unfix(generator.predict(zsamples)).transpose((0, 2, 3, 1))
    return xpred.reshape((10, 10) + xpred.shape[1:])
generator_cb =
    ImageGridCallback(os.path.join(path, "epoch-{:03d}.png"),
    generator_sampler, cmap=None)

```

现在，CIFAR-10 数据已加载，模型已拟合。如果后端是 TensorFlow，那么损失信息被保存到 TensorBoard 中，以检查损失如何随时间减少。历史也可以方便地保存为 CVS 格式，模型的重量也以`h5`格式存储:

```

# train model
xtrain, xtest = cifar10_data()
y = targets(xtrain.shape[0])
ytest = targets(xtest.shape[0])
callbacks = [generator_cb]
if K.backend() == "tensorflow":
    callbacks.append(TensorBoard(log_dir=os.path.join(path, 'logs'),
        histogram_freq=0, write_graph=True, write_images=True))
history = model.fit(x=dim_ordering_fix(xtrain),y=y,
    validation_data=(dim_ordering_fix(xtest), ytest),
    callbacks=callbacks, nb_epoch=nb_epoch,
    batch_size=32)
# save history to CSV
df = pd.DataFrame(history.history)
df.to_csv(csvpath)
# save models
generator.save(os.path.join(path, "generator.h5"))
d_d.save(os.path.join(path, "discriminator.h5"))

```

终于可以运行整个 GANs 了。生成器从一个具有 100 个潜在维度的空间中采样，我们使用`Adam`作为两个 gan 的优化器:

```

def main():
    # z in R^100
    latent_dim = 100
    # x in R^{28x28}
    # generator (z -> x)
    generator = model_generator()
    # discriminator (x -> y)
    discriminator = model_discriminator()
    example_gan(AdversarialOptimizerSimultaneous(), "output/gan-cifar10",
        opt_g=Adam(1e-4, decay=1e-5),
        opt_d=Adam(1e-3, decay=1e-5),
        nb_epoch=100, generator=generator, discriminator=discriminator,
        latent_dim=latent_dim)
if __name__ == "__main__":
main()

```

为了对开放源代码有一个完整的了解，我们需要包含一些简单的实用函数来存储图像网格:

```

from matplotlib import pyplot as plt, gridspec
import os

def write_image_grid(filepath, imgs, figsize=None, cmap='gray'):
    directory = os.path.dirname(filepath)
    if not os.path.exists(directory):
        os.makedirs(directory)
    fig = create_image_grid(imgs, figsize, cmap=cmap)
    fig.savefig(filepath)
    plt.close(fig)

def create_image_grid(imgs, figsize=None, cmap='gray'):
    n = imgs.shape[0]
    m = imgs.shape[1]
    if figsize is None:
        figsize=(n,m)
    fig = plt.figure(figsize=figsize)
    gs1 = gridspec.GridSpec(n, m)
    gs1.update(wspace=0.025, hspace=0.025) # set the spacing between axes.
    for i in range(n):
        for j in range(m):
            ax = plt.subplot(gs1[i, j])
            img = imgs[i, j, :]
    ax.imshow(img, cmap=cmap)
    ax.axis('off')
    return fig

```

此外，我们需要一些实用的方法来处理不同的图像排序(例如，Theano 或 TensorFlow):

```

import keras.backend as K
import numpy as np
from keras.layers import Input, Reshape

def dim_ordering_fix(x):
    if K.image_dim_ordering() == 'th':
        return x
    else:
        return np.transpose(x, (0, 2, 3, 1))

def dim_ordering_unfix(x):
    if K.image_dim_ordering() == 'th':
        return x
    else:
        return np.transpose(x, (0, 3, 1, 2))

def dim_ordering_shape(input_shape):
    if K.image_dim_ordering() == 'th':
        return input_shape
    else:
        return (input_shape[1], input_shape[2], input_shape[0])

def dim_ordering_input(input_shape, name):
    if K.image_dim_ordering() == 'th':
        return Input(input_shape, name=name)
    else:
        return Input((input_shape[1], input_shape[2], input_shape[0]), name=name)

def dim_ordering_reshape(k, w, **kwargs):
    if K.image_dim_ordering() == 'th':
        return Reshape((k, w, w), **kwargs)
    else:
        return Reshape((w, w, k), **kwargs)

# One more utility function is used to fix names
def fix_names(outputs, names):
    if not isinstance(outputs, list):
        outputs = [outputs]
    if not isinstance(names, list):
        names = [names]
    return [Activation('linear', name=name)(output) 
        for output, name in zip(outputs, names)]

```

以下屏幕截图显示了已定义网络的转储:

![](assets/keras_adversarial_CIFAR.png)

如果我们运行开源代码，第一次迭代就会产生不真实的图像。然而，在 99 次迭代之后，网络将学会伪造看起来像真实的 CIFAR-10 图像的图像，如下所示:

![](assets/B06258_04a_012.png)

在下面的图片中，我们看到右边是真实的 CIFAR-10 图像，左边是伪造的:

| 伪造的图像 | 真实 CIFAR-10 图像 |
| ![](assets/B06258_04a_013.png) | ![](assets/B06258_04a_014.png) |



# wave net——学习如何制作音频的生成模型

WaveNet 是用于产生原始音频波形的深度生成模型。这项突破性的技术是由谷歌 deep mind(【https://deepmind.com/】)推出的([https://deep mind . com/blog/wave net-generative-model-raw-audio/](https://deepmind.com/blog/wavenet-generative-model-raw-audio/))，用于教用户如何与计算机对话。结果确实令人印象深刻，你可以在网上找到合成声音的例子，在这些例子中，计算机学习如何与马特·达蒙等名人的声音交谈。所以，你可能想知道为什么学习合成音频如此困难。嗯，我们听到的每个数字声音都是基于每秒 16，000 个样本(有时是 48，000 个或更多)，建立一个预测模型，让我们学会根据所有以前的样本来重现样本是一个非常困难的挑战。然而，有实验表明，WaveNet 改进了当前最先进的**文本到语音** ( **TTS** )系统，将美国英语和中国普通话与人类语音的差异减少了 50%。更酷的是，DeepMind 证明了 WaveNet 也可以用来教计算机如何生成钢琴音乐等乐器的声音。现在是时候做一些定义了。TTS 系统通常分为两个不同的类别:

*   串联 TTS:这是单个语音片段首先被记忆的地方，然后当声音必须被再现时被重新组合。然而，这种方法不可扩展，因为只可能再现记忆的语音片段，而不可能再现新的说话者或不同类型的音频而不从头开始记忆这些片段。
*   **参数化 TTS** :这是创建模型的地方，用于存储要合成的音频的所有特征。在 WaveNet 之前，使用参数化 TTS 生成的音频不如拼接 TTS 自然。WaveNet 通过直接模拟音频声音的产生，而不是使用过去使用的中间信号处理算法，改进了最先进的技术。

原则上，WaveNet 可以被视为 1D 卷积层的堆叠(我们已经在[第 3 章](4be2a04a-4545-4051-bcd9-32764d21f0f2.xhtml)、*深度学习与 conv net*中看到了图像的 2D 卷积)，步长恒定为 1，没有池层。请注意，输入和输出在结构上具有相同的维度，因此 ConvNet 非常适合对音频等顺序数据进行建模。然而，已经表明，为了使输出神经元中的感受野达到大的尺寸(记住，一层中神经元的感受野是神经元提供输入的前一层的横截面),有必要使用大量的大滤波器或者过分地增加网络的深度。由于这个原因，纯 ConvNets 在学习如何合成音频方面并不那么有效。WaveNet 之外的关键直觉是扩张的因果卷积(有关更多信息，请参考 Fisher Yu，Vladlen Koltun，2016 年的文章:*通过扩张的卷积进行多尺度上下文聚合*，可在以下网址获得:[https://www . semantic scholar . org/paper/Multi-Scale-Context-Aggregation-by-expanded-Yu-kol tun/420 c46 D7 cafcb 841309 f 02 ad 04 cf 51 CB 190 a 48](https://www.semanticscholar.org/paper/Multi-Scale-Context-Aggregation-by-Dilated-Yu-Koltun/420c46d7cafcb841309f02ad04cf51cb1f190a48))或有时 atrous 卷积(。例如，在一个维度中，大小为 *3* 且具有膨胀 *1* 的过滤器 *w* 将计算以下总和:

![](assets/B06258_04a_015.png)

由于这个引入*孔*的简单想法，有可能用指数增长的滤波器堆叠多个扩展的卷积层，并且在没有过度深的网络的情况下学习长距离输入依赖性。因此，WaveNet 是一种 ConvNet，其中卷积层具有各种膨胀因子，允许感受野随深度呈指数增长，从而有效地覆盖数千个音频时间步长。当我们训练时，输入是从人类扬声器中录制的声音。波形被量化到固定的整数范围。波网定义了仅访问当前和先前输入的初始卷积层。然后，有一个扩展的 ConvNet 层堆栈，仍然只访问当前和以前的输入。最后，是一系列结合了先前结果的密集层，之后是用于分类输出的 softmax 激活函数。在每一步，从网络预测一个值，并反馈到输入中。同时，计算下一步的新预测。损失函数是当前步骤的输出和下一步骤的输入之间的交叉熵。Bas Veeling 开发的一个 Keras 实现可在:[https://github.com/basveeling/wavenet](https://github.com/basveeling/wavenet)获得，并可通过`git`轻松安装:

```

pip install virtualenv
mkdir ~/virtualenvs && cd ~/virtualenvs
virtualenv wavenet
source wavenet/bin/activate
cd ~
git clone https://github.com/basveeling/wavenet.git
cd wavenet
pip install -r requirements.txt

```

请注意，该代码与 Keras 1.x 兼容，请在[https://github.com/basveeling/wavenet/issues/29](https://github.com/basveeling/wavenet/issues/29)查看该问题，以了解将其移植到 Keras 2.x 上的进度。培训非常简单，但需要大量的计算能力(因此请确保您有良好的 GPU 支持):

```

$ python wavenet.py with 'data_dir=your_data_dir_name'

```

训练后对网络进行采样同样非常容易:

```

python wavenet.py predict with 'models/[run_folder]/config.json predict_seconds=1'

```

你可以在网上找到大量的超参数，可以用来微调我们的训练过程。这个网络真的很深，正如这个内部层的转储所解释的那样。注意输入波形分为(`fragment_length = 1152`和`nb_output_bins = 256`)，是传播到 WaveNet 的张量。WaveNet 由称为残差的重复块组成，每个块由两个扩展卷积模块(一个具有`sigmoid`，另一个具有`tanh`激活)的乘法合并组成，后面是求和合并卷积。请注意，每个扩张的卷积都有从 1 到 512 的指数大小(`2 ** i`)增长的孔洞，如这段文本中所定义:

```

def residual_block(x):
    original_x = x
    tanh_out = CausalAtrousConvolution1D(nb_filters, 2, atrous_rate=2 ** i,
        border_mode='valid', causal=True, bias=use_bias,
        name='dilated_conv_%d_tanh_s%d' % (2 ** i, s), activation='tanh',
        W_regularizer=l2(res_l2))(x)
    sigm_out = CausalAtrousConvolution1D(nb_filters, 2, atrous_rate=2 ** i,
        border_mode='valid', causal=True, bias=use_bias,
        name='dilated_conv_%d_sigm_s%d' % (2 ** i, s), activation='sigmoid',
        W_regularizer=l2(res_l2))(x)
    x = layers.Merge(mode='mul',
        name='gated_activation_%d_s%d' % (i, s))([tanh_out, sigm_out])
        res_x = layers.Convolution1D(nb_filters, 1, border_mode='same', bias=use_bias,
        W_regularizer=l2(res_l2))(x)
    skip_x = layers.Convolution1D(nb_filters, 1, border_mode='same', bias=use_bias,
        W_regularizer=l2(res_l2))(x)
    res_x = layers.Merge(mode='sum')([original_x, res_x])
    return res_x, skip_x

```

在残差扩张块之后，是一个合并卷积模块序列，后面是两个卷积模块，后面是一个`nb_output_bins`类别中的`softmax`激活函数。完整的网络结构如下:

```

Layer (type) Output Shape Param # Connected to
====================================================================================================
input_part (InputLayer) (None, 1152, 256) 0
____________________________________________________________________________________________________
initial_causal_conv (CausalAtrou (None, 1152, 256) 131328 input_part[0][0]
____________________________________________________________________________________________________
dilated_conv_1_tanh_s0 (CausalAt (None, 1152, 256) 131072 initial_causal_conv[0][0]
____________________________________________________________________________________________________
dilated_conv_1_sigm_s0 (CausalAt (None, 1152, 256) 131072 initial_causal_conv[0][0]
____________________________________________________________________________________________________
gated_activation_0_s0 (Merge) (None, 1152, 256) 0 dilated_conv_1_tanh_s0[0][0]
dilated_conv_1_sigm_s0[0][0]
______________________________________________________________________
_____________________________
convolution1d_1 (Convolution1D) (None, 1152, 256) 65536 gated_activation_0_s0[0][0]
____________________________________________________________________________________________________
merge_1 (Merge) (None, 1152, 256) 0 initial_causal_conv[0][0]
convolution1d_1[0][0]
____________________________________________________________________________________________________
dilated_conv_2_tanh_s0 (CausalAt (None, 1152, 256) 131072 merge_1[0][0]
____________________________________________________________________________________________________
dilated_conv_2_sigm_s0 (CausalAt (None, 1152, 256) 131072 merge_1[0][0]
____________________________________________________________________________________________________
gated_activation_1_s0 (Merge) (None, 1152, 256) 0 dilated_conv_2_tanh_s0[0][0]
dilated_conv_2_sigm_s0[0][0]
____________________________________________________________________________________________________
convolution1d_3 (Convolution1D) (None, 1152, 256) 65536 gated_activation_1_s0[0][0]
____________________________________________________________________________________________________
merge_2 (Merge) (None, 1152, 256) 0 merge_1[0][0]
convolution1d_3[0][0]
____________________________________________________________________________________________________
dilated_conv_4_tanh_s0 (CausalAt (None, 1152, 256) 131072 merge_2[0][0]
____________________________________________________________________________________________________
dilated_conv_4_sigm_s0 (CausalAt (None, 1152, 256) 131072 merge_2[0][0]
____________________________________________________________________________________________________
gated_activation_2_s0 (Merge) (None, 1152, 256) 0 dilated_conv_4_tanh_s0[0][0]
dilated_conv_4_sigm_s0[0][0]
____________________________________________________________________________________________________
convolution1d_5 (Convolution1D) (None, 1152, 256) 65536 gated_activation_2_s0[0][0]
____________________________________________________________________________________________________
merge_3 (Merge) (None, 1152, 256) 0 merge_2[0][0]
convolution1d_5[0][0]
____________________________________________________________________________________________________
dilated_conv_8_tanh_s0 (CausalAt (None, 1152, 256) 131072 merge_3[0][0]
____________________________________________________________________________________________________
dilated_conv_8_sigm_s0 (CausalAt (None, 1152, 256) 131072 merge_3[0][0]
____________________________________________________________________________________________________
gated_activation_3_s0 (Merge) (None, 1152, 256) 0 dilated_conv_8_tanh_s0[0][0]
dilated_conv_8_sigm_s0[0][0]
____________________________________________________________________________________________________
convolution1d_7 (Convolution1D) (None, 1152, 256) 65536 gated_activation_3_s0[0][0]
____________________________________________________________________________________________________
merge_4 (Merge) (None, 1152, 256) 0 merge_3[0][0]
convolution1d_7[0][0]
____________________________________________________________________________________________________
dilated_conv_16_tanh_s0 (CausalA (None, 1152, 256) 131072 merge_4[0][0]
____________________________________________________________________________________________________
dilated_conv_16_sigm_s0 (CausalA (None, 1152, 256) 131072 merge_4[0][0]
____________________________________________________________________________________________________
gated_activation_4_s0 (Merge) (None, 1152, 256) 0 dilated_conv_16_tanh_s0[0][0]
dilated_conv_16_sigm_s0[0][0]
____________________________________________________________________________________________________
convolution1d_9 (Convolution1D) (None, 1152, 256) 65536 gated_activation_4_s0[0][0]
____________________________________________________________________________________________________
merge_5 (Merge) (None, 1152, 256) 0 merge_4[0][0]
convolution1d_9[0][0]
____________________________________________________________________________________________________
dilated_conv_32_tanh_s0 (CausalA (None, 1152, 256) 131072 merge_5[0][0]
____________________________________________________________________________________________________
dilated_conv_32_sigm_s0 (CausalA (None, 1152, 256) 131072 merge_5[0][0]
____________________________________________________________________________________________________
gated_activation_5_s0 (Merge) (None, 1152, 256) 0 dilated_conv_32_tanh_s0[0][0]
dilated_conv_32_sigm_s0[0][0]
____________________________________________________________________________________________________
convolution1d_11 (Convolution1D) (None, 1152, 256) 65536 gated_activation_5_s0[0][0]
____________________________________________________________________________________________________
merge_6 (Merge) (None, 1152, 256) 0 merge_5[0][0]
convolution1d_11[0][0]
____________________________________________________________________________________________________
dilated_conv_64_tanh_s0 (CausalA (None, 1152, 256) 131072 merge_6[0][0]
____________________________________________________________________________________________________
dilated_conv_64_sigm_s0 (CausalA (None, 1152, 256) 131072 merge_6[0][0]
____________________________________________________________________________________________________
gated_activation_6_s0 (Merge) (None, 1152, 256) 0 dilated_conv_64_tanh_s0[0][0]
dilated_conv_64_sigm_s0[0][0]
____________________________________________________________________________________________________
convolution1d_13 (Convolution1D) (None, 1152, 256) 65536 gated_activation_6_s0[0][0]
____________________________________________________________________________________________________
merge_7 (Merge) (None, 1152, 256) 0 merge_6[0][0]
convolution1d_13[0][0]
____________________________________________________________________________________________________
dilated_conv_128_tanh_s0 (Causal (None, 1152, 256) 131072 merge_7[0][0]
____________________________________________________________________________________________________
dilated_conv_128_sigm_s0 (Causal (None, 1152, 256) 131072 merge_7[0][0]
____________________________________________________________________________________________________
gated_activation_7_s0 (Merge) (None, 1152, 256) 0 dilated_conv_128_tanh_s0[0][0]
dilated_conv_128_sigm_s0[0][0]
____________________________________________________________________________________________________
convolution1d_15 (Convolution1D) (None, 1152, 256) 65536 gated_activation_7_s0[0][0]
____________________________________________________________________________________________________
merge_8 (Merge) (None, 1152, 256) 0 merge_7[0][0]
convolution1d_15[0][0]
____________________________________________________________________________________________________
dilated_conv_256_tanh_s0 (Causal (None, 1152, 256) 131072 merge_8[0][0]
____________________________________________________________________________________________________
dilated_conv_256_sigm_s0 (Causal (None, 1152, 256) 131072 merge_8[0][0]
____________________________________________________________________________________________________
gated_activation_8_s0 (Merge) (None, 1152, 256) 0 dilated_conv_256_tanh_s0[0][0]
dilated_conv_256_sigm_s0[0][0]
____________________________________________________________________________________________________
convolution1d_17 (Convolution1D) (None, 1152, 256) 65536 gated_activation_8_s0[0][0]
____________________________________________________________________________________________________
merge_9 (Merge) (None, 1152, 256) 0 merge_8[0][0]
convolution1d_17[0][0]
____________________________________________________________________________________________________
dilated_conv_512_tanh_s0 (Causal (None, 1152, 256) 131072 merge_9[0][0]
____________________________________________________________________________________________________
dilated_conv_512_sigm_s0 (Causal (None, 1152, 256) 131072 merge_9[0][0]
____________________________________________________________________________________________________
gated_activation_9_s0 (Merge) (None, 1152, 256) 0 dilated_conv_512_tanh_s0[0][0]
dilated_conv_512_sigm_s0[0][0]
____________________________________________________________________________________________________
convolution1d_2 (Convolution1D) (None, 1152, 256) 65536 gated_activation_0_s0[0][0]
____________________________________________________________________________________________________
convolution1d_4 (Convolution1D) (None, 1152, 256) 65536 gated_activation_1_s0[0][0]
____________________________________________________________________________________________________
convolution1d_6 (Convolution1D) (None, 1152, 256) 65536 gated_activation_2_s0[0][0]
____________________________________________________________________________________________________
convolution1d_8 (Convolution1D) (None, 1152, 256) 65536 gated_activation_3_s0[0][0]
____________________________________________________________________________________________________
convolution1d_10 (Convolution1D) (None, 1152, 256) 65536 gated_activation_4_s0[0][0]
____________________________________________________________________________________________________
convolution1d_12 (Convolution1D) (None, 1152, 256) 65536 gated_activation_5_s0[0][0]
____________________________________________________________________________________________________
convolution1d_14 (Convolution1D) (None, 1152, 256) 65536 gated_activation_6_s0[0][0]
____________________________________________________________________________________________________
convolution1d_16 (Convolution1D) (None, 1152, 256) 65536 gated_activation_7_s0[0][0]
____________________________________________________________________________________________________
convolution1d_18 (Convolution1D) (None, 1152, 256) 65536 gated_activation_8_s0[0][0]
____________________________________________________________________________________________________
convolution1d_20 (Convolution1D) (None, 1152, 256) 65536 gated_activation_9_s0[0][0]
____________________________________________________________________________________________________
merge_11 (Merge) (None, 1152, 256) 0 convolution1d_2[0][0]
convolution1d_4[0][0]
convolution1d_6[0][0]
convolution1d_8[0][0]
convolution1d_10[0][0]
convolution1d_12[0][0]
convolution1d_14[0][0]
convolution1d_16[0][0]
convolution1d_18[0][0]
convolution1d_20[0][0]
____________________________________________________________________________________________________
activation_1 (Activation) (None, 1152, 256) 0 merge_11[0][0]
____________________________________________________________________________________________________
convolution1d_21 (Convolution1D) (None, 1152, 256) 65792 activation_1[0][0]
____________________________________________________________________________________________________
activation_2 (Activation) (None, 1152, 256) 0 convolution1d_21[0][0]
____________________________________________________________________________________________________
convolution1d_22 (Convolution1D) (None, 1152, 256) 65792 activation_2[0][0]
____________________________________________________________________________________________________
output_softmax (Activation) (None, 1152, 256) 0 convolution1d_22[0][0]
====================================================================================================
Total params: 4,129,536
Trainable params: 4,129,536
Non-trainable params: 0

```

DeepMind 试图用包括多个说话者的数据集进行训练，这大大提高了学习语言和音调的共享表示的能力，从而获得接近自然语音的结果。你会发现一个惊人的合成语音在线示例集合([https://deep mind . com/blog/wavenet-generative-model-raw-audio/](https://deepmind.com/blog/wavenet-generative-model-raw-audio/))，有趣的是，当 wave net 以附加文本为条件时，音频质量会提高，这些附加文本除了音频波形之外，还会转换为一系列语言和语音特征。我最喜欢的例子是，同一个句子在网络上用不同的声调发音。当然，听到 WaveNet 自己创作钢琴曲也是很迷人的。网上查一下！



# 摘要

在本章中，我们讨论了 gan。GAN 通常由两个网络组成；一个被训练来伪造看起来真实的合成数据，另一个被训练来区分真实数据和伪造数据。这两个网络不断地竞争，在这样做的过程中，他们不断地改进对方。我们回顾了一个开源代码，学习伪造看起来真实的 MNIST 和 CIFAR-10 图像。此外，我们讨论了 WaveNet，这是一个由 Google DeepMind 提出的深度生成网络，用于教计算机如何以令人印象深刻的质量再现人类的声音和乐器。WaveNet 使用基于扩展卷积网络的参数文本到语音方法直接生成原始音频。扩张卷积网络是一种特殊的卷积网络，其中卷积滤波器有孔，允许感受野在深度上呈指数增长，因此有效地覆盖数千个音频时间步长。DeepMind 展示了如何使用 WaveNet 合成人类语音和乐器，并改进了之前的最先进水平。在下一章，我们将讨论词嵌入——一套深度学习方法，用于检测单词之间的关系并将相似的单词分组在一起。