

# 下一步是什么？

强化学习是一种自动学习技术，旨在通过分配一种称为强化的奖励(包括评估其表现)来实现能够学习和适应其所处环境变化的系统。它可以通过不同的算法来实现，例如 Q-learning，被插入到要执行学习的系统中。这项技术越来越普及，因为它能够与环境互动。

在这一章中，我们将总结本书到目前为止所涵盖的内容，以及从这一点开始接下来的步骤。您将学习如何在构建和部署强化学习模型以及数据科学家经常使用的其他常用技术时，将您获得的技能应用于其他项目和现实生活中的挑战。在本章结束时，您将更好地了解构建和部署机器学习模型的现实挑战，以及学习提高机器学习技能的其他资源和技术。

涵盖了以下主题:

*   深心阿尔法零号
*   IBM 沃森
*   Unity 机器学习代理工具包
*   逆向强化学习
*   深度确定性政策梯度
*   马后炮经验回放

在本章的最后，读者将会探索一些基于强化学习的技术应用的实际例子，并且会理解在使用强化学习算法中的未来挑战。



# 强化学习在现实生活中的应用

正如我们已经说过的，强化学习是一种编程哲学，旨在创建能够学习和适应环境变化的算法。这种编程技术基于这样的假设，即能够根据算法的选择从外部接收刺激。因此，一个正确的选择将导致奖励，而一个错误的选择将导致系统的惩罚。该系统的目标是获得尽可能高的奖金，从而获得尽可能好的结果。

有了这样一个模型，计算机学习，例如，在游戏中击败对手(或驾驶车辆)，集中精力执行给定的任务，旨在实现最大的奖励值；换句话说，系统通过游戏(或驾驶)和所犯的错误来学习，精确地根据先前达到的结果来提高性能。

强化学习的应用已经数不胜数，其中一些在我们实际上没有意识到的情况下就进入了我们的日常生活。如前所述，基于强化学习的系统是自动驾驶汽车开发的基础，通过机器学习，学习识别周围环境(通过传感器、GPS 等收集的数据)，并根据他们面临/克服的具体情况调整他们的行为。

处理网络用户分析的程序也利用了机器学习，从在网站、平台或移动应用程序上冲浪的用户的行为和偏好中学习；一个例子是我们通常习惯于在电子商务平台上看到和使用的内容，如亚马逊，或娱乐和访问内容，如网飞或 Spotify。

在下面的章节中，我们将看到一些基于强化学习的技术应用的实际例子。



# 深心阿尔法零号

AlphaZero 是一种基于谷歌 DeepMind 开发的机器学习技术的人工智能算法。它是 AlphaGo Zero 的推广，AlphaGo Zero 是专门为围棋游戏开发的前身，而 alpha Go 是第一个能够在围棋游戏中实现超人表现的软件。像 AlphaGo Zero 一样，它使用**蒙特卡罗树搜索** ( **MCTS** )，由经过强化训练的深度卷积神经网络指导。

2017 年 12 月 5 日^月日，DeepMind 团队在 arXiv 上发表了一份预印本，其中 AlphaZero 获得的一些结果出现在几个经典的桌面游戏中，仅用几个小时的训练就在国际象棋、shogi 和 Go 的游戏中达到了超人的水平，克服了各自学科的世界冠军程序:国际象棋的 Stockfish、shogi 的 Elmo 和 Go 的 AlphaGo Zero。

Stockfish 是一个多平台开源 UCI 象棋引擎，最初由 Tord Romstad 和 Marco Costalba 开发，作为格劳龙的一个分支，另一个开源引擎此前由 Romstad 自己开发，目前由一个开源程序员社区维护。

AlphaGo Zero 是 DeepMind 的围棋软件 AlphaGo 的一个版本。AlphaGo 的团队于 2017 年 10 月 19 日在期刊 *Nature* 上发表了一篇文章，介绍了 AlphaGo Zero，这是一个没有使用人类游戏数据创建的版本，比以前的任何版本都更强[1]。通过与自己对弈，AlphaGo Zero 以 100 局 0 胜的成绩在三天内超越了 AlphaGo Lee 的实力，21 天达到 AlphaGo 大师的水平，40 天超过了所有旧版。

特别是，AlphaZero 的一个实例赢得了与 Stockfish 的 100 场比赛，白棋赢了 25 场，黑棋赢了 3 场，其余比赛都是平局。作者估计，AlphaZero 在前四个小时的训练(大约 30 万个迷你批次)后，就超过了 Stockfish 的游戏实力。

AlphaZero 与其前身的主要区别如下:

*   AlphaGo Zero 使用贝叶斯优化技术来调整每场比赛的搜索参数，而 AlphaZero 在所有比赛中使用恒定的参数。
*   在训练中，AlphaGo Zero 的游戏是由之前迭代中获得的最佳实例(最佳玩家)生成的，每次迭代后，新实例的性能都会与最佳玩家进行比较，如果它能够以至少 55%的差距击败最佳玩家，就会替换它。相反，AlphaZero 使用一个不断更新的单一神经网络，而无需等待每次迭代的结束。

*   围棋(不像象棋和松木)对于某些反射和旋转是对称的；AlphaGo Zero 在训练(通过每个位置的八个可能的旋转和反射来执行数据扩充)和评估阶段(在将输入提交给神经网络之前对输入应用随机对称，以删除由于旋转或反射导致的平均偏差)中都使用了这种对称性。AlphaZero 无法利用这些技术解决方案。
*   国际象棋比赛(不同于围棋)可能以平局告终；AlphaGo 估计并优化获胜概率，而 AlphaZero 估计并优化比赛的预期结果(以数字形式表示)。

DeepMind 解释说，AlphaGo 已经被展示了大约 10 万场比赛，以开始改进，而 AlphaGo Zero 只被教授了游戏规则。计算机已经开始与自己对弈，在数百万场游戏中犯错并自我修正，变得越来越能够相当轻松地击败它的前任。

AlphaGo Zero 程序员的经理大卫·西尔弗(David Silver)解释说，通过不使用人类数据，他们可以消除人类知识的限制，从而使这台超级计算机能够搜索和找到它的策略，即使是对最激进的围棋选手来说也是不公开的。这一切，使用了比以前更少的处理器:4 个而不是 AlphaGo 的 48 个**张量处理器单元** ( **TPU** )。谷歌为开发人工智能而制造的机器已经在使用 TPU，通过其翻译软件来理解和翻译不同的语言，或者在我们的谷歌照片中识别照片中的人脸。下图显示了一个谷歌 TPU:

![](Images/3e84394d-39d0-4de5-9baf-4fa5ad5f1f10.png)

目前，谷歌已经宣布，它能够将多达 64 个这样的处理器连接在一起，达到 11.5 petaFLOPS 的计算能力，相当于每秒 10 亿次运算；为了了解计算能力，一台普通的计算机，比如写这篇文章的那台，大约是 100 亿。

与其前辈类似，AlphaGo Zero 依靠深度神经网络来学习游戏背后的抽象概念。然后，它通过强化学习来击败新的对手，但忘记了如何击败以前版本的自己。与过去相比有了根本的不同。以前的软件使用两个独立的神经网络，一个预测最可能的行动，另一个决定哪一个能保证胜利。在第二种情况下，他们依赖于一个名为 rollout 的过程，即一系列游戏模拟来测试可能的后果。AlphaGo Zero 已经将这些功能合并到一个单一的神经网络中，它直接要求该网络预测将获胜的棋步。这就像是让它对单个专家做一个预测，而不是依靠普通人的 100 个假设。这种新软件可以在机器人、新材料制造和化学领域得到应用。事实上，它可以用于复杂的研究，例如蛋白质折叠的所有可能构型。



# IBM 沃森

Watson 是一个人工智能系统，能够回答用自然语言表达的问题，由 David Ferrucci 领导的研究团队在 IBM 的 DeepQA 项目中开发。这个名字是为了纪念 IBM 的第一任总裁托马斯·J·沃森。

2013 年 2 月，IBM 与 WellPoint 健康保险公司合作，在纪念斯隆-凯特琳癌症中心发布了首个用于管理肺癌治疗决策的沃森软件系统商业应用。

IBM 将其描述为自然语言处理、信息检索、知识表示、自动推理和机器学习技术在开放领域问答领域的高级应用，建立在 DeepQA、IBM 的技术假设公式、大量收集的反测试、分析和评分(实现目标的能力)的基础上。在下图中，显示了一个 IBM Watson 硬件:

![](Images/4497eff3-b4bb-4ac3-9515-761dfa62edfc.png)

Watson 使用 IBM DeepQA 软件和 Apache UIMA 框架。该系统已经用多种语言编程，例如 Java、C ++和 Prolog，并且使用 Apache Hadoop 作为分布式计算的框架在 SUSE Linux Enterprise Server 11 系统上运行。

该系统经过优化，能够管理生成假设、识别最大证据和分析数据所需的工作负载，集成了高度并行化的 POWER7 处理器。Watson 由 90 台 IBM Power 750 服务器组成的网格组成，每台服务器都配备了 3.5 GHz 八核 Power 7 处理器，每核 4 个线程。该系统总共有 2，880 个 POWER7 处理器线程和 16tb RAM。

根据约翰·雷尼的说法，沃森每秒可以分析 500 GB，相当于 100 万本书。IBM 设计师兼顾问托尼·皮尔森估计沃森的硬件成本为 300 万美元。然而，其 80 万亿次的性能不足以使其进入超级计算机 500 强的名单。

该项目的故事发生在 1997 年深蓝战胜冠军加里·卡斯帕罗夫之后。

深蓝是一台由 IBM 生产的计算机，专门用于下棋。

2004 年，IBM 的研究经理查尔斯·李克尔(Charles Lickel)在与同事共进晚餐后，注意到他们用餐的餐厅里一片死寂。他立刻发现了原因:肯·詹宁斯正在进行他的第 74 场^的危险游戏！事实上，整个餐厅的观众都被困在电视机前，在晚餐进行到一半的时候，都在关注这个节目。

危险！是一种美国电视问答比赛，由不同参赛者之间的一般比赛组成，自 1964 年以来在 NBC 播出。

这个电视智力竞赛的想法对 IBM 来说是一个可能的挑战，李克尔对此感到兴奋，他谈到了这个想法，2005 年，IBM 的首席执行官保罗·霍恩对李克尔的想法下了赌注，推出了在他的部门参加 Jeopardy 的想法！使用 IBM 系统。一开始，霍恩发现很难找到想要着手一项比没有文字的象棋游戏复杂得多的挑战的工作人员，然后大卫·费鲁奇接受了这一提议。在美国政府组织的一次挑战中，沃森的前任，一个名为 Piquant 的系统，只能正确回答 35%的问题，并且在每个问题上花费了几分钟。

要赢得挑战，沃森必须在不超过几秒钟的时间内回答，同时，游戏节目提出的问题将不再被认为是不可能解决的。在 2006 年由大卫·费鲁奇进行的最初测试中，沃森收到了前几期智力竞赛节目中的 500 个问题。虽然最好的人类竞争对手在一半的时间内给出了答案，并且正确回答了 95%的问题，但第一版沃森只能给出 15%的正确答案。2007 年，成立了一个由 15 人组成的小组，目标是在 3 至 5 年内解决这个问题。2008 年，研究人员称沃森可以与《危险边缘》相媲美！的人类冠军。

2010 年 2 月，沃森已经可以击败 Jeopardy 了！的竞争对手。虽然该系统主要是 IBM 的工作，但世界各地的几所大学已经进行了合作:伦斯勒理工学院、卡内基梅隆大学、马萨诸塞大学阿姆赫斯特分校、南加州大学信息科学学院、德克萨斯大学奥斯汀分校、麻省理工学院和特伦托大学纽约医学院。

沃森，2011 年 2 月，已经参加了《危险边缘》三集！，在该节目历史上唯一一次人机对抗中击败了它的人类对手。在 2 月 14 日至 16 日播出的三集节目中，沃森击败了布拉德·鲁特，后者被称为通过参加《危险边缘》(Jeopardy)赢得最多金钱的冠军！和该节目的记录保持者肯·詹宁斯。下图显示了三个竞争者之间的挑战:

![](Images/07be9d7d-aa32-46ea-9360-a2c2a589fa8e.png)

在反应速度方面，沃森一直远远超过其人类对手，但它在某些类别的问题上有些困难，特别是那些包含几个词的简短线索。对于每条线索，电视屏幕上都显示出三种最有可能的沃森反应。沃森可以访问 200，000，000 页的结构化和非结构化内容，包括维基百科的全文，占用 4tb 的磁盘空间。游戏过程中，沃森没有联网。沃森获得了 100 万美元的一等奖。

根据 IBM 的说法，目标是让计算机能够通过广泛的应用程序和过程，用自然语言与人进行交互，理解人类提出的问题，并以人类可以理解的语言提供答案。IBM 的总顾问罗伯特·c·韦伯(Robert C. Weber)提议在法律研究中使用沃森。该公司打算在其他拥有大量信息的领域使用沃森，如电信、金融服务和政府选择。

Watson 基于 2010 年 2 月上市的 IBM Power 750 服务器。IBM 也在考虑在市场上推出 1，000，000，000 美元的 DeepQA 应用程序，反映了购买满足运行 Watson 最低要求的服务器所需的 1，000，000 美元的价格。IBM 预计，由于技术进步，价格将在十年内大幅下降。

2013 年，宣布有三家公司正在与 IBM 合作，创建集成沃森技术的程序。Fluid 正在开发一个名为 North Face 的供应商应用程序，旨在为网络上的客户提供建议。Welltok 正在开发一个应用程序，为人们提供如何改善健康的建议。MD Buyline 正在开发一个应用程序，为医疗机构提供有关购买设备的决策建议。但是，这些只是众多 IBM Watson 应用程序中的一部分。

沃森的思想形成有四个过程:

*   解析应用程序，使其具有良好的格式和可解释性
*   文本与现有知识相联系，从确定的上下文中消除其含义的歧义
*   从文本中提取关系以加深对文本的理解
*   从文本中获取信息，从而增加知识基础，在此基础上形成后续的理解过程；这台机器能够学习

许多公司对 Watson 感兴趣，是因为它能够有效地管理在挖掘之前的分析过程中收集的大量数据，以及更好地管理公司的大量通信和内部数据。

数据的复杂性和异质性是最大的障碍，其中许多数据是非结构化和非整体性的，尽管传统的分析方法有所改进，但从分类数据中构建有用信息的工作仍然只是部分实现。如果我们考虑到数据是由人、信息系统、各种设备为超过 1 万亿个相互连接的对象无缝生成的，并且每天产生相当于 25 亿千兆字节的数量，那么很明显，体积效应以相关的方式导致了复杂性的增加。

因此，有必要将数据视为一种新的自然资源，从这种资源中可以获得竞争力。前提是你使用工具和流程拥有实时分析技能和提取知识的能力。

让我们详细看看已经实现的一些 IBM Watson 应用程序。一个例子是 Watson for Cyber Security，这是业内首个增强智能技术，旨在帮助分析师检查数千份自然语言搜索报告，此前即使是最现代的安全工具也无法访问这些报告。IBM 最近的一项研究发现，目前只有 7%的安全专业人员使用认知工具，但这种使用预计将在未来 2-3 年内增加两倍。在过去的一年里，沃森接受了网络安全语言的培训，拥有超过 100 万份关于该主题的文件，现在能够帮助分析师检查数千份自然语言研究报告。在过去的 5 年中，IBM 为几十个行业的客户创建了 300 多个安全运营中心，包括消费品分销、零售、银行和教育。

在健康领域，IBM 于 2015 年 4 月推出的 Watson Health 雇用了 2000 名专家，并分析了大量的医疗报告和案例研究。IBM Watson Health 的美国小组解释说，在几秒钟内，Watson 通过将正在检查的病例与档案中的所有病例进行比较，帮助制定诊断并找到最佳护理，这将花费医生 10，000 周的时间。2017 年 3 月，IBM 宣布在米兰的 Human Technopole Italy 2040 研究中心的展区启动沃森健康的第一个欧洲卓越中心，意大利政府旨在与该中心建立基因组学、大数据、人口老龄化和营养领域的国际中心。IBM 计划在未来几年投资高达 1.5 亿美元。

沃森的能力包括能够阅读所有脸书帖子和所有发布的推文的流量，被公司用来预测趋势。该公司只能访问已发布的内容，但不知道作者是谁。例如，这种潜力被应用于 Sanremo 意大利节，当时 IBM 在为期 5 天的节日期间提供了一些 Watson Analytic 的技能，这是一种认知工具，包括允许模拟和预测结果的人类语言，分析了网络上超过 290，000 条评论。



# Unity 机器学习代理工具包

2017 年 9 月，Unity 机器学习团队推出了机器学习代理工具包。作为世界上最受欢迎的创作引擎，Unity 正处于机器学习和游戏的十字路口。在创建这个工具包的过程中，Unity 的研究人员使用了强化学习算法。通过这种方式，游戏社区将能够使用最新的机器学习技术。

由于 Unity 的目的是使游戏的开发民主化，这个新的特性已经以一种简单和直接的方式提供给所有想沉浸其中的人。为此，机器学习团队创建了一些小型演示来展示它是如何自下而上地工作的。例如，你可以分析机器学习 Roguelike 演示，这是一款 2D 动作游戏，玩家必须与凶猛、智能的 slimes 进行战斗。这些被机器学习控制的敌人会毫不留情的攻击我们的英雄，当他们感觉到生命有危险的时候，他们会逃跑。在下面的屏幕截图中，显示了一个机器学习的 Roguelike 屏幕:

![](Images/01083e60-fee2-4771-88ba-32cdeaf32138.png)

Unity 的**机器学习代理** ( **ML-Agents** )工具包作为测试版发布。ML-Agents SDK 允许研究人员和开发人员将使用 Unity Editor 创建的游戏和模拟转换为智能代理可以通过简单易用的 Python API 使用深度强化学习、进化策略或其他机器学习方法进行训练的环境。Unity ML-Agents toolkit 的测试版已经作为开源软件发布，提供了一组示例项目和基线算法来帮助您入门。

工具包的基本要素如下:

*   **代理**:每个代理可以有一组唯一的状态和观察值，在环境中采取唯一的行动，并为环境中的事件接收唯一的奖励。一个智能体的行为是由它所连接的大脑决定的。
*   **大脑**:每个大脑都定义了一个特定的状态和动作空间，并负责决定其链接的每个代理将采取哪些动作。
*   **学院**:场景中的学院对象也包含了环境中所有作为孩子的大脑。每个环境都包含一个定义环境范围的学院。

各种各样的训练场景都是可能的，这取决于代理、大脑和奖励是如何连接的。这里有几个例子:

*   **单代理**:单代理连接单个大脑。训练特工的传统方式。
*   **同时单个智能体**:多个独立的智能体，具有独立的奖励功能，与单个大脑相连。
*   **对抗性的自我游戏**:两个相互作用的代理人，他们的报酬函数相反，并且与同一个大脑相连。
*   合作的多智能体:多个相互作用的智能体，共享奖励功能，链接到单个大脑或多个不同的大脑。
*   **竞争性多智能体**:多个相互作用的智能体，具有与单个大脑或多个不同大脑相关联的反向奖励功能。
*   **生态系统**:多个相互作用的智能体，具有独立的奖励功能，与单个大脑或多个不同的大脑相连。

Unity 的 ML-Agents 工具包仅处于开发的初期，Unity 团队计划快速迭代，并为那些对 Unity 作为机器学习研究平台感兴趣的人和那些专注于机器学习在游戏开发中的潜力的人提供额外的功能。



# FANUC 工业机器人

FANUC 是最大的工业机器人制造商之一。近年来，这家日本公司集中精力在机器人领域使用人工智能。特别是，他们正在开发使用强化学习来理解如何执行复杂任务的机器人。

只需给机器人一个任务，比如如何将物体从一个容器移动到另一个容器，花一晚上的时间了解如何最大限度地执行这个任务。到了早上，这台机器就能像专家给它编了程序一样工作了。FANUC 在 2017 年 12 月的东京国际机器人展上展示了一款通过强化学习训练的机器人。下图是 FANUC 机器人:

![](Images/137c9fad-43db-4a69-ad03-fdf856f0606f.png)

工业机器人配备了极高的精度和速度，但它们通常必须经过非常仔细的编程，才能执行看似简单的任务，如抓取物体。实际上，训练一个机器人做到这一点需要很长时间，最终这些机器人只能在严格控制的环境中工作。

FANUC 的机器人使用了一种基于深度强化学习的技术。在训练中，机器人试图在采集过程的视频镜头时收集物体。每当它成功或失败时，请记住该对象是如何出现的:这些知识随后被用于完善深度学习模型或大型神经网络，这些模型或网络控制着它的行动。近年来，深度学习已被证明是模式识别中一种强有力的方法。

在实践中，大约八个小时后，它达到了 90%或更高的准确率，这几乎就像一个专家给它编程一样。它在晚上工作，第二天早上就可以使用了。

这种学习方法的一个巨大的潜在好处是，如果几个机器人并行工作并分享他们所学到的东西，它可以被加速。所以，八个机器人一起工作一个小时可以做八小时机器做的事情。这种形式的分布式学习，称为云机器人，正在成为研究和行业的一个大趋势。

识别物体的能力对于训练机器人来说至关重要。这就是为什么 FANUC 开始与英伟达合作，将该公司的图形处理单元添加到其机器中。这项合作的目标是在 **FANUC 智能边缘链接和驱动** ( **场**)系统上实施人工智能，以提高机器人生产率，并为全球自动化工厂带来新的能力。

现场系统是一个用先进的人工智能提高工厂生产和效率的平台。通过结合人工智能和边缘计算技术，现场系统处理从各种机器收集的边缘密集型传感器数据，使机器智能和灵活地协作，以实现先进的制造能力。

英伟达的图形处理单元和深度学习技术将用于帮助 FANUC 机器人识别、处理和响应周围环境。这对于学习强化尤为重要，即机器使用人工智能通过实践来采用新技能的方式。

机器人可以捕捉自己的视频，看看它是如何工作的，然后随着时间的推移，分析和开发信息。工业机器人又大又危险，它们真的很擅长精确地一遍又一遍地执行一项任务。正如我们所说，这个想法是当 FANUC 的机器人一起学习时，他们学得更快。并且，NVIDIA 的技术特别适合并行处理，这意味着它可以同时处理成千上万的计算任务。下图显示了一个 NVIDIA GPU:

![](Images/9202f2e2-ea0c-4710-b96f-6f0df1c13ecb.png)

合作的主要特点如下:

*   用英伟达 GPU 进行深度学习机器人训练
*   FANUC 光纤陀螺中的 GPU 加速人工智能推理用于驱动机器人和机器
*   机器人本地推理的嵌入式系统

很明显，FANUC 不会是唯一一家投资此类项目的公司。强化学习在机器人管理中的应用代表了一个具有广阔发展前景的领域。



# 使用强化学习的自动交易系统

**外汇** ( **外汇**)是外汇代替货币的国际市场。在这个市场中，有不同的实体，包括国际银行、中央银行、商业公司、中介机构和小投资者。

外汇市场是世界上流动性最强的市场；外汇市场的每日合约量超过 1.9 万亿美元，由大量参与者组成。与股票市场不同，没有清算所或股票交易所进行实物交易，但后者是由大量地理上分散的市场参与者进行的。

外汇市场最初不对小投资者开放，因为要参与其中，就必须拥有大量资本。然而今天，由于杠杆作用，即使是小投资者也可以在这个市场上互动。

由于近几十年来信息技术的飞速发展，金融市场中诞生了自动交易的可能性。由于特殊软件的编程，这种新颖性使得以完全自动化的方式在市场上操作成为可能。

这种演变允许交易策略的实施，允许它在市场上操作，而不必必须出现在计算机前。

交易策略是指交易者用来决定在市场上采取什么行动的决策过程。它包括许多考虑市场和资本管理不同方面的因素。

使用自动化系统的优势有很多:

*   决策过程是自主的，因此消除了任何解释性成分
*   风险是有限的，因为它总是被系统地计算
*   没有留下任何判断的余地，因为总是遵循系统提供的指示

*   你有机会一天 24 小时同时分析和操作大量的金融工具
*   使用自动交易系统加快了策略测试的过程

此外，交易者可以选择是使用交易系统作为决策支持，还是作为对程序产生的任何信号进行即时操作的系统。

为了与外汇市场互动，可以使用 MetaTrader 软件平台，该平台允许进行直接和简单的交易。同一个平台还允许我们使用一种叫做**专家顾问** ( **EA** )的软件，用一种特殊的编程语言(MQL)编写，自动进行交易。下面的屏幕截图显示了该软件:

![](Images/f1d9a58d-dba9-4f05-b65b-5e4cba0cae42.png)

交易软件平台是一个允许你进行在线交易操作的程序。多年来，为了更好地适应投资者的需求，已经开发了许多产品，每个产品都有不同的特点。然而，其中最受欢迎的是 MetaTrader。这是因为这个平台除了免费和易于使用之外，也是经纪人提供自己的交易服务时使用最多的程序。

MetaTrader 提供了许多功能，都可以通过菜单栏访问，尤其是以下功能:

*   为每个可用的金融工具打开一个新图表
*   插入交易者使用的各种技术指标，作为能够预测价格变动的进一步信息
*   查看策略测试器
*   为 EA 的创建打开编辑器

编辑器包括一系列允许程序员创建 EA 的函数。这是一个通用的软件开发平台，它包括用于编写程序源代码的文本编辑器和用于编译这些代码的编译器。

用于开发我们的 EA 的编程语言叫做 MQL5。它是一种类似于 C 语言的编程语言，已经实现了一系列特殊功能，允许我们管理与市场的互动。

作为一个规则，EA 在市场的每一次变化时执行他们的算法。为了实现这种类型的行为，使用了一个名为 Ontick 的特殊函数，它在市场中的每个分笔成交点被调用。

MetaTrader 提供的另一个特性是策略测试器。这个工具为我们提供了测试其 EA 的可能性，在过去的数据上模拟其执行。在这个部分中，可以选择要测试的 EA、运行用户的金融工具、开始的初始预算、周期以及执行测试的精度(基于价格变化的频率)。选择所需设置后，可以确定相关电路板中 AE 的输入参数并开始模拟。

有了这个软件，就有可能创建一个 EA 来完全取代交易者的决策过程。交易者必须做出的决定包括几个方面:

*   在什么价位进入和退出市场
*   在单笔交易中，承担何种风险(就资本而言)，资本遭受的损失不超过某个百分比值
*   计算与先前确定的风险相适应的条目大小
*   使用平台提供的适当命令打开所需位置
*   管理对市场开放的贸易

因此，EA 确定要进入的价格水平，根据交易者评估的风险自动计算要购买的手数，根据它建议的策略执行买入和卖出操作，最后管理公开交易。

在 EA 的创建过程中，我们可以实现所有的技术来寻找在自动交易策略中使用的参数的最优值，以最大化性能。

为了实现这一点，有可能确定最影响策略性能的参数，将自动交易程序与强化学习算法相结合。



# 强化学习的后续步骤

如今，看到代替人类执行任务的机器已经成为一件很正常的事情。从生产过程的自动化，到装配线，到快速精确的计算，再到指令的执行，误差非常小，如果不是不存在的话。但是，当问题变得更加复杂时，我们会求助于人工智能技术，其中包括创建可以帮助软件从经验中学习的算法——这被称为机器学习。

机器自主学习，不是从预定义的规则列表开始，而是从模型和指令开始，通过模型和指令学习正确的规则来解决问题。这些技术已经得到广泛应用，例如，打击垃圾邮件和信用卡欺诈，进行经济和金融预测，进行语音识别和手写，进行自动图像分类，了解我们的口味并给出建议，以及改进我们的新闻源和搜索结果。

在这一点上，有理由问，“这项技术在未来会给我们带来什么？”谷歌已经宣布，它将从移动优先的方式转变为人工智能优先的方式。这种转变将通过它迄今为止向公众提供的产品来实现。谷歌地图等服务将通过机器学习自动识别来自外部环境的信号。在这种新方法中，使用人工智能和机器学习作为他们所有产品和服务的基础的意图很明显。让我们来看看强化学习算法的未来挑战是什么。



# 逆向强化学习

在**反向强化学习** ( **IRL** )中，奖励函数来源于观察到的行为。正如我们所了解的，在强化学习中，我们使用奖励来学习特定系统的行为。在 IRL 中，这个功能是相反的；事实上，代理观察系统的行为来理解它试图实现什么目标。

在 IRL 中，我们从以下问题着手:

*   衡量一段时间内代理的行为
*   对该药剂的感觉输入的测量
*   物理环境的模型

基于这些数据，我们确定代理正在优化的奖励函数。OpenAI 和 DeepMind 在 2017 年发布了一个应用于 Atari games 等简单领域的逆向强化学习的例子。DeepMind 的研究人员已经证明，这些技术可能在经济上适用于现代系统。



# 通过示范学习

通过示范学习可以看作是监督学习的一个特例；教师或监督者提供输入和期望的输出，这是监督学习范式的基本概念。然而，与监督学习的经典问题的不同之处在于，通过演示，解决问题的策略也被提供给机器人。在实践中，机器人模仿主管的行为，这就是为什么通过演示学习也被称为通过模仿学习。

一旦一般问题被分析和形式化，特别是在仔细选择了论证的主要方面之后，有必要从整体上考虑问题，选择哪种模仿过程对解决问题更有效。

这种学习模式对于训练机器人特别有用。假设机器人控制系统正在工作，一个人类操作员正在使用它。如果操作员抓住机器人的手臂，他们用它移动，然后机器人会在稍后的时间做同样的动作。在繁殖阶段，机器人模仿这种行为。



# 深度确定性政策梯度

**深度确定性策略梯度** ( **DDPG** )是一种采用随机行为策略进行探索的策略梯度算法。这个算法估计一个确定性的目标策略，这个策略要容易学得多。策略梯度算法使用以下策略迭代:

*   评估政策
*   然后，遵循策略梯度以最大化性能

DDPG 不在策略之内，而是使用确定性目标策略，因此允许使用确定性策略梯度定理。DDPG 也是一个关键的算法演员；它主要使用两个神经网络:一个用于演员，一个用于评论家。这些网络计算当前状态的行动预测，每次产生一个**时差** ( **TD** )误差信号。作为行动者网络的输入，采用当前状态，而作为输出，选择单个真实值来表示由连续动作空间选择的动作。批评家的输出仅仅是当前状态和行动者给出的动作的估计 Q 值。确定性策略梯度定理为参与者的网络权重提供了更新规则。临界网络由从误差信号 TD 获得的梯度更新。



# 基于人类偏好的强化学习

通常在日常问题中，不可能定义一个明确的奖励函数。事实上，许多任务涉及复杂的、定义不清的或难以指定的目标。例如，假设你想使用强化学习来训练一个机器人打扫房子。定义一个适当的奖励函数并不容易，这将取决于来自机器人传感器的数据。如果我们能够成功地向我们的代理人传达我们的真实目标，这将是解决这些问题的重要一步。

如果我们有期望任务的演示，我们可以使用反向强化学习提取奖励函数。此外，我们可以利用学习的模仿来克隆已证实的行为。然而，这些方法并不直接适用于难以向人类演示的行为(例如控制一个具有许多自由度但具有类似人类形态的机器人)。这个问题的解决方案可能是允许人类提供关于我们系统的当前行为的反馈，并使用这个反馈来定义活动。原则上，这是强化学习范式的一部分，尽管直接使用人类反馈作为奖励功能对于需要数百或数千小时经验的 RL 系统来说是禁止的。

在根据人类偏好的深度强化学习中，代理从人类反馈中学习奖励函数，然后优化该奖励函数。因此，它代表了一个解决顺序决策问题，没有一个明确规定的奖励函数。

该算法具有以下特点:

*   允许您解决我们只能识别所需行为，但不一定要演示的任务
*   允许非专业用户教授代理
*   扩展到大问题
*   有用户反馈就便宜

该算法使奖励函数适合人类的偏好，同时训练策略来优化当前预测的奖励函数。



# 马后炮经验回放

人类拥有的能力之一是从我们的错误中学习，并在下次适应以避免犯同样的错误。这是强化学习算法的基础。实现这些算法的最大困难是在处理分散的奖品时遇到的。考虑以下场景:一个学习代理必须控制一个机器人手臂打开一个盒子，并在里面放置一个对象。虽然定义这项任务的奖励简单明了，但潜在的学习问题却很困难。代理必须发现一长串正确的动作，以找到产生稀疏回报的环境配置——放在盒子里的物体。发现这种微弱的回报信号是一个困难的探索问题，通过随机探索获得成功是极不可能的。

一种被称为事后经验回放的新技术有望帮助解决这个问题。这种技术允许基于差和二进制奖励的有效学习，从而避免了对复杂奖励技术的需要。它可以与任意强化学习偏离策略算法相结合。

这种方法特别适用于用机械臂操纵物体的情况，特别是在推、滑和取放的任务中。

在任何情况下，仅使用二元奖励来指示活动是否完成。消融研究表明，后见之明经验重放是一个关键因素，使训练在这些困难的环境中成为可能。其他研究表明，在物理模拟上训练的策略可以在物理机器人上实现，并成功完成活动。



# 摘要

在这一章中，我们探索了一些基于强化学习的技术应用的实例。

首先，我们讨论了 DeepMind AlphaZero 项目。AlphaZero 是一种基于谷歌 DeepMind 开发的机器学习技术的人工智能算法。它是 AlphaGo Zero 的概括，AlphaGo Zero 是专门为围棋开发的前身，也是 AlphaGo 的演变，alpha Go 是第一个能够在围棋比赛中实现超人表现的软件。

然后，我们探索了 IBM Watson 项目。这是一个人工智能系统，能够回答用自然语言表达的问题，由 David Ferrucci 指导的研究团队在 IBM 的 DeepQA 项目中开发。我们研究了 Unity 的 ML-Agents 工具包、FANUC 工业机器人和一个自动交易系统。

最后，我们讨论了使用强化学习算法的一些未来挑战。我们学习了逆向强化学习、示范学习、人类偏好强化学习、DDPG 和后知之明经验回放。