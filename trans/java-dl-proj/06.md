

# 六、使用 YOLO、JavaCV 和 DL4J 的实时对象检测

```
Deep Convolutional Neural Networks (DCNN) have been used in computer vision—for example, image classification, image feature extraction, object detection, and semantic segmentation. Despite such successes of state-of-the-art approaches for object detection from still images, detecting objects in a video is not an easy job.
```

考虑到这个缺点，在本章中，我们将开发一个端到端的项目，当视频剪辑连续播放时，该项目将从视频帧中检测对象。为此，我们将在**deep learning 4j**(**DL4J**)的基础上，利用一个训练有素的 YOLO 迁移学习模型和 JavaCV 技术。简而言之，本端到端项目将涵盖以下主题:

*   目标检测
*   从视频中检测目标的挑战
*   将 YOLO 与 DL4J 一起使用
*   常见问题(FAQ)



# 图像和视频中的目标检测

深度学习已被广泛应用于各种计算机视觉任务，如图像分类、对象检测、语义分割和人体姿态估计。当我们打算解决图像中的目标检测问题时，整个过程是从目标分类开始的。然后，我们执行目标定位，最后，我们执行目标检测。

这个项目很大程度上受到了克莱维斯·拉莫([http://ramok.tech/](http://ramok.tech/))的 Java 自动驾驶-汽车检测文章的启发。此外，在作者许可的情况下，使用了一些理论概念(但为了满足这种需要，对其进行了显著扩展)。



# 对象分类、定位和检测

在对象分类问题中，从给定的图像(或视频片段)中，我们感兴趣的是知道它是否包含感兴趣的区域(ROI)或对象。更正式的说法是，“图像包含一辆汽车”，而不是“图像不包含任何汽车”为了解决这个问题，在过去的几年里，ImageNet 和 PASCAL VOC(详见[http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/))一直在使用，并且基于深度 CNN 架构。

此外，当然，最新的技术进步(即软件和硬件)有助于将性能提升到一个新的水平。尽管静态图像的最新方法如此成功，但检测视频中的对象并不容易。然而，视频中的目标检测带来了新的问题、可能性和挑战，即如何有效和鲁棒地解决视频中的目标检测问题。

回答这个问题不是一件容易的工作。首先，让我们试着一步一步地解决问题。首先让我们试着回答一个静态图像。当我们想用 CNN 来预测一幅图像是否包含一个特定的物体时，我们需要定位物体在图像中的位置。为此，我们需要指定对象在图像中的位置以及分类任务。这通常是通过用通常称为边界框的矩形框来标记对象来完成的。

现在，包围盒的想法是，在视频的每一帧，算法需要注释包围盒和每一类对象的置信度得分。为了更清楚地理解这个概念，我们来看看什么是边界框。边框通常由中心(*b^xT3、*b^yT7)、矩形高度( *b ^h* )和矩形宽度( *b ^w* )表示，如下图所示:**

![](assets/518cdb57-d806-492e-9d1d-57fe445afe3d.png)

边界框表示

既然我们知道了如何表示这样一个边界框，我们也可以理解我们需要在训练数据中为图像中的每个对象定义这些东西。只有这样，网络才能输出以下内容:

*   图像类别号的概率(例如，20%的概率是汽车，60%的概率是公共汽车，10%的概率是卡车，10%的概率是火车)
*   此外，定义对象边界框的四个变量

仅仅知道这些信息是不够的。有趣的是，有了这个关于边界框点的最小上下文信息(即中心、宽度和高度)，我们的模型仍然能够预测并为我们提供更详细的内容视图。换句话说，使用这种方法，我们可以解决对象定位问题，但它仍然只适用于单个对象。

因此，我们甚至可以更进一步，不仅定位图像中的单个对象，还定位图像中的多个或所有对象，这将有助于我们解决对象检测问题。尽管原始图像的结构保持不变，但我们需要处理单个图像中的多个边界框。

为了解决这个问题，一种最新的技术是将图像分割成更小的矩形。我们还有另外五个我们已经看到的变量( *P ^c ，b ^x ，b ^y* ， *b ^h* ， *b ^w* )当然，还有每个边界框的正常预测概率。

这个想法听起来很容易，但实际上它如何运作呢？如果我们只需要处理一个静态的图像分类问题，事情就变得简单了。使用一种天真的方法是从成千上万张汽车图像的集合中裁剪每张汽车图像，然后我们训练一个卷积神经网络(例如，VGG-19)来训练所有这些图像的模型(尽管每张图像可能有不同的大小)。

![](assets/182e01a5-2ec9-4892-8184-ef2d89b6918c.png)

典型的公路交通

现在，为了处理这个场景，我们可以用一个滑动矩形窗口扫描图像，每次都让我们的模型预测里面是否有汽车。正如我们看到的，通过使用不同大小的矩形，我们可以为汽车和它们的位置找出完全不同的形状。

虽然，这在只检测汽车时工作得很好，但是想象一下一个更实际的问题，比如开发一个自动驾驶应用程序。在一条典型的高速公路上，在一个城市，甚至在一个郊区，会有许多汽车，公共汽车，卡车，摩托车，自行车和其他车辆。此外，还会有其他物体，如行人、交通标志、桥梁、分隔物和灯柱。这些将使情况变得更加复杂。然而，真实图像的大小与裁剪后的图像相比会有很大的不同(也就是说，要大得多)。此外，在前面，许多汽车可能正在接近，所以手动调整大小，特征提取，然后手工培训将是必要的。

另一个问题是训练算法的缓慢，因此它不能用于实时视频对象检测。这个应用程序的构建是为了让你们可以学到一些有用的东西，以便同样的知识可以扩展并应用到新兴的应用程序中，例如自动驾驶。

不管怎样，还是回到最初的讨论吧。当移动矩形(向右、向左、向上和向下)时，许多共享像素可能不会被重用，但它们只是被重复重新计算。即使具有非常精确和不同的边界框尺寸，这种方法也不能非常精确地用边界框标记对象。

因此，该模型可能不会非常准确地输出车辆的类别，就好像该盒子可能仅包括对象的一部分一样。这可能会导致自动驾驶汽车容易发生事故，也就是说，可能会与其他车辆或物体相撞。现在，为了摆脱这种限制，最先进的方法之一是使用**卷积滑动窗口** ( **CSW** )解决方案，这在 YOLO 得到了广泛应用(我们将在后面看到)。



# 卷积滑动窗口(CSW)

在前一小节中，我们已经看到，基于滑动窗口的简单方法具有严重的性能缺陷，因为这种类型的方法不能重用许多已经计算的值。

然而，当每个单独的窗口移动时，我们需要对所有像素执行数百万个超参数，以便得到预测。实际上，通过引入卷积可以重用大部分计算(参见[第 5 章](2d4fc6f2-3753-456c-8774-3f41dbe13cfb.xhtml)、*使用迁移学习进行图像分类*，以了解更多关于使用预训练 DCNN 架构进行图像分类的迁移学习)。这可以通过两种渐进的方式实现:

*   通过将全连接的 CNN 层变成卷积
*   使用 CSW

我们已经看到，无论人们使用什么样的 DCNN 架构(例如，DarkNet、VGG-16、AlexNet、ImageNet、ResNet 和 Inception)，无论它们的大小和配置如何，最终，它们都被用来为完全连接的神经网络提供不同数量的层，并根据类别输出几个预测。

此外，这些深层架构通常有很多层，很难很好地解释它们。所以，走小一点的网络听起来是合理的选择。在下图中，网络将大小为 32 x 32 x 3 的彩色图像(即 RGB)作为输入。然后，它使用相同的卷积，将前两个尺寸(即宽度 x 高度)保持为 3 x 3 x 64 不变，以获得 32 x 32 x 64 的输出。这样，第三维度(即 64)与 conv 矩阵保持一致。

![](assets/14c94add-449d-4c09-ab47-49d25838b4d0.png)

然后，放置一个最大的池层以减少宽度和高度，但保持第三维度不变，为 16 x 16 x 64。之后，缩减的层被馈送到具有两个隐藏层的密集层，每个隐藏层具有 256 和 128 个神经元。最后，网络使用 Softmax 层输出五个类别的概率。

现在，让我们看看如何用 conv 层替换**完全连接的** ( **FC** )层，同时保持输入的线性函数为 16 x 16 x 64，如下图所示:

![](assets/a8745d97-47a6-4e71-9f1b-3e913238ab0f.png)

在上图中，我们刚刚用 conv 滤波器替换了 FC 层。实际上，16×16×256 conv 滤波器相当于 16×16×64×256 矩阵。在这种情况下，第三维度 64 始终与第三维度输入 16 x 16 x 64 相同。因此，可以通过省略 64(实际上相当于相应的 FC 层)将其写入 16×16×256 conv 滤波器。下面的数学公式提供了原因的答案:

*输出:1 x 1 x 256 =输入:[16 x 16 x 64]* conv:[16 x 16 x 64 x 256]*

前面的数学表示输出 1×1×256 的每个元素是输入 16×16×64 的相应元素的线性函数。我们之所以费心将 FC 层转换为卷积层，是因为这将使我们在网络生成输出的方式上更加灵活:对于 FC 层，我们将始终具有相同的输出大小，即多个类。

现在，为了查看用卷积滤波器替换 FC 层的效果，我们必须获取一个更大尺寸的输入图像，比如 36 x 36 x 3。现在，如果我们使用简单的滑动窗口技术，其中 stride 是 2，我们有 FC，我们需要移动原始图像大小九次以覆盖所有图像，因此，也要执行模型九次。因此，采用这种方法肯定是没有意义的。相反，让我们现在尝试应用这个新的更大的矩阵作为我们只有卷积层的新模型的输入。

![](assets/7025aace-6796-4bb6-bca7-0181884071df.png)

现在，我们可以看到输出从 1 x 1 x 5 变为 3 x 3 x 5，这是与 FC 的比较。同样，如果我们回想一下基于 CSW 的方法，我们必须移动滑动窗口九次以覆盖所有图像，有趣的是，这等于 conv 的 3×3 输出，尽管如此，每个 3×3 单元代表 1×1×5 类滑动窗口的概率预测结果！因此，不是只有一个 1×1×5 的 9 次滑动窗口移动，现在一次拍摄的输出是 3×3×5。

现在，使用基于 CSW 的方法，我们已经能够解决从图像中检测物体的问题。然而，这种方法不是很精确，但仍然会产生一个可接受的结果，其精确度很低。然而，当涉及到实时视频时，事情变得复杂得多。我们将在本章后面看到 YOLO 是如何解决剩下的限制的。目前，让我们尝试理解从视频剪辑中检测对象的潜在复杂性。



# 视频中的目标检测

在深入挖掘之前，让我们想一个简单的场景。假设我们有一个包含猫或狼在森林里的运动的视频剪辑。现在我们想检测这种动物，但是在每个时间步都在移动。

下图显示了这种情况下的挑战。红框是地面真相注解。该图的上部(即 **a** )示出了静止图像对象检测方法在帧之间具有大的时间波动，甚至在地面真实边界框上也是如此。波动可能是由运动模糊、视频散焦、部分遮挡和不良姿势造成的。视频中的目标检测需要利用来自相邻帧上相同目标的盒子的信息。

另一方面，( **b** )显示跟踪能够关联同一对象的框。然而，由于遮挡、外观变化和姿态变化，被跟踪的框可能漂移到非目标对象。物体检测器应该被结合到跟踪算法中，以在漂移发生时不断地开始新的轨迹。

![](assets/6e9d0c22-358d-4b9c-ad96-984e81c33202.png)

从视频中检测目标的挑战(来源:康凯等人，用卷积神经网络从视频小管中检测目标)

有很多方法可以解决这个问题。然而，它们中的大多数集中于检测一类特定的对象，如行人、汽车或有动作的人。

幸运的是，与静止图像中的对象检测能够辅助包括图像分类、定位和对象检测在内的任务类似，准确检测视频中的对象也可能提高视频分类的性能。通过定位视频中的对象，视频的语义也可以被更清楚地描述，这导致基于视频的任务的更鲁棒的性能。

换句话说，现有的一般目标检测方法不能有效地解决这个问题。它们的性能可能会受到视频中对象的大的外观变化的影响。例如，在上图( **a** )中，如果猫首先面对摄像机，然后转身，则它的背部图像不能有效地被识别为猫，因为它包含很少的纹理信息，并且不太可能被包括在训练中。然而，这是一个更简单的场景，其中我们必须检测单个对象(即动物)。

当我们想要开发一个自动驾驶汽车的应用程序时，我们将不得不处理这么多的对象和考虑因素。无论如何，因为我们不能通过这一章涵盖所有的方面，让我们移动到仅用少量的知识解决问题。

此外，从头开始实施和培训这些类型的应用程序非常耗时，也极具挑战性。因此，如今，迁移学习技术正成为流行和可行的选择。通过利用一个经过训练的模型，我们可以更容易地进行开发。一个这样的训练对象检测框架是 YOLO，它是最先进的实时对象检测系统之一。这些挑战和 YOLO 式的框架激励我用最少的努力开发这个项目。



# 你只看一次(YOLO)

虽然我们已经通过引入卷积滑动窗口解决了从静态图像中检测对象的问题，但是我们的模型仍然不能输出非常精确的边界框，即使有几个边界框大小。让我们看看 YOLO 是如何很好地解决这个问题的:

![](assets/a8d5df7e-72c6-47c5-a951-4a387edd4f3b.jpg)

使用边界框规范，我们去每个图像，并标记我们想要检测的对象

我们需要以某种特定的方式标记我们的训练数据，这样 YOLO 算法才能正确工作。YOLO V2 格式要求包围盒尺寸为*b^xT3、*b^yT7、 *b ^h* 、 *b* ^(*w*) ，以便相对于原始图像的宽度和高度。**

首先，我们通常会进入每张图像，并标记我们想要检测的对象。之后，每个图像被分割成更小数量的矩形(盒子)，通常是 13×13 的矩形，但是这里，为了简单起见，我们有 8×9。边界框(蓝色)和对象都可以是几个框(绿色)的一部分，所以我们可以将对象和边界框只分配给拥有对象中心的框(黄色框)。

这样，我们用四个额外的变量(*b^xT3、*b^yT7、 *b ^h* 和 *b ^w* )来训练我们的模型，并将这些变量分配给拥有中心的盒子 *b ^x* 、 *b ^y 由于神经网络是用这种标记的数据训练的，所以它也预测这四个变量(除了对象是什么之外)的值或边界框。***

我们让模型学习如何用边界框标记对象，而不是用预定义的边界框大小扫描并试图适应对象。因此，边界框现在是灵活的。这绝对是一个更好的方法，而且边界框的精确度更高，也更灵活。

现在让我们看看如何表示输出，我们已经添加了四个变量( *b ^x* 、 *b ^y* 、 *b ^h* 和 *b ^w* )除了类 1-car、2-pedestrian 之外。实际上，还添加了另一个变量，*P^c，它只是简单地告诉我们图像中是否有任何我们想要检测的对象。*

*   **P ^c =1(红色)**:这意味着至少有一个物体，所以值得看看概率和包围盒
*   **P ^c =0(红色)**:图像中没有我们想要的物体，所以我们不关心概率或边界框规格

得到的预测值， *b [w，]和 *b [h]* ，通过图像的高度和宽度归一化。(训练标签是这样选的。)所以，如果包含汽车的盒子的预测*b[x]和*b[y]为(0.3，0.8)，那么 13 x 13 特征图上的实际宽度和高度为(13 x 0.3，13 x 0.8)。***

YOLO 预测的边界框是相对于拥有对象中心(黄色)的框来定义的。盒子的左上角从(0，0)开始，右下角从(1，1)开始。由于该点在框内，在这种情况下，sigmoid 激活函数确保中心(*b[x]、 *b [y]* )在范围 0-1 内，如下图所示:*

![](assets/4ff68531-32ea-4470-8ffb-db6422a055b9.png)

Sigmoid 激活功能确保中心(b [x] ，b [y] ) 在 0-1 范围内

当 *b [h]* 、 *b [^w]* 与盒子的 *w* 和 *h* 值(黄色)成比例计算时，值可以大于 1(指数用于正值)。图中我们可以看到，包围盒的宽度*b[w]几乎是盒子宽度 *w* 的 1.8 倍。同样，*b[h]大约是箱体高度 *h* 的 1.6 倍，如下图所示:**

![](assets/2925cbfc-4935-4abb-a3ab-5a696df75677.png)

现在，问题是物体包含在包围盒中的概率是多少？嗯，要回答这个问题，我们需要知道对象得分，它代表一个对象包含在一个边界框内的概率。对于红色和相邻的网格，它应该接近 1，而对于例如角落处的网格，它应该接近 0。以下公式描述了如何变换网络输出以获得边界框预测:

![](assets/320f4dc8-05b4-4843-9e59-cf0253df142b.png)

在前面的公式中*，b [x] ，b [y] ，b [w] ，*和 *b [h]* 分别是我们预测的 *x* ， *y* 中心坐标，宽度和高度。另一方面， *t [x] ，t [y] ，t [w，]* 和 *t [h]* 是网络输出的。再者， *c [x]* 和 *c [y]* 是网格的左上角坐标。最后，*p[w]和*p[h]是盒子的锚定尺寸。**

abjectness 分数也通过一个 sigmoid，因为它被解释为一个概率。然后，使用类别置信度来表示检测到的对象属于特定类别的概率。预测后，我们看到预测框与开头标注的真实包围盒相交的程度。我们试图最大化它们之间的交集，因此理想情况下，预测的边界框与标记的边界框完全相交。

简而言之，我们用包围盒(*b^xT3、*b^yT7、 *b ^h* 、 *b ^w* )提供足够多的标记数据，然后我们分割图像并将其分配给包含中心的盒子，使用 CSW 网络进行训练并预测物体及其位置。所以首先我们分类，然后定位物体，检测。**

到目前为止，我们可以看到，我们已经能够克服使用 YOLO 的大部分障碍。然而，实际上，还有两个小问题需要解决。首先，即使在训练时间内将对象分配给一个盒子(一个包含对象的中心)，在推理期间，训练的模型假定几个盒子(即黄色)具有对象的中心(带有红色)。因此，这种混淆为同一对象引入了自己的边界框。

幸运的是，非最大抑制算法可以解决这个问题:首先，该算法选择具有最大*P^c概率的预测框，以便它具有 0 和 1 之间的值，而不是二进制 0 或 1 值。然后，移除相对于该框高于某个阈值的每个相交框。重复相同的逻辑，直到不再有边界框。其次，由于我们预测多个对象(汽车、火车、公共汽车等等)，两个或更多对象的中心可能位于单个框中。这个问题可以通过引入锚盒来解决:*

![](assets/15810d89-ef14-47f1-bdfa-ee44007e1f80.png)

锚箱规格

使用锚定框，我们选择几种形状的边界框，我们发现它们经常用于我们想要检测的对象。然后，通过对输出应用对数空间变换，然后将它们乘以锚点，来预测边界框的尺寸。



# 开发实时对象检测项目

在本节中，我们将使用预训练的 YOLO 模型(即迁移学习)、DL4J 和 OpenCV 开发一个视频对象分类应用程序，该应用程序可以检测视频帧中的汽车和树木等标签。坦率地说，这个应用也是关于将图像检测问题扩展到视频检测。所以让我们开始吧。



# 步骤 1–加载预训练的 YOLO 模型

自从 Alpha 1 . 0 . 0 发布以来，DL4J 通过 ZOO 提供了一个微型 YOLO 模型。为此，我们需要向 Maven 友好的`pom.xml`文件添加一个依赖项:

```
<dependency>
  <groupId>org.deeplearning4j</groupId>
  <artifactId>deeplearning4j-zoo</artifactId>
  <version>${dl4j.version}</version>
</dependency>
```

除此之外，如果可能的话，确保您通过添加以下依赖项来利用 CUDA 和 cuDNN(参见[第 2 章](e27fb252-7892-4659-81e2-2289de8ce570.xhtml)、*使用循环类型网络预测癌症类型*，了解更多详细信息):

```
<dependency>
  <groupId>org.nd4j</groupId>
  <artifactId>nd4j-cuda-9.0-platform</artifactId>
  <version>${nd4j.version}</version>
</dependency>
<dependency>
  <groupId>org.deeplearning4j</groupId>
  <artifactId>deeplearning4j-cuda-9.0</artifactId>
  <version>${dl4j.version}</version>
</dependency>
```

然后，我们准备加载预训练的小 YOLO 模型作为一个`ComputationGraph`，代码如下:

```
private ComputationGraph model; 

private TinyYoloModel() { 

        try { 

            model = (ComputationGraph) new TinyYOLO().initPretrained(); 

            createObjectLabels(); 

        } catch (IOException e) { 

            throw new RuntimeException(e); 

        } 

    }  
```

在前面的代码段中，`createObjectLabels()`方法引用了用于训练 YOLO 2 模型的 PASCAL 可视对象类(PASCAL VOC)数据集的标签。该方法的特征如下所示:

```
private HashMap<Integer, String> labels;  

void createObjectLabels() { 

        if (labels == null) { 

            String label = "aeroplanen" + "bicyclen" + "birdn" + "boatn" + "bottlen" + "busn" + "carn" + 

                    "catn" + "chairn" + "cown" + "diningtablen" + "dogn" + "horsen" + "motorbiken" + 

                    "personn" + "pottedplantn" + "sheepn" + "sofan" + "trainn" + "tvmonitor"; 

            String[] split = label.split("\n"); 

            int i = 0; 

            labels = new HashMap<>(); 

            for(String label1 : split) { 

                labels.put(i++, label1); 

            } 

        } 

    } 
```

现在，让我们创建一个小的 YOLO 模型实例:

```
 static final TinyYoloModel yolo = new TinyYoloModel(); 

    public static TinyYoloModel getPretrainedModel() { 

        return yolo; 

    } 
```

现在，出于好奇，我们来看看模型架构和每层中超参数的数量:

```
TinyYoloModel model = TinyYoloModel.getPretrainedModel(); 

System.out.println(TinyYoloModel.getSummary()); 
```

![](assets/f8ad0f0e-daa2-4a32-8575-441734544417.png)

预训练微型 YOLO 模型的网络概要和层结构

因此，我们的微型 YOLO 模型在其 29 层网络中有大约 160 万个参数。然而，最初的 YOLO 2 模型有更多的层。有兴趣的读者可以到[https://github . com/yhcc/yolo 2/blob/master/model _ data/model . png](https://github.com/yhcc/yolo2/blob/master/model_data/model.png)看一下 YOLO 2 的原文。



# 步骤 2–从视频剪辑生成帧

现在，为了处理实时视频，我们可以使用视频处理工具或框架，如 JavaCV 框架，它可以将视频分割成单独的帧，我们获取图像的高度和宽度。为此，我们必须在`pom.xml`文件中包含以下依赖关系:

```
<dependency>
  <groupId>org.bytedeco</groupId>
  <artifactId>javacv-platform</artifactId>
  <version>1.4.1</version>
</dependency>
```

JavaCV 使用计算机视觉领域研究人员常用的库(例如 OpenCV 和 FFmpeg)的 JavaCPP 预设的包装器，并提供实用程序类，使其功能在 Java 平台(包括 Android)上更容易使用。更多细节可以在[https://github.com/bytedeco/javacv](https://github.com/bytedeco/javacv)找到。

为了这个项目，我收集了两个视频剪辑(每个 1 分钟长)，应该可以让你一窥自动驾驶汽车。我从 YouTube 的以下链接下载了数据集:

*   *建造自动驾驶汽车-本地数据集-日期*:【https://www.youtube.com/watch?v=7BjNbkONCFw 
*   *建造自动驾驶汽车-本地数据集-夜晚*:【https://www.youtube.com/watch?v=ev5nddpQQ9I 

从 YouTube downloader(左右)下载后，我给它们重新命名如下:

*   `SelfDrivingCar_Night.mp4`
*   `SelfDrivingCar_Day.mp4`

现在，如果你播放这些片段，你会看到德国人如何以 160 公里/小时甚至更快的速度驾驶汽车。现在，让我们分析视频(首先我们使用第 1 天的视频)并查看一些属性，以了解视频质量硬件要求:

```
String videoPath = "data/SelfDrivingCar_Day.mp4"; 

FFmpegFrameGrabber frameGrabber = new FFmpegFrameGrabber(videoPath); 

frameGrabber.start(); 

Frame frame; 

double frameRate = frameGrabber.getFrameRate(); 

System.out.println("The inputted video clip has " + frameGrabber.getLengthInFrames() + " frames"); 

System.out.println("Frame rate " + framerate + "fps"); 
```

```
>>>
 The inputted video clip has 1802 frames.
 The inputted video clip has frame rate of 29.97002997002997.
```

然后我们抓取每一帧并使用`Java2DFrameConverter`；它帮助我们将帧转换成 JPEG 图像:

```
Java2DFrameConverter converter = new Java2DFrameConverter(); 

// grab the first frame 

frameGrabber.setFrameNumber(1); 

frame = frameGrabber.grab(); 
BufferedImage bufferedImage = converter.convert(frame); 

System.out.println("First Frame" + ", Width: " + bufferedImage.getWidth() + ", Height: " + bufferedImage.getHeight()); 

// grab the second frame 

frameGrabber.setFrameNumber(2); 

frame = frameGrabber.grab(); 

bufferedImage = converter.convert(frame); 

System.out.println("Second Frame" + ", Width: " + bufferedImage.getWidth() + ", Height: " + bufferedImage.getHeight()); 
```

```
>>>
 First Frame: Width-640, Height-360
 Second Frame: Width-640, Height-360
```

这样，前面的代码将针对相同数量的帧生成 1，802 幅 JPEG 图像。让我们看看生成的图像:

![](assets/66694124-6340-48c9-9cc2-2ace25135ed5.png)

从视频剪辑到视频帧再到图像

因此，1 分钟长的视频剪辑有相当数量(即 1，800)的帧，每秒 30 帧。简而言之，这个视频剪辑有 720p 的视频质量。所以你可以理解处理这个视频应该需要好的硬件；特别是，配置一个 GPU 应该会有所帮助。



# 步骤 3–将生成的帧输入到微型 YOLO 模型中

现在我们知道了剪辑的一些属性，我们可以开始生成要传递给微小的 YOLO 预训练模型的帧。首先，让我们看一个不太重要但透明的方法:

```
private volatile Mat[] v = new Mat[1]; 

private String windowName = "Object Detection from Video"; 

try { 

    for(int i = 1; i < frameGrabber.getLengthInFrames();     

    i+ = (int)frameRate) { 

                frameGrabber.setFrameNumber(i); 

                frame = frameGrabber.grab(); 

                v[0] = new OpenCVFrameConverter.ToMat().convert(frame); 

                model.markObjectWithBoundingBox(v[0], frame.imageWidth, 
                                               frame.imageHeight, true, windowName); 

                imshow(windowName, v[0]); 

                char key = (char) waitKey(20); 

                // Exit on escape: 

                if (key == 27) { 

                    destroyAllWindows(); 

                    break; 

                } 

            } 

        } catch (IOException e) { 

            e.printStackTrace(); 

        } finally { 

            frameGrabber.stop(); 

        } 

        frameGrabber.close(); 
```

在前面的代码块中，我们将每个帧发送给模型。然后我们用`Mat`类把每一帧表示成一个 n 维的、密集的、数值化的多通道(也就是 RGB)数组。

欲知详情，请访问[https://docs . opencv . org/trunk/D3/d63/classcv _ 1 _ 1 mat . html # details](https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#details)。

换句话说，我们将视频剪辑分成多个帧，并传递到微小的 YOLO 模型中逐个处理它们。这样，我们将单个神经网络应用于整个图像。



# 步骤 4–从图像帧中检测目标

微小的 YOLO 从每一帧中提取特征，作为一个 n 维，密集，数字多通道阵列。然后，每个图像被分割成更小数量的矩形(方框):

```
public void markObjectWithBoundingBox(Mat file, int imageWidth, int imageHeight, boolean newBoundingBOx,
                                      String winName) throws Exception { 

        // parameters matching the pretrained TinyYOLO model
        int W = 416; // width of the video frame  

        int H = 416; // Height of the video frame 

        int gW = 13; // Grid width 

        int gH = 13; // Grid Height 

        double dT = 0.5; // Detection threshold 

        Yolo2OutputLayer outputLayer = (Yolo2OutputLayer) model.getOutputLayer(0); 

        if (newBoundingBOx) { 

            INDArray indArray = prepareImage(file, W, H); 

            INDArray results = model.outputSingle(indArray); 

            predictedObjects = outputLayer.getPredictedObjects(results, dT); 
            System.out.println("results = " + predictedObjects); 

            markWithBoundingBox(file, gW, gH, imageWidth, imageHeight); 

        } else { 

            markWithBoundingBox(file, gW, gH, imageWidth, imageHeight); 

        } 

        imshow(winName, file); 

    }
```

在前面的代码中，`prepareImage()`方法将视频帧作为图像，使用`NativeImageLoader class`解析它们，进行必要的预处理，并提取图像特征，这些特征进一步转换为模型可使用的`INDArray`格式:

```
INDArray prepareImage(Mat file, int width, int height) throws IOException { 

        NativeImageLoader loader = new NativeImageLoader(height, width, 3); 

        ImagePreProcessingScaler imagePreProcessingScaler = new ImagePreProcessingScaler(0, 1); 

        INDArray indArray = loader.asMatrix(file); 

        imagePreProcessingScaler.transform(indArray); 

        return indArray; 

    } 
```

然后，在不止一个边界框的情况下，使用`markWithBoundingBox()`方法进行非最大值抑制。



# 步骤 5–超过一个边界框时的非最大抑制

由于 YOLO 预测每个对象不止一个边界框，所以实现了非最大值抑制；它合并属于同一对象的所有检测。所以不用*b^x、 *b ^y* 、 *b ^h* 、 *b ^w* **、**我们可以用左上右下的点。`gridWidth`和`gridHeight`是我们将图像分割成的小盒子的数量。在我们的例子中，这是 13×13，其中`w`和`h`是原始图像帧尺寸:*

```
void markObjectWithBoundingBox(Mat file, int gridWidth, int gridHeight, int w, int h, DetectedObject obj) {  

        double[] xy1 = obj.getTopLeftXY(); 

        double[] xy2 = obj.getBottomRightXY(); 

        int predictedClass = obj.getPredictedClass(); 
        int x1 = (int) Math.round(w * xy1[0] / gridWidth); 

        int y1 = (int) Math.round(h * xy1[1] / gridHeight); 

        int x2 = (int) Math.round(w * xy2[0] / gridWidth); 

        int y2 = (int) Math.round(h * xy2[1] / gridHeight); 
        rectangle(file, new Point(x1, y1), new Point(x2, y2), Scalar.RED); 

        putText(file, labels.get(predictedClass), new Point(x1 + 2, y2 - 2), 
                                 FONT_HERSHEY_DUPLEX, 1, Scalar.GREEN); 

    } 
```

最后，我们移除那些与最大抑制相交的对象，如下所示:

```
static void removeObjectsIntersectingWithMax(ArrayList<DetectedObject> detectedObjects, 
                                             DetectedObject maxObjectDetect) { 

        double[] bottomRightXY1 = maxObjectDetect.getBottomRightXY(); 

        double[] topLeftXY1 = maxObjectDetect.getTopLeftXY(); 

        List<DetectedObject> removeIntersectingObjects = new ArrayList<>(); 
        for(DetectedObject detectedObject : detectedObjects) { 

            double[] topLeftXY = detectedObject.getTopLeftXY(); 

            double[] bottomRightXY = detectedObject.getBottomRightXY(); 

            double iox1 = Math.max(topLeftXY[0], topLeftXY1[0]); 

            double ioy1 = Math.max(topLeftXY[1], topLeftXY1[1]); 

            double iox2 = Math.min(bottomRightXY[0], bottomRightXY1[0]); 

            double ioy2 = Math.min(bottomRightXY[1], bottomRightXY1[1]); 

            double inter_area = (ioy2 - ioy1) * (iox2 - iox1); 

            double box1_area = (bottomRightXY1[1] - topLeftXY1[1]) * (bottomRightXY1[0] - topLeftXY1[0]); 

            double box2_area = (bottomRightXY[1] - topLeftXY[1]) * (bottomRightXY[0] - topLeftXY[0]); 

            double union_area = box1_area + box2_area - inter_area; 

            double iou = inter_area / union_area;  

            if(iou > 0.5) { 

                removeIntersectingObjects.add(detectedObject); 

            } 

        } 

        detectedObjects.removeAll(removeIntersectingObjects); 

    } 
```

在第二个块中，我们将每个图像缩放为 416 x 416 x 3(即 W x H x 3 RGB 通道)。然后，该缩放图像被传递给微小的 YOLO，用于预测和标记边界框，如下所示:

![](assets/7b10e461-0e7c-42ed-b47f-bde5329922f3.png)

我们微小的 YOLO 模型可以预测在边界框中检测到的物体的类别

一旦执行了`markObjectWithBoundingBox()`方法，将生成包含预测类、 *b ^x* 、 *b ^y* 、 *b ^h* 、 *b ^w* 和置信度(即检测阈值)的日志，并显示在控制台上:

```
[4.6233e-11]], predictedClass=6),
DetectedObject(exampleNumber=0,
centerX=3.5445247292518616, centerY=7.621537864208221,
width=2.2568163871765137, height=1.9423424005508423,
confidence=0.7954192161560059,
classPredictions=[[ 1.5034e-7], [ 3.3064e-9]...
```



# 步骤 6——完成所有工作并运行应用程序

到目前为止，我们已经知道了我们方法的整个工作流程，现在我们可以总结所有的东西，看看它是否真的有效。然而，在此之前，让我们看一下不同 Java 类的功能:

*   这个程序展示了如何从视频剪辑中抓取帧并将每一帧保存为 JPEG 图像。此外，它还显示了视频剪辑的一些探索性质。
*   `TinyYoloModel.java`:这将实例化小 YOLO 模型并生成标签。它还用边界框创建和标记对象。尽管如此，它显示了如何为每个对象的多个边界框处理非最大抑制。
*   `ObjectDetectorFromVideo.java`:主类。它不断地抓取帧，并将它们馈送给微型 YOLO 模型(也就是说，直到用户按下 *Esc* 键)。然后，它使用非最大抑制(如果需要)预测在正常或重叠边界框内成功检测到的每个对象的相应类别。

简而言之，首先，我们创建并实例化小 YOLO 模型。然后，我们抓取帧，并将每一帧视为一个单独的 JPEG 图像。接下来，我们将所有的图像传递给模型，模型像前面描述的那样完成它的任务。现在可以用一些 Java 代码来描述整个工作流，如下所示:

```
// ObjectDetectorFromVideo.java
public class ObjectDetectorFromVideo{ 

    private volatile Mat[] v = new Mat[1]; 

    private String windowName; 

    public static void main(String[] args) throws java.lang.Exception { 

        String videoPath = "data/SelfDrivingCar_Day.mp4"; 

        TinyYoloModel model = TinyYoloModel.getPretrainedModel(); 

        System.out.println(TinyYoloModel.getSummary()); 

        new ObjectDetectionFromVideo().startRealTimeVideoDetection(videoPath, model); 

    } 

    public void startRealTimeVideoDetection(String videoFileName, TinyYoloModel model) 
           throws java.lang.Exception { 

        windowName = "Object Detection from Video"; 

        FFmpegFrameGrabber frameGrabber = new FFmpegFrameGrabber(videoFileName); 

        frameGrabber.start(); 

        Frame frame; 

        double frameRate = frameGrabber.getFrameRate(); 

        System.out.println("The inputted video clip has " + frameGrabber.getLengthInFrames() + " frames"); 

        System.out.println("The inputted video clip has frame rate of " + frameRate); 

        try { 

            for(int i = 1; i < frameGrabber.getLengthInFrames(); i+ = (int)frameRate) { 

                frameGrabber.setFrameNumber(i); 

                frame = frameGrabber.grab(); 

                v[0] = new OpenCVFrameConverter.ToMat().convert(frame); 

                model.markObjectWithBoundingBox(v[0], frame.imageWidth, frame.imageHeight, 
                                                true, windowName); 

                imshow(windowName, v[0]); 

                char key = (char) waitKey(20); 

                // Exit on escape: 

                if(key == 27) { 

                    destroyAllWindows(); 

                    break; 

                } 

            } 

        } catch (IOException e) { 

            e.printStackTrace(); 

        } finally { 

            frameGrabber.stop(); 

        } 

        frameGrabber.close(); 

    } 

} 
```

一旦执行了前面的类，应用程序应该加载预训练的模型，并且应该加载 UI，显示每个被分类的对象:

![](assets/2cdfb2ce-0c92-46aa-93cb-9c2fd1af9a3b.png)

我们的微型 YOLO 模型可以从一个视频剪辑中同时预测多辆汽车(一天)

现在，为了查看我们的模型在夜间模式下的有效性，我们可以在夜间数据集上执行第二个实验。为此，只需更改`main()`方法中的一行，如下所示:

```
String videoPath = "data/SelfDrivingCar_Night.mp4";
```

一旦使用这个片段执行了前面的类，应用程序应该加载预训练模型，并且应该加载 UI，显示每个被分类的对象:

![](assets/6f66961b-2b0a-4755-b510-9be340da10c5.png)

我们的微型 YOLO 模型可以从视频剪辑中同时预测多辆汽车(夜间)

此外，要查看实时输出，请执行显示应用程序输出的给定屏幕记录剪辑。



# 常见问题(FAQ)

在这一部分，我们将看到一些你可能已经想到的常见问题。这些问题的答案可以在附录 a 中找到

1.  我们不能从头开始训练 YOLO 吗？
2.  我想知道我们是否可以使用 YOLO v3 型。

3.  我需要对代码做什么修改才能让它适用于我自己的视频剪辑？
4.  提供的应用程序可以检测视频剪辑中的汽车和另一辆汽车。但是，加工并不顺利。它似乎停止了。我该如何解决这个问题？
5.  我可以扩展这个应用程序，使其适用于网络摄像头的实时视频吗？



# 摘要

在本章中，我们看到了如何开发一个端到端的项目，当视频剪辑连续播放时，该项目将从视频帧中检测对象。我们看到了如何利用预先训练好的微型 YOLO 模型，它是原始 YOLO v2 模型的一个较小的变体。

此外，我们还讨论了从静态图像和视频中检测对象的一些典型挑战，以及如何使用包围盒和非最大值抑制技术来解决这些挑战。我们学习了如何使用 DL4J 之上的 JavaCV 库处理视频剪辑。最后，我们看到了一些常见问题，这些问题应该对这个项目的实现和扩展有用。

在下一章中，我们将了解如何开发异常检测，这对于银行、保险和信用合作社等金融公司的欺诈分析非常有用。发展业务是一项重要的任务。我们将使用无监督学习算法，如变分自编码器和重建概率。



# 问题的答案

**回答** **问题 1** :我们可以从零开始训练一个 YOLO 网络，但这需要大量的工作(以及昂贵的 GPU 时间)。作为工程师和数据科学家，我们希望尽可能多地利用预构建的库和机器学习模型，因此我们将使用预训练的 YOLO 模型来更快、更便宜地将我们的应用投入生产。

**回答** **问题 2** :也许是的，但是最新的 DL4J 版本只提供 YOLO v2。然而，当我和他们的 Gitter(见 https://deeplearning4j.org/)交谈时，他们告诉我，只要再多做一些努力，你就能让它工作起来。我的意思是你可以用 Keras import 导入 YOLO v3。不幸的是，我试了试，但没能成功。

**回答** **问题 3** :你应该可以直接馈入自己的视频。但是，如果它不起作用，或者抛出任何不需要的异常，那么视频属性(如帧速率、宽度和每帧的高度)应该与边界框规格相同。

**回答** **第四个问题**:嗯，我已经说过了，你的机器应该有良好的硬件规格，处理不应该造成任何延迟。例如，我的机器有 32 GB 的内存，酷睿 i7 处理器，GeForce GTX 1050 GPU 和 4 GB 的主内存，应用程序运行非常流畅。

**回答** **问题 5:** 也许，是的。在这种情况下，视频的主要来源应该是直接来自网络摄像头。根据在 https://github.com/bytedeco/javacv[提供的文档，JavaCV 还配备了硬件加速的全屏图像显示，易于使用的在多个内核上并行执行代码的方法，用户友好的相机、投影仪等的几何和颜色校准。](https://github.com/bytedeco/javacv)