

# 四、基于 Word2Vec 和 LSTM 网络的情感分析

情感分析是一种识别、提取、量化和研究有效状态和主观信息的系统方法。这在**自然语言处理** ( **NLP** )、文本分析和计算语言学中被广泛使用。本章演示了如何实施和部署一个实践深度学习项目，该项目根据评论文本包含的单词将它们分为正面或负面。将使用包含 50k 评论(训练加测试)的大规模电影评论数据集。

使用 Word2Vec(即 NLP 中广泛使用的单词嵌入技术)和**长短期记忆** ( **LSTM** )网络进行建模的组合方法将被应用:预训练的谷歌新闻向量模型将被用作神经单词嵌入。然后，训练向量和标签一起，将被输入 LSTM 网络，以将它们分类为消极情绪或积极情绪。最后，在测试集上对训练好的模型进行评估。

此外，它还展示了如何应用文本预处理技术，如分词器、停用词移除、**词频-逆文档频率** ( **TF-IDF** )，以及在**deep learning 4j**(**DL4J**)中的单词嵌入操作。

尽管如此，它还展示了如何保存已训练好的 DL4J 模型。稍后，保存的模型将从磁盘中恢复，以对来自亚马逊细胞、Yelp 和 IMDb 的其他小规模评论文本进行情感预测。最后，它回答了一些与项目和可能的前景相关的常见问题。

本端到端项目将涵盖以下主题:

*   自然语言处理中的情感分析
*   使用 Word2Vec 进行神经单词嵌入
*   数据集收集和描述
*   使用 DL4J 保存和恢复预训练模型
*   使用 Word2Vec 和 LSTM 开发情感分析模型
*   常见问题(FAQ)



# 情感分析是一项具有挑战性的任务

NLP 中的文本分析是关于处理和分析大规模结构化和非结构化文本，以发现隐藏的模式和主题，并得出上下文含义和关系。文本分析有许多潜在的用例，如情感分析、主题建模、TF-IDF、命名实体识别和事件提取。

情感分析包括许多示例用例，例如分析脸书、Twitter 和其他社交媒体上人们的政治观点。同样，分析 Yelp 上餐馆的评论也是情感分析的另一个很好的例子。诸如 OpenNLP 和 Stanford NLP 之类的 NLP 框架和库通常用于实现情感分析。

然而，对于使用文本，尤其是非结构化文本来分析情感，我们必须找到一种健壮且有效的特征工程方法来将文本转换成数字。然而，在模型被训练、随后被部署以及最终执行预测分析之前，数据转换的几个阶段是可能的。此外，我们应该期待特性和模型属性的细化。我们甚至可以探索一种完全不同的算法，重复整个任务序列，作为新工作流的一部分。

当你看一行文本时，我们看到句子、短语、单词、名词、动词、标点符号等等，当它们放在一起时，就有了意义和目的。人类非常擅长理解句子、单词、俚语、注释和上下文。这来自多年的练习和学习如何读/写正确的语法、标点符号、感叹词等等。

例如，两个句子: *DL4J 使预测分析变得容易*和*预测分析使 DL4J 变得容易*，可能导致相同的句子向量具有相同的长度，等于我们选择的词汇量的大小。第二个缺点是单词“is”和“DL4J”具有相同的数字索引值 1，但是我们的直觉告诉我们单词“is”与“DL4J”相比并不重要。让我们看一下第二个例子:当您在 Google 中搜索柏林的*酒店*时，我们也想要与柏林的 *bnb* 、*汽车旅馆*、*住宿*和*住宿*相关的结果。

当外行术语来到党，自然语言学习变得更加复杂。以银行这个词为例。这与一家金融机构和一片水域边上的土地有几个联系。现在，如果一个自然语句包含“银行”一词以及金融、货币、国库和利率等词，我们可以理解为它的本意是前者。然而，如果相邻的词是水、岸、河和湖等等，情况是后者。现在，问题是:我们能否利用这个概念来处理多义词和同义词，并使我们的模型学习得更好？

![](assets/f620eb24-b4e5-4a8d-9a95-ef5685415cd2.png)

经典机器学习与基于深度学习的 NPL

尽管如此，自然语言句子也包含模糊词、俚语、琐碎词和特殊字符，所有这些都使得整体理解和机器学习很麻烦。

我们已经看到了如何使用 one-hot 编码或 StringIndexer 技术将分类变量(甚至是单词)转换成数字形式。然而，这种程序往往无法解释复杂句子中的语义，尤其是对于长句甚至单词。因此，人类的词汇没有相似的自然概念。因此，我们自然不会试图复制这种能力，对吗？

我们如何才能建立一种简单、可扩展、更快的方法来处理常规文本或句子，并推导出单词与其上下文单词之间的关系，然后将它们嵌入到数十亿个单词中，这些单词将产生非常好的数字向量空间的单词表示，以便机器学习模型可以使用它们？让我们看看 Word2Vec 模型来寻找这个问题的答案。



# 使用 Word2Vec 进行神经单词嵌入

Word2Vec 是一个两层神经网络，它处理文本并将其转化为数字特征。这样，Word2Vec 的输出是一个词汇表，其中每个单词都嵌入在向量空间中。生成的向量可以输入神经网络，以便更好地理解自然语言。小说家埃尔·多克托罗在他的书《比利·巴斯盖特》中颇有诗意地表达了这个想法:

“就像数字是一种语言，就像语言中的所有字母都变成了数字，所以这是每个人都以同样的方式理解的事情。你失去了字母的声音，不管它们是咔嚓声、砰砰声还是碰上颚的声音，或者发出“哦”或“啊”的声音，以及任何可能被误读或被它的音乐或它放在你脑海中的图片欺骗的声音，所有这些都消失了，连同口音，你有了一种完全新的理解，一种数字语言，一切对每个人来说都变得像墙上的文字一样清晰。所以正如我所说的，阅读数字的时间到了。”

当使用 BOW 和 TF-IDF 时，所有的单词被投影到相同的位置，并且它们的向量被平均:我们处理单词重要性，而不考虑单词顺序在文档集合或单个文档中的重要性。

由于历史中的单词顺序不影响投影，BOW 和 TF-IDF 都没有这样的功能来处理这个问题。Word2Vec 通过使用**连续词袋** ( **CBOW** )或者使用单词预测目标上下文(称为**连续跳格**)来使用上下文预测目标单词，从而将每个单词编码成向量。

*   **N-gram 对 skip-gram** :单词被一次读入一个向量，并在一定范围内来回扫描
*   CBOW 技术使用上下文的连续分布式表示
*   **连续跳格**:与 CBOW 不同，这种方法试图根据同一个句子中的另一个单词最大化地分类一个单词

我的经验是，范围的增加提高了结果词向量的质量，但也增加了计算的复杂性。由于距离较远的单词通常比距离较近的单词与当前单词的关系更小，因此我们在训练示例中通过从这些单词中抽取较少的样本来降低距离较远的单词的权重。由于模型的建立和预测，次数也增加了。

从体系结构的角度进行的比较分析可以在下图中看到，其中体系结构根据上下文预测当前单词，而跳过程序根据当前单词预测周围的单词:

![](assets/bba3319f-913c-4a4d-af80-de53f74b9cfd.png)

CBOW 与 skip-gram(来源:Tomas Mikolov 等人，向量空间中单词表示的有效估计，https://arxiv.org/pdf/1301.3781.pdf)



# 数据集和预训练模型描述

我们将使用大型电影评论数据集来训练和测试该模式。此外，我们将使用情感标签句子数据集对产品、电影和餐馆的评论进行单一预测。



# 用于训练和测试的大型电影评论数据集

前者是用于二元情感分类的数据集，其包含的数据比先前的基准数据集多得多。数据集可以从[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)下载。或者，我使用了一个来自 DL4J 示例的 Java 方法，该方法也下载并提取这个数据集。

我要感谢下列出版物:安德鲁·马斯、雷蒙德·戴利、彼得·范、黄丹、安德鲁·吴和克里斯托弗·波茨。(2011)，*面向情感分析的词向量学习*，计算语言学协会第 49 届^(年会(ACL 2011)。)

该数据集包含 50，000 条电影评论及其相关的二元情感极性标签。对于训练集和测试集，评论被平均分成 25，000 条。标签的总体分布是平衡的(25，000 个正标签和 25，000 个负标签)。我们还包括额外的 5 万个无标签文档，用于无监督学习。在标记的训练/测试集中，如果评论具有分数<= 4 out of 10, it is treated as a negative review, but having a score >= 10 分中的 7 分被视为正面评论。然而，具有更中性评级的评论不包括在数据集中。



# 数据集的文件夹结构

有两个文件夹，即分别用于训练集和测试集的`train`和`test`。每个文件夹都有两个独立的子文件夹，分别叫做`pos`和`neg`，包含带有二进制标签(pos，neg)的评论。评论存储在名为`id_rating.txt`的文本文件中，其中`id`是唯一的 ID，`rating`是 1-10 级的星级。请看下图，以便更清楚地了解目录的结构:

![](assets/3c758053-b424-4acf-b7d1-76fa4dcc7225.png)

大型电影评论数据集中的文件夹结构

例如，`test/pos/200_8.txt`文件是一个阳性标签测试集示例的文本，它的惟一 ID 是 200，来自 IMDb 的星级是 8/10。由于数据集的这一部分省略了评级，因此`train/unsup/`目录中的所有评级为零。让我们来看一个来自 IMDb 的正面评价样本:

“布罗姆韦尔高中是一部卡通喜剧。它与其他一些关于学校生活的节目同时播出，比如《教师》。我 35 年的教学生涯让我相信，布罗姆韦尔高中的讽刺作品比《教师》更接近现实。在经济上生存的竞争，有洞察力的学生能看穿他们可怜的老师的浮华，整个情况的琐碎，都让我想起了我所知道的学校和他们的学生。当我看到一个学生多次试图烧毁学校的那一集时，我立即回忆起来.........在..........高。经典台词:督察:我是来解雇你的一个老师的。学生:欢迎来到布罗姆威尔高中。我想很多和我同龄的成年人会认为布罗姆韦尔高中很牵强。可惜不是！”

因此，从前面的评论文本中，我们可以理解，各自的观众给了布罗姆韦尔高中(一部关于伦敦南部一所英国高中的英国-加拿大成人动画系列，你可以在[https://en.wikipedia.org/wiki/Bromwell_High](https://en.wikipedia.org/wiki/Bromwell_High)看到更多)一个积极的评价，即一种积极的情绪。



# 情感标签数据集的描述

情感标签句子数据集是从 UCI 机器学习知识库下载的，网址为[http://archive . ics . UCI . edu/ml/datasets/perspective+Labelled+Sentences](http://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)。该数据集是 Kotzias 的研究成果，并在以下出版物中使用:*使用深度特征从群体到个体标签*，Kotzias 等人。阿尔，KDD 2015。

该数据集包含标记有积极或消极情绪的句子，这些句子是从产品、电影和餐馆的评论中提取的。该审核是一个制表符分隔的审核，包含审核句子，得分为 1(表示积极)或 0(表示消极)。让我们来看看 Yelp 的一篇带有相关标签的示例评论:

“我很反感，因为我很确定那是人的头发。”

在前面的评论文本中，分数为 0，因此这是一个负面评论，表达了客户的负面情绪。另一方面，有 500 个肯定句和 500 个否定句。

这些都是随机选择的大型评论数据集。作者试图选择具有明确肯定或否定含义的句子；目标是不选择中性的句子。复习句子收集自三个不同的网站/领域，具体如下:

*   [https://www.imdb.com/](https://www.imdb.com/)
*   [https://www.amazon.com/](https://www.amazon.com/)
*   [https://www.yelp.com/](https://www.yelp.com/)



# Word2Vec 预训练模型

可以使用 Google 预先训练的新闻单词向量模型，而不是从头生成新的 Word2Vec 模型，该模型为计算单词的向量表示提供了 CBOW 和 skip-gram 架构的有效实现。这些表示随后可以用于许多 NLP 应用和进一步的研究。

该模型可以从[https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/)手动下载。Word2Vec 模型将文本语料库作为输入，并将单词向量作为输出。它首先从训练文本数据构建词汇表，然后学习单词的向量表示。

有两种方法可以实现 Word2Vec 模型:使用连续的词包和连续的跳格。Skip-gram 速度较慢，但对于不常用的单词更好，尽管 CBOW 速度更快。

得到的单词向量文件可以用作许多自然语言处理和机器学习应用中的特征。



# 使用 Word2Vec 和 LSTM 的情感分析

首先，我们来定义一下问题。给定一个电影评论(原始文本)，我们必须根据它包含的词，即情感，将该电影评论分为正面或负面。我们通过结合 Word2Vec 模型和 LSTM 来做到这一点:使用 Word2Vec 模型将评论中的每个单词矢量化，并输入到 LSTM 网络中。如前所述，我们将在大型电影评论数据集中训练数据。现在，这是整个项目的工作流程:

*   首先，我们下载电影/产品评论数据集
*   然后，我们创建或重用现有的 Word2Vec 模型(例如，Google News word vectors)
*   然后，我们加载每个评论文本，将单词转换为向量，将评论转换为向量序列
*   然后我们创建并训练 LSTM 网络
*   然后我们保存训练好的模型
*   然后我们在测试集上评估模型
*   然后，我们恢复训练好的模型，并评估来自标记为数据集
    的情感的样本评论文本

现在，让我们来看看如果我们按照前面的工作流程进行，`main()`方法会是什么样子:

```
public static void main(String[] args) throws Exception {
       Nd4j.getMemoryManager().setAutoGcWindow(10000);// see more in the FAQ section
       wordVectors = WordVectorSerializer.loadStaticModel(new File(WORD_VECTORS_PATH)); // Word2vec path   
       downloadAndExtractData(); // download and extract the dataset
       networkTrainAndSaver(); // create net, train and save the model
       networkEvaluator(); // evaluate the model on test set
       sampleEvaluator(); // evaluate a simple review from text/file.
}
```

让我们把前面的步骤分成更小的步骤。我们将使用 Word2Vec 模型从数据集准备开始。



# 使用 Word2Vec 模型准备训练和测试集

现在，为了准备用于训练和测试的数据集，首先，我们必须下载三个文件，概述如下:

*   谷歌训练的 Word2Vec 模型
*   大型电影评论数据集
*   一个情感标签数据集

预先训练好的 Word2Vec 从[https://code.google.com/p/word2vec/](https://code.google.com/p/word2vec/)下载，然后我们可以手动设置谷歌新闻向量的位置:

```
public static final String WORD_VECTORS_PATH = "/Downloads/GoogleNews-vectors-negative300.bin.gz";
```

然后，我们将从以下 URL 下载并提取大型电影评论数据集。

```
public static final String DATA_URL = "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz";
```

现在，让我们设置保存和提取训练/测试数据的位置:

```
public static final String DATA_PATH = FilenameUtils.concat(System.getProperty("java.io.tmpdir"), "dl4j_w2vSentiment/");
```

现在，我们可以在我们的首选位置手动下载或提取数据集，或者，下面的方法以自动化的方式完成。请注意，我稍微修改了最初的 DL4J 实现:

```
public static void downloadAndExtractData() throws Exception {
  //Create directory if required
  File directory = new File(DATA_PATH);

  if(!directory.exists()) directory.mkdir();
  //Download file:
  String archizePath = DATA_PATH + "aclImdb_v1.tar.gz";
  File archiveFile = new File(archizePath);
  String extractedPath = DATA_PATH + "aclImdb";
  File extractedFile = new File(extractedPath);

  if( !archiveFile.exists() ){
    System.out.println("Starting data download (80MB)...");
    FileUtils.copyURLToFile(new URL(DATA_URL), archiveFile);
    System.out.println("Data (.tar.gz file) downloaded to " + archiveFile.getAbsolutePath());

    //Extract tar.gz file to output directory
    DataUtilities.extractTarGz(archizePath, DATA_PATH);
  } else {
    //Assume if archive (.tar.gz) exists, then data has already been extracted
    System.out.println("Data (.tar.gz file) already exists at " + archiveFile.getAbsolutePath());

    if( !extractedFile.exists()){
    //Extract tar.gz file to output directory
      DataUtilities.extractTarGz(archizePath, DATA_PATH);
    } else {
      System.out.println("Data (extracted) already exists at " + extractedFile.getAbsolutePath());
    }
  }
}
```

在前面的方法中，使用 HTTP 协议从我提到的 URL 下载数据集。然后，将数据集提取到我们提到的位置。为此，我使用了 Apache Commons 的`TarArchiveEntry`、`TarArchiveInputStream`和`GzipCompressorInputStream`工具。感兴趣的读者可以在[http://commons.apache.org/](http://commons.apache.org/)了解更多详情。

简而言之，我提供了一个名为`DataUtilities.java`的类，它有两个方法`downloadFile()`和`extractTarGz()`，用于下载和提取数据集。

首先，`downloadFile()`方法以远程 URL(即远程文件的 URL)和本地路径(即下载文件的位置)为参数，如果远程文件不存在就下载一个远程文件。现在，让我们看看签名是什么样子的:

```
public static boolean downloadFile(String remoteUrl, String localPath) throws IOException {
  boolean downloaded = false;

  if (remoteUrl == null || localPath == null)
       return downloaded;

  File file = new File(localPath);
  if (!file.exists()) {
    file.getParentFile().mkdirs();
    HttpClientBuilder builder = HttpClientBuilder.create();
    CloseableHttpClient client = builder.build();
    try (CloseableHttpResponse response = client.execute(new HttpGet(remoteUrl))) {
      HttpEntity entity = response.getEntity();
      if (entity != null) {
        try (FileOutputStream outstream = new FileOutputStream(file)) {
          entity.writeTo(outstream);
          outstream.flush();
          outstream.close();
        }
      }
    }
    downloaded = true;
  }
  if (!file.exists())
  throw new IOException("File doesn't exist: " + localPath);
  return downloaded;
}
```

其次，`extractTarGz()`方法将输入路径(`ism`输入文件路径)和输出路径(即输出目录路径)作为参数，并将`tar.gz`文件提取到本地文件夹。现在，让我们看看签名是什么样子的:

```
public static void extractTarGz(String inputPath, String outputPath) throws IOException {
  if (inputPath == null || outputPath == null)
       return;

  final int bufferSize = 4096;
  if (!outputPath.endsWith("" + File.separatorChar))
      outputPath = outputPath + File.separatorChar;

  try (TarArchiveInputStream tais = new TarArchiveInputStream( new GzipCompressorInputStream(new BufferedInputStream(
                                      new FileInputStream(inputPath))))) {
    TarArchiveEntry entry;
    while ((entry = (TarArchiveEntry) tais.getNextEntry()) != null) {
      if (entry.isDirectory()) {
        new File(outputPath + entry.getName()).mkdirs();
      } else {
        int count;
        byte data[] = newbyte[bufferSize];
        FileOutputStream fos = new FileOutputStream(outputPath + entry.getName());
        BufferedOutputStream dest = new BufferedOutputStream(fos, bufferSize);
        while ((count = tais.read(data, 0, bufferSize)) != -1) {
              dest.write(data, 0, count);
        }
        dest.close();
      }
    }
  }
}
```

现在，要利用前面的方法，您必须导入以下包:

```
import org.apache.commons.compress.archivers.tar.TarArchiveEntry;
import org.apache.commons.compress.archivers.tar.TarArchiveInputStream;
import org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream;
```

顺便说一下，Apache Commons 是一个 Apache 项目，关注可重用 Java 组件的所有方面。更多见[https://commons.apache.org/](https://commons.apache.org/)。

最后，标注了数据集的情感可以从[https://archive . ics . UCI . edu/ml/machine-learning-databases/00331/](https://archive.ics.uci.edu/ml/machine-learning-databases/00331/)下载。完成这些步骤后，下一个任务将是准备培训和测试集。为此，我编写了一个名为`SentimentDatasetIterator`的类，它是一个`DataSetIterator`，专门用于我们项目中使用的 IMDb 评论数据集。然而，这也可以应用于 NLP 中文本分析的任何文本数据集。这个类是由 DL4J 示例提供的`SentimentExampleIterator.java`类的一个小扩展。感谢 DL4J 的乡亲们让我们的生活更轻松。

`SentimentDatasetIterator`类从情感标签数据集和 Google 预先训练的 Word2Vec 中获取训练或测试集，并生成训练数据集。另一方面，使用单个类别(负面或正面)作为标签来预测每个评论的最终时间步长。此外，由于我们处理不同长度的评论，并且在最后的时间步只有一个输出，所以我们使用填充数组。简而言之，我们的训练数据集应该包含以下项目，即 4D 对象:

*   每个评论文本的特征
*   1 或 0 中的标签(即分别代表阳性和阴性)
*   特征遮罩
*   标签遮罩

因此，让我们从下面的构造函数开始，它用于以下目的:

```
private final WordVectors wordVectors;
private final int batchSize;
private final int vectorSize;
private final int truncateLength;
private int cursor = 0;
private final File[] positiveFiles;
private final File[] negativeFiles;
private final TokenizerFactory tokenizerFactory;

public SentimentDatasetIterator(String dataDirectory, WordVectors wordVectors, 
                                 int batchSize, int truncateLength, boolean train) throws IOException {
  this.batchSize = batchSize;
  this.vectorSize = wordVectors.getWordVector(wordVectors.vocab().wordAtIndex(0)).length;
  File p = new File(FilenameUtils.concat(dataDirectory, "aclImdb/" + (train ? "train" : "test") 
                                         + "/pos/") + "/");
  File n = new File(FilenameUtils.concat(dataDirectory, "aclImdb/" + (train ? "train" : "test")
                                         + "/neg/") + "/");
  positiveFiles = p.listFiles();
  negativeFiles = n.listFiles();

  this.wordVectors = wordVectors;
  this.truncateLength = truncateLength;
  tokenizerFactory = new DefaultTokenizerFactory();
  tokenizerFactory.setTokenPreProcessor(new CommonPreprocessor());
}
```

在前面的构造函数签名中，我们使用了以下目的:

*   跟踪 IMDb 审查数据集目录中的正面和负面审查文件
*   将复习文本标记为去除了停用词和未知词的单词
*   如果最长回顾超过`truncateLength`，只取前`truncateLength`个字
*   Word2Vec 对象
*   批次大小，即用于训练的每个迷你批次的大小

初始化完成后，我们将每个回顾测试作为一个字符串加载。然后，我们在正面和负面评价之间交替:

```
List<String> reviews = new ArrayList<>(num);
boolean[] positive = newboolean[num];

for(int i=0; i<num && cursor<totalExamples(); i++ ){
  if(cursor % 2 == 0){
    //Load positive review
    int posReviewNumber = cursor / 2;
    String review = FileUtils.readFileToString(positiveFiles[posReviewNumber]);
    reviews.add(review);
    positive[i] = true;
  } else {
    //Load negative review
    int negReviewNumber = cursor / 2;
    String review = FileUtils.readFileToString(negativeFiles[negReviewNumber]);
    reviews.add(review);
    positive[i] = false;
  }
  cursor++;
}
```

然后，我们对评论进行标记化，过滤掉未知词(即未包含在预先训练好的 Word2Vec 模型中的词，例如停用词):

```
List<List<String>> allTokens = new ArrayList<>(reviews.size());
int maxLength = 0;

for(String s : reviews){
  List<String> tokens = tokenizerFactory.create(s).getTokens();
  List<String> tokensFiltered = new ArrayList<>();
 for(String t : tokens ){
 if(wordVectors.hasWord(t)) tokensFiltered.add(t);
  }
  allTokens.add(tokensFiltered);
  maxLength = Math.*max*(maxLength,tokensFiltered.size());
}
```

然后，如果最长的评论超过了阈值`truncateLength`，我们只取第一个`truncateLength`单词:

```
if(maxLength > truncateLength) 
    maxLength = truncateLength;
```

然后，我们为训练创建数据。这里，我们有`reviews.size()`个不同长度的例子，因为我们有两个标签，阳性或阴性:

```
INDArray features = Nd4j.create(newint[]{reviews.size(), vectorSize, maxLength}, 'f');
INDArray labels = Nd4j.create(newint[]{reviews.size(), 2, maxLength}, 'f');
```

现在，我们正在处理不同长度的检查，并且在最后的时间步只有一个输出，我们使用填充数组，其中对于该示例，如果在该时间步存在数据，掩码数组包含 1，如果数据只是填充，则包含 0:

```
INDArray featuresMask = Nd4j.*zeros*(reviews.size(), maxLength);
INDArray labelsMask = Nd4j.*zeros*(reviews.size(), maxLength);
```

应当注意，为特征和标签创建掩模阵列是可选的，也可以是空的。然后，我们获得第 *i* ^(th) 文档的截断序列长度，获得当前文档的所有单词向量，并转置它们以适合第二和第三特征形状。

一旦我们准备好了单词向量，我们将它们放入 features 数组的三个索引处，这相当于`NDArrayIndex.interval(0, vectorSize)`拥有 0 和当前序列长度之间的所有元素。然后，我们将 1 分配给特征出现的每个位置，即，在 0 和序列长度的间隔处。

现在，当谈到标签编码时，我们为负面评论文本设置[0，1]，为正面评论文本设置[1，0]。最后，我们指定在本例的最后一个时间步存在一个输出:

```
for( int i=0; i<reviews.size(); i++ ){
  List<String> tokens = allTokens.get(i);
  int seqLength = Math.min(tokens.size(), maxLength);
  final INDArray vectors = wordVectors.getWordVectors(tokens.subList(0, seqLength)).transpose();
  features.put(new INDArrayIndex[] {
      NDArrayIndex.point(i), NDArrayIndex.all(), NDArrayIndex.interval(0, seqLength)
    }, vectors);

  featuresMask.get(new INDArrayIndex[] {NDArrayIndex.point(i), NDArrayIndex.interval(0,      
                   seqLength)}).assign(1);
  int idx = (positive[i] ? 0 : 1);
  int lastIdx = Math.min(tokens.size(),maxLength);

  labels.putScalar(newint[]{i,idx,lastIdx-1},1.0);
  labelsMask.putScalar(newint[]{i,lastIdx-1},1.0);
}
```

请注意，阻碍 NLP 中丢失的主要问题是它不能应用于循环连接，因为聚合丢失掩码会随着时间的推移有效地清除嵌入，因此，在前面的代码块中使用了特征掩码。

现在，到此为止，所有必需的元素都已准备好，所以最后，我们将数据集作为一个包含特征、标签、`featuresMask`和`labelsMask`的`NDArray`(即 4D)返回:

```
return new DataSet(features,labels,featuresMask,labelsMask);
```

更详细地说，使用`DataSet`，我们将创建一个具有指定输入`INDArray`和标签(输出)`INDArray`的数据集，以及(可选的)用于特征和标签的掩码数组。

最后，我们的训练集将使用以下调用:

```
SentimentDatasetIterator train = new SentimentDatasetIterator(DATA_PATH, wordVectors, 
                                                              batchSize, truncateReviewsToLength, true);
```

太棒了。现在，我们可以在下一步中通过指定层和超参数来创建我们的神经网络。



# 网络构建、训练和保存模型

正如在泰坦尼克号生存预测一节中所讨论的，一切都从`MultiLayerConfiguration`开始，它组织那些层和它们的超参数。我们的 LSTM 网络由五层组成。输入层之后是三个 LSTM 层。然后，最后一层是 RNN 层，这也是输出层。

从技术上讲，第一层是输入层，然后放置三层作为 LSTM 层。对于 LSTM 层，我们使用 Xavier 初始化权重，使用 SGD 作为 Adam 更新器的优化算法，使用 Tanh 作为激活函数。最后，RNN 输出图层有一个 softmax 激活函数，它给出了类的概率分布(也就是说，它将总和输出到 1.0)和 MCXENT，它是多类交叉熵损失函数。

为了创建 LSTM 层，DL4J 提供了 LSTM 和`GravesLSTM`类。后者是一个 LSTM 循环网络，基于 Graves，但没有 CUDA 支持:用 RNN 监督序列标记(更多信息见[http://www.cs.toronto.edu/~graves/phd.pdf](http://www.cs.toronto.edu/~graves/phd.pdf))。现在，在我们开始创建网络之前，首先让我们定义所需的超参数，例如输入/隐藏/输出节点(即神经元)的数量:

```
// Network hyperparameters: Truncate reviews with length greater than this
static int truncateReviewsToLength = 30;
static int numEpochs = 10; // number of training epochs
static int batchSize = 64; //Number of examples in each minibatch
static int vectorSize = 300; //Size of word vectors in Google Word2Vec
static int seed = 12345; //Seed for reproducibility
static int numClasses = 2; // number of classes to be predicted
static int numHiddenNodes = 256;
```

我们现在将创建网络配置并进行网络培训。使用 DL4J，您可以通过调用`NeuralNetConfiguration.Builder()`上的层来添加层，指定它在层顺序中的位置(以下代码中的零索引层是输入层):

```
MultiLayerConfiguration LSTMconf = new NeuralNetConfiguration.Builder()
     .seed(seed)
     .updater(new Adam(1e-8)) // Gradient updater with Adam
     .l2(1e-5) // L2 regularization coefficient for weights
     .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
     .weightInit(WeightInit.XAVIER)
     .gradientNormalization(GradientNormalization.ClipElementWiseAbsoluteValue)
     .gradientNormalizationThreshold(1.0)     
     .trainingWorkspaceMode(WorkspaceMode.SEPARATE).inferenceWorkspaceMode(WorkspaceMode.SEPARATE)
     .list()
     .layer(0, new LSTM.Builder()
           .nIn(vectorSize)
           .nOut(numHiddenNodes)
           .activation(Activation.TANH)
           .build())
     .layer(1, new LSTM.Builder()
           .nIn(numHiddenNodes)
           .nOut(numHiddenNodes)
           .activation(Activation.TANH)
           .build())
     .layer(2, new RnnOutputLayer.Builder()
          .activation(Activation.SOFTMAX)
          .lossFunction(LossFunction.XENT)
          .nIn(numHiddenNodes)
          .nOut(numClasses)
          .build())
    .pretrain(false).backprop(true).build();
```

最后，我们还指定我们不需要做任何预训练(在深度信任网络或堆叠自编码器中通常需要)。然后，我们初始化网络，并在训练集上开始训练:

```
MultiLayerNetwork model = new MultiLayerNetwork(LSTMconf);
model.init();
```

通常，这种类型的网络有许多超参数。让我们打印网络中的参数数量(以及每一层):

```
Layer[] layers = model.getLayers();
int totalNumParams = 0;
for(int i=0; i<layers.length; i++ ){
  int nParams = layers[i].numParams();
  System.out.println("Number of parameters in layer " + i + ": " + nParams);
  totalNumParams += nParams;
}
System.out.println("Total number of network parameters: " + totalNumParams);

>>
 Number of parameters in layer 0: 570,368
 Number of parameters in layer 1: 525,312
 Number of parameters in layer 2: 514
 Total number of network parameters: 1,096,194
```

我说过，我们的网络有 100 万个参数，这是巨大的。这也给优化超参数带来了巨大的挑战。然而，我们将在 FAQ 部分看到一些技巧。

```
MultiLayerNetwork net = new MultiLayerNetwork(LSTMconf);
net.init();
net.setListeners(new ScoreIterationListener(1));
for (int i = 0; i < numEpochs; i++) {
  net.fit(train);
  train.reset();
  System.out.println("Epoch " + (i+1) + " finished ...");
}
System.out.println("Training has been completed");
```

一旦训练完成，我们可以保存训练好的模型，用于模型持久化和后续重用。为此，DL4J 使用来自`ModelSerializer`类的`writeModel()`方法为训练好的模型消毒提供支持。此外，它还提供了使用`restoreMultiLayerNetwork()`方法恢复已保存模型的功能。

我们将在接下来的步骤中看到更多。然而，我们也可以保存网络更新程序，即 momentum、RMSProp、Adagrad 等的状态:

```
File locationToSave = new File(modelPath); //location and file format
boolean saveUpdater = true; // we save the network updater too
ModelSerializer.writeModel(net, locationToSave, saveUpdater);
```



# 恢复训练好的模型并在测试集上对其进行评估

一旦培训完成，下一个任务将是评估模型。我们将在测试集上评估模型的性能。对于评估，我们将使用`Evaluation()`，它用两个可能的类创建一个评估对象。

首先，让我们对每个测试样本进行迭代评估，并从训练好的模型中获得网络预测。最后，`eval()`方法对照真实类检查预测:

```
public static void networkEvaluator() throws Exception {
      System.out.println("Starting the evaluation ...");
      boolean saveUpdater = true;

      //Load the model
      MultiLayerNetwork restoredModel = ModelSerializer.restoreMultiLayerNetwork(modelPath, saveUpdater);
      //WordVectors wordVectors = getWord2Vec();
      SentimentDatasetIterator test = new SentimentDatasetIterator(DATA_PATH, wordVectors, batchSize,   
                                                                   truncateReviewsToLength, false);
      Evaluation evaluation = restoredModel.evaluate(test);
      System.out.println(evaluation.stats());
      System.out.println("----- Evaluation completed! -----");
}

>>>
 ==========================Scores========================================
 # of classes: 2
 Accuracy: 0.8632
 Precision: 0.8632
 Recall: 0.8632
 F1 Score: 0.8634
 Precision, recall, and F1: Reported for positive class (class 1 -"negative") only
 ========================================================================
```

使用 LSTM 进行情感分析的预测准确率约为 87%,考虑到我们并未关注超参数调优，这已经很不错了！现在，让我们看看分类器如何预测每个类别:

```
Predictions labeled as positive classified by model as positive: 10,777 times
 Predictions labeled as positive classified by model as negative: 1,723 times
 Predictions labeled as negative classified by model as positive: 1,696 times
 Predictions labeled as negative classified by model as negative: 10,804 times
```

类似于[第 2 章](e27fb252-7892-4659-81e2-2289de8ce570.xhtml)、*使用循环类型网络的癌症类型预测*，我们现在将为这个二进制分类问题计算另一个称为马修斯相关系数的度量:

```
// Compute Matthews correlation coefficient
EvaluationAveraging averaging = EvaluationAveraging.*Macro*;
double MCC = eval.matthewsCorrelation(averaging);
System.*out*.println("Matthews correlation coefficient: "+ MCC);

>>
 Matthews's correlation coefficient: 0.22308172619187497
```

这显示了弱正关系，表明我们的模型表现得相当好。接下来，我们将使用训练好的模型进行推理，也就是说，我们将对示例评论文本执行预测。



# 对样本评论文本进行预测

现在，让我们看看我们训练的模型是如何概括的，也就是说，它是如何在情感标签句子数据集的看不见的评论文本上执行的。首先，我们需要从磁盘恢复训练模型:

```
System.*out*.println("Starting the evaluation on sample texts ...");
boolean saveUpdater = true;

MultiLayerNetwork restoredModel = ModelSerializer.*restoreMultiLayerNetwork*(*modelPath*, saveUpdater);
SentimentDatasetIterator test = new SentimentDatasetIterator(*DATA_PATH*, *wordvectors*, *batchSize*, 
                                                             *truncateReviewsToLength*, false);
```

现在，我们可以从 IMDb、亚马逊和 Yelp 中随机提取两个评论文本，其中第一个表达了积极的情绪，第二个表达了消极的情绪(根据已知的标签)。然后，我们可以创建一个包含评论字符串和标签的 HashMap:

```
String IMDb_PositiveReview = "Not only did it only confirm that the film would be unfunny and generic, but 
                              it also managed to give away the ENTIRE movie; and I'm not exaggerating - 
                              every moment, every plot point, every joke is told in the trailer";

String IMDb_NegativeReview = "One character is totally annoying with a voice that gives me the feeling of 
                              fingernails on a chalkboard.";

String Amazon_PositiveReview = "This phone is very fast with sending any kind of messages and web browsing 
                                is significantly faster than previous phones i have used";

String Amazon_NegativeReview = "The one big drawback of the MP3 player is that the buttons on the phone's 
                             front cover that let you pause and skip songs lock out after a few seconds.";

String Yelp_PositiveReview = "My side Greek salad with the Greek dressing was so tasty, and the pita and 
                              hummus was very refreshing.";

String Yelp_NegativeReview = "Hard to judge whether these sides were good because we were grossed out by 
                              the melted styrofoam and didn't want to eat it for fear of getting sick.";
```

然后，我们创建前面字符串的数组:

```
String[] reviews = {IMDb_PositiveReview, IMDb_NegativeReview, Amazon_PositiveReview, 
                    Amazon_NegativeReview, Yelp_PositiveReview, Yelp_NegativeReview};

String[] sentiments = {"Positive", "Negative", "Positive", "Negative", "Positive", "Negative"};
Map<String, String> reviewMap = new HashMap<String, String>();

reviewMap.put(reviews[0], sentiments[0]);
reviewMap.put(reviews[1], sentiments[1]);
reviewMap.put(reviews[2], sentiments[2]);
reviewMap.put(reviews[3], sentiments[3]);
```

然后，我们迭代映射，并使用预先训练的模型进行样本评估，如下所示:

```
System.out.println("Starting the evaluation on sample texts ...");         
for (Map.Entry<String, String> entry : reviewMap.entrySet()) {
            String text = entry.getKey();
            String label = entry.getValue();

            INDArray features = test.loadFeaturesFromString(text, truncateReviewsToLength);
            INDArray networkOutput = restoredModel.output(features);

            int timeSeriesLength = networkOutput.size(2);
            INDArray probabilitiesAtLastWord = networkOutput.get(NDArrayIndex.point(0), 
                              NDArrayIndex.all(), NDArrayIndex.point(timeSeriesLength - 1));

            System.out.println("-------------------------------");
            System.out.println("\n\nProbabilities at last time step: ");
            System.out.println("p(positive): " + probabilitiesAtLastWord.getDouble(0));
            System.out.println("p(negative): " + probabilitiesAtLastWord.getDouble(1));

            Boolean flag = false;
            if(probabilitiesAtLastWord.getDouble(0) > probabilitiesAtLastWord.getDouble(1))
                flag = true;
            else
                flag = false;
            if (flag == true) {
                System.out.println("The text express a positive sentiment, actually it is " + label);
            } else {
                System.out.println("The text express a negative sentiment, actually it is " + label);
            }
        }
    System.out.println("----- Sample evaluation completed! -----");
    }
```

如果仔细观察前面的代码块，您会发现我们通过提取特征将每个评论文本转换为一个时间序列。然后，我们计算网络输出(即概率)。然后，我们比较概率，也就是说，如果概率是它是一个积极的情绪，我们设置一个标志为真，否则为假。这样，我们就可以对最终的类预测做出决定。

我们还在前面的代码块中使用了`loadFeaturesFromString()`方法，该方法将评论字符串转换为`INDArray`格式的特性。它需要两个参数，`reviewContents`和`maxLength`，前者是要矢量化的评论内容，后者是评论文本的最大长度。最后，它为给定的输入字符串返回一个`features`数组:

```
public INDArray loadFeaturesFromString(String reviewContents, int maxLength){
        List<String> tokens = tokenizerFactory.create(reviewContents).getTokens();
        List<String> tokensFiltered = new ArrayList<>();
        for(String t : tokens ){
            if(wordVectors.hasWord(t)) tokensFiltered.add(t);
        }
        int outputLength = Math.max(maxLength,tokensFiltered.size());
        INDArray features = Nd4j.create(1, vectorSize, outputLength);

        for(int j=0; j<tokens.size() && j<maxLength; j++ ){
            String token = tokens.get(j);
            INDArray vector = wordVectors.getWordVectorMatrix(token);
            features.put(new INDArrayIndex[]{NDArrayIndex.point(0), 
                          NDArrayIndex.all(), NDArrayIndex.point(j)}, vector);
        }
        return features;
    }
```

如果不想截断，就直接用`Integer.MAX_VALUE`。

现在，让我们回到最初的讨论。有趣的是，我们让它更人性化，也就是说，没有使用激活功能。最后，我们打印每个评论文本及其相关标签的结果:

```
> Probabilities at last time step:
 p(positive): 0.003569001331925392
 p(negative): 0.9964309930801392
 The text express a negative sentiment, actually, it is Positive

p(positive): 0.003569058608263731
 p(negative): 0.9964308738708496
 The text express a negative sentiment, actually, it is Negative
 -------------------------------
 Probabilities at last time step:
 p(positive): 0.003569077467545867
 p(negative): 0.9964308738708496
 The text express a negative sentiment, actually, it is Negative

p(positive): 0.003569045104086399
 p(negative): 0.9964308738708496
 The text express a negative sentiment, actually, it is Positive
 -------------------------------
 Probabilities at last time step:
 p(positive): 0.003570008557289839
 p(negative): 0.996429979801178
 The text express a negative sentiment, actually, it is Positive

p(positive): 0.0035690285731106997
 p(negative): 0.9964309930801392
 The text express a negative sentiment, actually, it is Negative

----- Sample evaluation completed! -----
```

因此，我们训练过的模型做出了 50%的错误预测，特别是因为它总是将正面的评论预测为负面的。简而言之，它并不那么擅长推广到未知文本，可以看到 50%的准确率。

现在，一个愚蠢的问题可能会浮现在脑海里。我们的网络是否不足？有什么方法可以观察训练进行的怎么样？换句话说，问题应该是:为什么我们的 LSTM 神经网络没有表现出更高的准确性？我们将在下一节尝试回答这些问题。所以和我在一起吧！



# 常见问题(FAQ)

既然我们已经以可接受的准确度解决了情感分析问题，那么还需要考虑该问题的其他实际方面和整体深度学习现象。在这一部分，我们将看到一些你可能已经想到的常见问题。这些问题的答案可以在附录 A 中找到:

1.  我理解使用 LSTM 的情绪分析的预测准确性仍然是合理的。然而，它在情感标签数据集上表现不佳。我们的网络超载了吗？有什么方法可以观察训练进行的怎么样？
2.  考虑到大量的复习文本，我们可以在 GPU 上进行训练吗？
3.  关于问题 2，我们甚至可以使用 Spark 完成整个过程吗？
4.  从哪里可以获得更多用于情感分析的训练数据集？
5.  不需要手动下载`.zip`格式的训练数据，可以用`extractTarGz()`的方法吗？
6.  我的机器内存有限。给我一点关于 DL4J 中内存管理和垃圾收集如何工作的线索。



# 摘要

在这一章中，我们已经看到了如何实施和部署一个实践深度学习项目，该项目根据评论文本包含的单词将它们分类为正面或负面。我们使用了包含 50，000 条评论的大规模电影评论数据集(训练加测试)。使用 Word2Vec(即 NLP 中广泛使用的单词嵌入技术)和 LSTM 网络进行建模的组合方法被应用:预训练的 Google 新闻向量模型被用作神经单词嵌入。

然后，训练向量和标签一起被输入 LSTM 网络，该网络成功地将它们分类为消极情绪或积极情绪。然后，在测试集上对训练好的模型进行评估。此外，我们还了解了如何应用基于文本的预处理技术，如 tokenizer、停用词移除和 TF-IDF，以及 DL4J 中的单词嵌入操作。

在下一章，我们将看到一个完整的例子，如何开发一个深度学习项目，使用 DL4J 迁移学习 API 对图像进行分类。通过该应用，用户将能够修改现有模型的架构，微调现有模型的学习配置，并在训练期间保持指定层的参数不变，这也称为冻结。



# 问题的答案

**对问题 1** 的回答:我们已经看到，我们训练的模型在测试集上表现得相当好，准确率为 87%。现在，如果我们从下图中看到模型与迭代得分和其他参数，那么我们可以看到我们的模型没有过度拟合:

![](assets/45849f1d-1af8-415e-ba55-2557cb8acf40.png)

LSTM 情感分析器的模型与迭代得分和其他参数

现在，对于情绪标记的句子，训练的模型表现不佳。这可能有几个原因。例如，我们的模型只使用电影评论数据集进行训练，但在这里，我们试图强制我们的模型也在不同类型的数据集上执行，例如，Amazon 和 Yelp。然而，我们没有仔细调整超参数。

**问题 2 答案**:可以，其实这样会很有帮助。为此，我们必须确保我们的编程环境已经准备就绪。换句话说，首先，我们必须在我们的机器上配置 CUDA 和 cuDNN。

但是，请确保您的机器安装了 NVIDIA GPU，并配置了足够的内存和 CUDA 计算能力。如果您不知道如何配置这些先决条件，请参考位于[https://docs.nvidia.com/deeplearning/sdk/cudnn-install/](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/)的 URL。一旦您的机器安装了 CUDA/cuDNN，在`pom.xml`文件中，您必须添加两个条目:

*   项目属性中的后端
*   CUDA 作为平台依赖

对于步骤 1，属性现在应该如下所示:

```
<properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <java.version>1.8</java.version>
        <nd4j.backend>nd4j-cuda-9.0-platform</nd4j.backend>
        <nd4j.version>1.0.0-alpha</nd4j.version>
        <dl4j.version>1.0.0-alpha</dl4j.version>
        <datavec.version>1.0.0-alpha</datavec.version>
        <arbiter.version>1.0.0-alpha</arbiter.version>
        <logback.version>1.2.3</logback.version>
 </properties>
```

现在，对于第 2 步，在`pop.xml`文件中添加以下依赖项(即，在依赖项标记内):

```
<dependency>
         <groupId>org.nd4j</groupId>
         <artifactId>nd4j-cuda-9.0-platform</artifactId>
         <version>${nd4j.version}</version>
</dependency>
```

然后，更新 Maven 项目，所需的依赖项就会自动下载。现在，除非我们在多个 GPU 上执行训练，否则我们不需要进行任何更改。但是，只需再次运行相同的脚本来执行培训。然后，您将在控制台上看到以下日志:

```
17:03:55.317 [main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [JCublasBackend] backend
 17:03:55.360 [main] WARN org.reflections.Reflections - given scan urls are empty. set urls in the configuration
 17:04:06.410 [main] INFO org.nd4j.nativeblas.NativeOpsHolder - Number of threads used for NativeOps: 32
 17:04:08.118 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [18] to device [0], out of [1] devices...
 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [19] to device [0], out of [1] devices...
 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [20] to device [0], out of [1] devices...
 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [21] to device [0], out of [1] devices...
 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [22] to device [0], out of [1] devices...
 17:04:08.119 [main] DEBUG org.nd4j.jita.concurrency.CudaAffinityManager - Manually mapping thread [23] to device [0], out of [1] devices...
 17:04:08.123 [main] INFO org.nd4j.nativeblas.Nd4jBlas - Number of threads used for BLAS: 0
 17:04:08.127 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Backend used: [CUDA]; OS: [Windows 10]
 17:04:08.127 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Cores: [8]; Memory: [7.0GB];
 17:04:08.127 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [CUBLAS]
 17:04:08.127 [main] INFO org.nd4j.linalg.jcublas.ops.executioner.CudaExecutioner - Device opName: [GeForce GTX 1050]; CC: [6.1]; Total/free memory: [4294967296]
```

然而，在第八章、*的[中，分布式深度学习——使用卷积 LSTM 网络的视频分类](a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml)*，我们将看到如何在多个 GPU 上让一切更快、更可扩展。

**Aswer**对问题 3 :是的，其实这样会很有帮助。为此，我们必须确保我们的编程环境已经准备就绪。换句话说，首先，我们必须在我们的机器上配置 Spark。一旦你的机器安装了 CUDA/cuDNN，我们只需要配置 Spark。在`pom.xml`文件中，您必须添加两个条目:

*   项目属性中的后端
*   火花依赖性

对于步骤 1，属性现在应该如下所示:

```
<properties>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
        <java.version>1.8</java.version>
        <nd4j.backend>nd4j-cuda-9.0-platform</nd4j.backend>
        <nd4j.version>1.0.0-alpha</nd4j.version>
        <dl4j.version>1.0.0-alpha</dl4j.version>
        <datavec.version>1.0.0-alpha</datavec.version>
        <arbiter.version>1.0.0-alpha</arbiter.version>
        <dl4j.spark.version>1.0.0-alpha_spark_2</dl4j.spark.version>
        <logback.version>1.2.3</logback.version>
 </properties>
```

现在，对于第 2 步，在`pop.xml`文件中添加以下依赖项(即，在依赖项标记内):

```
<dependency>
      <groupId>org.deeplearning4j</groupId>
      <artifactId>dl4j-spark_2.11</artifactId>
      <version>1.0.0-alpha_spark_2</version>
</dependency>
```

然后，更新 Maven 项目，所需的依赖项就会自动下载。现在，除非我们在多个 GPU 上执行训练，否则我们不需要进行任何更改。然而，我们需要将训练/测试数据集转换成 Spark 兼容的 JavaRDD。

我已经在`SentimentAnalyzerSparkGPU.java`文件中写了所有的步骤，可以用来查看整个步骤是如何工作的。一个常见的警告是，如果您在 Spark 上执行培训，DL4J UI 将无法工作，因为它与 Jackson 库存在交叉依赖。为此，我们必须首先使用`sparkSession()`方法创建`JavaSparkContext`，如下所示:

```
public static JavaSparkContext *spark*;
static int *batchSizePerWorker* = 16;

public static JavaSparkContext getJavaSparkContext () {
                    SparkConf sparkConf = new SparkConf();
                    sparkConf.set("spark.locality.wait", "0");
                    sparkConf.setMaster("local[*]").setAppName("DL4J Spark");
 *spak* = new JavaSparkContext(sparkConf);
 return *spark*;
}
```

然后，我们必须将情感训练数据集迭代器转换为数据集的 JavaRDD。首先，我们创建一个数据集列表，然后将每个训练样本添加到列表中，如下所示:

```
List<DataSet> trainDataList = new ArrayList<>();
while(train.hasNext()) {
       trainDataList.add(train.next());
    }
```

然后，我们通过调用`sparkSession()`方法创建一个`JavaSparkContext`，如下所示:

```
spark = createJavaSparkContext();
```

最后，我们利用 Spark 的`parallelize()`方法来创建数据集的 JavaRDD，然后使用 Spark 执行训练:

```
JavaRDD<DataSet> trainData = *spark*.parallelize(trainDataList);
```

然后，Spark `TrainingMaster`使用`ParameterAveragingTrainingMaster`，这有助于使用 Spark 执行训练。请参见[第八章](a59fb1f2-b585-44f5-a467-903b8c25867b.xhtml)、*分布式深度学习——使用卷积 LSTM 网络的视频分类*，了解更多详情:

```
TrainingMaster<?, ?> tm = (TrainingMaster<?, ?>) new ParameterAveragingTrainingMaster
               .Builder(*batchSizePerWorker*)             
               .averagingFrequency(5).workerPrefetchNumBatches(2)
               .batchSizePerWorker(*batchSizePerWorker*).build();
```

然后，我们创建`SparkDl4jMultiLayer`,而不是像之前那样只创建`MultilayerNetwork`:

```
SparkDl4jMultiLayer sparkNet = new SparkDl4jMultiLayer(*spark*, LSTMconf, tm);
```

然后，我们创建一个训练监听器，它记录每个迭代的分数，如下所示:

```
sparkNet.setListeners(Collections.<IterationListener>*singletonList*(new ScoreIterationListener(1)));
sparkNet.setListeners(new ScoreIterationListener(1));
```

最后，我们按如下方式开始培训:

```
for (int i = 0; i < *numEpochs*; i++) {
         sparkNet.fit(trainData);
         System.*out*.println("Epoch " + (i+1) + " has been finished ...");
       }
```

然而，使用这种方法有一个缺点，即我们不能像这样直接保存训练过的模型，但首先，我们必须使用训练数据拟合网络，并收集输出作为如下的`MultiLayerNetwork`:

```
MultiLayerNetwork outputNetwork = sparkNet.fit(trainData);

//Save the model
File locationToSave = new File(*modelPath*);

boolean saveUpdater = true;
ModelSerializer.*writeModel*(outputNetwork, locationToSave, saveUpdater);
```

**问题 4 答案**:有很多来源可以让你得到一个情感分析数据集。下面列出了其中的一些:

*   来自谷歌的巨大的 n-grams 数据集:[storage.googleapis.com/books/ngrams/books/datasetsv2.html](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)
*   推特情绪:[http://www.sananalytics.com/lab/twitter-sentiment/](http://www.sananalytics.com/lab/twitter-sentiment/)
*   UMICH si 650—ka ggle 上的情感分类数据集:[http://inclass.kaggle.com/c/si650winter11/data](http://inclass.kaggle.com/c/si650winter11/data)
*   多领域情感数据集:[http://www.cs.jhu.edu/~mdredze/datasets/sentiment/](http://www.cs.jhu.edu/~mdredze/datasets/sentiment/)

**对问题 5** 的回答:答案是否定的，但只要稍加努力，我们就能让它运转起来。为此，我们可以使用 Apache commons 中的`ZipArchiveInputStream`和`GzipCompressorInputStream`类，如下所示:

```
public static void extractZipFile(String inputPath, String outputPath) 
               throws IOException { if (inputPath == null || outputPath == null)
 return;
 final int bufferSize = 4096;
 if (!outputPath.endsWith("" + File.*separatorChar*))
                            outputPath = outputPath + File.*separatorChar*; 
 try (ZipArchiveInputStream tais = new ZipArchiveInputStream(new 
                         GzipCompressorInputStream(
                             new BufferedInputStream(new FileInputStream(inputPath))))) {
                             ZipArchiveEntry entry;
 while ((entry = (ZipArchiveEntry) tais.getNextEntry()) != null) {
 if (entry.isDirectory()) {
 new File(outputPath + entry.getName()).mkdirs();
                               } else {
                                int count; 
                                byte data[] = newbyte[bufferSize];
                                FileOutputStream fos = new FileOutputStream(outputPath + entry.getName());
                                BufferedOutputStream dest = new BufferedOutputStream(fos, bufferSize);
                                while ((count = tais.read(data, 0, bufferSize)) != -1) {
                                       dest.write(data, 0, count);
                                       }
                            dest.close();
                       }
                 }
           }
}
```

第六个问题的答案:如果你的机器没有足够的内存，这确实是一个问题。对于这个应用程序，我在运行这个项目时没有遇到任何 OOP 类型的问题，因为我的笔记本电脑有 32 GB 的 RAM。

除了这一步，我们还可以选择 DL4J 垃圾收集，特别是因为内存在您这一端受到限制。DL4J 提供了一个名为`getMemoryManager()`的方法，该方法为低级内存管理返回一个后端特定的`MemoryManager`实现。此外，我们必须启用周期性的`System.gc()`调用，使窗口在调用之间的时间最短，以毫秒为单位。让我们看一个例子:

```
Nd4j.getMemoryManager().setAutoGcWindow(10000); // min 10s between calls
```

但是，只需将`windowMillis`设置为`0`即可禁用该选项。