

# 九、使用深度强化学习玩 GridWorld 游戏

作为人类，我们从经验中学习。我们不是一夜之间或偶然变得如此迷人的。多年的赞美和批评都帮助塑造了今天的我们。我们通过尝试不同的肌肉运动来学习如何骑自行车，直到它发出咔嗒声。当你执行动作时，你有时会立即得到回报，这就是所谓的**强化学习** **(** **RL** **)。**

这一章是关于设计一个由批评和奖励驱动的机器学习系统。我们将看到如何使用**deep learning 4j**(**DL4J**)、**强化学习 4j** ( **RL4J** )和充当 Q 函数的神经 Q 学习来开发一个演示 GridWorld 游戏。我们将从强化学习及其理论背景开始，这样概念更容易掌握。总之，本章将涵盖以下主题:

*   强化学习中的符号、策略和效用
*   深度 Q 学习算法
*   使用深度 Q 学习开发 GridWorld 游戏
*   常见问题(FAQ)



# RL 的符号、策略和实用程序

监督学习和非监督学习出现在光谱的两端，而 RL 存在于中间。它不是监督学习，因为训练数据来自于在探索和利用之间做出决定的算法。

此外，它不是无人监督的，因为算法接收来自环境的反馈。只要你处于在一种状态下执行一个动作会产生奖励的情况下，你就可以使用强化学习来发现一个好的动作序列，以获得最大的预期奖励。RL 代理的目标是最大化它最终获得的总回报。第三个主要的子元素是价值函数。

虽然奖励决定了状态的即时可取性，但值指示了状态的长期可取性，考虑了可能跟随的状态和在这些状态中可用的奖励。价值函数是相对于所选择的策略而指定的。在学习阶段，代理尝试确定具有最高值的状态的动作，因为这些动作最终将获得最好的奖励。

强化学习技术正在许多领域得到应用。目前正在追求的一个普遍想法是创建一个除了任务描述之外不需要任何东西的算法。当这种性能实现后，它将被应用到几乎所有地方。



# 强化学习中的符号

你可能会注意到，强化学习的行话包括将算法具体化为在某些情况下采取行动以获得奖励。事实上，算法通常被称为与环境一起行动的代理。

你可以认为它是一个智能硬件代理，通过传感器进行感知，并使用其执行器与环境进行交互。因此，许多 RL 理论应用于机器人领域并不令人惊讶。现在，为了进一步延长我们的讨论，我们需要了解一些术语:

*   **环境**:这是一个具有多种状态和状态间转换机制的系统。例如，对于玩 GridWorld 游戏的代理来说，环境就是网格空间本身，它定义了代理达到目标的状态和奖励方式。
*   代理人:这是一个与环境互动的自治系统。例如，在我们的 GridWorld 游戏中，代理就是玩家。
*   **状态**:环境中的状态是一组完整描述环境的变量。
*   **目标**:也是一个州，提供比其他任何州都高的折扣累积奖励。对于我们的 GridWorld 游戏，目标是玩家希望最终达到的状态，但要通过积累尽可能高的奖励。
*   **动作**:动作定义了不同状态之间的转换。因此，在执行一个动作时，代理可以从环境中得到奖励或惩罚。
*   **Policy** :它定义了一组规则，这些规则基于要为环境中的给定状态执行的动作。
*   **奖励**:这是一个正或负的量(即分数)，分别表示好的和坏的动作/移动。最终，学习目标是达到分数最高的目标(奖励)。这样，奖励本质上就是对代理的训练。
*   **插曲(又称试炼)**:这是从初始状态(即一个代理人的位置)达到目标状态所必需的步数。

我们将在本节稍后讨论更多关于策略和效用的内容。下图展示了状态、行动和奖励之间的相互作用。如果在状态*s[1]开始，可以执行动作*a[1]获得奖励 *r (s [1] ，a [1] )* 。箭头表示动作，状态由圆圈表示:**

![](assets/9ff64270-027c-4839-b06b-01795d189ff3.png)

当一个代理执行一个动作时，国家会产生一个奖励

机器人执行动作以在不同状态之间改变。但是它如何决定采取哪种行动呢？嗯，这是关于使用不同的或具体的政策。



# 政策

在强化学习中，策略是一组规则或策略。因此，学习的结果之一是发现一个好的策略，观察每个状态下行动的长期后果。因此，从技术上讲，策略定义了在给定状态下要采取的操作。下图显示了给定任何状态下的最佳操作:

![](assets/2762d760-cfcf-4ac5-9d6b-fba6784b50fd.png)

策略定义了在给定状态下要采取的动作

短期后果很容易计算:这只是回报。尽管执行一个行为会产生直接的回报，但贪婪地选择这个行为并不总是一个好主意。根据您的 RL 问题公式，可能有不同类型的策略，如下所述:

*   当一个代理总是试图通过执行一个动作来获得最高的即时回报时，我们称之为**贪婪策略**。
*   如果任意执行一个动作，则该策略称为**随机策略**。
*   当神经网络通过反向传播和来自环境的显式反馈更新权重来学习挑选动作的策略时，我们将此称为**策略梯度**。

如果我们想提出一个健壮的 a 策略来解决 RL 问题，我们必须找到一个比随机的和贪婪的策略表现更好的最优策略。在这一章中，我们将看到为什么政策梯度更直接和乐观。



# 效用

长期回报是**效用**。为了决定采取哪一个行动，代理可以贪婪地选择产生最高效用的行动。在状态 *s* 执行动作 *a* 的效用被写成函数 *Q(s，a)* ，称为效用函数。效用函数根据由状态和行动组成的输入生成的最优策略预测即时和最终奖励，如下图所示:

![](assets/b01b6793-f371-4a12-96a4-7a9f74c661e4.png)

使用效用函数



# 神经 Q 学习

大多数强化学习算法可以归结为三个主要步骤:推断、执行和学习。在第一步中，该算法使用迄今为止的知识选择给定状态 *s* 中的最佳动作 *a* 。接下来，它执行一个动作来找到奖励 *r* 以及下一个状态*s’*。

然后它利用新获得的知识 *(s，r，a，s’)*提高对世界的理解。使用 QLearning 算法可以更好地制定这些步骤，这或多或少是深度强化学习的核心。



# QLearning 简介

用 *(s，r，a，s’)*计算获得的知识，只是一种计算效用的幼稚方式。因此，我们需要找到一种更健壮的方法来计算它，以便我们通过递归地考虑未来行动的效用来计算特定状态-行动对 *(s，a)* 的效用。你当前行动的效用不仅受到直接回报的影响，还受到下一个最佳行动的影响，如下式所示，称为 **Q 函数**:

![](assets/54fe0db0-e6ab-426a-a24a-fbe697d49bcf.png)

上式中，*s’*表示下一个状态，*a’*表示下一个动作，在状态 *s* 下采取动作 *a* 的奖励用 *r(s，a)表示。*而*，* γ是一个超参数，称为**贴现因子**。如果 *γ* 是 *0* ，那么代理人选择一个特定的行动来最大化即时报酬。 *γ* 的值越高，代理人越重视考虑长期后果。

实际上，我们需要考虑更多这样的超参数。例如，如果期望真空吸尘器机器人快速学习解决任务，但不一定是最优的，我们可能希望设置更快的学习速率。

或者，如果允许机器人有更多的时间探索和利用，我们可能会降低学习速度。让我们称学习率 *α* ，并如下改变我们的效用函数(注意当 *α = 1* 时，两个方程是相同的):

![](assets/52146aac-6bcb-412c-b6f9-b5d38208f74a.jpg)

总之，如果我们知道这个 *Q(s，a)* 函数，就可以解决一个 RL 问题。这促使研究人员提出了一种更先进的 **QLearning** 算法，称为**神经 QLearning** ，这是一种用于计算状态-动作值的算法。它属于**时间差** ( **TD** )算法的类别，这表明所采取的行动和收到的奖励之间存在时间差。



# 作为 Q 函数的神经网络

现在我们知道了状态和要执行的操作。然而，`QLearning`代理需要知道表单的搜索空间(states x actions)。下一步是创建图或搜索空间，它是负责任何状态序列的容器。`QLSpace`类定义了`QLearning`算法的搜索空间(状态 x 动作)，如下图所示:

![](assets/20fae0f1-19b4-47ff-a3f5-b8a228aec422.png)

带 QLData 的状态转移矩阵(Q 值、回报、概率)

具有状态和动作列表的终端用户可以提供搜索空间。或者，通过提供状态数，采用以下参数，自动创建它:

*   **状态**:在 Q 学习搜索空间中定义的所有可能状态的序列
*   **目标**:作为目标的状态的标识符列表

然而，这种搜索空间(或查找表)的经典符号有时效率不高；和大多数有趣的问题一样，我们的状态-动作空间太大了，无法存储在一个表中，例如,*吃豆人*游戏。相反，我们无论如何都需要在状态之间进行归纳和模式匹配。换句话说，我们需要我们的 Q 学习算法说，*这种状态的值是 X* 而不是说，*这种精确的，超特定的状态的值是 X* 。

在这里，基于神经网络的 Q-learning 可以用来代替查找表作为我们的 *Q* ( *s* ， *a* )，从而它接受状态 *s* 和动作 *a* ，并吐出该状态-动作的值。然而，正如我前面提到的，一个神经网络有时有数百万个相关的参数。这些是重量。所以我们的 *Q* 函数其实看起来就像 *Q* ( *s* ， *a* ， *θ* )，其中 *θ* 是一个参数向量。

我们将迭代更新我们的神经网络的 *θ* 参数，而不是迭代更新表中的值，以便它学习为我们提供状态-动作值的更好估计。顺便说一下，我们可以使用梯度下降(反向传播)来训练这样一个深度 Q 学习网络，就像任何其他神经网络一样。

例如，如果状态(搜索空间)由图像表示，神经网络可以对代理可能采取的行动进行排序，从而可以预测可能的奖励。比如向左跑得五分，向上跳得七分，向下跳得两分，向左跑得零分。

![](assets/2dd9f59b-8fc8-4f23-a414-c19e938a5bc4.png)

使用神经网络进行基于强化学习的游戏

为了实现这一点，我们不需要为每一个动作向前运行我们的网络，我们只需要得到*max Q*(*s*′，*a*′)，即在新状态*s’*中每一个可能动作的 *max Q* 值，就可以向前运行它。

我们将看到如何使用 DL4J 的`MultiLayerNetwork`和`MultiLayerConfiguration`配置创建这样的深度 Q 学习网络。因此，神经网络将充当我们的 Q *-* 函数。既然我们已经对 RL 和 Q-learning 有了最基本的理论了解，那么是时候开始编码了。



# 使用深度 Q 网络开发 GridWorld 游戏

我们现在将开始潜入**深度 Q 网** ( **DQN** )来训练一个代理玩 GridWorld，这是一个简单的基于文本的游戏。有一个 4 x 4 的瓷砖网格，放置了四个对象。有代理人(玩家)，有坑，有球门，有墙。

![](assets/109cb766-7ba7-4c1d-b8b6-4d53282de1e5.png)

GridWorld 项目结构

该项目的结构如下:

*   `DeepQNetwork.java`:为 DQN 提供参考架构
*   `Replay.java`:为 DQN 生成重放记忆，以确保深层网络的梯度稳定且不会跨情节发散
*   用于训练 DQN 和玩游戏的主职业。

顺便说一下，我们在 GPU 和 cuDNN 上进行训练，以加快收敛速度。但是，如果您的机器没有 GPU，也可以随意使用 CPU 后端。



# 生成网格

我们将开发一个简单的游戏，每次都以完全相同的方式初始化一个网格。游戏从代理( *A* )、球门( *+* )、坑(-)、墙( *W* )开始。在每个游戏中，所有的元素都被随机放置在网格上。这使得 Q-learning 只需要学习如何将代理从已知的起始位置移动到已知的目标，而不会撞到坑，这给出了负的回报。看看这张截图:

![](assets/a59c6f73-11ff-4199-8fd7-25476b7e85b0.png)

GridWorld 游戏的网格显示了各种元素(即代理、目标、坑和墙)

简而言之，游戏的目标是达到目标，代理人将获得数字奖励。为简单起见，我们将避免一个坑；如果代理落在坑上，它会受到负奖励的惩罚。

这堵墙也可以阻挡代理的路径，但它不提供奖励或惩罚，所以我们是安全的。由于这是定义状态的一种简单方式，代理可以进行以下移动(即动作):

*   向上
*   向下
*   左边的
*   对吧

这样，一个动作*一个*可以定义如下:`a ∈ A {up, down, left, right}`。现在，根据前面的假设，让我们看看网格是什么样子的:

```
// Generate the GridMap
int size = 4;
float[][] generateGridMap() {
        int agent = rand.nextInt(size * size);
        int goal = rand.nextInt(size * size);

        while(goal == agent)
            goal = rand.nextInt(size * size);
        float[][] map = new float[size][size];

        for(int i = 0; i < size * size; i++)
            map[i / size][i % size] = 0;
        map[goal / size][goal % size] = -1;
        map[agent / size][agent % size] = 1;

        return map;
    }
```

一旦构建了网格，就可以按如下方式打印:

```
void printGrid(float[][] Map) {
        for(int x = 0; x < size; x++) {
            for(int y = 0; y < size; y++) {
                System.out.print((int) Map[x][y]);
            }
            System.out.println(" ");
        }
        System.out.println(" ");
    }
```



# 计算座席代表和目标位置

现在代理的搜索空间已经准备好了。那么我们来计算一下代理的初始位置和目标。首先，我们计算代理在网格中的初始位置，如下所示:

```
// Calculate the position of agent
int calcAgentPos(float[][] Map) {
        int x = -1;
        for(int i = 0; i < size * size; i++) {
            if(Map[i / size][i % size] == 1)
                return i;
        }
        return x;
    }
```

然后我们计算目标的位置，如下所示:

```
// Calculate the position of goal. The method takes the grid space as input
int calcGoalPos(float[][] Map) {
        int x = -1;// start from the initial position

        // Then we loop over the grid size say 4x4 times
        for(int i = 0; i < size * size; i++) {
            // If the mapped position is the initial position, we update the position 
            if(Map[i / size][i % size] == -1)
                return i;
        }
        return x; // agent cannot move to any other cell
    }
```

现在，可以将生成的网格视为四个独立的网格平面，其中每个平面代表每个元素的位置。在下图中，代理的当前网格位置是(3，0)，墙在(0，0)，坑在(0，1)，目标在(1，0)，这也意味着所有其他元素都是 0:

![](assets/826ed961-0272-49c2-9a9c-d8bd9a35bd10.png)

生成的网格可以被视为四个独立的网格平面

因此，我们开发了网格，使得一些对象在相同的 *x* 、 *y* 位置(但不同的 *z* 位置)包含一个 *1* ，这表明它们在网格上的相同位置。



# 计算动作掩码

这里，我们将所有输出设置为 0，除了我们实际看到的动作的输出，这样网络将其输出乘以对应于独热码编码动作的掩码。然后，我们可以将 0 作为所有未知动作的目标，这样我们的神经网络应该会运行良好。当我们想要预测所有动作时，我们可以简单地传递一个全 1 的掩码:

```
// Get action mask
int[] getActionMask(float[][] CurrMap) {
        int retVal[] = { 1, 1, 1, 1 };

        int agent = calcAgentPos(CurrMap); //agent current position
        if(agent < size) // if agent's current pos is less than 4, action mask is set to 0
            retVal[0] = 0;
        if(agent >= (size * size - size)) // if agent's current pos is 12, we set action mask to 0 too
            retVal[1] = 0;
        if(agent % size == 0) // if agent's current pos is 0 or 4, we set action mask to 0 too
            retVal[2] = 0;
        if(agent % size == (size - 1))// if agent's current pos is 7/11/15, we set action mask to 0 too
            retVal[3] = 0;

        return retVal; // finally, we return the updated action mask. 
    }
```



# 提供指导行动

现在代理的行动计划是已知的。下一个任务是为代理从当前位置向目标移动提供一些指导。例如，并不是所有的动作都是准确的，也就是一个糟糕的举动:

```
// Show guidance move to agent 
float[][] doMove(float[][] CurrMap, int action) {
        float nextMap[][] = new float[size][size];
        for(int i = 0; i < size * size; i++)
            nextMap[i / size][i % size] = CurrMap[i / size][i % size];

        int agent = calcAgentPos(CurrMap);
        nextMap[agent / size][agent % size] = 0;

        if(action == 0) {
            if(agent - size >= 0)
                nextMap[(agent - size) / size][agent % size] = 1;
            else {
                System.out.println("Bad Move");
                System.exit(0);
            }
        } else if(action == 1) {
            if(agent + size < size * size)
                nextMap[(agent + size) / size][agent % size] = 1;
            else {
                System.out.println("Bad Move");
                System.exit(0);
            }
        } else if (action == 2) {
            if((agent % size) - 1 >= 0)
                nextMap[agent / size][(agent % size) - 1] = 1;
            else {
                System.out.println("Bad Move");
                System.exit(0);
            }
        } else if(action == 3) {
            if((agent % size) + 1 < size)
                nextMap[agent / size][(agent % size) + 1] = 1;
            else {
                System.out.println("Bad Move");
                System.exit(0);
            }
        }
        return nextMap;
    }
```

在前面的代码块中，我们对动作编码如下:`0`向上，`1`向下，`2`向左，`3`向右。否则，我们会将该行为视为一个错误的举动，因此该代理会受到处罚。



# 计算奖励

既然已经为代理提供了一些指导——强化——下一个任务是计算代理所做的每个动作的奖励。看一下这段代码:

```
// Compute reward for an action 
float calcReward(float[][] CurrMap, float[][] NextMap) {
        int newGoal = calcGoalPos(NextMap);// first, we calculate goal position for each map
        if(newGoal == -1) // if goal position is the initial position (i.e. no move)
            return (size * size + 1); // we reward the agent to 4*4+ 1 = 17 (i.e. maximum reward)
        return -1f; // else we reward -1.0 for each bad move 
    }
```



# 输入图层的拼合输入

然后，我们需要将网络的输出转换成 1D 特征向量，以供 DQN 使用。这种扁平化得到了网络的输出；它展平其所有结构，以创建供密集层使用的单个长特征向量。看一下这段代码:

```
INDArray flattenInput(int TimeStep) {
        float flattenedInput[] = new float[size * size * 2 + 1];

        for(int a = 0; a < size; a++) {
            for(int b = 0; b < size; b++) {
                if(FrameBuffer[a][b] == -1)
                    flattenedInput[a * size + b] = 1;
                else
                    flattenedInput[a * size + b] = 0;
                if(FrameBuffer[a][b] == 1)
                    flattenedInput[size * size + a * size + b] = 1;
                else
                    flattenedInput[size * size + a * size + b] = 0;
            }
        }
        flattenedInput[size * size * 2] = TimeStep;
        return Nd4j.create(flattenedInput);
    }
```

到目前为止，我们只是为`GridWorld`创建了逻辑框架。因此，我们在开始玩游戏之前创建了`DQN`。



# 网络建设和培训

正如我所说的，我们将使用 DL4J 的`MultiLayerNetwork`和`MultiLayerConfiguration`配置创建一个 DQN 网络，它将作为我们的 Q 函数。因此，第一步是通过定义`MultiLayerConfiguration`来创建一个`MultiLayerNetwork`。由于状态有 64 个元素—4x 4x 4—我们的网络必须有一个 64 个单元的输入层，两个分别为 164 和 150 个单元的隐藏层，以及一个 4 个单元的输出层，用于四种可能的动作(上、下、左和右)。这里概述了这一点:

![](assets/e4134ff8-c90a-4cbb-947b-d7b4efa6c89d.png)

DQN 网络的结构，显示了一个输入层、两个隐藏层和一个输出层

然而，我们将使用体验回放记忆来训练我们的 DQN，这将帮助我们存储代理观察到的转换。这将允许 DQN 在以后重复使用这些数据。通过从中随机采样，构成一个批次的跃迁被去相关。已经表明，这极大地稳定和改善了 DQN 训练程序。按照前面的配置，下面的代码可以用来创建这样一个`MultiLayerConfiguration`:

```
int InputLength = size * size * 2 + 1;
int HiddenLayerCount = 150;

MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
                .seed(12345)    //Random number generator seed for improved repeatability. Optional.
                .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
                .weightInit(WeightInit.XAVIER)
                .updater(new Adam(0.001))
                .l2(0.001) // l2 regularization on all layers
                .list()
                .layer(0, new DenseLayer.Builder()
                        .nIn(InputLength)
                        .nOut(HiddenLayerCount)
                        .weightInit(WeightInit.XAVIER)
                        .activation(Activation.RELU)
                        .build())
                .layer(1, new DenseLayer.Builder()
                        .nIn(HiddenLayerCount)
                        .nOut(HiddenLayerCount)
                        .weightInit(WeightInit.XAVIER)
                        .activation(Activation.RELU)
                        .build())
                .layer(2,new OutputLayer.Builder(LossFunction.MSE)
                        .nIn(HiddenLayerCount)
                        .nOut(4) // for 4 possible actions
                        .weightInit(WeightInit.XAVIER)
                        .activation(Activation.IDENTITY)
                        .weightInit(WeightInit.XAVIER)
                        .build())
                .pretrain(false).backprop(true).build();
```

然后，我们使用此配置创建一个 DQN:

```
DeepQNetwork RLNet = new DeepQNetwork(conf, 100000, .99f, 1d, 1024, 500, 1024, InputLength, 4);
```

我们将很快讨论参数，但在此之前，我们将看看如何创建这样一个深层次的架构。首先，我们定义一些参数:

```
int ReplayMemoryCapacity;
List<Replay> ReplayMemory;
double Epsilon;
float Discount;

MultiLayerNetwork DeepQ; // Initial DeepQNet
MultiLayerNetwork TargetDeepQ; // Target DeepQNet

int BatchSize;
int UpdateFreq;
int UpdateCounter;
int ReplayStartSize;
Random r;

int InputLength;
int NumActions;

INDArray LastInput;
int LastAction;
```

然后我们定义构造函数来初始化这些参数:

```
DeepQNetwork(MultiLayerConfiguration conf, int replayMemoryCapacity, float discount, double epsilon, int batchSize, int updateFreq, int replayStartSize, int inputLength, int numActions){
        // First, we initialize both the DeepQNets
 DeepQ = new MultiLayerNetwork(conf);
        DeepQ.init();

        TargetDeepQ = new MultiLayerNetwork(conf);
        TargetDeepQ.init();

        // Then we initialize the target DeepQNet's params
        TargetDeepQ.setParams(DeepQ.params());
        ReplayMemoryCapacity = replayMemoryCapacity;

        Epsilon = epsilon;
        Discount = discount;

        r = new Random();
        BatchSize = batchSize;
        UpdateFreq = updateFreq;
        UpdateCounter = 0;

        ReplayMemory = new ArrayList<Replay>();
        ReplayStartSize = replayStartSize;
        InputLength = inputLength;
        NumActions = numActions;
    }
```

下面是算法主循环的实现:

1.  我们在游戏进行过程中设置了一个`for`循环来循环播放剧集的数量。
2.  我们向前运行 Q 网络。
3.  我们使用ε贪婪实现，因此在时间 *t* 以概率 *ϵ，*代理*选择随机动作。然而，以概率 1-*ϵ,*执行与来自我们的神经网络的最高 q 值相关联的动作。*
4.  然后代理采取一个动作 *a* ，这是在前面的步骤中确定的；我们观察一个新的状态 *s* ，奖励 *r [t]* [+1] 。
5.  然后使用 *s* 执行 Q 网络前向传递，并存储最高 Q 值(maxQ)。
6.  然后将代理的目标值计算为 reward + (gamma * maxQ)来训练网络，其中 gamma 是一个参数(0 <= *γ* < =1)。
7.  我们的目标是为四个可能的输出更新与我们刚刚采取的行动相关联的输出。这里，代理的目标输出向量与第一次执行的输出向量相同，除了与 *reward + (gamma * maxQ)* 的动作相关联的一个输出。

前面的步骤针对一集，然后循环针对用户定义的集进行迭代。此外，首先构建网格，然后计算并保存每次移动的下一个奖励。简而言之，前面的步骤可以表示如下:

```
GridWorld grid = new GridWorld();
grid.networkConstruction();

// We iterate for 100 episodes
for(int m = 0; m < 100; m++) {
            System.out.println("Episode: " + m);
            float CurrMap[][] = grid.generateGridMap();

            grid.FrameBuffer = CurrMap;
            int t = 0;
            grid.printGrid(CurrMap);

            for(int i = 0; i < 2 * grid.size; i++) {
                int a = grid.RLNet.getAction(grid.flattenInput(t), grid.getActionMask(CurrMap));

                float NextMap[][] = grid.doMove(CurrMap, a);
                float r = grid.calcReward(CurrMap, NextMap);
                grid.addToBuffer(NextMap);
                t++;

                if(r == grid.size * grid.size + 1) {
                    grid.RLNet.observeReward(r, null, grid.getActionMask(NextMap));
                    break;
                }

                grid.RLNet.observeReward(r, grid.flattenInput(t), grid.getActionMask(NextMap));
                CurrMap = NextMap;
            }
}
```

在前面的代码块中，network 为每个小批量的展平输入数据计算观察到的回报。看看这个:

```
void observeReward(float Reward, INDArray NextInputs, int NextActionMask[]){
        addReplay(Reward, NextInputs, NextActionMask);

        if(ReplayStartSize <  ReplayMemory.size())
            networkTraining(BatchSize);
        UpdateCounter++;
        if(UpdateCounter == UpdateFreq){
            UpdateCounter = 0;
            System.out.println("Reconciling Networks");
            reconcileNetworks();
        }
    }    
```

计算前面的奖励是为了估计最佳的未来价值:

```
int getAction(INDArray Inputs , int ActionMask[]){
        LastInput = Inputs;
        INDArray outputs = DeepQ.output(Inputs);

        System.out.print(outputs + " ");
        if(Epsilon > r.nextDouble()) {
             LastAction = r.nextInt(outputs.size(1));
             while(ActionMask[LastAction] == 0)
                 LastAction = r.nextInt(outputs.size(1));
             System.out.println(LastAction);
             return LastAction;
        }        
        LastAction = findActionMax(outputs , ActionMask);
        System.out.println(LastAction);
        return LastAction;
    }
```

在前面的代码块中，通过取神经网络输出的最大值来计算未来奖励。看看这个:

```
int findActionMax(INDArray NetOutputs , int ActionMask[]){
        int i = 0;
        while(ActionMask[i] == 0) i++;

        float maxVal = NetOutputs.getFloat(i);
        int maxValI = i;

        for(; i < NetOutputs.size(1) ; i++){
            if(NetOutputs.getFloat(i) > maxVal && ActionMask[i] == 1){
                maxVal = NetOutputs.getFloat(i);
                maxValI = i;
            }
        }
        return maxValI;
    }    
```

如前所述，一旦网络训练开始，就计算观察到的奖励。组合输入计算如下:

```
INDArray combineInputs(Replay replays[]){
        INDArray retVal = Nd4j.create(replays.length , InputLength);
        for(int i = 0; i < replays.length ; i++){
            retVal.putRow(i, replays[i].Input);
        }
        return retVal;
    }
```

那么网络需要计算下一遍的组合输入。看一下这段代码:

```
INDArray combineNextInputs(Replay replays[]){
        INDArray retVal = Nd4j.create(replays.length , InputLength);
        for(int i = 0; i < replays.length ; i++){
            if(replays[i].NextInput != null)
                retVal.putRow(i, replays[i].NextInput);
        }
        return retVal;
    }
```

在前面的代码块中，每个时间步的地图都是用`addToBuffer()`方法保存的，如下:

```
void addToBuffer(float[][] nextFrame) { 
          FrameBuffer = nextFrame;
}
```

然后 DQNet 对每集分批取扁平化输入，训练开始。然后，基于当前和目标输入，通过最大化奖励来计算当前和目标输出。看一下这个代码块:

```
void networkTraining(int BatchSize){
        Replay replays[] = getMiniBatch(BatchSize);
        INDArray CurrInputs = combineInputs(replays);
        INDArray TargetInputs = combineNextInputs(replays);

        INDArray CurrOutputs = DeepQ.output(CurrInputs);
        INDArray TargetOutputs = TargetDeepQ.output(TargetInputs);

        float y[] = new float[replays.length];
        for(int i = 0 ; i < y.length ; i++){
            int ind[] = { i , replays[i].Action };
            float FutureReward = 0 ;
            if(replays[i].NextInput != null)
                FutureReward = findMax(TargetOutputs.getRow(i) , replays[i].NextActionMask);
            float TargetReward = replays[i].Reward + Discount * FutureReward ;
            CurrOutputs.putScalar(ind , TargetReward ) ;
        }
        //System.out.println("Avgerage Error: " + (TotalError / y.length) );

        DeepQ.fit(CurrInputs, CurrOutputs);
    }
```

在前面的代码块中，未来奖励是通过最大化神经网络输出的值来计算的，如下所示:

```
float findMax(INDArray NetOutputs , int ActionMask[]){
        int i = 0;
        while(ActionMask[i] == 0) i++;

        float maxVal = NetOutputs.getFloat(i);
        for(; i < NetOutputs.size(1) ; i++){
            if(NetOutputs.getFloat(i) > maxVal && ActionMask[i] == 1){
                maxVal = NetOutputs.getFloat(i);
            }
        }
        return maxVal;
    }
```

正如我前面所说，这是一个非常简单的游戏，如果代理人采取行动 2(即左)，一步的结果是达到目标。因此，我们只需保持所有其他输出与之前相同，并针对我们采取的操作更改一个输出。因此，实现经验回放是一个更好的想法，它为我们提供了在线学习方案中的小批量更新。

它的工作方式是，我们运行代理来收集足够的转换以填充重放内存，而无需训练。例如，我们的记忆容量可能是 10，000。然后，在每一步，代理将获得一个转换；我们将把这个添加到内存的末尾，并弹出最早的一个。看一下这段代码:

```
void addReplay(float reward , INDArray NextInput , int NextActionMask[]){
        if(ReplayMemory.size() >= ReplayMemoryCapacity )
            ReplayMemory.remove( r.nextInt(ReplayMemory.size()) );

        ReplayMemory.add(new Replay(LastInput , LastAction , reward , NextInput , NextActionMask));
    }
```

然后，从记忆中随机抽取一个小批量的体验，并在此基础上更新我们的 Q 函数，类似于小批量梯度下降。看一下这段代码:

```
Replay[] getMiniBatch(int BatchSize){
        int size = ReplayMemory.size() < BatchSize ? ReplayMemory.size() : BatchSize ;
        Replay[] retVal = new Replay[size];

        for(int i = 0 ; i < size ; i++){
            retVal[i] = ReplayMemory.get(r.nextInt(ReplayMemory.size()));
        }
        return retVal;        
    }
```



# 玩 GridWorld 游戏

对于这个项目，我没有使用任何可视化来演示状态和动作。正如我之前提到的，这是一款基于文本的游戏。然后您可以使用下面的调用运行`GridWorld.java`类(包含 main 方法):

```
DeepQNetwork RLNet = new DeepQNetwork(conf, 100000, .99f, 1d, 1024, 500, 1024, InputLength, 4);
```

在这个调用中，参数描述如下:

*   `conf`:这是用于创建 DQN 的`MultiLayerConfiguration`
*   `100000`:这是回放记忆容量
*   `.99f`:折扣
*   这是ε
*   `1024`:批量大小
*   `500`:这是更新频率；第二个 1，024 是重播开始大小
*   `InputLength`:这是尺寸 x 尺寸 x 2 + 1= 33 的输入长度(考虑尺寸=4)
*   `4`:这是代理可以执行的可能操作的数量。

我们将 epsilon(*ϵ*-贪婪动作选择)初始化为 1，它将在每一集减少一点。这样最终会达到 0.1，饱和。根据前面的设置，应该开始训练，这将开始生成一个网格，表示每个时间戳的地图和 DQN 的输出，按上/下/左/右的顺序排列，后面是最高值的索引。

我们没有任何游戏的图形表示模块。所以在前面的结果中，0，1，-1，以此类推，网格表示五集每个时间戳的地图。括号中的数字只是 DQN 的输出，后面是最高值的索引。看一下这个代码块:

```
Scanner keyboard = new Scanner(System.in);
for(int m = 0; m < 10; m++) {
            grid.RLNet.SetEpsilon(0);
            float CurrMap[][] = grid.generateGridMap();
            grid.FrameBuffer = CurrMap;

            int t = 0;
            float tReward = 0;

            while(true) {
                grid.printGrid(CurrMap);
                keyboard.nextLine();

                int a = grid.RLNet.getAction(grid.flattenInput(t), grid.getActionMask(CurrMap));
                float NextMap[][] = grid.doMove(CurrMap, a);
                float r = grid.calcReward(CurrMap, NextMap);

                tReward += r;
                grid.addToBuffer(NextMap);
                t++;
                grid.RLNet.observeReward(r, grid.flattenInput(t), grid.getActionMask(NextMap));

                if(r == grid.size * grid.size + 1)
                    break;
                CurrMap = NextMap;
            }
            System.out.println("Net Score: " + (tReward));
        }
        keyboard.close();
    }

```

```
>>>
 Episode: 0
 0000
 01-10
 0000
 0000
 [[ 0.2146, 0.0337, -0.0444, -0.0311]] 2
 [[ 0.1105, 0.2139, -0.0454, 0.0851]] 0
 [[ 0.0678, 0.3976, -0.0027, 0.2667]] 1
 [[ 0.0955, 0.3379, -0.1072, 0.2957]] 3
 [[ 0.2498, 0.2510, -0.1891, 0.4132]] 0
 [[ 0.2024, 0.4142, -0.1918, 0.6754]] 2
 [[ 0.1141, 0.6838, -0.2850, 0.6557]] 1
 [[ 0.1943, 0.6514, -0.3886, 0.6868]] 0
 Episode: 1
 0000
 0000
 1000
 00-10
 [[ 0.0342, 0.1792, -0.0991, 0.0369]] 0
 [[ 0.0734, 0.2147, -0.1360, 0.0285]] 1
 [[ 0.0044, 0.1295, -0.2472, 0.1816]] 3
 [[ 0.0685, 0.0555, -0.2153, 0.2873]] 0
 [[ 0.1479, 0.0558, -0.3495, 0.3527]] 3
 [[ 0.0978, 0.3776, -0.4362, 0.4475]] 0
 [[ 0.1010, 0.3509, -0.4134, 0.5363]] 2
 [[ 0.1611, 0.3717, -0.4411, 0.7929]] 3
 ....
 Episode: 9
 0000
 1-100
 0000
 0000
 [[ 0.0483, 0.2899, -0.1125, 0.0281]] 3
 0000
 0000
 0-101
 0000
 [[ 0.0534, 0.2587, -0.1539, 0.1711]] 1
 Net Score: 10.0
```

因此，该代理已经能够得到总分 10(即，正)。



# 常见问题(FAQ)

现在我们已经解决了 GridWorld 问题，在强化学习和整体深度学习现象中还有其他实际的方面需要考虑。在这一部分，我们将看到一些你可能已经想到的常见问题。这些问题的答案可以在附录中找到。

1.  Q-learning 中的 Q 是什么？
2.  我知道我们在 GPU 和 cuDNN 上进行培训是为了更快地收敛。但是，我的机器上没有 GPU。我能怎么做呢？
3.  没有可视化，所以很难跟踪代理向目标移动。
4.  再举几个强化学习的例子。
5.  我如何协调我们的小批量处理获得的结果？
6.  我该如何调和 DQN？
7.  我想保存训练好的网络。我能做到吗？
8.  我想恢复保存的(即训练过的)网络。我能做到吗？



# 摘要

在本章中，我们看到了如何使用 DL4J、RL4J 和神经 Q-learning 开发一个演示 GridWorld 游戏，神经 Q-learning 充当 Q 函数。我们还提供了开发用于玩 GridWorld 游戏的深度学习网络所必需的一些基本理论背景。然而，我们没有开发任何模块来可视化整个剧集中特工的动作。

在下一章中，我们将开发一个非常普通的端到端电影推荐系统项目，但是使用神经**因式分解机** ( **FM** )算法。movie lens 100 万数据集将用于该项目。我们将使用 RankSys 和基于 Java 的 FM 库来预测用户对电影的评分和排名。然而，Spark ML 将用于数据集的探索性分析。



# 问题的答案

**回答** **问题 1:** 不要将 Q-learning 中的 Q 与我们在前面部分讨论过的 Q-function 混淆。Q 函数总是接受状态和动作并给出状态-动作对的值的函数的名称。RL 方法涉及 Q 函数，但不一定是 Q 学习算法。

**回答** **问题 2:** 不用担心，你也可以在 CPU 后端执行训练。在这种情况下，只需从`pom.xml`文件中删除 CUDA 和 cuDNN 依赖项，并用 CPU 依赖项替换它们。这些属性将是:

```
<properties>
       <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
       <java.version>1.8</java.version>
        <nd4j.version>1.0.0-alpha</nd4j.version>
       <dl4j.version>1.0.0-alpha</dl4j.version>
      <datavec.version>1.0.0-alpha</datavec.version>
       <arbiter.version>1.0.0-alpha</arbiter.version>
      <logback.version>1.2.3</logback.version>
</properties>
```

不要使用这两个依赖项:

```
<dependency>
       <groupId>org.nd4j</groupId>
        <artifactId>nd4j-cuda-9.0-platform</artifactId>
        <version>${nd4j.version}</version>
</dependency>
<dependency>
       <groupId>org.deeplearning4j</groupId>
       <artifactId>deeplearning4j-cuda-9.0</artifactId>
       <version>${dl4j.version}</version>
</dependency>
```

仅使用一个，如下所示:

```
<dependency>
     <groupId>org.nd4j</groupId>
     <artifactId>nd4j-native</artifactId>
     <version>${nd4j.version}</version>
</dependency>
```

然后，您就可以开始使用 CPU 后端了。

**回答** **问题 3:** 如前所述，最初的目标是开发一个简单的基于文本的游戏。然而，通过一些努力，所有的动作都可以被可视化。我想把这个留给读者。尽管如此，可视化模块将很快被添加到 GitHub 库。

**回答问题 4 的****:**在 https://github.com/deeplearning4j/dl4j-examples/的 DL4J GitHub 库中有一些 RL4J 的基本例子。请随意尝试扩展它们以满足您的需求。

**回答问题 5 的****:**处理每个小批量给我们提供了该小批量中使用的输入的最佳权重/偏差结果。这个问题衍生出几个与此相关的子问题:I)我们如何调和所有小批量获得的结果？ii)我们是否取平均值来得出经过训练的网络的最终权重/偏差？

因此，每个小批量包含单个误差梯度的平均值。如果有两个小批量，可以取两个小批量梯度更新的平均值来调整权重，以减少这些样品的误差。

**回答** **第 6 题:**参考第 5 题得到理论上的理解。但是，在我们的示例中，使用 DL4J 中的`setParams()`方法，它可以帮助您协调网络:

```
void reconcileNetworks(){
     TargetDeepQ.setParams(DeepQ.params());
    }
```

现在的问题是:我们在哪里使用这种调和？嗯，答案是在计算奖励时(见`observeReward()`方法)。

**回答** **问题 7:** 拯救 DQN 类似于拯救另一个基于 DL4J 的网络。为此，我编写了一个名为`saveNetwork()`的方法，将网络参数保存为 JSON 格式的单个 ND4J 对象。看看这个:

```
public boolean saveNetwork(String ParamFileName , String JSONFileName){
        //Write the network parameters for later use:
        try(DataOutputStream dos = new DataOutputStream(Files.newOutputStream(Paths.get(ParamFileName)))){
            Nd4j.write(DeepQ.params(),dos);
        } catch(IOException e) {
            System.out.println("Failed to write params");
            return false;
        }

        //Write the network configuration:
        try{
            FileUtils.write(new File(JSONFileName), DeepQ.getLayerWiseConfigurations().toJson());
        } catch (IOException e) {
            System.out.println("Failed to write json");
            return false;
        }
        return true;
    }
```

**回答** **问题 8:** 恢复 DQN 类似于拯救另一个基于 DL4J 的网络。为此，我编写了一个名为`restoreNetwork()`的方法来协调参数，并将保存的网络重新加载为`MultiLayerNetwork`。这是:

```
public boolean restoreNetwork(String ParamFileName , String JSONFileName){
        //Load network configuration from disk:
        MultiLayerConfiguration confFromJson;
        try{
            confFromJson = MultiLayerConfiguration.fromJson(FileUtils.readFileToString(new 
                                                            File(JSONFileName)));
        } catch(IOException e1) {
            System.out.println("Failed to load json");
            return false;
        }

        //Load parameters from disk:
        INDArray newParams;
        try(DataInputStream dis = new DataInputStream(new FileInputStream(ParamFileName))){
            newParams = Nd4j.read(dis);
        } catch(FileNotFoundException e) {
            System.out.println("Failed to load parems");
            return false;
        } catch (IOException e) {
            System.out.println("Failed to load parems");
            return false;
        }
        //Create a MultiLayerNetwork from the saved configuration and parameters 
        DeepQ = new MultiLayerNetwork(confFromJson); 
        DeepQ.init(); 

        DeepQ.setParameters(newParams); 
        reconcileNetworks();
        return true;        
    }   
```