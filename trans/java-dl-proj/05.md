

# 五、用于图像分类的迁移学习

在[第 3 章](5e051ef6-a3bf-4fe3-b888-8cebdb4240e6.xhtml)、*使用卷积神经网络的多标签*、*图像分类*中，我们看到了如何使用基于 Java 的 CNN 和真实 Yelp 图像数据集上的**deep learning 4j**(**DL4J**)框架开发一个端到端的项目来处理多标签图像分类问题。为此，我们从头开始开发了一个 CNN 模型。

不幸的是，从头开始开发这样一个模型非常耗时，并且需要大量的计算资源。其次，有时候，我们甚至可能没有足够的数据来训练这样的深度网络。例如，ImageNet 是目前最大的图像数据集之一，拥有数百万张带标签的图像。

因此，我们将开发一个端到端的项目，使用预先训练的 VGG-16 模型来解决狗和猫的图像分类，该模型已经用 ImageNet 进行了训练。最后，我们将把所有东西包装在一个 Java JFrame 和 JPanel 应用程序中，以使整个管道易于理解。简而言之，我们将在整个端到端项目中学习以下主题:

*   用于图像分类的迁移学习
*   使用迁移学习开发图像分类器
*   数据集收集和描述
*   开发一个狗和猫的检测器界面
*   **常见问题** ( **常见问题**)



# 基于预训练 VGG16 的图像分类

当今最有用和新兴的应用之一是使用迁移学习技术；它提供了不同框架和平台之间的高可移植性。

一旦你训练了一个神经网络，你得到的是一组训练好的超参数值。比如 **LeNet-5** 有 60k 个参数值， **AlexNet** 有 6000 万， **VGG- 16** 大约有 1.38 亿个参数。这些架构使用从 1，000 到数百万的图像进行训练，通常具有非常深的架构，具有数百个层，构成如此多的超参数。

有许多开源社区的人，甚至是科技巨头，已经将这些预训练的模型公之于众，以供研究(和行业)使用，这样它们就可以被恢复和重用来解决类似的问题。例如，假设我们想要将新图像分类到 1，000 个类别中的一个类别(在 AlexNet 的情况下)和 10 个类别(在 LeNet-5 的情况下)。我们通常不需要处理这么多参数，只需要处理几个选定的参数(我们很快就会看到一个例子)。

简而言之，我们不需要从零开始训练这样的深度网络，而是重用现有的预训练模型；尽管如此，我们还是设法达到了可接受的分类精度。从技术上来说，我们可以使用预训练模型的权重作为特征提取器，或者我们可以用它来初始化我们的架构，然后根据我们的新任务对它们进行微调。

在这方面，当使用 TL 技术来解决您自己的问题时，可能有三种选择:

*   **使用深度 CNN 作为固定特征提取器**:如果我们不再对其拥有的 1000 个类别感兴趣，我们可以通过移除输出层来重用具有完全连接层的预训练 ImageNet。这样，我们可以将所有其他层视为特征提取器。即使您已经使用预训练模型提取了特征，您也可以将这些特征提供给任何线性分类器，例如 softmax 分类器，甚至线性 SVM！
*   **微调深度 CNN** :试图微调整个网络，甚至大部分层，都可能导致过拟合。因此，需要额外的努力来使用反向传播对新任务的预训练权重进行微调。
*   **使用检查点重用预先训练好的模型**:第三种广泛使用的场景是下载人们在互联网上提供的检查点。如果您没有强大的计算能力来从零开始训练模型，那么您可能会选择这种场景，因此您只需使用释放的检查点来初始化模型，然后进行一些微调。

此时，你可能会想到一个有趣的问题:传统的 ML 和使用迁移学习的 ML 有什么区别？在传统的 ML 中，你不会将任何知识或表征转移到任何其他任务中，这与迁移学习不同。

与传统的机器学习不同，源和目标任务或域不必来自相同的分布，但它们必须相似。此外，在训练样本较少或者没有必要的计算能力的情况下，可以使用迁移学习。

![](assets/792d1f27-123a-4af5-b400-7bd0cb749722.png)

传统机器学习与迁移学习



# DL4J 与迁移学习

现在，让我们看看 DL4J 是如何通过它拥有的迁移学习 API 为我们提供这些功能的。DL4J 迁移学习 API 使用户能够(在[https://deeplearning4j.org/transfer-learning](https://deeplearning4j.org/transfer-learning)了解更多信息):

*   修改现有模型的架构
*   微调现有模型的学习配置
*   在训练期间保持指定层(也称为**冻结层**)的参数不变

下图描述了这些功能，其中我们使用迁移学习技术解决了任务 B(类似于任务 A ):

![](assets/74b69371-b7bf-47e7-8f0a-9ec9bde897ac.png)

迁移学习的工作原理

在下一节中，我们将提供更多关于如何使用 DL4J 的这种预训练模型来帮助我们进行迁移学习的见解。



# 使用迁移学习开发图像分类器

在下一节中，我们将看到如何根据狗和猫的原始图像来区分它们。我们还将看到如何实现我们的第一个 CNN 模型来处理具有三个通道的原始和彩色图像。

这个项目受到了 Klevis Ramo([http://ramok.tech/](http://ramok.tech/))的文章《用深度神经网络进行 Java 图像猫狗识别》的极大启发(但也有了显著扩展)。

`code`文件夹有三个包，每个包中有几个 Java 文件。它们的功能概述如下:

*   `com.packt.JavaDL.DogvCatClassification.Train`:
    *   `TrainCatvsDogVG16.java`:用于训练网络，训练好的模型保存到用户指定的位置。最后，它打印结果。
    *   `PetType.java`:包含一个`enum`类型，指定宠物类型(即猫、狗、未知)。
    *   `VG16CatvDogEvaluator.java`:恢复`TrainCatvsDogVG16.java`类保存在指定位置的训练好的模型。然后对测试集和验证集进行评估。最后，它打印结果。
*   `com.packt.JavaDL.DogvCatClassification.Classifier`:
    *   `PetClassfier.java`:让用户有机会上传一个样本图像(即狗或猫)。然后，用户可以从高级 UI 进行检测。
*   `com.packt.JavaDL.DogvCatClassification.UI`:
    *   `ImagePanel.java`:通过扩展 Java JPanel 充当图像面板
    *   `UI.java`:创建上传图像的用户界面并显示结果
    *   `ProgressBar.java`:显示进度条

我们将一步一步地探索它们。首先，让我们看看数据集描述。



# 数据集收集和描述

对于这个端到端项目，我们将使用微软的狗与猫数据集，该数据集是为臭名昭著的狗与猫分类问题提供的，作为一个操场竞赛。数据集可以从 https://www.microsoft.com/en-us/download/details.aspx?下载 id=54765。

train 文件夹包含 25k 张狗和猫的图像，其中标签是文件名的一部分。但是，测试文件夹包含根据数字 id 命名的 12.5k 图像。现在，让我们来看一些来自 25k 图像的示例快照:

![](assets/56ec1ba5-7445-4ac4-82ae-dc4d575a03ae.png)

显示随机选择的图像的真实标签

对于测试集中的每个图像，我们必须预测一个图像是否包含一只狗( *1 =狗，0 =猫*)。简而言之，这是一个二元分类问题。



# 架构选择和采用

如前所述，我们将重用 VGG-16 预训练模型，该模型已经用来自 ImageNet 的不同猫和狗品种的图像进行了训练(参见此处的列表:http://www . image-net . org/challenges/LSVRC/2014/results # cls loc)。最初的 VGG-16 模型有 1000 类图像需要预测，如下图所示:

![](assets/8de28bf5-65da-4908-9b80-d3edef23304d.png)

最初的 VGG-16 模型结构

好在 DL4J 网站上已经有训练好的模型和网络权重了(见[http://blob . deep learning 4j . org/models/vgg 16 _ DL4J _ inference . zip](http://blob.deeplearning4j.org/models/vgg16_dl4j_inference.zip))大小在 500 MB 左右。

您可以手动下载和恢复，或者更好的方法是使用 DL4J 的方式，其中您只需要指定预训练的类型(直到 DL4J 1.0.0 alpha，只有四种预训练的类型可用，如 ImageNet、CIFAR、MNIST 和 VGG-Face)。

后者非常简单明了；只需使用下面几行代码，训练好的模型就会自动下载(这需要一段时间，取决于网速):

```
ZooModel zooModel = new VGG16();
LOGGER.info(" VGG16 model is getting downloaded...");
ComputationGraph preTrainedNet = (ComputationGraph) zooModel.initPretrained(PretrainedType.IMAGENET);
```

在前面的代码片段中，`ComputationGraph`类用于实例化计算图，这是一个具有任意(即有向无环图)连接结构的神经网络。该图结构也可以具有任意数量的输入和输出。

```
LOGGER.info(preTrainedNet.summary());
```

现在，让我们看看网络架构，包括输入/输出的神经元数量、参数形状和参数数量:

![](assets/913bd9a8-7cd0-4b01-8494-d4f46594a1ea.png)

作为计算图的 VGG-16 模型结构

现在我们有了预训练模型，使用它，我们将预测多达 1，000 个类。并且可训练参数等于总参数:1.38 亿。训练如此多的参数是一项困难的工作。

然而，由于我们只需要预测两个类，我们需要稍微修改一下模型架构，使它只输出两个类，而不是 1000 个。所以我们保持一切不变。修改后的 VGG-16 网络将如下所示:

![](assets/7a85deba-fab7-4f1c-91ec-adacd11c161a.png)

从输入到最后完全连接的层(即 fc2)是冻结的

在上图中，我们冻结到最后一个池层，并使用初始权重。绿色部分是我们要训练的感兴趣的话题，所以我们准备只训练两个类的最后一层。换句话说，在我们的例子中，我们将从输入冻结到最后一个完全连接的层，即`fc2`。也就是说，`featurizeExtractionLayer`变量值将是`fc2`。

但是，在此之前，让我们定义一些属性，如种子、类的数量以及我们想要冻结的层:

```
private static final long seed = 12345;
private static final String FREEZE_UNTIL_LAYER = "fc2";
private static final int NUM_CLASS = 2;
```

然后，我们实例化配置进行微调，这将使用此处设置的值覆盖所有非冻结层的值:

```
FineTuneConfiguration fineTuneConf = new FineTuneConfiguration.Builder()    
         .optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT)
         .updater(new Adam(0.001))
         .seed(seed)
         .build();
```

**FineTuneConfiguration** is the configuration for fine-tuning. Values set in this configuration will override the values in each non-frozen layer. Interested readers can take a look at [https://deeplearning4j.org/doc/org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.html](https://deeplearning4j.org/doc/org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.html).

然后，我们创建一个配置图来完成这个任务:它将作为使用预训练的 VGG-16 模型的迁移学习者:

```
ComputationGraph vgg16Transfer = new TransferLearning.GraphBuilder(preTrainedNet)
       .fineTuneConfiguration(fineTuneConf)
       .setFeatureExtractor(FREEZE_UNTIL_LAYER)
       .removeVertexKeepConnections("predictions")
       .setWorkspaceMode(WorkspaceMode.SEPARATE)
       .addLayer("predictions", new OutputLayer
                  .Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
                  .nIn(4096).nOut(NUM_CLASS)
                  .weightInit(WeightInit.XAVIER)
                  .activation(Activation.SOFTMAX).build(), FREEZE_UNTIL_LAYER)
       .build();
vgg16Transfer.setListeners(new ScoreIterationListener(5));
LOGGER.info(vgg16Transfer.summary());
```

下面的屏幕截图显示了前面代码片段的输出:

![](assets/55a7cae8-7caf-4ffe-9693-5f9134fd91f5.png)

冻结网络只有 8194 个可训练参数

在前面的代码中，我们删除了以前计算的预测，而是使用我们的方法，以便修改后的网络通过重新添加新的预测层仅预测两个类。

此外，`setFeatureExtractor`方法通过指定一个层顶点设置为特征提取器来冻结权重。然后，指定的图层顶点和从输入顶点到它的路径上的图层将被冻结，参数保持不变。

因此，我们将只训练从最后一层到两个输出的 8192 个参数(13800 万个参数中的);两个额外的参数用于两个类别的偏差。简而言之，通过冻结到 fc2 层，现在可训练参数从 1.38 亿急剧减少到 8194(即*8192 个网络参数+ 2 个偏置参数*)。



# 训练和测试设备准备

现在我们已经创建了一个**计算图**，我们需要为微调阶段准备训练和测试集。但在此之前，我们定义了一些参数，比如允许的格式和数据路径:

```
public static final Random RAND_NUM_GEN = new Random(*seed*);
public static final String[] ALLOWED_FORMATS = BaseImageLoader.*ALLOWED_FORMATS*;
public static ParentPathLabelGenerator *LABEL_GENERATOR_MAKER* = new ParentPathLabelGenerator();
public static BalancedPathFilter *PATH_FILTER* = new BalancedPathFilter(RAND_NUM_GEN, ALLOWED_FORMATS, LABEL_GENERATOR_MAKER);
```

让我们简单讨论一下多层网络和计算图的区别。在 DL4J 中，有两种由多层组成的网络:

*   多层网络(multilayer network):目前为止我们使用的一堆神经网络层。
*   **计算图**:这允许构建具有以下特征的网络:多个网络输入阵列和多个网络输出(用于分类和回归)。在这种网络类型中，各层使用有向无环图连接结构相互连接。

无论如何，让我们进入正题。一旦设置了参数，下一个任务就是定义文件路径。读者应遵循此路径或在培训过程中展示准确的路径:

```
public static String DATA_PATH = "data/DoG_CaT/data";
public static final String TRAIN_FOLDER = DATA_PATH + "/train";
public static final String *TEST_FOLDER* = DATA_PATH + "/test";
File trainData = new File(TRAIN_FOLDER);
```

然后我们将使用基于`JavaCV`库的`NativeImageLoader`类来加载图像，允许的格式有`.bmp`、`.gif`、`.jpg`、`.jpeg`、`.jp2`、`.pbm`、`.pgm`、`.ppm`、`.pnm`、`.png`、`.tif`、`.tiff`、`.exr`和`.webp`:

**JavaCV** uses wrappers from the JavaCPP presets of several libraries for computer vision (for example, OpenCV and FFmpeg). More details can be found at [https://github.com/bytedeco/javacv](https://github.com/bytedeco/javacv).

```
FileSplit train = new FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS, RAND_NUM_GEN);
```

从图像中提取特征后，我们将特征空间随机分为 80%用于训练，剩下的 20%用于验证训练本身，以防止过度拟合:

```
private static final int TRAIN_SIZE = 80;
InputSplit[] sample = train.sample(*PATH_FILTER*, TRAIN_SIZE, 100 - TRAIN_SIZE);
```

此外，我们的 DL4J 网络将无法使用这种格式的数据，但我们需要将其转换为`DataSetIterator`格式:

```
DataSetIterator trainIterator = getDataSetIterator(sample[0]);
DataSetIterator devIterator = getDataSetIterator(sample[1]);
```

在前面几行中，我们通过`getDataSetIterator()`方法将训练集和验证集都转换成了`DataSetIterator`。这种方法的特征如下所示:

```
public static DataSetIterator getDataSetIterator(InputSplit sample) throws IOException {
    ImageRecordReader imageRecordReader = new ImageRecordReader(224, 224, 3, *LABEL_GENERATOR_MAKER*);
    imageRecordReader.initialize(sample);

    DataSetIterator iterator = new RecordReaderDataSetIterator(imageRecordReader, 
                               BATCH_SIZE, 1, NUM_CLASS);
    iterator.setPreProcessor(new VGG16ImagePreProcessor());
    return iterator;
}
```

太棒了。到目前为止，我们已经准备好了训练集。然而，请记住，这将需要一段时间，因为它必须处理 12，500 张图像。

现在我们可以开始训练了。然而，您可能想知道为什么我们没有谈论测试集。嗯，是的！毫无疑问，我们也需要测试设备。但是，让我们在网络评估步骤中讨论这一点。



# 网络培训和评估

既然训练和测试集已经准备好了，我们可以开始训练了。然而，在此之前，我们为数据集准备定义了一些超参数:

```
private static final int EPOCH = 100;
private static final int BATCH_SIZE = 128;
private static final int SAVING_INTERVAL = 100;
```

此外，我们指定保存训练模型的路径，以供将来重用:

```
private static final String SAVING_PATH = "bin/CatvsDog_VG16_TrainedModel_Epoch100_v1.zip";
```

现在我们可以开始训练网络了。我们将结合使用训练集进行训练，并使用验证集进行验证。最后，网络将使用测试集评估网络性能。因此，为此，我们也需要准备测试集:

```
File testData = new File(TEST_FOLDER);
FileSplit test = new FileSplit(testData, NativeImageLoader.ALLOWED_FORMATS, RAND_NUM_GEN);
DataSetIterator testIterator = *getDataSetIterator*(test.sample(*PATH_FILTER*, 1, 0)[0]);
```

然后我们开始训练；我们使用 128 和 100 个时期的批量大小。因此，第一个 while 循环将执行 100 次。然后，第二个内`while`循环将被执行 196 次(25000 个猫狗图像/128):

```
int iEpoch = 0;
int i = 0;
while (iEpoch < EPOCH) {
 while (trainIterator.hasNext()) {
        DataSet trained = trainIterator.next();
        vgg16Transfer.fit(trained);
 if (i % SAVING_INTERVAL == 0 && i != 0) {
            ModelSerializer.*writeModel*(vgg16Transfer, new File(SAVING_PATH), false);
 *evaluateOn*(vgg16Transfer, devIterator, i);
        }
        i++;
    }
    trainIterator.reset();
    iEpoch++;
    evaluateOn(vgg16Transfer, testIterator, iEpoch);
}
```

通过这种方式，我们已经尝试让训练更快，但仍然可能需要几个小时甚至几天，这取决于设定的纪元数量。而且，如果训练是在 CPU 而不是 GPU 上进行的，可能需要几天时间。对我来说，100 个纪元花了 48 个小时。顺便说一下，我的机器有一个酷睿 i7 处理器，32 GB 的内存和 GeForce GTX 1050 GPU。

Epoch versus iteration
An epoch is a full traversal through the data, and one iteration is one forward and one back propagation on the batch size specified.

无论如何，一旦训练完成，被训练的模型将被保存在先前指定的位置。现在让我们来看看训练进行得如何。为此，我们将看到验证集的性能(如前所述，我们使用总训练集的 15%作为验证集，即 5，000 幅图像):

```
>>>
 Cat classified by model as cat: 2444 times
 Cat classified by model as dog: 56 times
 Dog classified by model as cat: 42 times
 Dog classified by model as dog: 2458 times
 ==========================Scores==========================
 # of classes: 2
 Accuracy: 0.9800
 Precision: 0.9804
 Recall: 0.9806
 F1 Score: 0.9800
 ========================================================

```

然后，当我们在一个完整的测试集(即 12500 张图像)上评估我们的模型时，我体验了以下性能指标:

```
>>>
 Cat classified by model as cat: 6178 times
 Cat classified by model as dog: 72 times
 Dog classified by model as cat: 261 times
 Dog classified by model as dog: 5989 times
 ==========================Scores===================
 # of classes: 2
 Accuracy: 0.9693
 Precision: 0.9700
 Recall: 0.9693
 F1 Score: 0.9688
 ==================================================
```



# 恢复训练好的模型和推理

既然我们已经看到了我们的模型是如何执行的，那么探索恢复已经训练好的模型的可行性是值得的。换句话说，我们将恢复经过训练的模型，并在验证和测试集上评估网络性能:

```
private staticfinal String TRAINED_PATH_MODEL = "bin/CatvsDog_VG16_TrainedModel_Epoch100_v1.zip";
ComputationGraph computationGraph = ModelSerializer.restoreComputationGraph(new File(TRAINED_PATH_MODEL));

VG16CatvDogEvaluator().runOnTestSet(computationGraph);
VG16CatvDogEvaluator().runOnValidationSet(computationGraph);
```

在前面的代码行中，首先，我们从磁盘中恢复了训练好的模型；然后，我们对测试集(完整的测试集)和验证集(20%的训练集)进行了评估。

现在，让我们来看看`runOnTestSet()`方法的签名，它很简单，因为我们已经在前一小节中描述了类似的工作流:

```
private void runOnTestSet(ComputationGraph computationGraph) throws IOException {
        File trainData = new File(TrainCatvsDogVG16.TEST_FOLDER);
        FileSplit test = new FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS,             
                                       TrainCatvsDogVG16.RAND_NUM_GEN);

        InputSplit inputSplit = test.sample(TrainCatvsDogVG16.*PATH_FILTER*, 100, 0)[0];
        DataSetIterator dataSetIterator = TrainCatvsDogVG16.getDataSetIterator(inputSplit);
        TrainCatvsDogVG16.evaluateOn(computationGraph, dataSetIterator, 1);
}
```

现在，让我们来看看`runOnValidationSet`方法的签名:

```
private void runOnValidationSet(ComputationGraph computationGraph) throws IOException {
        File trainData = new File(TrainCatvsDogVG16.TRAIN_FOLDER);
        FileSplit test = new FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS,     
                                       TrainCatvsDogVG16.RAND_NUM_GEN);

        InputSplit inputSplit = test.sample(TrainCatvsDogVG16.*PATH_FILTER*, 15, 80)[0];
        DataSetIterator dataSetIterator = TrainCatvsDogVG16.getDataSetIterator(inputSplit);
        TrainCatvsDogVG16.evaluateOn(computationGraph, dataSetIterator, 1);
}
```



# 进行简单的推理

现在我们已经看到，我们的训练模型在测试集和验证集上都显示出了出色的准确性。那么我们为什么不开发一个 UI 来帮助我们使事情变得更简单呢？如前所述，我们将开发一个简单的 UI，允许我们卸载一个样本图像，然后我们应该能够通过一个简单的按钮检测它。这部分是纯 Java，这里就不讨论细节了。

如果我们运行`PetClassifier.java`类，它首先加载我们训练好的模型，并作为部署模型的后端。然后它调用`UI.java`类来加载用户界面，如下所示:

![](assets/dd775f02-53c6-4827-b0b2-5b3bdc1593f6.png)

猫和狗识别器的用户界面

在控制台中，您应该会看到以下日志/消息:

```
19:54:52.496 [pool-1-thread-1] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [CpuBackend] backend
19:54:52.534 [pool-1-thread-1] WARN org.reflections.Reflections - given scan urls are empty. set urls in the configuration
19:54:52.865 [pool-1-thread-1] INFO org.nd4j.nativeblas.NativeOpsHolder - Number of threads used for NativeOps: 4
19:54:53.249 [pool-1-thread-1] INFO org.nd4j.nativeblas.Nd4jBlas - Number of threads used for BLAS: 4
19:54:53.252 [pool-1-thread-1] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Backend used: [CPU]; OS: [Windows 10]
19:54:53.252 [pool-1-thread-1] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Cores: [8]; Memory: [7.0GB];
19:54:53.252 [pool-1-thread-1] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [OPENBLAS]
19:55:09.015 [pool-1-thread-1] DEBUG org.reflections.Reflections - going to scan these urls:
 ...
9:55:13.394 [pool-1-thread-1] INFO org.deeplearning4j.nn.graph.ComputationGraph - Starting ComputationGraph with WorkspaceModes set to [training: NONE; inference: SEPARATE]
19:55:13.394 [pool-1-thread-1] DEBUG org.reflections.Reflections - going to scan these urls:
19:55:13.779 [pool-1-thread-1] INFO com.packt.JavaDL.DogvCatClassification.UI.UI - Model loaded successfully!
```

现在，让我们从测试集中上传一些照片(这更有意义，因为我们正在重用经过训练的模型，该模型被训练为仅识别训练集，因此测试集仍然不可见):

![](assets/78c5ac06-bae8-46c2-85f3-8f7c5afb8c4b.png)

我们的猫与狗识别器从具有不同形状和颜色的狗的图像中识别狗

因此，我们训练的模型已经能够根据图像识别具有不同形状、大小和颜色的狗。现在，让我们试着上传几张猫的图片，看看是否可行:

![](assets/399ca5c8-189f-4585-b8be-d13a23744b0f.png)

我们的猫与狗识别器从具有不同形状和颜色的猫的图像中识别猫



# 常见问题(FAQ)

现在，我们已经非常准确地解决了狗与猫的分类问题，还需要考虑迁移学习和整体深度学习现象的其他实际方面。在这一部分，我们将看到一些你可能已经想到的常见问题。这些问题的答案可以在附录 a 中找到

1.  可以用自己的动物形象训练模型吗？
2.  使用所有图像进行训练花费的时间太长。我能怎么做呢？
3.  我可以将这个应用程序包装成 web 应用程序吗？
4.  我可以用 VGG-19 来完成这项任务吗？
5.  我们有多少个超参数？我还想看看每一层。



# 摘要

在这一章中，我们用迁移学习技术解决了一个有趣的狗和猫的分类问题。我们使用了预训练的 VGG16 模型及其权重，随后我们使用 Kaggle 的真实猫对狗数据集对训练进行了微调。

一旦训练完成，我们保存训练好的模型用于模型持久化和随后的重用。我们看到，经过训练的模型可以成功地检测和区分大小、质量和形状非常不同的猫和狗的图像。

甚至经过训练的模型/分类器也可以用于解决现实生活中的猫对狗的问题。要点是，这种技术只需很少的努力就可以扩展并用于解决类似的图像分类问题，这适用于二进制和多类分类问题。

在下一章中，我们将看到如何开发一个端到端的项目，当一个视频剪辑连续播放时，它将从视频帧中检测对象。我们还将看到如何利用一个预训练的`TinyYOLO`模型，它是原始 YOLOv2 模型的一个较小的变体。

此外，还将讨论从静止图像和视频中检测对象的一些典型挑战。然后我们将演示如何使用包围盒和非最大抑制技术来解决它们。然而，我们将看到如何使用 DL4J 之上的 JavaCV 库来处理视频剪辑。最后，我们将看到一些常见问题，这些问题对于采用和扩展这个项目应该是有用的。



# 问题的答案

**回答** **问题 1** :可以，当然可以。但是，请注意，您必须提供足够数量的图像，最好每种动物类型至少有几千张图像。否则模型训练不好。

**回答** **第二个问题**:一个可能的原因是你试图一次输入所有的图像，或者你正在进行 CPU 训练(你的机器没有一个好的配置)。前者很容易解决；我们可以采用批量模式进行训练，这是深度学习时代的建议。

后一种情况可以通过将训练从 CPU 迁移到 GPU 来解决。但是，如果您的机器没有 GPU，您可以尝试迁移到 Amazon GPU 实例，以获得对单个(p2.xlarge)或多个 GPU(例如，p2.8xlarge)的支持。

**回答** **问题 3** :所提供的申请应该足以了解申请的有效性。然而，这个应用程序仍然可以包装成一个 web 应用程序，在后端可以提供经过训练的模型。

为此，我经常使用 Spring Boot 框架(参见 https://projects.spring.io/spring-boot/的)。除此之外，Java CUBA studio 也可以使用(参见 https://www.cuba-platform.com/)。

正如本章前面提到的，VGG-16 是 VGG-19 的小型变体。不幸的是，没有办法直接使用 VGG-19。不过，读者可以尝试加载 VGG-19 可以进口与 Keras 进口。

**问题 6 答案:**网络初始化后立即使用以下代码即可:

```
//Print the number of parameters in the network (and for each layer)
Layer[] layers = model.getLayers();
int totalNumParams = 0;

for( int i=0; i<layers.length; i++ ){
         int nParams = layers[i].numParams();
         System.*out*.println("Number of parameters in layer " + i + ": " + nParams);
         totalNumParams += nParams;
}
System.*out*.println("Total number of network parameters: " + totalNumParams);
```

```
>>>
 Number of parameters in layer 0: 1792
 Number of parameters in layer 1: 36928
 Number of parameters in layer 2: 0
 Number of parameters in layer 3: 73856
 Number of parameters in layer 4: 147584
 Number of parameters in layer 5: 0
 Number of parameters in layer 6: 295168
 Number of parameters in layer 7: 590080
 Number of parameters in layer 8: 590080
 Number of parameters in layer 9: 0
 Number of parameters in layer 10: 1180160
 Number of parameters in layer 11: 2359808
 Number of parameters in layer 12: 2359808
 Number of parameters in layer 13: 0
 Number of parameters in layer 14: 2359808
 Number of parameters in layer 15: 2359808
 Number of parameters in layer 16: 2359808
 Number of parameters in layer 17: 0
 Number of parameters in layer 18: 102764544
 Number of parameters in layer 19: 16781312
 Number of parameters in layer 20: 8194
 Total number of network parameters: 134268738
```