

# 二、现实世界深度学习：预测比特币价格

概观

本章将帮助您为深度学习模型准备数据，选择正确的模型架构，使用 Keras-tensor flow 2.0 的默认 API，并使用训练好的模型进行预测。本章结束时，您将已经准备好一个模型来进行预测，我们将在接下来的章节中探讨这个模型。

# 简介

基于第一章、*神经网络和深度学习介绍*中的基本概念，现在让我们转向现实世界的场景，并确定我们是否可以建立一个预测比特币价格的深度学习模型。

我们将学习为深度学习模型准备数据的原则，以及如何选择正确的模型架构。我们将使用 Keras—tensor flow 2.0 的默认 API，并用训练好的模型进行预测。我们将通过将所有这些组件放在一起并构建一个基本的、完整的深度学习应用的第一个版本来结束本章。

深度学习是一个正在经历激烈研究活动的领域。除此之外，研究人员致力于发明新的神经网络架构，既可以解决新问题，也可以提高以前实现的架构的性能。

在这一章中，我们将研究新旧架构。旧的架构已经被用来解决大量的问题，并且通常被认为是开始一个新项目的正确选择。较新的架构在特定的问题上显示了巨大的成功，但是很难推广。后者作为下一步探索的参考很有趣，但在开始一个项目时几乎不是一个好的选择。

以下主题讨论了这些体系结构的细节，以及如何为特定的问题陈述确定最佳的体系结构。

# 选择正确的模型架构

考虑到可用的架构可能性，有两种流行的架构经常被用作几种应用的起点:**卷积神经网络**(**CNN**)和**循环神经网络** ( **RNNs** )。这些是基础网络，应该被视为大多数项目的起点。

我们也包括了另外三个网络的描述，由于它们在这个领域的相关性:**长短期记忆** ( **LSTM** )网络(一个 RNN 变种)；**生成对抗网络**(**GANs**)；以及**深度强化学习** ( **DRL** )。这些后一种架构在解决当代问题方面表现出了巨大的成功，然而，它们使用起来有些困难。下一节将介绍不同类型的架构在不同问题中的使用。

## 卷积神经网络(CNN)

CNN 因处理网格状结构的问题而声名狼藉。它们最初是为了对图像进行分类而创建的，但已经被用于其他几个领域，从语音识别到无人驾驶汽车。

CNN 的本质是使用密切相关的数据作为训练过程的一个要素，而不仅仅是单个的数据输入。这种想法在图像的上下文中特别有效，其中位于中心的像素与位于其右侧和左侧的像素相关。名称**卷积**用于以下过程的数学表示:

![Figure 2.1: Illustration of the convolution process
](image/B15911_02_01.jpg)

图 2.1:卷积过程的图示

注意

图像来源:volodymyr mnih，*等。*

你可以在[https://packt.live/3fivWLB](https://packt.live/3fivWLB)找到这张图片

有关深度强化学习的更多信息，请参考*通过深度强化学习的人类级控制*。【2015 年 2 月、*自然*，可在[https://storage . Google APIs . com/deep mind-media/dqn/dqnnaturepaper . pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)查阅。

## 循环神经网络

CNN 使用一组输入，不断改变网络各层和节点的权重和偏差。这种方法的一个已知限制是，当确定网络权重和偏差的变化时，其架构忽略了这些输入的顺序。

rnn 正是为了解决这个问题而创建的。它们设计用于处理顺序数据。这意味着在每个时期，层会受到前一层的输出的影响。对每个序列中先前观察值的记忆在评估后验观察值时起着重要作用。

由于该问题的顺序性质，rnn 已经成功地应用于语音识别。此外，它们还用于解决翻译问题。谷歌翻译目前的算法——Transformer，使用 RNN 将文本从一种语言翻译成另一种语言。2018 年末，谷歌推出了另一种基于 Transformer 算法的算法，称为来自 Transformers ( **BERT** )的**双向编码器表示，这是目前在**自然语言处理** ( **NLP** )领域的最新技术。**

注意

有关 RNN 应用的更多信息，请参阅以下内容:

*Transformer:用于语言理解的新型神经网络架构*， *Jakob Uszkoreit* ，*谷歌研究博客*，*2017 年 8 月*，可在[https://ai . Google Blog . com/2017/08/Transformer-Novel-Neural-Network . html](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)获得。

*BERT:开源 BERT:自然语言处理最先进的前期培训*，可在[https://ai . Google blog . com/2018/11/Open-Sourcing-BERT-State-Art-Pre . html](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html)获得。

下图根据单词在句子中出现的位置，说明了英语单词与法语单词之间的联系。rnn 在语言翻译问题中非常流行:

![Figure 2.2: Illustration from distill.pub linking words in English and French
](image/B15911_02_02.jpg)

图 2.2:来自 distill.pub 链接英语和法语单词的插图

注意

图片来源:[https://distill.pub/2016/augmented-rnns/](https://distill.pub/2016/augmented-rnns/)

## 长短期记忆(LSTM)网络

**LSTM** 网络是为解决消失梯度问题而创建的 RNN 变体。这个问题是由于距离当前步骤太远的内存组件由于它们的距离而接收到较低的权重。LSTMs 是 RNNs 的一种变体，包含一个称为**遗忘门**的存储组件。该组件可用于评估新旧元素如何影响权重和偏差，具体取决于观察值在序列中的位置。

注意

LSTM 建筑是由 Sepp Hochreiter 和 Jürgen Schmidhuber 于 1997 年首次推出的。当前的实现已经有了一些修改。关于 LSTM 每个组成部分如何工作的详细数学解释，请参考克里斯托弗·奥拉 2015 年 8 月撰写的文章*了解 LSTM 网络*，可在 https://colah.github.io/posts/2015-08-Understanding-LSTMs/的[获得。](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## 生成性对抗网络

生成对抗网络 ( **GANs** )是由蒙特利尔大学的 Ian Goodfellow 和他的同事在 2014 年发明的。GANs 的工作基于这样一种方法，即不是有一个神经网络来优化权重和偏差，以最小化其误差，而是应该有两个神经网络为此目的相互竞争。

注意

有关 GANs 的更多信息，参见*生成对抗网络*、*伊恩·古德菲勒等人*、 *arXiv* 。*2014 年 6 月 10 日*，在[https://arxiv.org/pdf/1406.2661.pdf](https://arxiv.org/pdf/1406.2661.pdf)发售。

gan 生成新数据(*假*数据)和一个网络，该网络评估第一个网络生成的数据是*真*或*假*的可能性。他们竞争是因为双方都在学习:一方学习如何更好地生成*假*数据，另一方学习如何区分呈现的数据是否真实。他们在每个时期迭代，直到收敛。这就是评估生成数据的网络无法进一步区分*假*和*真*数据的原因。

GANs 已经成功地应用于数据具有清晰拓扑结构的领域。最初，gan 用于创建物体、人脸和动物的合成图像，这些图像与这些事物的真实图像相似。你会在下面的图片中看到，在 **StarGAN** 项目中，脸上的表情发生了变化:

![Figure 2.3: Changes in people's faces based on emotion, using GAN algorithms
](image/B15911_02_03.jpg)

图 2.3:基于情感的人脸变化，使用 GAN 算法

图像创建的这个领域是 GANs 使用最频繁的地方，但是其他领域的应用偶尔会出现在研究论文中。

注意

图片来源:StarGAN 项目，可在[https://github.com/yunjey/StarGAN](https://github.com/yunjey/StarGAN)获得。

## 深度强化学习(DRL)

最初的 DRL 架构是由总部位于英国的谷歌旗下的人工智能研究机构 DeepMind 支持的。DRL 网络的关键思想是，它们在本质上是无人监管的，它们从反复试验中学习，只对奖励函数进行优化。

也就是说，与其他网络(使用监督方法来优化不正确的预测，而不是已知的正确预测)不同，DRL 网络不知道解决问题的正确方法。他们只是被给予一个系统的规则，然后每当他们正确地执行一个功能时就会得到奖励。这个过程需要大量的迭代，最终训练网络在几项任务中表现出色。

注意

有关 DRL 的更多信息，请参考*通过深度强化学习的人类级控制*、 *Volodymyr Mnih 等人*、【2015 年 2 月、 *Nature* ，可在:[https://storage . Google APIs . com/Deep mind-media/dqn/dqnnaturepaper . pdf](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)获得。

DRL 模型在 DeepMind 创造了 AlphaGo 之后变得流行起来，alpha Go 是一个比职业选手玩围棋更好的系统。DeepMind 还创建了 DRL 网络，学习如何在超人的水平上完全靠自己玩视频游戏。

注意

想了解更多关于 DQN 的信息，可以去看看 DeepMind 为了击败 Atari games 而创建的 DQN。该算法使用 DRL 解来不断增加其奖励。

图片来源:[https://keon.io/deep-q-learning/](https://keon.io/deep-q-learning/)。

以下是神经网络架构及其应用的概述:

![Figure 2.4: Different neural network architectures, data structures, and their successful applications
](image/B15911_02_04.jpg)

图 2.4:不同的神经网络架构、数据结构及其成功应用

## 数据规范化

在构建深度学习模型之前，数据规范化是重要的一步。数据规范化是机器学习系统中的一种常见做法。对于神经网络，研究人员提出，归一化是训练 RNNs(和 LSTMs)的一种基本技术，主要是因为它减少了网络的训练时间，提高了网络的整体性能。

注意

如需了解更多信息，请参考*批量标准化:通过减少内部协变量偏移加速深度网络训练*，*谢尔盖·约菲等人*， *arXiv* ，2015 年 3 月，可在[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)获得。

哪种规范化技术最有效取决于数据和手头的问题。这里列出了一些常用的技术:

### Z 分数

当数据呈正态分布(即高斯分布)时，可以将每个观测值之间的距离计算为其平均值的标准差。当确定数据点距离分布中更可能出现的位置有多远时，这种归一化很有用。Z 得分由以下公式定义:

![Figure 2.5: Formula for Z-Score
](image/B15911_02_05.jpg)

图 2.5:Z 分数的公式

这里， *x* i 是第 *i* 次观测，![1](image/B15911_02_Formula_01.png)是均值，![2](image/B15911_02_Formula_02.png)是数列的标准差。

注意

有关更多信息，请参考标准分数文章( *Z-Score:定义、公式和计算*)，可在[https://www . statisticshowto . datasciencecentral . com/probability-and-statistics/Z-Score/](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/)获得。

### 点相对归一化

这种标准化计算给定观察值相对于系列中第一个观察值的差异。这种标准化对于识别相对于起点的趋势是有用的。点相对归一化由下式定义:

![Figure 2.6: Formula for point-relative normalization
](image/B15911_02_06.jpg)

图 2.6:点相对标准化公式

这里， *o* i 是第 *i* 次观测， *o* o 是系列的第一次观测。

注意

有关预测的更多信息，请观看*如何轻松预测股票价格——深度学习介绍#7* 、 *Siraj Raval* ，可在 YouTube 上的【https://www.youtube.com/watch?v=ftMq5ps503w 网站获得。

### 最大和最小归一化

这种类型的归一化计算给定观测值与序列的最大值和最小值之间的距离。这在处理最大值和最小值不是异常值并且对未来预测很重要的序列时非常有用。这种归一化技术可以应用于以下公式:

![Figure 2.7: Formula for calculating normalization
](image/B15911_02_07.jpg)

图 2.7:计算标准化的公式

这里， *O* i 是第 *i* 次观测， *O* 代表一个具有所有 *O* 值的向量，函数 *min (O)* 和 *max (O)* 分别代表序列的最小值和最大值。

在*练习**2.01**探索比特币数据集并为模型准备数据*期间，我们将准备可用的比特币数据用于我们的 LSTM 模型。这包括选择感兴趣的变量，选择相关的周期，并应用前面的点相对归一化技术。

# 构建你的问题

与研究人员相比，从业者在开始一个新的深度学习项目时，花在确定选择哪种架构上的时间要少得多。在开发这些系统时，获取正确代表给定问题的数据是要考虑的最重要的因素，其次是对数据集固有的偏见和局限性的理解。开始开发深度学习系统时，考虑以下问题进行反思:

*   我有正确的数据吗？这是训练深度学习模型时最难的挑战。首先，用数学规则定义你的问题。使用精确的定义，将问题组织成类别(分类问题)或连续范围(回归问题)。现在，您如何收集与这些指标相关的数据呢？
*   我有足够的数据吗？通常，深度学习算法在大型数据集上的表现要比在小型数据集上好得多。知道训练高性能算法需要多少数据取决于你试图解决的问题的类型，但目标是收集尽可能多的数据。
*   我可以使用预先训练好的模型吗？如果您正在处理的问题是一个更一般的应用的子集，但是在同一个领域内。考虑使用预先训练好的模型。预先训练好的模型可以让你在处理你的问题的特定模式上领先一步，而不是更一般的领域特征。一个好的起点是官方的 TensorFlow 资源库([https://github.com/tensorflow/models](https://github.com/tensorflow/models))。

当你用这样的问题来构建你的问题时，你将对任何新的深度学习项目有一个循序渐进的方法。以下是这些问题和任务的代表性流程图:

![Figure 2.8: Decision tree of key reflection questions to be asked at the beginning of a deep learning project
](image/B15911_02_08.jpg)

图 2.8:深度学习项目开始时要问的关键反思问题的决策树

在某些情况下，数据可能根本不可用。根据具体情况，可以使用一系列技术从输入数据中有效地创建更多数据。这个过程被称为**数据扩充**，可以成功应用于处理图像识别问题。

注意

一个很好的参考是文章*用深度神经网络*对浮游生物进行分类，可以在 https://benanne.github.io/2015/03/17/plankton.html[的](https://benanne.github.io/2015/03/17/plankton.html)找到。作者展示了一系列用于扩充一小组图像数据的技术，以增加模型具有的训练样本的数量。

一旦问题结构良好，你就可以开始准备模型了。

## Jupyter 笔记本电脑

在本节中，我们将使用 Jupyter 笔记本进行编码。Jupyter 笔记本通过 web 浏览器提供 Python 会话，允许您交互式地处理数据。它们是探索数据集的流行工具。整本书的练习都会用到它们。

## 练习 2.01:探索比特币数据集并为模型准备数据

在本练习中，我们将准备数据，然后将其传递给模型。准备好的数据将有助于我们在本章进行预测。在准备数据之前，我们会对它做一些可视化的分析，比如看比特币的价值什么时候最高，什么时候开始下跌。

注意

我们将使用最初从雅虎财经网站([https://finance.yahoo.com/quote/BTC-USD/history/](https://finance.yahoo.com/quote/BTC-USD/history/))检索的公开数据集。数据集已经做了轻微的修改，因为它已经和本章一起提供了，并将在本书的其余部分使用。

数据集可以从[https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r)下载。

以下是完成本练习的步骤:

1.  Using your Terminal, navigate to the `Chapter02/Exercise2.01` directory. Activate the environment created in the previous chapter and execute the following command to start a Jupyter Notebook instance:

    ```
    $ jupyter notebook
    ```

    这应该会在您的浏览器中自动打开 Jupyter 实验室服务器。从那里你可以开始一个 Jupyter 笔记本。

    您应该看到以下输出或类似内容:

    ![Figure 2.9: Terminal image after starting a Jupyter lab instance
    ](image/B15911_02_09.jpg)

    图 2.9:启动 Jupyter 实验室实例后的终端图像

2.  Select the `Exercise2.01_Exploring_Bitcoin_Dataset.ipynb` file. This is a Jupyter Notebook file that will open in a new browser tab. The application will automatically start a new Python interactive session for you:![Figure 2.10: Landing page of your Jupyter Notebook instance
    ](image/B15911_02_10.jpg)

    图 2.10:Jupyter 笔记本实例的登录页面

3.  Click the Jupyter Notebook file:![Figure 2.11: Image of Jupyter Notebook
    ](image/B15911_02_11.jpg)

    图 2.11:Jupyter 笔记本的图像

4.  Opening our Jupyter Notebook, consider the Bitcoin data made available with this chapter. The dataset `data/bitcoin_historical_prices.csv` ([https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r)) contains the details of Bitcoin prices since early 2013\. It contains eight variables, two of which (`date` and `week`) describe a time period of the data. These can be used as indices—and six others (`open`, `high`, `low`, `close`, `volume`, and `market_capitalization`) can be used to understand changes in the price and value of Bitcoin over time:![Figure 2.12: Available variables (that is, columns) in the Bitcoin historical prices dataset
    ](image/B15911_02_12.jpg)

    图 2.12:比特币历史价格数据集中的可用变量(即列)

5.  使用 open Jupyter Notebook 实例，考虑其中两个变量的时间序列:`close`和`volume`。从那些时间序列入手，探究价格波动模式，即比特币的价格在历史数据中的不同时间是如何变化的。
6.  Navigate to the open instance of the Jupyter Notebook, `Exercise2.01_Exploring_Bitcoin_Dataset.ipynb`. Now, execute all cells under the `Introduction` header. This will import the required libraries and import the dataset into memory:![Figure 2.13: Output from the first cells of the notebook time-series plot 
    of the closing price for Bitcoin from the close variable](image/B15911_02_13.jpg)

    图 2.13:笔记本时间序列图第一个单元格的输出，显示了收盘变量中比特币的收盘价

7.  After the dataset has been imported into memory, move to the `Exploration` section. You will find a snippet of code that generates a time series plot for the `close` variable:

    ```
    bitcoin.set_index('date')['close'].plot(linewidth=2, \
                                            figsize=(14, 4),\
                                            color='#d35400')
    #plt.plot(bitcoin['date'], bitcoin['close'])
    ```

    输出如下所示:

    ![Figure 2.14: Time series plot of the closing price for Bitcoin from the close variable
    ](image/B15911_02_14.jpg)

    图 2.14:收盘变量中比特币收盘价的时间序列图

8.  Reproduce this plot but using the `volume` variable in a new cell below this one. You will have most certainly noticed that price variables surge in 2017 and then the downfall starts:

    ```
    bitcoin.set_index('date')['volume'].plot(linewidth=2, \
                                             figsize=(14, 4), \
                                             color='#d35400')
    ```

    ![Figure 2.15: The total daily volume of Bitcoin coins 
    ](image/B15911_02_15.jpg)

    图 2.15:比特币的每日总成交量

    *图 2.15* 显示，自 2017 年以来，比特币交易在市场上显著增加。每日总成交量的变化比每日收盘价大得多。

9.  Execute the remaining cells in the Exploration section to explore the range from 2017 to 2018.

    近年来，比特币价格的波动越来越常见。虽然神经网络可以使用这些时期来理解某些模式，但我们将排除旧的观察结果，因为我们对预测不太遥远时期的未来价格感兴趣。仅过滤 2016 年以后的数据。导航至`Preparing Dataset for a Model`部分。使用 pandas API 来过滤数据。Pandas 提供了一个直观的 API 来执行这个操作。

10.  Extract recent data and save it into a variable:

    ```
    bitcoin_recent = bitcoin[bitcoin['date'] >= '2016-01-04']
    ```

    `bitcoin_recent`变量现在有了我们原始比特币数据集的副本，但被过滤为更新或等于 2016 年 1 月 4 日的观察值。

    使用 Jupyter 笔记本中*数据标准化*部分描述的点相对标准化技术标准化数据。你也不会贬低两个变量——`close`和`volume`——因为这些是我们正在努力预测的变量。

11.  Run the next cell in the notebook to ensure that we only keep the close and volume variables.

    在包含本章的同一个目录中，我们放置了一个名为`normalizations.py`的脚本。该脚本包含本章中描述的三种规范化技术。我们将该脚本导入到我们的 Jupyter 笔记本中，并将这些函数应用到我们的系列中。

12.  导航至`Preparing Dataset for a Model`部分。现在，使用熊猫`groupby()`方法，使用`iso_week`变量对给定一周的每日观察进行分组。我们现在可以将标准化函数`normalizations.point_relative_normalization()`直接应用于该周内的序列。我们可以使用下面的代码将规范化输出作为一个新变量存储在同一个 pandas 数据帧中:

    ```
    bitcoin_recent['close_point_relative_normalization'] = \
    bitcoin_recent.groupby('iso_week')['close']\
    .apply(lambda x: normalizations.point_relative_normalization(x))
    ```

13.  The `close_point_relative_normalization` variable now contains the normalized data for the `close` variable:

    ```
    bitcoin_recent.set_index('date')\
    ['close_point_relative_normalization'].plot(linewidth=2, \
                                                figsize=(14,4), \
                                                color='#d35400')
    ```

    这将导致以下输出:

    ![Figure 2.16: Image of the Jupyter Notebook focusing on the section where the normalization function is applied
    ](image/B15911_02_16.jpg)

    图 2.16:Jupyter 笔记本的图像，聚焦于应用规范化函数的部分

14.  Do the same with the `volume` variable (`volume_point_relative_normalization`). The normalized `close` variable contains an interesting variance pattern every week. We will be using that variable to train our LSTM model:

    ```
    bitcoin_recent.set_index('date')\
                            ['volume_point_relative_normalization'].\
                            plot(linewidth=2, \
                            figsize=(14,4), \
                            color='#d35400')
    ```

    您的输出应该如下所示。

    ![Figure 2.17: Plot that displays the series from the normalized variable
    ](image/B15911_02_17.jpg)

    图 2.17:显示标准化变量序列的图

15.  In order to evaluate how well the model performs, you need to test its accuracy versus some other data. Do this by creating two datasets: a training set and a test set. You will use 90 percent of the dataset to train the LSTM model and 10 percent to evaluate its performance. Given that the data is continuous and in the form of a time series, use the last 10 percent of available weeks as a test set and the first 90 percent as a training set:

    ```
    boundary = int(0.9 * bitcoin_recent['iso_week'].nunique())
    train_set_weeks = bitcoin_recent['iso_week'].unique()[0:boundary]
    test_set_weeks = bitcoin_recent[~bitcoin_recent['iso_week']\
                     .isin(train_set_weeks)]['iso_week'].unique()
    test_set_weeks
    train_set_weeks
    ```

    这将显示以下输出:

    ![Figure 2.18: Output of the test set weeks
    ](image/B15911_02_18.jpg)

    图 2.18:测试集周的输出

    ![Figure 2.19: Using weeks to create a training set
    ](image/B15911_02_19.jpg)

    图 2.19:使用周来创建训练集

16.  为每个操作创建单独的数据集:

    ```
    train_dataset = bitcoin_recent[bitcoin_recent['iso_week']\
                                                 .isin(train_set_weeks)]
    test_dataset = bitcoin_recent[bitcoin_recent['iso_week'].\
                                                isin(test_set_weeks)]
    ```

17.  Finally, navigate to the `Storing Output` section and save the filtered variable to disk, as follows:

    ```
    test_dataset.to_csv('data/test_dataset.csv', index=False)
    train_dataset.to_csv('data/train_dataset.csv', index=False)
    bitcoin_recent.to_csv('data/bitcoin_recent.csv', index=False)
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/3ehbgCi](https://packt.live/3ehbgCi)。

    你也可以在[https://packt.live/2ZdGq9s](https://packt.live/2ZdGq9s)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

在这个练习中，我们探索了比特币数据集，并为深度学习模型做了准备。

我们了解到，2017 年，比特币价格暴涨。这一现象花了很长时间才发生，并且可能受到了许多外部因素的影响，这些因素仅靠这一数据无法解释(例如，其他加密货币的出现)。在经历了 2017 年的大涨之后，2018 年我们看到了比特币价值的大幅下跌。

我们还使用了相对于点的归一化技术来处理每周块中的比特币数据集。我们这样做是为了训练一个 LSTM 网络来学习比特币价格变化的每周模式，以便它可以预测未来一整周的情况。

然而，比特币统计数据显示，每周都有显著波动。我们能预测未来比特币的价格吗？七天后价格会是多少？我们将在下一节中使用 Keras 构建一个深度学习模型来探索这些问题。

# 使用 Keras 作为TensorFlow接口

我们使用 Keras 是因为它将 TensorFlow 接口简化为通用抽象，并且在 TensorFlow 2.0 中，这是该版本中的默认 API。在后端，计算仍然在 TensorFlow 中执行，但我们花更少的时间来担心单个组件，如变量和操作，而花更多的时间来构建作为计算单元的网络。Keras 使得试验不同的架构和超参数变得容易，更快地向高性能解决方案迈进。

从 TensorFlow 2.0.0 开始，Keras 现在正式以 TensorFlow 为`tf.keras`发行。这表明 Keras 现在已经与 TensorFlow 紧密集成，并且很可能在很长一段时间内继续作为开源工具开发。组件是构建模型时不可或缺的一部分。现在让我们深入探讨一下这个概念。

## 模型组件

正如我们在*第一章*、*神经网络和深度学习介绍*中看到的，LSTM 网络也有输入层、隐藏层和输出层。每个隐藏层都有一个激活函数，用于评估该层的相关权重和偏差。正如预期的那样，网络按顺序将数据从一层移动到另一层，并通过每次迭代(即一个时期)的输出来评估结果。

Keras 提供了直观的类，表示下表中列出的每个组件:

![Figure 2.20: Description of key components from the Keras API
](image/B15911_02_20.jpg)

图 2.20:Keras API 关键组件的描述

我们将使用这些组件来构建深度学习模型。

Keras '组件代表一个完整的时序神经网络。这个 Python 类可以独立实例化，并随后添加其他组件。

我们对构建 LSTM 网络感兴趣，因为这些网络在处理序列数据时表现良好，而时间序列就是一种序列数据。使用 Keras，完整的 LSTM 网络将按如下方式实施:

```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dense, Activation
model = Sequential()
model.add(LSTM(units=number_of_periods, \
               input_shape=(period_length, number_of_periods) \
               return_sequences=False), stateful=True)
model.add(Dense(units=period_length)) \
          model.add(Activation("linear"))
model.compile(loss="mse", optimizer="rmsprop")
```

这个实现将在*第三章*、【TensorFlow 和 Keras 的真实世界深度学习:评估比特币模型中进一步优化。

Keras 抽象允许您专注于使深度学习系统更具性能的关键元素:确定组件的正确顺序，包括多少层和节点，以及使用哪个激活函数。所有这些选择要么由组件添加到实例化的`keras.models` `.Sequential()`类的顺序决定，要么由传递给每个组件实例化的参数决定(即`Activation("linear")`)。最后的`model.compile()`步骤使用 TensorFlow 组件构建神经网络。

在网络建立之后，我们使用`model.fit()`方法训练我们的网络。这将产生可用于进行预测的训练模型:

```
model.fit(X_train, Y_train,
          batch_size=32, epochs=epochs)
```

`X_train`和`Y_train`变量分别是用于训练的一组和用于评估损失函数(即测试网络预测数据的好坏)的较小的一组。最后，我们可以使用`model.predict()`方法进行预测:

```
model.predict(x=X_train)
```

前面的步骤涵盖了使用神经网络的 Keras 范例。尽管不同的架构可以用非常不同的方式处理，但 Keras 通过使用三个组件简化了与不同架构一起工作的界面—`Network Architecture`、`Fit`和`Predict`:

![Figure 2.21: The Keras neural network paradigm
](image/B15911_02_21.jpg)

图 2.21:Keras 神经网络范例

Keras 神经网络图包括以下三个步骤:

*   神经网络体系结构
*   训练一个神经网络(或**拟合**
*   做预测

Keras 允许在每个步骤中有更大的控制。然而，它的重点是让用户在尽可能短的时间内尽可能容易地创建神经网络。这意味着我们可以从一个简单的模型开始，然后在前面的每一个步骤中增加复杂性，使最初的模型表现得更好。

我们将在接下来的练习和章节中利用这个范例。在下一个练习中，我们将尽可能创建最简单的 LSTM 网络。然后，在*第 3 章*、*真实世界深度学习:评估比特币模型*中，我们将不断评估和改变该网络，使其更加健壮和高效。

## 练习 2.02:使用 Keras 创建TensorFlow模型

在这本笔记本中，我们使用 Keras 作为 TensorFlow 的接口，设计并编译了一个深度学习模型。在接下来的章节和练习中，我们将通过试验不同的优化技术来继续修改这个模型。然而，该模型的基本组件完全在该笔记本中设计:

1.  打开一个新的 Jupyter 笔记本并导入以下库:

    ```
    import warnings
    warnings.filterwarnings("ignore", category=DeprecationWarning)
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM
    from tensorflow.keras.layers import Dense, Activation
    ```

2.  Our dataset contains daily observations and each observation influences a future observation. Also, we are interested in predicting a week—that is, 7 days—of Bitcoin prices in the future:

    ```
    period_length = 7
    number_of_periods = 208 - 21 - 1
    number_of_periods
    ```

    我们根据数据集中可用的周数计算了`number_of_observations`。鉴于我们将使用上周来测试每个时期的 LSTM 网络，我们将使用 208–21–1。您将获得:

    ```
    186
    ```

3.  Build the LSTM model using Keras. We have the batch size as one because we are passing the whole data in a single iteration. If data is big, then we can pass the data with multiple batches, That's why we used batch_input_shape:

    ```
    def build_model(period_length, number_of_periods, batch_size=1):
        model = Sequential()
        model.add(LSTM(units=period_length,\
                       batch_input_shape=(batch_size, \
                                          number_of_periods, \
                                          period_length),\
                       input_shape=(number_of_periods, \
                                    period_length),\
                       return_sequences=False, stateful=False))
        model.add(Dense(units=period_length))
        model.add(Activation("linear"))
        model.compile(loss="mse", optimizer="rmsprop")
        return model
    ```

    这应该会返回一个编译好的 Keras 模型，它可以被训练并存储在磁盘中。

4.  Let's store the model on the output of the model to a disk:

    ```
    model = build_model(period_length=period_length, \
                        number_of_periods=number_of_periods)
    model.save('bitcoin_lstm_v0.h5')
    ```

    注意`bitcoin_lstm_v0.h5`模型还没有训练好。在没有事先培训的情况下保存模型时，您实际上只保存了模型的架构。稍后可以使用 Keras' `load_model()`函数加载相同的模型，如下所示:

    ```
    model = keras.models.load_model('bitcoin_lstm_v0.h5')
    ```

    注意

    要访问该特定部分的源代码，请参考[https://packt.live/38KQI3Y](https://packt.live/38KQI3Y)。

    你也可以在[https://packt.live/3fhEL89](https://packt.live/3fhEL89)在线运行这个例子。您必须执行整个笔记本才能获得想要的结果。

这结束了我们的 Keras 模型的创建，我们现在可以使用它来进行预测。

注意

加载 Keras 库时，您可能会遇到以下警告:

`Using TensorFlow backend`

Keras 可以配置成使用另一个后端，而不是 TensorFlow(也就是 Theano)。为了避免这个消息，您可以创建一个名为`keras.json`的文件，并在那里配置它的后端。该文件的正确配置取决于您的系统。因此，建议您在[https://keras.io/backend/](https://keras.io/backend/)访问 Keras 关于该主题的官方文档。

在本节中，我们已经学习了如何使用 Keras(tensor flow 的一个接口)建立深度学习模型。我们研究了 Keras 的核心组件，并使用这些组件建立了基于 LSTM 模型的比特币价格预测系统的第一个版本。

在我们的下一部分，我们将讨论如何将本章的所有组件整合到一个(接近完整的)深度学习系统中。该系统将产生我们的第一个预测，作为未来改进的起点。

# 从数据准备到建模

本节重点介绍深度学习系统的实现方面。我们将使用来自*选择正确的模型架构*部分的比特币数据，以及来自前一部分的 Keras 知识，*使用 Keras 作为 TensorFlow 接口*，将这两个组件放在一起。本节通过构建一个从磁盘读取数据并将其作为一个单独的软件馈入模型的系统来结束本章。

## 训练一个神经网络

神经网络可能需要很长时间来训练。许多因素影响这个过程需要多长时间。其中，三个因素通常被认为是最重要的:

*   网络的架构
*   网络有多少层和神经元
*   在训练过程中会用到多少数据

其他因素也可能极大地影响网络训练所需的时间，但神经网络在解决业务问题时可以进行的大多数优化来自于探索这三个因素。

我们将使用上一节中的标准化数据。回想一下，我们已经将训练数据存储在一个名为`train_dataset.csv`的文件中。

注意:

您可以通过访问此链接下载培训数据:[https://packt.live/2Zgmm6r](https://packt.live/2Zgmm6r)。

为了便于研究，我们将使用`pandas`库将数据集加载到内存中:

```
import pandas as pd
train = pd.read_csv('data/train_dataset.csv')
```

注意

确保根据您下载或保存 CSV 文件的位置更改路径(突出显示)。

您将看到如下表格形式的输出:

![Figure 2.22: Table showing the first five rows of the training dataset
](image/B15911_02_22.jpg)

图 2.22:显示训练数据集的前五行的表格

我们将使用来自`close_point_relative_normalization`变量的序列，这是自 2016 年初以来比特币收盘价的归一化序列——来自`close`变量。

`close_point_relative_normalization`变量已经每周标准化。一周期间的每个观察值都是相对于该期间第一天收盘价的差值而言的。这一标准化步骤非常重要，将有助于我们的网络训练更快:

![Figure 2.23: Plot that displays the series from the normalized variable.
](image/B15911_02_23.jpg)

图 2.23:显示标准化变量序列的图。

这个变量将用于训练我们的 LSTM 模型。

## 重塑时间序列数据

神经网络通常使用向量和张量——两者都是在多个维度上组织数据的数学对象。在 Keras 中实现的每个神经网络将具有根据规范组织的向量或张量作为输入。

首先，理解如何将数据重新整形为给定图层所期望的格式可能会令人困惑。为避免混淆，建议从组件尽可能少的网络开始，然后逐渐添加组件。Keras 的官方文档(在`Layers`部分)对于了解每种层的要求是必不可少的。

注意

Keras 官方文档可在 https://keras.io/layers/core/获得。该链接直接将您带到`Layers`部分。

NumPy 是一个流行的 Python 库，用于执行数值计算。它被深度学习社区用来操纵向量和张量，并为深度学习系统做准备。

特别是在为深度学习模型调整数据时，`numpy.reshape()`方法非常重要。该模型允许操纵 NumPy 数组，NumPy 数组是类似于向量和张量的 Python 对象。

我们现在将使用 2016 年后的周数来组织来自`close_point_relative_normalization`变量的价格。我们将创建不同的组，每个组包含 7 个观察值(一周中的每一天一个),总共 208 个完整周。我们这样做是因为我们对预测一周的交易价格感兴趣。

注意

我们使用 ISO 标准来确定一周的开始和结束。其他类型的组织是完全可能的。这种方法简单直观，但仍有改进的空间。

LSTM 网络使用三维张量。这些维度中的每一个都代表了网络的一个重要属性。这些尺寸如下:

*   **周期长度**:周期长度，即一个周期内有多少次观察
*   **周期数**:数据集中有多少个周期
*   **特征数量**:数据集中可用的特征数量

来自`close_point_relative_normalization`变量的数据目前是一个一维向量。我们需要重塑它来匹配这三个维度。

我们将用一周的时间。所以，我们的周期长度是 7 天(周期长度= 7)。我们的数据中有 208 个完整的星期。在训练期间，我们将利用最后几周来测试我们的模型。我们还有 187 周的时间。最后，我们将在该网络中使用单个特征(特征数量= 1)；我们将在未来的版本中包含更多的特性。

为了对数据进行整形以匹配这些维度，我们将使用基本 Python 属性和来自`numpy`库的`reshape()`方法的组合。首先，我们用 7 天创建 186 个不同的周组，每个组使用纯 Python:

```
group_size = 7
samples = list()
for i in range(0, len(data), group_size):
sample = list(data[i:i + group_size])
         if len(sample) == group_size:samples\
                           .append(np.array(sample)\
                           .reshape(group_size, 1).tolist())
data = np.array(samples)
```

这段代码创建了不同的周组。产生的变量数据是包含所有正确维度的变量。

注意

每个 Keras 层都希望其输入以特定的方式组织。然而，在大多数情况下，Keras 会相应地重塑数据。添加新层之前，请务必参考 Keras 关于层的文档([https://keras.io/layers/core/](https://keras.io/layers/core/))。

Keras LSTM 图层希望这些维度按照特定的顺序进行组织:要素数量、观测数量和周期长度。重塑数据集以匹配该格式:

```
X_train = data[:-1,:].reshape(1, 186, 7)
Y_validation = data[-1].reshape(1, 7)
```

前面的代码片段还选择了我们集合的最后一周作为验证集合(通过`data[-1]`)。我们将尝试使用之前的 76 周来预测数据集中的最后一周。下一步是使用这些变量来拟合我们的模型:

```
model.fit(x=X_train, y=Y_validation, epochs=100)
```

LSTMs 是计算昂贵的模型。他们可能需要 5 分钟在现代计算机上用我们的数据集进行训练。当算法创建完整的计算图时，大部分时间花费在计算的开始。该过程在开始训练后会加速:

![Figure 2.24: Graph that shows the results of the loss function evaluated at each epoch
](image/B15911_02_24.jpg)

图 2.24:显示每个时期损失函数评估结果的图表

注意

这将比较模型在每个时期的预测，然后使用一种称为均方误差的技术将其与真实数据进行比较。这张图显示了这些结果。

乍一看，我们的网络似乎表现得很好；它从一个非常小的错误率开始，并不断降低。现在我们已经降低了错误率，让我们继续做一些预测。

## 做预测

在我们的网络被训练之后，我们可以继续进行预测。我们将对未来一周做出超出我们时间段的预测。

一旦我们用`model.fit()`方法训练了我们的模型，做出预测就很简单了:

```
model.predict(x=X_train)
```

我们使用与训练数据相同的数据进行预测(变量`X_train`)。如果我们有更多可用的数据，我们可以使用这些数据——假设我们将其重新整形为 LSTM 要求的格式。

### 过度拟合

当神经网络过度适应验证集时，这意味着它学习训练集中存在的模式，但无法将其推广到看不见的数据(例如，测试集)。在下一章中，我们将学习如何避免过度拟合，并创建一个系统来评估我们的网络并提高其性能:

![Figure 2.25: Graph showing the weekly performance of Bitcoin
](image/B15911_02_25.jpg)

图 2.25:显示比特币周表现的图表

在上图中，横轴代表周数，纵轴代表比特币的预测表现。既然我们已经探索了数据，准备了模型，并学习了如何进行预测，让我们将这些知识付诸实践。

## 活动 2.01:组装一个深度学习系统

在这项活动中，我们将汇集构建基本深度学习系统的所有基本要素——数据、模型和预测:

1.  开始一个 Jupyter 笔记本。
2.  将训练数据集加载到内存中。
3.  检查定型集，以查看其形式是否为周期长度、周期数或功能数。
4.  如果训练集的格式不符合要求，请将其转换。
5.  加载先前训练的模型。
6.  使用您的训练数据集训练模型。
7.  对训练集进行预测。
8.  对值进行反规格化并保存模型。

最终输出如下所示，横轴表示天数，纵轴表示比特币的价格:

six

![Figure 2.26: Expected output
](image/B15911_02_26.jpg)

图 2.26:预期产出

注意

这项活动的解决方案可在第 136 页找到。

# 总结

在这一章中，我们组装了一个完整的深度学习系统，从数据到预测。本练习中创建的模型需要进行大量改进才能被认为是有用的。但是，这是一个很好的起点，我们将在此基础上不断改进。

下一章将探索测量模型性能的技术，并将继续进行修改，直到我们得到一个既有用又健壮的模型。